{
  "name": "drone-cv-expert",
  "type": "folder",
  "path": "drone-cv-expert",
  "children": [
    {
      "name": "references",
      "type": "folder",
      "path": "drone-cv-expert/references",
      "children": [
        {
          "name": "navigation-algorithms.md",
          "type": "file",
          "path": "drone-cv-expert/references/navigation-algorithms.md",
          "size": 14746,
          "content": "# Navigation Algorithms Reference\n\n## GPS-Based Waypoint Navigation\n\n```python\nclass PIDController:\n    \"\"\"Standard PID controller for position control\"\"\"\n    def __init__(self, kp: float, ki: float, kd: float):\n        self.kp = kp\n        self.ki = ki\n        self.kd = kd\n        self.integral = 0.0\n        self.prev_error = 0.0\n\n    def update(self, error: float, dt: float = 0.02) -> float:\n        self.integral += error * dt\n        derivative = (error - self.prev_error) / dt\n        self.prev_error = error\n        return self.kp * error + self.ki * self.integral + self.kd * derivative\n\n\nclass WaypointNavigator:\n    \"\"\"GPS waypoint navigation with PID control\"\"\"\n    def __init__(self, kp=1.0, ki=0.1, kd=0.05):\n        self.pid_lat = PIDController(kp, ki, kd)\n        self.pid_lon = PIDController(kp, ki, kd)\n        self.pid_alt = PIDController(kp, ki, kd)\n        self.waypoint_threshold = 2.0  # meters\n\n    def navigate_to_waypoint(self, current_pos, target_pos):\n        \"\"\"Generate velocity commands to reach waypoint\"\"\"\n        lat_error = target_pos.lat - current_pos.lat\n        lon_error = target_pos.lon - current_pos.lon\n        alt_error = target_pos.alt - current_pos.alt\n\n        return {\n            'north': self.pid_lat.update(lat_error),\n            'east': self.pid_lon.update(lon_error),\n            'up': self.pid_alt.update(alt_error)\n        }\n\n    def is_waypoint_reached(self, current_pos, target_pos) -> bool:\n        \"\"\"Check if within threshold of waypoint\"\"\"\n        import math\n        distance = math.sqrt(\n            (current_pos.lat - target_pos.lat)**2 +\n            (current_pos.lon - target_pos.lon)**2\n        ) * 111000  # Rough meters conversion\n        return distance < self.waypoint_threshold\n```\n\n## Visual SLAM Pipeline\n\n```python\nimport cv2\nimport numpy as np\nfrom typing import List, Tuple, Optional\n\nclass VisualSLAM:\n    \"\"\"ORB-SLAM style visual SLAM for GPS-denied navigation\"\"\"\n    def __init__(self, camera_matrix: np.ndarray = None):\n        self.orb = cv2.ORB_create(nfeatures=2000)\n        self.matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n        self.map_points: List[np.ndarray] = []\n        self.camera_poses: List[np.ndarray] = []\n        self.prev_frame = None\n        self.prev_keypoints = None\n        self.prev_descriptors = None\n\n        # Camera intrinsics (default for 640x480)\n        self.K = camera_matrix if camera_matrix is not None else np.array([\n            [500, 0, 320],\n            [0, 500, 240],\n            [0, 0, 1]\n        ], dtype=np.float32)\n\n    def process_frame(self, frame: np.ndarray) -> Tuple[np.ndarray, List]:\n        \"\"\"Process frame and return estimated pose\"\"\"\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) if len(frame.shape) == 3 else frame\n\n        # Feature detection\n        keypoints, descriptors = self.orb.detectAndCompute(gray, None)\n\n        if self.prev_frame is None:\n            # Initialize with first frame\n            self.prev_frame = gray\n            self.prev_keypoints = keypoints\n            self.prev_descriptors = descriptors\n            pose = np.eye(4)\n            self.camera_poses.append(pose)\n            return pose, []\n\n        # Match features\n        matches = self.matcher.match(self.prev_descriptors, descriptors)\n        matches = sorted(matches, key=lambda x: x.distance)[:100]\n\n        if len(matches) < 10:\n            return self.camera_poses[-1], self.map_points\n\n        # Extract matched points\n        pts1 = np.float32([self.prev_keypoints[m.queryIdx].pt for m in matches])\n        pts2 = np.float32([keypoints[m.trainIdx].pt for m in matches])\n\n        # Estimate pose\n        E, mask = cv2.findEssentialMat(pts1, pts2, self.K, method=cv2.RANSAC)\n        _, R, t, mask = cv2.recoverPose(E, pts1, pts2, self.K)\n\n        # Build pose matrix\n        pose = np.eye(4)\n        pose[:3, :3] = R\n        pose[:3, 3] = t.flatten()\n\n        # Chain with previous pose\n        if self.camera_poses:\n            pose = self.camera_poses[-1] @ pose\n\n        self.camera_poses.append(pose)\n\n        # Update state\n        self.prev_frame = gray\n        self.prev_keypoints = keypoints\n        self.prev_descriptors = descriptors\n\n        return pose, self.map_points\n\n\nclass VisualInertialOdometry:\n    \"\"\"VIO combining camera and IMU for robust tracking\"\"\"\n    def __init__(self):\n        self.visual_slam = VisualSLAM()\n        self.imu_buffer = []\n        self.gravity = np.array([0, 0, -9.81])\n\n    def process_imu(self, accel: np.ndarray, gyro: np.ndarray, dt: float):\n        \"\"\"Buffer IMU measurements for fusion\"\"\"\n        self.imu_buffer.append({\n            'accel': accel,\n            'gyro': gyro,\n            'dt': dt\n        })\n\n    def process_frame(self, frame: np.ndarray):\n        \"\"\"Fuse visual with IMU for robust pose\"\"\"\n        visual_pose, _ = self.visual_slam.process_frame(frame)\n\n        # Integrate IMU between frames\n        imu_delta = self._integrate_imu()\n\n        # Simple fusion (production would use EKF/UKF)\n        fused_pose = visual_pose.copy()\n        if imu_delta is not None:\n            # Weight visual more when feature tracking is good\n            fused_pose[:3, 3] = 0.8 * visual_pose[:3, 3] + 0.2 * imu_delta\n\n        self.imu_buffer = []\n        return fused_pose\n\n    def _integrate_imu(self) -> Optional[np.ndarray]:\n        if not self.imu_buffer:\n            return None\n\n        position = np.zeros(3)\n        velocity = np.zeros(3)\n\n        for meas in self.imu_buffer:\n            accel = meas['accel'] - self.gravity\n            dt = meas['dt']\n            velocity += accel * dt\n            position += velocity * dt\n\n        return position\n```\n\n## Path Planning Algorithms\n\n### A* 3D Path Planning\n\n```python\nimport heapq\nimport numpy as np\nfrom typing import List, Tuple, Set, Optional\n\nclass Drone3DPathPlanner:\n    \"\"\"A* path planning with 3D obstacle avoidance\"\"\"\n    def __init__(self, grid_resolution: float = 0.5, safety_margin: float = 1.0):\n        self.resolution = grid_resolution\n        self.safety_margin = safety_margin\n        self.obstacle_map: Set[Tuple[int, int, int]] = set()\n\n    def add_obstacles(self, point_cloud: np.ndarray):\n        \"\"\"Add obstacles from 3D point cloud\"\"\"\n        for point in point_cloud:\n            grid_cell = self._to_grid(point)\n            self.obstacle_map.add(grid_cell)\n            # Add safety margin\n            for dx in range(-2, 3):\n                for dy in range(-2, 3):\n                    for dz in range(-2, 3):\n                        self.obstacle_map.add((\n                            grid_cell[0] + dx,\n                            grid_cell[1] + dy,\n                            grid_cell[2] + dz\n                        ))\n\n    def _to_grid(self, pos: Tuple[float, float, float]) -> Tuple[int, int, int]:\n        return (\n            int(pos[0] / self.resolution),\n            int(pos[1] / self.resolution),\n            int(pos[2] / self.resolution)\n        )\n\n    def _from_grid(self, cell: Tuple[int, int, int]) -> Tuple[float, float, float]:\n        return (\n            cell[0] * self.resolution,\n            cell[1] * self.resolution,\n            cell[2] * self.resolution\n        )\n\n    def a_star(self, start: Tuple[float, float, float],\n               goal: Tuple[float, float, float]) -> Optional[List[Tuple[float, float, float]]]:\n        \"\"\"Find optimal path avoiding obstacles\"\"\"\n        start_cell = self._to_grid(start)\n        goal_cell = self._to_grid(goal)\n\n        open_set = [(0, start_cell)]\n        came_from = {}\n        g_score = {start_cell: 0}\n        f_score = {start_cell: self._heuristic(start_cell, goal_cell)}\n\n        while open_set:\n            current = heapq.heappop(open_set)[1]\n\n            if self._is_goal_reached(current, goal_cell):\n                return self._reconstruct_path(came_from, current)\n\n            for neighbor in self._get_neighbors(current):\n                if neighbor in self.obstacle_map:\n                    continue\n\n                tentative_g = g_score[current] + self._distance(current, neighbor)\n\n                if neighbor not in g_score or tentative_g < g_score[neighbor]:\n                    came_from[neighbor] = current\n                    g_score[neighbor] = tentative_g\n                    f_score[neighbor] = tentative_g + self._heuristic(neighbor, goal_cell)\n                    heapq.heappush(open_set, (f_score[neighbor], neighbor))\n\n        return None  # No path found\n\n    def _heuristic(self, pos1: Tuple, pos2: Tuple) -> float:\n        \"\"\"Euclidean distance heuristic\"\"\"\n        return np.sqrt(sum((a - b) ** 2 for a, b in zip(pos1, pos2)))\n\n    def _distance(self, pos1: Tuple, pos2: Tuple) -> float:\n        return self._heuristic(pos1, pos2)\n\n    def _is_goal_reached(self, current: Tuple, goal: Tuple, threshold: int = 2) -> bool:\n        return all(abs(a - b) <= threshold for a, b in zip(current, goal))\n\n    def _get_neighbors(self, cell: Tuple[int, int, int]) -> List[Tuple[int, int, int]]:\n        \"\"\"26-connected neighbors in 3D grid\"\"\"\n        neighbors = []\n        for dx in [-1, 0, 1]:\n            for dy in [-1, 0, 1]:\n                for dz in [-1, 0, 1]:\n                    if dx == 0 and dy == 0 and dz == 0:\n                        continue\n                    neighbors.append((cell[0] + dx, cell[1] + dy, cell[2] + dz))\n        return neighbors\n\n    def _reconstruct_path(self, came_from: dict, current: Tuple) -> List[Tuple[float, float, float]]:\n        path = [self._from_grid(current)]\n        while current in came_from:\n            current = came_from[current]\n            path.append(self._from_grid(current))\n        return list(reversed(path))\n```\n\n### RRT (Rapidly-exploring Random Tree)\n\n```python\nimport random\nimport numpy as np\nfrom typing import List, Tuple, Optional\n\nclass RRTPlanner:\n    \"\"\"RRT for dynamic obstacle avoidance\"\"\"\n    def __init__(self, bounds: Tuple[Tuple, Tuple], step_size: float = 1.0):\n        self.bounds = bounds  # ((xmin, ymin, zmin), (xmax, ymax, zmax))\n        self.step_size = step_size\n        self.tree = []\n        self.obstacle_check_fn = None\n\n    def plan(self, start: Tuple, goal: Tuple,\n             obstacle_check: callable, max_iterations: int = 1000) -> Optional[List[Tuple]]:\n        \"\"\"Plan path using RRT\"\"\"\n        self.obstacle_check_fn = obstacle_check\n        self.tree = [{'pos': start, 'parent': None}]\n\n        for _ in range(max_iterations):\n            # Sample random point (bias toward goal)\n            if random.random() < 0.1:\n                sample = goal\n            else:\n                sample = self._random_sample()\n\n            # Find nearest node\n            nearest_idx = self._nearest_node(sample)\n            nearest = self.tree[nearest_idx]\n\n            # Extend toward sample\n            new_pos = self._steer(nearest['pos'], sample)\n\n            # Check collision\n            if self._collision_free(nearest['pos'], new_pos):\n                self.tree.append({'pos': new_pos, 'parent': nearest_idx})\n\n                # Check if we reached goal\n                if self._distance(new_pos, goal) < self.step_size:\n                    self.tree.append({'pos': goal, 'parent': len(self.tree) - 1})\n                    return self._extract_path()\n\n        return None\n\n    def _random_sample(self) -> Tuple:\n        return tuple(\n            random.uniform(self.bounds[0][i], self.bounds[1][i])\n            for i in range(3)\n        )\n\n    def _nearest_node(self, sample: Tuple) -> int:\n        distances = [self._distance(node['pos'], sample) for node in self.tree]\n        return int(np.argmin(distances))\n\n    def _steer(self, from_pos: Tuple, to_pos: Tuple) -> Tuple:\n        direction = np.array(to_pos) - np.array(from_pos)\n        distance = np.linalg.norm(direction)\n        if distance <= self.step_size:\n            return to_pos\n        return tuple(np.array(from_pos) + direction / distance * self.step_size)\n\n    def _collision_free(self, from_pos: Tuple, to_pos: Tuple, steps: int = 10) -> bool:\n        for i in range(steps + 1):\n            t = i / steps\n            point = tuple(\n                from_pos[j] + t * (to_pos[j] - from_pos[j])\n                for j in range(3)\n            )\n            if self.obstacle_check_fn(point):\n                return False\n        return True\n\n    def _distance(self, p1: Tuple, p2: Tuple) -> float:\n        return np.sqrt(sum((a - b) ** 2 for a, b in zip(p1, p2)))\n\n    def _extract_path(self) -> List[Tuple]:\n        path = []\n        idx = len(self.tree) - 1\n        while idx is not None:\n            path.append(self.tree[idx]['pos'])\n            idx = self.tree[idx]['parent']\n        return list(reversed(path))\n```\n\n## AprilTag Localization\n\n```python\nimport cv2\nimport numpy as np\nfrom typing import Dict, List, Optional\n\n# Requires: pip install pupil-apriltags\nfrom pupil_apriltags import Detector\n\nclass AprilTagLocalizer:\n    \"\"\"Localization using AprilTag markers for GPS-denied environments\"\"\"\n    def __init__(self, tag_size: float = 0.16, camera_matrix: np.ndarray = None):\n        self.detector = Detector(\n            families='tag36h11',\n            nthreads=4,\n            quad_decimate=1.0,\n            quad_sigma=0.0,\n            refine_edges=True,\n            decode_sharpening=0.25\n        )\n        self.tag_size = tag_size\n        self.K = camera_matrix\n\n        # Known tag positions in world frame\n        self.tag_world_positions: Dict[int, np.ndarray] = {}\n\n    def register_tag(self, tag_id: int, world_position: np.ndarray):\n        \"\"\"Register known tag position for localization\"\"\"\n        self.tag_world_positions[tag_id] = world_position\n\n    def localize(self, frame: np.ndarray) -> Optional[np.ndarray]:\n        \"\"\"Estimate drone position from visible tags\"\"\"\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) if len(frame.shape) == 3 else frame\n\n        # Detect tags\n        detections = self.detector.detect(\n            gray,\n            estimate_tag_pose=True,\n            camera_params=(self.K[0, 0], self.K[1, 1], self.K[0, 2], self.K[1, 2]),\n            tag_size=self.tag_size\n        )\n\n        poses = []\n        for det in detections:\n            if det.tag_id in self.tag_world_positions:\n                # Get tag-to-camera transform\n                R = det.pose_R\n                t = det.pose_t\n\n                # Invert to get camera-in-tag frame\n                R_inv = R.T\n                t_inv = -R_inv @ t\n\n                # Transform to world frame\n                tag_world = self.tag_world_positions[det.tag_id]\n                camera_world = tag_world + t_inv.flatten()\n\n                poses.append(camera_world)\n\n        if not poses:\n            return None\n\n        # Average multiple tag observations\n        return np.mean(poses, axis=0)\n```\n"
        },
        {
          "name": "object-detection-tracking.md",
          "type": "file",
          "path": "drone-cv-expert/references/object-detection-tracking.md",
          "size": 17655,
          "content": "# Object Detection & Tracking Reference\n\n## Real-Time Aerial Object Detection\n\n```python\nimport torch\nimport numpy as np\nfrom ultralytics import YOLO\nfrom typing import List, Dict, Tuple, Optional\n\nclass AerialObjectDetector:\n    \"\"\"\n    YOLOv8-based object detection optimized for drone platforms.\n    Handles altitude-dependent scaling and aerial-specific classes.\n    \"\"\"\n    def __init__(self, model_path: str = 'yolov8n.pt', device: str = 'cuda'):\n        self.model = YOLO(model_path)\n        self.model.to(device)\n        self.device = device\n\n        # Confidence thresholds by altitude (lower when higher)\n        self.altitude_conf_map = {\n            0: 0.5,    # Ground level\n            50: 0.4,   # 50m\n            100: 0.35, # 100m\n            200: 0.3   # 200m+\n        }\n\n    def detect(self, frame: np.ndarray, altitude: float = 0,\n               classes: Optional[List[int]] = None) -> List[Dict]:\n        \"\"\"\n        Detect objects in frame with altitude-adaptive confidence.\n\n        Args:\n            frame: BGR image from drone camera\n            altitude: Current drone altitude in meters\n            classes: Filter for specific class IDs (None for all)\n\n        Returns:\n            List of detections with bbox, class, confidence\n        \"\"\"\n        # Get altitude-appropriate confidence threshold\n        conf = self._get_confidence_threshold(altitude)\n\n        # Run detection\n        results = self.model(\n            frame,\n            conf=conf,\n            classes=classes,\n            verbose=False\n        )[0]\n\n        # Parse results\n        detections = []\n        for box in results.boxes:\n            det = {\n                'bbox': box.xyxy[0].cpu().numpy().tolist(),\n                'class_id': int(box.cls[0].item()),\n                'class_name': results.names[int(box.cls[0].item())],\n                'confidence': float(box.conf[0].item()),\n                'center': self._get_center(box.xyxy[0].cpu().numpy())\n            }\n            detections.append(det)\n\n        return detections\n\n    def _get_confidence_threshold(self, altitude: float) -> float:\n        \"\"\"Interpolate confidence threshold based on altitude\"\"\"\n        sorted_alts = sorted(self.altitude_conf_map.keys())\n        for i, alt in enumerate(sorted_alts[:-1]):\n            if altitude < sorted_alts[i + 1]:\n                # Linear interpolation\n                t = (altitude - alt) / (sorted_alts[i + 1] - alt)\n                return self.altitude_conf_map[alt] * (1 - t) + self.altitude_conf_map[sorted_alts[i + 1]] * t\n        return self.altitude_conf_map[sorted_alts[-1]]\n\n    def _get_center(self, bbox: np.ndarray) -> Tuple[float, float]:\n        return ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)\n\n\nclass TensorRTDetector:\n    \"\"\"\n    TensorRT-optimized detector for edge deployment.\n    3-5x faster than PyTorch inference on Jetson.\n    \"\"\"\n    def __init__(self, engine_path: str, input_size: Tuple[int, int] = (640, 640)):\n        import tensorrt as trt\n\n        self.input_size = input_size\n        self.logger = trt.Logger(trt.Logger.WARNING)\n\n        # Load engine\n        with open(engine_path, 'rb') as f:\n            self.engine = trt.Runtime(self.logger).deserialize_cuda_engine(f.read())\n\n        self.context = self.engine.create_execution_context()\n\n        # Allocate buffers\n        self._allocate_buffers()\n\n    def _allocate_buffers(self):\n        import pycuda.driver as cuda\n\n        self.inputs = []\n        self.outputs = []\n        self.bindings = []\n\n        for binding in self.engine:\n            size = trt.volume(self.engine.get_binding_shape(binding))\n            dtype = trt.nptype(self.engine.get_binding_dtype(binding))\n\n            # Allocate host and device buffers\n            host_mem = cuda.pagelocked_empty(size, dtype)\n            device_mem = cuda.mem_alloc(host_mem.nbytes)\n\n            self.bindings.append(int(device_mem))\n\n            if self.engine.binding_is_input(binding):\n                self.inputs.append({'host': host_mem, 'device': device_mem})\n            else:\n                self.outputs.append({'host': host_mem, 'device': device_mem})\n\n    def detect(self, frame: np.ndarray) -> List[Dict]:\n        \"\"\"Run TensorRT inference\"\"\"\n        import pycuda.driver as cuda\n\n        # Preprocess\n        input_tensor = self._preprocess(frame)\n        np.copyto(self.inputs[0]['host'], input_tensor.ravel())\n\n        # Transfer to GPU\n        cuda.memcpy_htod(self.inputs[0]['device'], self.inputs[0]['host'])\n\n        # Execute\n        self.context.execute_v2(bindings=self.bindings)\n\n        # Transfer results back\n        cuda.memcpy_dtoh(self.outputs[0]['host'], self.outputs[0]['device'])\n\n        # Postprocess\n        return self._postprocess(self.outputs[0]['host'], frame.shape)\n\n    def _preprocess(self, frame: np.ndarray) -> np.ndarray:\n        \"\"\"Preprocess frame for inference\"\"\"\n        import cv2\n        img = cv2.resize(frame, self.input_size)\n        img = img.astype(np.float32) / 255.0\n        img = img.transpose(2, 0, 1)  # HWC to CHW\n        return np.expand_dims(img, axis=0)\n\n    def _postprocess(self, output: np.ndarray, orig_shape: Tuple) -> List[Dict]:\n        \"\"\"Parse YOLO output format\"\"\"\n        # Implementation depends on YOLO version export format\n        detections = []\n        # ... parse output tensor ...\n        return detections\n```\n\n## Multi-Object Tracking\n\n### ByteTrack Implementation\n\n```python\nimport numpy as np\nfrom typing import List, Dict, Tuple\nfrom scipy.optimize import linear_sum_assignment\n\nclass KalmanBoxTracker:\n    \"\"\"Kalman filter for tracking bounding box state\"\"\"\n    count = 0\n\n    def __init__(self, bbox: np.ndarray):\n        from filterpy.kalman import KalmanFilter\n\n        self.kf = KalmanFilter(dim_x=7, dim_z=4)\n\n        # State: [x, y, s, r, dx, dy, ds]\n        # x, y = center, s = area, r = aspect ratio\n        self.kf.F = np.array([\n            [1, 0, 0, 0, 1, 0, 0],\n            [0, 1, 0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0, 0, 1],\n            [0, 0, 0, 1, 0, 0, 0],\n            [0, 0, 0, 0, 1, 0, 0],\n            [0, 0, 0, 0, 0, 1, 0],\n            [0, 0, 0, 0, 0, 0, 1]\n        ])\n\n        self.kf.H = np.array([\n            [1, 0, 0, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0, 0, 0],\n            [0, 0, 1, 0, 0, 0, 0],\n            [0, 0, 0, 1, 0, 0, 0]\n        ])\n\n        self.kf.R[2:, 2:] *= 10.0\n        self.kf.P[4:, 4:] *= 1000.0\n        self.kf.P *= 10.0\n        self.kf.Q[-1, -1] *= 0.01\n        self.kf.Q[4:, 4:] *= 0.01\n\n        self.kf.x[:4] = self._bbox_to_z(bbox)\n\n        self.time_since_update = 0\n        self.id = KalmanBoxTracker.count\n        KalmanBoxTracker.count += 1\n        self.history = []\n        self.hits = 0\n        self.hit_streak = 0\n        self.age = 0\n\n    def update(self, bbox: np.ndarray):\n        \"\"\"Update with matched detection\"\"\"\n        self.time_since_update = 0\n        self.history = []\n        self.hits += 1\n        self.hit_streak += 1\n        self.kf.update(self._bbox_to_z(bbox))\n\n    def predict(self) -> np.ndarray:\n        \"\"\"Predict next state\"\"\"\n        if (self.kf.x[6] + self.kf.x[2]) <= 0:\n            self.kf.x[6] *= 0.0\n\n        self.kf.predict()\n        self.age += 1\n\n        if self.time_since_update > 0:\n            self.hit_streak = 0\n        self.time_since_update += 1\n\n        self.history.append(self._z_to_bbox(self.kf.x))\n        return self.history[-1]\n\n    def get_state(self) -> np.ndarray:\n        return self._z_to_bbox(self.kf.x)\n\n    def _bbox_to_z(self, bbox: np.ndarray) -> np.ndarray:\n        \"\"\"Convert [x1, y1, x2, y2] to [cx, cy, s, r]\"\"\"\n        w = bbox[2] - bbox[0]\n        h = bbox[3] - bbox[1]\n        x = bbox[0] + w / 2\n        y = bbox[1] + h / 2\n        s = w * h\n        r = w / float(h) if h > 0 else 1\n        return np.array([x, y, s, r]).reshape((4, 1))\n\n    def _z_to_bbox(self, z: np.ndarray) -> np.ndarray:\n        \"\"\"Convert [cx, cy, s, r] to [x1, y1, x2, y2]\"\"\"\n        w = np.sqrt(z[2] * z[3])\n        h = z[2] / w if w > 0 else z[2]\n        return np.array([\n            z[0] - w / 2,\n            z[1] - h / 2,\n            z[0] + w / 2,\n            z[1] + h / 2\n        ]).flatten()\n\n\nclass ByteTracker:\n    \"\"\"\n    ByteTrack multi-object tracker.\n    Associates detections across frames using IoU and appearance.\n    \"\"\"\n    def __init__(self, max_age: int = 30, min_hits: int = 3, iou_threshold: float = 0.3):\n        self.max_age = max_age\n        self.min_hits = min_hits\n        self.iou_threshold = iou_threshold\n        self.trackers: List[KalmanBoxTracker] = []\n        self.frame_count = 0\n\n    def update(self, detections: List[Dict]) -> List[Dict]:\n        \"\"\"\n        Update tracks with new detections.\n\n        Args:\n            detections: List of detections with 'bbox' and 'confidence'\n\n        Returns:\n            List of tracks with track_id, bbox, and status\n        \"\"\"\n        self.frame_count += 1\n\n        # Get predicted locations from existing trackers\n        trks = np.zeros((len(self.trackers), 5))\n        to_del = []\n        for t, trk in enumerate(self.trackers):\n            pos = trk.predict()\n            trks[t, :] = [pos[0], pos[1], pos[2], pos[3], 0]\n            if np.any(np.isnan(pos)):\n                to_del.append(t)\n\n        trks = np.ma.compress_rows(np.ma.masked_invalid(trks))\n        for t in reversed(to_del):\n            self.trackers.pop(t)\n\n        # Split detections by confidence\n        dets = np.array([d['bbox'] for d in detections])\n        confs = np.array([d['confidence'] for d in detections])\n\n        high_conf_mask = confs >= 0.5\n        low_conf_mask = ~high_conf_mask\n\n        high_dets = dets[high_conf_mask] if len(dets) > 0 else np.empty((0, 4))\n        low_dets = dets[low_conf_mask] if len(dets) > 0 else np.empty((0, 4))\n\n        # First association with high confidence detections\n        matched, unmatched_dets, unmatched_trks = self._associate(\n            high_dets, trks, self.iou_threshold\n        )\n\n        # Update matched trackers\n        for t, d in matched:\n            self.trackers[t].update(high_dets[d])\n\n        # Second association with low confidence detections\n        if len(low_dets) > 0 and len(unmatched_trks) > 0:\n            left_trks = trks[unmatched_trks]\n            matched2, _, _ = self._associate(low_dets, left_trks, 0.5)\n\n            for t_idx, d in matched2:\n                t = unmatched_trks[t_idx]\n                self.trackers[t].update(low_dets[d])\n                unmatched_trks = unmatched_trks[unmatched_trks != t]\n\n        # Create new trackers for unmatched high-confidence detections\n        for i in unmatched_dets:\n            trk = KalmanBoxTracker(high_dets[i])\n            self.trackers.append(trk)\n\n        # Return active tracks\n        ret = []\n        for trk in reversed(self.trackers):\n            d = trk.get_state()\n            if (trk.time_since_update < 1) and \\\n               (trk.hit_streak >= self.min_hits or self.frame_count <= self.min_hits):\n                ret.append({\n                    'track_id': trk.id,\n                    'bbox': d.tolist(),\n                    'status': 'tracked'\n                })\n\n            # Remove dead tracks\n            if trk.time_since_update > self.max_age:\n                self.trackers.remove(trk)\n\n        return ret\n\n    def _associate(self, detections: np.ndarray, trackers: np.ndarray,\n                   iou_threshold: float) -> Tuple[List, np.ndarray, np.ndarray]:\n        \"\"\"Associate detections with trackers using Hungarian algorithm\"\"\"\n        if len(trackers) == 0:\n            return [], np.arange(len(detections)), np.empty((0,), dtype=int)\n\n        if len(detections) == 0:\n            return [], np.empty((0,), dtype=int), np.arange(len(trackers))\n\n        # Compute IoU matrix\n        iou_matrix = self._iou_batch(detections, trackers[:, :4])\n        cost_matrix = 1 - iou_matrix\n\n        # Hungarian algorithm\n        row_indices, col_indices = linear_sum_assignment(cost_matrix)\n\n        # Filter low IoU matches\n        matched = []\n        unmatched_dets = list(range(len(detections)))\n        unmatched_trks = list(range(len(trackers)))\n\n        for r, c in zip(row_indices, col_indices):\n            if iou_matrix[r, c] >= iou_threshold:\n                matched.append((c, r))  # (tracker_idx, detection_idx)\n                unmatched_dets.remove(r)\n                unmatched_trks.remove(c)\n\n        return matched, np.array(unmatched_dets), np.array(unmatched_trks)\n\n    def _iou_batch(self, bb_test: np.ndarray, bb_gt: np.ndarray) -> np.ndarray:\n        \"\"\"Compute IoU between all pairs of boxes\"\"\"\n        bb_gt = np.expand_dims(bb_gt, 0)\n        bb_test = np.expand_dims(bb_test, 1)\n\n        xx1 = np.maximum(bb_test[..., 0], bb_gt[..., 0])\n        yy1 = np.maximum(bb_test[..., 1], bb_gt[..., 1])\n        xx2 = np.minimum(bb_test[..., 2], bb_gt[..., 2])\n        yy2 = np.minimum(bb_test[..., 3], bb_gt[..., 3])\n        w = np.maximum(0., xx2 - xx1)\n        h = np.maximum(0., yy2 - yy1)\n        wh = w * h\n        o = wh / ((bb_test[..., 2] - bb_test[..., 0]) * (bb_test[..., 3] - bb_test[..., 1])\n                  + (bb_gt[..., 2] - bb_gt[..., 0]) * (bb_gt[..., 3] - bb_gt[..., 1]) - wh)\n        return o\n\n\nclass TargetTracker:\n    \"\"\"\n    Single-target tracker with prediction for fast-moving objects.\n    Uses Kalman filter + optical flow for robust tracking.\n    \"\"\"\n    def __init__(self):\n        self.kalman = KalmanBoxTracker(np.array([0, 0, 100, 100]))\n        self.last_bbox = None\n        self.lost_frames = 0\n        self.max_lost = 30\n\n    def update(self, detection: Optional[Dict] = None) -> Dict:\n        \"\"\"Update tracker with detection (or None if lost)\"\"\"\n        predicted = self.kalman.predict()\n\n        if detection is not None:\n            self.kalman.update(np.array(detection['bbox']))\n            self.last_bbox = detection['bbox']\n            self.lost_frames = 0\n            status = 'tracking'\n        else:\n            self.lost_frames += 1\n            self.last_bbox = predicted.tolist()\n            status = 'predicting' if self.lost_frames < self.max_lost else 'lost'\n\n        return {\n            'bbox': self.last_bbox,\n            'predicted_bbox': predicted.tolist(),\n            'status': status,\n            'lost_frames': self.lost_frames\n        }\n```\n\n## Optical Flow for Motion Estimation\n\n```python\nimport cv2\nimport numpy as np\nfrom typing import Tuple, Optional\n\nclass OpticalFlowTracker:\n    \"\"\"\n    Optical flow for visual motion estimation.\n    Useful for velocity estimation and feature tracking.\n    \"\"\"\n    def __init__(self, feature_params: Optional[dict] = None, lk_params: Optional[dict] = None):\n        self.feature_params = feature_params or {\n            'maxCorners': 100,\n            'qualityLevel': 0.3,\n            'minDistance': 7,\n            'blockSize': 7\n        }\n\n        self.lk_params = lk_params or {\n            'winSize': (15, 15),\n            'maxLevel': 2,\n            'criteria': (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)\n        }\n\n        self.prev_gray = None\n        self.prev_points = None\n\n    def compute_flow(self, frame: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Compute sparse optical flow between frames.\n\n        Returns:\n            flow_vectors: Motion vectors for each tracked point\n            good_points: Points successfully tracked\n        \"\"\"\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) if len(frame.shape) == 3 else frame\n\n        if self.prev_gray is None:\n            self.prev_gray = gray\n            self.prev_points = cv2.goodFeaturesToTrack(gray, mask=None, **self.feature_params)\n            return np.array([]), np.array([])\n\n        if self.prev_points is None or len(self.prev_points) < 10:\n            self.prev_points = cv2.goodFeaturesToTrack(gray, mask=None, **self.feature_params)\n\n        # Calculate optical flow\n        next_points, status, _ = cv2.calcOpticalFlowPyrLK(\n            self.prev_gray, gray, self.prev_points, None, **self.lk_params\n        )\n\n        # Select good points\n        if next_points is not None:\n            good_old = self.prev_points[status == 1]\n            good_new = next_points[status == 1]\n\n            # Compute flow vectors\n            flow_vectors = good_new - good_old\n\n            # Update for next frame\n            self.prev_gray = gray.copy()\n            self.prev_points = good_new.reshape(-1, 1, 2)\n\n            return flow_vectors, good_new\n\n        return np.array([]), np.array([])\n\n    def estimate_motion(self, frame: np.ndarray) -> Tuple[float, float, float]:\n        \"\"\"\n        Estimate camera motion from optical flow.\n\n        Returns:\n            dx, dy: Translation in pixels\n            rotation: Rotation in radians\n        \"\"\"\n        flow_vectors, points = self.compute_flow(frame)\n\n        if len(flow_vectors) < 4:\n            return 0.0, 0.0, 0.0\n\n        # Average translation\n        dx = np.median(flow_vectors[:, 0])\n        dy = np.median(flow_vectors[:, 1])\n\n        # Estimate rotation from flow field\n        # Points moving clockwise = positive rotation\n        center = np.mean(points, axis=0)\n        relative_pos = points - center\n\n        # Cross product for rotation direction\n        rotation_components = []\n        for i, (pos, flow) in enumerate(zip(relative_pos, flow_vectors)):\n            cross = pos[0] * flow[1] - pos[1] * flow[0]\n            dist = np.linalg.norm(pos)\n            if dist > 10:  # Avoid center points\n                rotation_components.append(cross / dist)\n\n        rotation = np.median(rotation_components) if rotation_components else 0.0\n\n        return float(dx), float(dy), float(rotation)\n```\n"
        },
        {
          "name": "sensor-fusion-ekf.md",
          "type": "file",
          "path": "drone-cv-expert/references/sensor-fusion-ekf.md",
          "size": 11687,
          "content": "# Sensor Fusion & State Estimation Reference\n\n## Extended Kalman Filter for Drone State\n\n```python\nimport numpy as np\nfrom typing import Tuple, Optional\n\nclass DroneEKF:\n    \"\"\"\n    Extended Kalman Filter for drone state estimation.\n    State vector: [x, y, z, vx, vy, vz, roll, pitch, yaw]\n    \"\"\"\n    def __init__(self):\n        # State: position (3), velocity (3), attitude (3)\n        self.state = np.zeros(9)\n        self.P = np.eye(9) * 1.0  # State covariance\n\n        # Process noise\n        self.Q = np.diag([\n            0.01, 0.01, 0.01,    # Position noise\n            0.05, 0.05, 0.05,    # Velocity noise\n            0.001, 0.001, 0.001  # Attitude noise\n        ])\n\n        # Measurement noise matrices\n        self.R_gps = np.eye(3) * 2.5       # GPS position (meters)\n        self.R_baro = np.array([[0.5]])     # Barometer altitude (meters)\n        self.R_imu_accel = np.eye(3) * 0.1  # Accelerometer\n        self.R_imu_gyro = np.eye(3) * 0.01  # Gyroscope\n\n        self.gravity = np.array([0, 0, 9.81])\n\n    def predict(self, dt: float):\n        \"\"\"Predict step using motion model\"\"\"\n        # State transition matrix (linearized)\n        F = self._get_state_transition_matrix(dt)\n\n        # Predict state\n        self.state = self._motion_model(self.state, dt)\n\n        # Predict covariance\n        self.P = F @ self.P @ F.T + self.Q\n\n    def _motion_model(self, state: np.ndarray, dt: float) -> np.ndarray:\n        \"\"\"Non-linear motion model\"\"\"\n        new_state = state.copy()\n\n        # Position update: x += v * dt\n        new_state[0:3] += state[3:6] * dt\n\n        return new_state\n\n    def _get_state_transition_matrix(self, dt: float) -> np.ndarray:\n        \"\"\"Jacobian of motion model\"\"\"\n        F = np.eye(9)\n        F[0, 3] = dt  # dx/dvx\n        F[1, 4] = dt  # dy/dvy\n        F[2, 5] = dt  # dz/dvz\n        return F\n\n    def update_gps(self, gps_measurement: np.ndarray):\n        \"\"\"Update with GPS position measurement [lat, lon, alt] in local frame\"\"\"\n        # Observation matrix: GPS measures position\n        H = np.zeros((3, 9))\n        H[0:3, 0:3] = np.eye(3)\n\n        # Innovation\n        y = gps_measurement - (H @ self.state)\n\n        # Kalman gain\n        S = H @ self.P @ H.T + self.R_gps\n        K = self.P @ H.T @ np.linalg.inv(S)\n\n        # Update\n        self.state = self.state + K @ y\n        self.P = (np.eye(9) - K @ H) @ self.P\n\n    def update_barometer(self, altitude: float):\n        \"\"\"Update with barometer altitude measurement\"\"\"\n        H = np.zeros((1, 9))\n        H[0, 2] = 1  # Measures z position\n\n        y = altitude - self.state[2]\n        S = H @ self.P @ H.T + self.R_baro\n        K = self.P @ H.T / S\n\n        self.state = self.state + (K * y).flatten()\n        self.P = (np.eye(9) - K @ H) @ self.P\n\n    def update_imu(self, accel: np.ndarray, gyro: np.ndarray, dt: float):\n        \"\"\"Update with IMU measurements\"\"\"\n        # Accelerometer updates velocity (after gravity compensation)\n        roll, pitch, yaw = self.state[6:9]\n        R_body_to_world = self._rotation_matrix(roll, pitch, yaw)\n\n        # Compensate for gravity and transform to world frame\n        accel_world = R_body_to_world @ accel - self.gravity\n\n        # Simple integration for velocity update\n        H_accel = np.zeros((3, 9))\n        H_accel[0:3, 3:6] = np.eye(3) / dt\n\n        innovation = accel_world - (self.state[3:6] - self._prev_velocity) / dt if hasattr(self, '_prev_velocity') else accel_world\n        self._prev_velocity = self.state[3:6].copy()\n\n        # Gyroscope updates attitude rates\n        self.state[6:9] += gyro * dt\n\n        # Normalize angles\n        self.state[6:9] = np.mod(self.state[6:9] + np.pi, 2 * np.pi) - np.pi\n\n    def _rotation_matrix(self, roll: float, pitch: float, yaw: float) -> np.ndarray:\n        \"\"\"Create rotation matrix from Euler angles\"\"\"\n        cr, sr = np.cos(roll), np.sin(roll)\n        cp, sp = np.cos(pitch), np.sin(pitch)\n        cy, sy = np.cos(yaw), np.sin(yaw)\n\n        return np.array([\n            [cy*cp, cy*sp*sr - sy*cr, cy*sp*cr + sy*sr],\n            [sy*cp, sy*sp*sr + cy*cr, sy*sp*cr - cy*sr],\n            [-sp, cp*sr, cp*cr]\n        ])\n\n    def get_position(self) -> np.ndarray:\n        return self.state[0:3]\n\n    def get_velocity(self) -> np.ndarray:\n        return self.state[3:6]\n\n    def get_attitude(self) -> np.ndarray:\n        return self.state[6:9]\n\n    def get_covariance(self) -> np.ndarray:\n        return self.P\n\n\nclass UnscentedKalmanFilter:\n    \"\"\"\n    Unscented Kalman Filter - better for highly nonlinear systems.\n    Use when EKF linearization introduces significant errors.\n    \"\"\"\n    def __init__(self, n: int = 9):\n        self.n = n\n        self.state = np.zeros(n)\n        self.P = np.eye(n) * 1.0\n\n        # UKF parameters\n        self.alpha = 0.001\n        self.beta = 2.0\n        self.kappa = 0.0\n        self.lambda_ = self.alpha**2 * (n + self.kappa) - n\n\n        # Weights for mean and covariance\n        self.Wm = np.zeros(2 * n + 1)\n        self.Wc = np.zeros(2 * n + 1)\n        self.Wm[0] = self.lambda_ / (n + self.lambda_)\n        self.Wc[0] = self.Wm[0] + (1 - self.alpha**2 + self.beta)\n        for i in range(1, 2 * n + 1):\n            self.Wm[i] = 1 / (2 * (n + self.lambda_))\n            self.Wc[i] = self.Wm[i]\n\n    def _sigma_points(self) -> np.ndarray:\n        \"\"\"Generate sigma points\"\"\"\n        sigma_pts = np.zeros((2 * self.n + 1, self.n))\n        sigma_pts[0] = self.state\n\n        sqrt_P = np.linalg.cholesky((self.n + self.lambda_) * self.P)\n\n        for i in range(self.n):\n            sigma_pts[i + 1] = self.state + sqrt_P[i]\n            sigma_pts[self.n + i + 1] = self.state - sqrt_P[i]\n\n        return sigma_pts\n\n    def predict(self, motion_model: callable, Q: np.ndarray, dt: float):\n        \"\"\"Predict using unscented transform\"\"\"\n        # Generate sigma points\n        sigma_pts = self._sigma_points()\n\n        # Transform through motion model\n        transformed = np.array([motion_model(sp, dt) for sp in sigma_pts])\n\n        # Compute predicted mean\n        self.state = np.sum(self.Wm[:, np.newaxis] * transformed, axis=0)\n\n        # Compute predicted covariance\n        self.P = Q.copy()\n        for i in range(2 * self.n + 1):\n            diff = transformed[i] - self.state\n            self.P += self.Wc[i] * np.outer(diff, diff)\n\n    def update(self, measurement: np.ndarray, measurement_model: callable, R: np.ndarray):\n        \"\"\"Update with measurement\"\"\"\n        # Generate sigma points\n        sigma_pts = self._sigma_points()\n\n        # Transform through measurement model\n        transformed = np.array([measurement_model(sp) for sp in sigma_pts])\n\n        # Predicted measurement\n        z_pred = np.sum(self.Wm[:, np.newaxis] * transformed, axis=0)\n\n        # Innovation covariance\n        Pzz = R.copy()\n        for i in range(2 * self.n + 1):\n            diff = transformed[i] - z_pred\n            Pzz += self.Wc[i] * np.outer(diff, diff)\n\n        # Cross covariance\n        Pxz = np.zeros((self.n, len(measurement)))\n        for i in range(2 * self.n + 1):\n            diff_x = sigma_pts[i] - self.state\n            diff_z = transformed[i] - z_pred\n            Pxz += self.Wc[i] * np.outer(diff_x, diff_z)\n\n        # Kalman gain\n        K = Pxz @ np.linalg.inv(Pzz)\n\n        # Update\n        self.state = self.state + K @ (measurement - z_pred)\n        self.P = self.P - K @ Pzz @ K.T\n\n\nclass ComplementaryFilter:\n    \"\"\"\n    Simple complementary filter for attitude estimation.\n    Faster than Kalman for basic stabilization.\n    \"\"\"\n    def __init__(self, alpha: float = 0.98):\n        self.alpha = alpha  # Trust gyroscope vs accelerometer\n        self.roll = 0.0\n        self.pitch = 0.0\n        self.yaw = 0.0\n\n    def update(self, accel: np.ndarray, gyro: np.ndarray, dt: float,\n               mag: Optional[np.ndarray] = None) -> Tuple[float, float, float]:\n        \"\"\"Update attitude estimate\"\"\"\n        # Accelerometer-based attitude (gravity direction)\n        accel_roll = np.arctan2(accel[1], accel[2])\n        accel_pitch = np.arctan2(-accel[0], np.sqrt(accel[1]**2 + accel[2]**2))\n\n        # Gyroscope integration\n        self.roll += gyro[0] * dt\n        self.pitch += gyro[1] * dt\n        self.yaw += gyro[2] * dt\n\n        # Complementary fusion\n        self.roll = self.alpha * self.roll + (1 - self.alpha) * accel_roll\n        self.pitch = self.alpha * self.pitch + (1 - self.alpha) * accel_pitch\n\n        # Magnetometer for yaw (if available)\n        if mag is not None:\n            # Compensate for tilt\n            mx = mag[0] * np.cos(self.pitch) + mag[2] * np.sin(self.pitch)\n            my = (mag[0] * np.sin(self.roll) * np.sin(self.pitch) +\n                  mag[1] * np.cos(self.roll) -\n                  mag[2] * np.sin(self.roll) * np.cos(self.pitch))\n            mag_yaw = np.arctan2(-my, mx)\n            self.yaw = self.alpha * self.yaw + (1 - self.alpha) * mag_yaw\n\n        return self.roll, self.pitch, self.yaw\n```\n\n## Multi-Sensor Fusion Architecture\n\n```python\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Callable\nimport time\n\n@dataclass\nclass SensorReading:\n    sensor_type: str\n    timestamp: float\n    data: np.ndarray\n    covariance: np.ndarray\n\nclass MultiSensorFusion:\n    \"\"\"\n    Asynchronous multi-sensor fusion framework.\n    Handles GPS, IMU, Barometer, Optical Flow, LiDAR, etc.\n    \"\"\"\n    def __init__(self):\n        self.ekf = DroneEKF()\n        self.sensor_queue: List[SensorReading] = []\n        self.last_update_time = time.time()\n\n        # Sensor update callbacks\n        self.update_functions: Dict[str, Callable] = {\n            'gps': self._update_gps,\n            'imu': self._update_imu,\n            'barometer': self._update_baro,\n            'optical_flow': self._update_optical_flow,\n            'lidar': self._update_lidar\n        }\n\n    def add_reading(self, reading: SensorReading):\n        \"\"\"Add sensor reading to fusion queue\"\"\"\n        self.sensor_queue.append(reading)\n        self.sensor_queue.sort(key=lambda x: x.timestamp)\n\n    def process(self) -> np.ndarray:\n        \"\"\"Process all pending sensor readings\"\"\"\n        while self.sensor_queue:\n            reading = self.sensor_queue.pop(0)\n\n            # Predict to reading time\n            dt = reading.timestamp - self.last_update_time\n            if dt > 0:\n                self.ekf.predict(dt)\n                self.last_update_time = reading.timestamp\n\n            # Update with sensor reading\n            if reading.sensor_type in self.update_functions:\n                self.update_functions[reading.sensor_type](reading)\n\n        return self.ekf.state\n\n    def _update_gps(self, reading: SensorReading):\n        self.ekf.R_gps = reading.covariance\n        self.ekf.update_gps(reading.data)\n\n    def _update_imu(self, reading: SensorReading):\n        accel = reading.data[:3]\n        gyro = reading.data[3:6]\n        self.ekf.update_imu(accel, gyro, 0.01)\n\n    def _update_baro(self, reading: SensorReading):\n        self.ekf.update_barometer(reading.data[0])\n\n    def _update_optical_flow(self, reading: SensorReading):\n        \"\"\"Update with optical flow velocity measurement\"\"\"\n        # Optical flow gives ground-relative velocity\n        H = np.zeros((2, 9))\n        H[0, 3] = 1  # vx\n        H[1, 4] = 1  # vy\n\n        R = reading.covariance[:2, :2]\n        y = reading.data[:2] - self.ekf.state[3:5]\n        S = H @ self.ekf.P @ H.T + R\n        K = self.ekf.P @ H.T @ np.linalg.inv(S)\n        self.ekf.state = self.ekf.state + K @ y\n        self.ekf.P = (np.eye(9) - K @ H) @ self.ekf.P\n\n    def _update_lidar(self, reading: SensorReading):\n        \"\"\"Update with LiDAR altitude\"\"\"\n        self.ekf.update_barometer(reading.data[0])\n```\n"
        }
      ]
    },
    {
      "name": "CHANGELOG.md",
      "type": "file",
      "path": "drone-cv-expert/CHANGELOG.md",
      "size": 1548,
      "content": "# Changelog\n\n## [2.0.0] - 2024-12-XX\n\n### Changed\n- **BREAKING**: Rewrote SKILL.md from 430 → 193 lines using skill-coach methodology\n- Updated frontmatter from `tools:` to `allowed-tools:` format\n- Added comprehensive NOT clause for skill differentiation from drone-inspection-specialist\n\n### Added\n- Decision tree for skill selection (drone-cv-expert vs drone-inspection-specialist vs others)\n- 6 anti-patterns with corrections:\n  1. Simulation-Only Syndrome\n  2. EKF Overkill\n  3. Max Resolution Assumption\n  4. Single-Thread Processing\n  5. GPS Trust\n  6. One Model Fits All\n- Quick reference tables (MAVLink messages, Kalman tuning, coordinate frames)\n- Algorithm selection matrix (classical vs deep learning)\n- Safety checklist for pre-flight\n- Reference files with detailed implementations:\n  - `references/navigation-algorithms.md` - SLAM, A*, RRT, VIO, AprilTag localization\n  - `references/sensor-fusion-ekf.md` - EKF, UKF, complementary filter, multi-sensor fusion\n  - `references/object-detection-tracking.md` - YOLO, TensorRT, ByteTrack, optical flow\n\n### Removed\n- Inline code examples (moved to reference files)\n- Redundant tool configuration (triggers, integrates_with, python_dependencies in frontmatter)\n- Verbose problem-solving examples\n\n### Migration Guide\nCode examples previously inline are now in `references/`:\n- `WaypointNavigator`, `VisualSLAM` → `references/navigation-algorithms.md`\n- `DroneEKF` → `references/sensor-fusion-ekf.md`\n- `AerialObjectDetector`, tracking → `references/object-detection-tracking.md`\n"
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "drone-cv-expert/SKILL.md",
      "size": 8136,
      "content": "---\nname: drone-cv-expert\ndescription: Expert in drone systems, computer vision, and autonomous navigation. Specializes in flight control, SLAM, object detection, sensor fusion, and path planning. Activate on \"drone\", \"UAV\", \"SLAM\", \"visual odometry\", \"PID control\", \"MAVLink\", \"Pixhawk\", \"path planning\", \"A*\", \"RRT\", \"EKF\", \"sensor fusion\", \"optical flow\", \"ByteTrack\". NOT for domain-specific inspection tasks like fire detection, roof damage assessment, or thermal analysis (use drone-inspection-specialist), GPU shader optimization (use metal-shader-expert), or general image classification without drone context (use clip-aware-embeddings).\nallowed-tools:\n  - Read\n  - Write\n  - Edit\n  - Bash\n  - Grep\n  - Glob\n  - mcp__firecrawl__firecrawl_search\n  - WebFetch\n---\n\n# Drone CV Expert\n\nExpert in robotics, drone systems, and computer vision for autonomous aerial platforms.\n\n## Decision Tree: When to Use This Skill\n\n```\nUser mentions drones or UAVs?\n├─ YES → Is it about inspection/detection of specific things (fire, roof damage, thermal)?\n│        ├─ YES → Use drone-inspection-specialist\n│        └─ NO → Is it about flight control, navigation, or general CV?\n│                ├─ YES → Use THIS SKILL (drone-cv-expert)\n│                └─ NO → Is it about GPU rendering/shaders?\n│                        ├─ YES → Use metal-shader-expert\n│                        └─ NO → Use THIS SKILL as default drone skill\n└─ NO → Is it general object detection without drone context?\n        ├─ YES → Use clip-aware-embeddings or other CV skill\n        └─ NO → Probably not a drone question\n```\n\n## Core Competencies\n\n### Flight Control & Navigation\n- **PID Tuning**: Position, velocity, attitude control loops\n- **SLAM**: ORB-SLAM, LSD-SLAM, visual-inertial odometry (VIO)\n- **Path Planning**: A*, RRT, RRT*, Dijkstra, potential fields\n- **Sensor Fusion**: EKF, UKF, complementary filters\n- **GPS-Denied Navigation**: AprilTags, visual odometry, LiDAR SLAM\n\n### Computer Vision\n- **Object Detection**: YOLO (v5/v8/v10), EfficientDet, SSD\n- **Tracking**: ByteTrack, DeepSORT, SORT, optical flow\n- **Edge Deployment**: TensorRT, ONNX, OpenVINO optimization\n- **3D Vision**: Stereo depth, point clouds, structure-from-motion\n\n### Hardware Integration\n- **Flight Controllers**: Pixhawk, Ardupilot, PX4, DJI\n- **Protocols**: MAVLink, DroneKit, MAVSDK\n- **Edge Compute**: Jetson (Nano/Xavier/Orin), Coral TPU\n- **Sensors**: IMU, GPS, barometer, LiDAR, depth cameras\n\n## Anti-Patterns to Avoid\n\n### 1. \"Simulation-Only Syndrome\"\n**Wrong**: Testing only in Gazebo/AirSim, then deploying directly to real drone.\n**Right**: Simulation → Bench test → Tethered flight → Controlled environment → Field.\n\n### 2. \"EKF Overkill\"\n**Wrong**: Using Extended Kalman Filter when complementary filter suffices.\n**Right**: Match filter complexity to requirements:\n- Complementary filter: Basic stabilization, attitude only\n- EKF: Multi-sensor fusion, GPS+IMU+baro\n- UKF: Highly nonlinear systems, aggressive maneuvers\n\n### 3. \"Max Resolution Assumption\"\n**Wrong**: Processing 4K frames at 30fps expecting real-time performance.\n**Right**: Resolution trade-offs by altitude/speed:\n| Altitude | Speed | Resolution | FPS | Rationale |\n|----------|-------|------------|-----|-----------|\n| &lt;30m | Slow | 1920x1080 | 30 | Detail needed |\n| 30-100m | Medium | 1280x720 | 30 | Balance |\n| &gt;100m | Fast | 640x480 | 60 | Speed priority |\n\n### 4. \"Single-Thread Processing\"\n**Wrong**: Sequential detect → track → control in one loop.\n**Right**: Pipeline parallelism:\n```\nThread 1: Camera capture (async)\nThread 2: Object detection (GPU)\nThread 3: Tracking + state estimation\nThread 4: Control commands\n```\n\n### 5. \"GPS Trust\"\n**Wrong**: Assuming GPS is always accurate and available.\n**Right**: Multi-source position estimation:\n- GPS: 2-5m accuracy outdoor, unavailable indoor\n- Visual odometry: 0.1-1% drift, lighting dependent\n- AprilTags: cm-level accuracy where deployed\n- IMU: Short-term only, drift accumulates\n\n### 6. \"One Model Fits All\"\n**Wrong**: Using same YOLO model for all scenarios.\n**Right**: Model selection by constraint:\n| Constraint | Model | Notes |\n|------------|-------|-------|\n| Latency critical | YOLOv8n | 6ms inference |\n| Balanced | YOLOv8s | 15ms, better accuracy |\n| Accuracy first | YOLOv8x | 50ms, highest mAP |\n| Edge device | YOLOv8n + TensorRT | 3ms on Jetson |\n\n## Problem-Solving Framework\n\n### 1. Constraint Analysis\n- **Compute**: What hardware? (Jetson Nano = ~5 TOPS, Xavier = 32 TOPS)\n- **Power**: Battery capacity? Flight time impact?\n- **Latency**: Control loop rate? Detection response time?\n- **Weight**: Payload capacity? Center of gravity?\n- **Environment**: Indoor/outdoor? GPS available? Lighting conditions?\n\n### 2. Algorithm Selection Matrix\n\n| Problem | Classical Approach | Deep Learning | When to Use Each |\n|---------|-------------------|---------------|------------------|\n| Feature tracking | KLT optical flow | FlowNet | Classical: Real-time, limited compute. DL: Robust, more compute |\n| Object detection | HOG+SVM | YOLO/SSD | Classical: Simple objects, no GPU. DL: Complex, GPU available |\n| SLAM | ORB-SLAM | DROID-SLAM | Classical: Mature, debuggable. DL: Better in challenging scenes |\n| Path planning | A*, RRT | RL-based | Classical: Known environments. DL: Complex, dynamic |\n\n### 3. Safety Checklist\n- [ ] Kill switch tested and accessible\n- [ ] Geofence configured\n- [ ] Return-to-home altitude set\n- [ ] Low battery action defined\n- [ ] Signal loss action defined\n- [ ] Propeller guards (if applicable)\n- [ ] Pre-flight sensor calibration\n- [ ] Weather conditions checked\n\n## Quick Reference Tables\n\n### MAVLink Message Types\n| Message | Purpose | Frequency |\n|---------|---------|-----------|\n| HEARTBEAT | Connection alive | 1 Hz |\n| ATTITUDE | Roll/pitch/yaw | 10-100 Hz |\n| LOCAL_POSITION_NED | Position | 10-50 Hz |\n| GPS_RAW_INT | Raw GPS | 1-10 Hz |\n| SET_POSITION_TARGET | Commands | As needed |\n\n### Kalman Filter Tuning\n| Matrix | High Values | Low Values |\n|--------|-------------|------------|\n| Q (process noise) | Trust measurements more | Trust model more |\n| R (measurement noise) | Trust model more | Trust measurements more |\n| P (initial covariance) | Uncertain initial state | Confident initial state |\n\n### Common Coordinate Frames\n| Frame | Origin | Axes | Use |\n|-------|--------|------|-----|\n| NED | Takeoff point | North-East-Down | Navigation |\n| ENU | Takeoff point | East-North-Up | ROS standard |\n| Body | Drone CG | Forward-Right-Down | Control |\n| Camera | Lens center | Right-Down-Forward | Vision |\n\n## Reference Files\n\nDetailed implementations in `references/`:\n- `navigation-algorithms.md` - SLAM, path planning, localization\n- `sensor-fusion-ekf.md` - Kalman filters, multi-sensor fusion\n- `object-detection-tracking.md` - YOLO, ByteTrack, optical flow\n\n## Simulation Tools\n\n| Tool | Strengths | Weaknesses | Best For |\n|------|-----------|------------|----------|\n| Gazebo | ROS integration, physics | Graphics quality | ROS development |\n| AirSim | Photorealistic, CV-focused | Windows-centric | Vision algorithms |\n| Webots | Multi-robot, accessible | Less drone-specific | Swarm simulations |\n| MATLAB/Simulink | Control design | Not real-time | Controller tuning |\n\n## Emerging Technologies (2024-2025)\n\n- **Event cameras**: 1μs temporal resolution, no motion blur\n- **Neuromorphic computing**: Loihi 2 for ultra-low-power inference\n- **4D Radar**: Velocity + 3D position, works in all weather\n- **Swarm autonomy**: Decentralized coordination, emergent behavior\n- **Foundation models**: SAM, CLIP for zero-shot detection\n\n## Integration Points\n\n- **drone-inspection-specialist**: Domain-specific detection (fire, damage, thermal)\n- **metal-shader-expert**: GPU-accelerated vision processing, custom shaders\n- **collage-layout-expert**: Report generation, visual composition\n\n---\n\n**Key Principle**: In drone systems, reliability trumps performance. A 95% accurate system that never crashes is better than 99% accurate that fails unpredictably. Always have fallbacks.\n"
    }
  ]
}