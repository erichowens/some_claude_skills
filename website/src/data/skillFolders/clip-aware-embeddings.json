{
  "name": "clip-aware-embeddings",
  "type": "folder",
  "path": "clip-aware-embeddings",
  "children": [
    {
      "name": "scripts",
      "type": "folder",
      "path": "clip-aware-embeddings/scripts",
      "children": [
        {
          "name": "validate_clip_usage.py",
          "type": "file",
          "path": "clip-aware-embeddings/scripts/validate_clip_usage.py",
          "size": 7094,
          "content": "#!/usr/bin/env python3\n\"\"\"\nCLIP Usage Validator - Checks if CLIP is appropriate for a given query\n\nThis demonstrates domain-specific validation that encodes expert knowledge.\n\"\"\"\n\nimport sys\nimport re\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n\nclass TaskType(Enum):\n    SEMANTIC_SEARCH = \"semantic_search\"\n    COUNTING = \"counting\"\n    FINE_GRAINED = \"fine_grained\"\n    SPATIAL = \"spatial\"\n    COMPOSITIONAL = \"compositional\"\n    ZERO_SHOT = \"zero_shot\"\n\n\n@dataclass\nclass ValidationResult:\n    is_appropriate: bool\n    task_type: TaskType\n    confidence: float\n    reason: str\n    alternative: Optional[str] = None\n\n\nclass CLIPValidator:\n    \"\"\"Validates whether CLIP is appropriate for a given task.\"\"\"\n    \n    # Keywords that indicate specific task types\n    COUNTING_KEYWORDS = [\n        'how many', 'count', 'number of', 'total', 'quantity',\n        'several', 'few', 'multiple'\n    ]\n    \n    SPATIAL_KEYWORDS = [\n        'left', 'right', 'above', 'below', 'next to', 'beside',\n        'between', 'in front', 'behind', 'under', 'over', 'near'\n    ]\n    \n    FINE_GRAINED_DOMAINS = [\n        'celebrity', 'celebrities', 'actor', 'actress',\n        'car model', 'vehicle model', 'car make',\n        'flower species', 'bird species', 'dog breed',\n        'person', 'face', 'people'\n    ]\n    \n    COMPOSITIONAL_PATTERNS = [\n        r'(\\w+)\\s+(\\w+)\\s+and\\s+(\\w+)\\s+(\\w+)',  # \"red car and blue truck\"\n        r'both\\s+',\n        r'neither\\s+',\n        r'either\\s+',\n    ]\n    \n    GOOD_USE_CASES = [\n        'find images', 'search for', 'similar to', 'looks like',\n        'classify', 'categorize', 'what is this', 'identify',\n        'semantic', 'concept', 'theme'\n    ]\n    \n    def validate(self, query: str) -> ValidationResult:\n        \"\"\"\n        Validate if CLIP is appropriate for the query.\n        \n        Args:\n            query: Natural language query\n            \n        Returns:\n            ValidationResult with recommendation\n        \"\"\"\n        query_lower = query.lower()\n        \n        # Check for counting tasks\n        if any(kw in query_lower for kw in self.COUNTING_KEYWORDS):\n            return ValidationResult(\n                is_appropriate=False,\n                task_type=TaskType.COUNTING,\n                confidence=0.95,\n                reason=\"Query requires counting objects. CLIP cannot preserve spatial information needed for counting.\",\n                alternative=\"Use object detection models: DETR, Faster R-CNN, YOLO\"\n            )\n        \n        # Check for spatial reasoning\n        if any(kw in query_lower for kw in self.SPATIAL_KEYWORDS):\n            return ValidationResult(\n                is_appropriate=False,\n                task_type=TaskType.SPATIAL,\n                confidence=0.90,\n                reason=\"Query requires spatial understanding. CLIP's embeddings lose spatial topology.\",\n                alternative=\"Use spatial reasoning models: GQA, SWIG, Visual Genome models\"\n            )\n        \n        # Check for fine-grained classification\n        if any(domain in query_lower for domain in self.FINE_GRAINED_DOMAINS):\n            return ValidationResult(\n                is_appropriate=False,\n                task_type=TaskType.FINE_GRAINED,\n                confidence=0.85,\n                reason=\"Query requires fine-grained classification. CLIP trained on coarse categories.\",\n                alternative=\"Use specialized models: Fine-tuned ResNet/EfficientNet for the specific domain\"\n            )\n        \n        # Check for compositional reasoning\n        if any(re.search(pattern, query_lower) for pattern in self.COMPOSITIONAL_PATTERNS):\n            return ValidationResult(\n                is_appropriate=False,\n                task_type=TaskType.COMPOSITIONAL,\n                confidence=0.80,\n                reason=\"Query requires attribute binding. CLIP cannot bind attributes to specific objects.\",\n                alternative=\"Use compositional models: DCSMs (Dense Cosine Similarity Maps), PC-CLIP\"\n            )\n        \n        # Check if it's a good CLIP use case\n        if any(use_case in query_lower for use_case in self.GOOD_USE_CASES):\n            return ValidationResult(\n                is_appropriate=True,\n                task_type=TaskType.SEMANTIC_SEARCH,\n                confidence=0.90,\n                reason=\"Query is appropriate for CLIP: semantic search or broad categorization.\",\n                alternative=None\n            )\n        \n        # Default: probably okay but lower confidence\n        return ValidationResult(\n            is_appropriate=True,\n            task_type=TaskType.ZERO_SHOT,\n            confidence=0.60,\n            reason=\"Query appears suitable for CLIP, but verify results carefully.\",\n            alternative=\"If results are poor, consider task-specific models\"\n        )\n\n\ndef print_result(query: str, result: ValidationResult):\n    \"\"\"Pretty-print validation results.\"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(f\"CLIP USAGE VALIDATION\")\n    print(\"=\"*70)\n    print(f\"\\nQuery: {query}\")\n    print(f\"Task Type: {result.task_type.value}\")\n    print(f\"Confidence: {result.confidence:.0%}\")\n    print()\n    \n    if result.is_appropriate:\n        print(\"‚úÖ CLIP IS APPROPRIATE\")\n        print(f\"\\nReason: {result.reason}\")\n        if result.alternative:\n            print(f\"\\nüí° Note: {result.alternative}\")\n    else:\n        print(\"‚ùå CLIP IS NOT APPROPRIATE\")\n        print(f\"\\nReason: {result.reason}\")\n        print(f\"\\nüí° Use Instead: {result.alternative}\")\n    \n    print(\"\\n\" + \"=\"*70 + \"\\n\")\n\n\ndef run_examples():\n    \"\"\"Run validation on example queries.\"\"\"\n    examples = [\n        \"Find images of beaches at sunset\",\n        \"How many cars are in this image?\",\n        \"Identify which celebrity this is\",\n        \"Is the cat to the left or right of the dog?\",\n        \"Find images with a red car and a blue truck\",\n        \"Classify this image as indoor or outdoor\",\n    ]\n    \n    validator = CLIPValidator()\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"EXAMPLE VALIDATIONS\")\n    print(\"=\"*70)\n    \n    for query in examples:\n        result = validator.validate(query)\n        print(f\"\\n{query}\")\n        print(f\"  ‚Üí {'‚úÖ CLIP' if result.is_appropriate else '‚ùå Alternative'}: {result.task_type.value}\")\n        if not result.is_appropriate:\n            print(f\"  ‚Üí {result.alternative}\")\n    \n    print(\"\\n\" + \"=\"*70 + \"\\n\")\n\n\ndef main():\n    if len(sys.argv) < 2:\n        print(\"Usage:\")\n        print(\"  python validate_clip_usage.py 'your query here'\")\n        print(\"  python validate_clip_usage.py --examples\")\n        print(\"\\nExample:\")\n        print(\"  python validate_clip_usage.py 'Find images of mountains'\")\n        sys.exit(1)\n    \n    if sys.argv[1] == '--examples':\n        run_examples()\n        return\n    \n    query = ' '.join(sys.argv[1:])\n    validator = CLIPValidator()\n    result = validator.validate(query)\n    print_result(query, result)\n    \n    # Exit code: 0 if appropriate, 1 if not\n    sys.exit(0 if result.is_appropriate else 1)\n\n\nif __name__ == '__main__':\n    main()\n"
        }
      ]
    },
    {
      "name": "CHANGELOG.md",
      "type": "file",
      "path": "clip-aware-embeddings/CHANGELOG.md",
      "size": 985,
      "content": "# Changelog\n\nAll notable changes to the clip-aware-embeddings skill will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [1.3.0] - 2025-11-26\n\n### Changed\n- Updated frontmatter to standard `allowed-tools` format\n- Added activation keywords to description\n\n### Added\n- **MCP Integrations** section (Firecrawl, Hugging Face)\n- Moved inline changelog to separate CHANGELOG.md file\n\n## [1.2.0] - 2025-03-15\n\n### Added\n- DCSMs and PC-CLIP alternatives\n- Updated for 2025 best practices\n- Improved validation scripts\n\n## [1.1.0] - 2024-06-10\n\n### Added\n- Anti-pattern detection\n- Expanded troubleshooting\n\n## [1.0.0] - 2024-01-15\n\n### Added\n- Initial release\n- CLIP usage decision tree\n- Anti-patterns for counting, fine-grained, spatial, attribute binding\n- Model selection guide\n- Performance notes\n- Reference documentation structure\n"
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "clip-aware-embeddings/SKILL.md",
      "size": 8980,
      "content": "---\nname: clip-aware-embeddings\ndescription: Semantic image-text matching with CLIP and alternatives. Use for image search, zero-shot classification, similarity matching. NOT for counting objects, fine-grained classification (celebrities, car models), spatial reasoning, or compositional queries. Activate on \"CLIP\", \"embeddings\", \"image similarity\", \"semantic search\", \"zero-shot classification\", \"image-text matching\".\nallowed-tools: Read,Write,Edit,Bash\n---\n\n# CLIP-Aware Image Embeddings\n\nSmart image-text matching that knows when CLIP works and when to use alternatives.\n\n## MCP Integrations\n\n| MCP | Purpose |\n|-----|---------|\n| **Firecrawl** | Research latest CLIP alternatives and benchmarks |\n| **Hugging Face** (if configured) | Access model cards and documentation |\n\n## Quick Decision Tree\n\n```\nYour task:\n‚îú‚îÄ Semantic search (\"find beach images\") ‚Üí CLIP ‚úì\n‚îú‚îÄ Zero-shot classification (broad categories) ‚Üí CLIP ‚úì\n‚îú‚îÄ Counting objects ‚Üí DETR, Faster R-CNN ‚úó\n‚îú‚îÄ Fine-grained ID (celebrities, car models) ‚Üí Specialized model ‚úó\n‚îú‚îÄ Spatial relations (\"cat left of dog\") ‚Üí GQA, SWIG ‚úó\n‚îî‚îÄ Compositional (\"red car AND blue truck\") ‚Üí DCSMs, PC-CLIP ‚úó\n```\n\n## When to Use This Skill\n\n‚úÖ **Use for**:\n- Semantic image search\n- Broad category classification\n- Image similarity matching\n- Zero-shot tasks on new categories\n\n‚ùå **Do NOT use for**:\n- Counting objects in images\n- Fine-grained classification\n- Spatial understanding\n- Attribute binding\n- Negation handling\n\n## Installation\n\n```bash\npip install transformers pillow torch sentence-transformers --break-system-packages\n```\n\n**Validation**: Run `python scripts/validate_setup.py`\n\n## Basic Usage\n\n### Image Search\n\n```python\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n\n# Embed images\nimages = [Image.open(f\"img{i}.jpg\") for i in range(10)]\ninputs = processor(images=images, return_tensors=\"pt\")\nimage_features = model.get_image_features(**inputs)\n\n# Search with text\ntext_inputs = processor(text=[\"a beach at sunset\"], return_tensors=\"pt\")\ntext_features = model.get_text_features(**text_inputs)\n\n# Compute similarity\nsimilarity = (image_features @ text_features.T).softmax(dim=0)\n```\n\n## Common Anti-Patterns\n\n### Anti-Pattern 1: \"CLIP for Everything\"\n\n**‚ùå Wrong**:\n```python\n# Using CLIP to count cars in an image\nprompt = \"How many cars are in this image?\"\n# CLIP cannot count - it will give nonsense results\n```\n\n**Why wrong**: CLIP's architecture collapses spatial information into a single vector. It literally cannot count.\n\n**‚úì Right**:\n```python\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\n# Detect objects\nresults = model(**processor(images=image, return_tensors=\"pt\"))\n# Filter for cars and count\ncar_detections = [d for d in results if d['label'] == 'car']\ncount = len(car_detections)\n```\n\n**How to detect**: If query contains \"how many\", \"count\", or numeric questions ‚Üí Use object detection\n\n---\n\n### Anti-Pattern 2: Fine-Grained Classification\n\n**‚ùå Wrong**:\n```python\n# Trying to identify specific celebrities with CLIP\nprompts = [\"Tom Hanks\", \"Brad Pitt\", \"Morgan Freeman\"]\n# CLIP will perform poorly - not trained for fine-grained face ID\n```\n\n**Why wrong**: CLIP trained on coarse categories. Fine-grained faces, car models, flower species require specialized models.\n\n**‚úì Right**:\n```python\n# Use a fine-tuned face recognition model\nfrom transformers import AutoFeatureExtractor, AutoModelForImageClassification\n\nmodel = AutoModelForImageClassification.from_pretrained(\n    \"microsoft/resnet-50\"  # Then fine-tune on celebrity dataset\n)\n# Or use dedicated face recognition: ArcFace, CosFace\n```\n\n**How to detect**: If query asks to distinguish between similar items in same category ‚Üí Use specialized model\n\n---\n\n### Anti-Pattern 3: Spatial Understanding\n\n**‚ùå Wrong**:\n```python\n# CLIP cannot understand spatial relationships\nprompts = [\n    \"cat to the left of dog\",\n    \"cat to the right of dog\"\n]\n# Will give nearly identical scores\n```\n\n**Why wrong**: CLIP embeddings lose spatial topology. \"Left\" and \"right\" are treated as bag-of-words.\n\n**‚úì Right**:\n```python\n# Use a spatial reasoning model\n# Examples: GQA models, Visual Genome models, SWIG\nfrom swig_model import SpatialRelationModel\n\nmodel = SpatialRelationModel()\nresult = model.predict_relation(image, \"cat\", \"dog\")\n# Returns: \"left\", \"right\", \"above\", \"below\", etc.\n```\n\n**How to detect**: If query contains directional words (left, right, above, under, next to) ‚Üí Use spatial model\n\n---\n\n### Anti-Pattern 4: Attribute Binding\n\n**‚ùå Wrong**:\n```python\nprompts = [\n    \"red car and blue truck\",\n    \"blue car and red truck\"\n]\n# CLIP often gives similar scores for both\n```\n\n**Why wrong**: CLIP cannot bind attributes to objects. It sees \"red, blue, car, truck\" as a bag of concepts.\n\n**‚úì Right - Use PC-CLIP or DCSMs**:\n```python\n# PC-CLIP: Fine-tuned for pairwise comparisons\nfrom pc_clip import PCCLIPModel\n\nmodel = PCCLIPModel.from_pretrained(\"pc-clip-vit-l\")\n# Or use DCSMs (Dense Cosine Similarity Maps)\n```\n\n**How to detect**: If query has multiple objects with different attributes ‚Üí Use compositional model\n\n---\n\n## Evolution Timeline\n\n### 2021: CLIP Released\n- Revolutionary: zero-shot, 400M image-text pairs\n- Widely adopted for everything\n- Limitations not yet understood\n\n### 2022-2023: Limitations Discovered\n- Cannot count objects\n- Poor at fine-grained classification\n- Fails spatial reasoning\n- Can't bind attributes\n\n### 2024: Alternatives Emerge\n- **DCSMs**: Preserve patch/token topology\n- **PC-CLIP**: Trained on pairwise comparisons\n- **SpLiCE**: Sparse interpretable embeddings\n\n### 2025: Current Best Practices\n- Use CLIP for what it's good at\n- Task-specific models for limitations\n- Compositional models for complex queries\n\n**LLM Mistake**: LLMs trained on 2021-2023 data will suggest CLIP for everything because limitations weren't widely known. This skill corrects that.\n\n---\n\n## Validation Script\n\nBefore using CLIP, check if it's appropriate:\n\n```bash\npython scripts/validate_clip_usage.py \\\n    --query \"your query here\" \\\n    --check-all\n```\n\nReturns:\n- ‚úÖ CLIP is appropriate\n- ‚ùå Use alternative (with suggestion)\n\n## Task-Specific Guidance\n\n### Image Search (CLIP ‚úì)\n```python\n# Good use of CLIP\nqueries = [\"beach\", \"mountain\", \"city skyline\"]\n# Works well for broad semantic concepts\n```\n\n### Zero-Shot Classification (CLIP ‚úì)\n```python\n# Good: Broad categories\ncategories = [\"indoor\", \"outdoor\", \"nature\", \"urban\"]\n# CLIP excels at this\n```\n\n### Object Counting (CLIP ‚úó)\n```python\n# Use object detection instead\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n# See /references/object_detection.md\n```\n\n### Fine-Grained Classification (CLIP ‚úó)\n```python\n# Use specialized models\n# See /references/fine_grained_models.md\n```\n\n### Spatial Reasoning (CLIP ‚úó)\n```python\n# Use spatial relation models\n# See /references/spatial_models.md\n```\n\n---\n\n## Troubleshooting\n\n### Issue: CLIP gives unexpected results\n\n**Check**:\n1. Is this a counting task? ‚Üí Use object detection\n2. Fine-grained classification? ‚Üí Use specialized model\n3. Spatial query? ‚Üí Use spatial model\n4. Multiple objects with attributes? ‚Üí Use compositional model\n\n**Validation**:\n```bash\npython scripts/diagnose_clip_issue.py --image path/to/image --query \"your query\"\n```\n\n### Issue: Low similarity scores\n\n**Possible causes**:\n1. Query too specific (CLIP works better with broad concepts)\n2. Fine-grained task (not CLIP's strength)\n3. Need to adjust threshold\n\n**Solution**: Try broader query or use alternative model\n\n---\n\n## Model Selection Guide\n\n| Model | Best For | Avoid For |\n|-------|----------|-----------|\n| CLIP ViT-L/14 | Semantic search, broad categories | Counting, fine-grained, spatial |\n| DETR | Object detection, counting | Semantic similarity |\n| DINOv2 | Fine-grained features | Text-image matching |\n| PC-CLIP | Attribute binding, comparisons | General embedding |\n| DCSMs | Compositional reasoning | Simple similarity |\n\n## Performance Notes\n\n**CLIP models**:\n- ViT-B/32: Fast, lower quality\n- ViT-L/14: Balanced (recommended)\n- ViT-g-14: Highest quality, slower\n\n**Inference time** (single image, CPU):\n- ViT-B/32: ~100ms\n- ViT-L/14: ~300ms\n- ViT-g-14: ~1000ms\n\n## Further Reading\n\n- `/references/clip_limitations.md` - Detailed analysis of CLIP's failures\n- `/references/alternatives.md` - When to use what model\n- `/references/compositional_reasoning.md` - DCSMs and PC-CLIP deep dive\n- `/scripts/validate_clip_usage.py` - Pre-flight validation tool\n- `/scripts/diagnose_clip_issue.py` - Debug unexpected results\n\n---\n\n*See CHANGELOG.md for version history.*\n"
    }
  ]
}