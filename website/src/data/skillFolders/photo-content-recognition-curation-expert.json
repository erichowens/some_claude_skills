{
  "name": "photo-content-recognition-curation-expert",
  "type": "folder",
  "path": "photo-content-recognition-curation-expert",
  "children": [
    {
      "name": "references",
      "type": "folder",
      "path": "photo-content-recognition-curation-expert/references",
      "children": [
        {
          "name": "content-detection.md",
          "type": "file",
          "path": "photo-content-recognition-curation-expert/references/content-detection.md",
          "size": 9880,
          "content": "# Content Detection Reference\n\n## Pet Recognition & Clustering\n\n```python\nfrom ultralytics import YOLO\nfrom transformers import CLIPProcessor, CLIPModel\n\nclass PetRecognizer:\n    \"\"\"Pet detection and clustering.\"\"\"\n\n    def __init__(self):\n        self.yolo = YOLO('yolov8n.pt')\n        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n    def detect_animals(self, image):\n        \"\"\"Detect animals in image.\"\"\"\n        results = self.yolo(image)\n\n        animals = []\n        animal_classes = ['cat', 'dog', 'horse', 'bird', 'cow', 'sheep',\n                         'elephant', 'bear', 'zebra', 'giraffe']\n\n        for result in results:\n            for box in result.boxes:\n                class_name = result.names[int(box.cls)]\n                if class_name in animal_classes:\n                    animals.append({\n                        'type': class_name,\n                        'bbox': box.xyxy[0].cpu().numpy(),\n                        'confidence': float(box.conf)\n                    })\n        return animals\n\n    def extract_pet_embedding(self, image, bbox):\n        \"\"\"Extract embedding for individual animal using CLIP.\"\"\"\n        x1, y1, x2, y2 = map(int, bbox)\n        crop = image.crop((x1, y1, x2, y2))\n\n        inputs = self.clip_processor(images=crop, return_tensors=\"pt\")\n        with torch.no_grad():\n            embedding = self.clip_model.get_image_features(**inputs)\n        return embedding.cpu().numpy().flatten()\n\n    def cluster_pets(self, pet_embeddings, min_cluster_size=5):\n        \"\"\"Cluster pet embeddings (same individual = cluster).\"\"\"\n        clusterer = hdbscan.HDBSCAN(\n            min_cluster_size=min_cluster_size,\n            min_samples=2,\n            metric='cosine'\n        )\n        return clusterer.fit_predict(np.array(pet_embeddings))\n```\n\n---\n\n## Burst Photo Selection\n\n**Problem:** Burst mode creates 10-50 nearly identical photos. Need to select best frame.\n\n**Solution:** Multi-criteria scoring: sharpness, face quality, aesthetics, composition.\n\n```python\nclass BurstPhotoSelector:\n    \"\"\"Select best photo from camera burst sequence.\"\"\"\n\n    def __init__(self):\n        self.face_detector = FaceEmbeddingExtractor()\n        self.aesthetic_scorer = NIMAPredictor()\n\n    def detect_bursts(self, photos_with_timestamps, max_gap_seconds=0.5):\n        \"\"\"Detect burst sequences from timestamps.\"\"\"\n        sorted_photos = sorted(photos_with_timestamps, key=lambda x: x[1])\n\n        bursts = []\n        current_burst = [sorted_photos[0]]\n\n        for photo in sorted_photos[1:]:\n            time_gap = (photo[1] - current_burst[-1][1]).total_seconds()\n            if time_gap <= max_gap_seconds:\n                current_burst.append(photo)\n            else:\n                if len(current_burst) >= 3:\n                    bursts.append(current_burst)\n                current_burst = [photo]\n\n        if len(current_burst) >= 3:\n            bursts.append(current_burst)\n\n        return bursts\n\n    def select_best_from_burst(self, burst_photos):\n        \"\"\"\n        Select best photo from burst.\n        Criteria: Sharpness, Face quality, Aesthetics, Position, Exposure\n        \"\"\"\n        scores = []\n\n        for idx, (photo_id, timestamp, image) in enumerate(burst_photos):\n            score = 0.0\n\n            # 1. SHARPNESS (30%)\n            gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n            sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()\n            score += min(1.0, sharpness / 1000) * 0.30\n\n            # 2. FACE QUALITY (35%)\n            faces = self.face_detector.extract_faces(image)\n            if faces:\n                face_scores = [self.assess_face_quality(f) for f in faces]\n                score += np.mean(face_scores) * 0.35\n            else:\n                score += 0.5 * 0.35\n\n            # 3. AESTHETIC SCORE (20%)\n            score += self.aesthetic_scorer.predict(image) * 0.20\n\n            # 4. POSITION BONUS - middle frames (10%)\n            position = idx / len(burst_photos)\n            center_bonus = 1.0 - abs(position - 0.5) * 2\n            score += center_bonus * 0.10\n\n            # 5. EXPOSURE (5%)\n            score += self.assess_exposure(image) * 0.05\n\n            scores.append((photo_id, score))\n\n        return max(scores, key=lambda x: x[1])[0]\n\n    def assess_face_quality(self, face_dict):\n        \"\"\"Assess quality: eyes open, not blurry, smiling.\"\"\"\n        face_crop = face_dict['crop']\n        gray_face = cv2.cvtColor(np.array(face_crop), cv2.COLOR_RGB2GRAY)\n        face_sharpness = cv2.Laplacian(gray_face, cv2.CV_64F).var()\n        sharpness_score = min(1.0, face_sharpness / 500)\n\n        # Production: Use landmarks or emotion classifier\n        eyes_open_score = 0.8\n        smiling_score = 0.7\n\n        return (sharpness_score * 0.4 + eyes_open_score * 0.3 + smiling_score * 0.3)\n\n    def assess_exposure(self, image):\n        \"\"\"Check if image is properly exposed.\"\"\"\n        gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n        hist = hist.flatten() / hist.sum()\n\n        clipping = np.sum(hist[:20]) + np.sum(hist[235:])\n        return max(0, 1.0 - clipping / 0.05)\n```\n\n---\n\n## Screenshot vs Photo Classification\n\n**Signals:**\n1. EXIF metadata (camera info missing)\n2. UI elements (status bars, buttons)\n3. Text density\n4. Perfect rectangles\n5. Device-specific aspect ratios\n6. Sharpness (screenshots are perfectly sharp)\n\n```python\nclass ScreenshotDetector:\n    \"\"\"Classify image as screenshot vs photo.\"\"\"\n\n    def __init__(self):\n        self.text_detector = self.init_text_detector()\n        self.ui_detector = self.init_ui_detector()\n\n    def is_screenshot(self, image, metadata=None):\n        \"\"\"\n        Determine if image is screenshot.\n        Returns: (bool, confidence)\n        \"\"\"\n        signals = []\n\n        # SIGNAL 1: EXIF metadata\n        if metadata:\n            has_camera_info = any(k in metadata for k in\n                                ['Make', 'Model', 'LensModel', 'FocalLength'])\n            if not has_camera_info:\n                signals.append(('no_camera_exif', 0.6))\n        else:\n            signals.append(('no_metadata', 0.5))\n\n        # SIGNAL 2: UI elements\n        ui_elements = self.detect_ui_elements(image)\n        if ui_elements:\n            signals.append(('ui_elements', 0.85))\n\n        # SIGNAL 3: Text density\n        text_coverage = self.compute_text_coverage(image)\n        if text_coverage > 0.25:\n            signals.append(('high_text', 0.7))\n\n        # SIGNAL 4: Perfect rectangles\n        perfect_rects = self.detect_perfect_rectangles(image)\n        if perfect_rects > 5:\n            signals.append(('perfect_rects', 0.75))\n\n        # SIGNAL 5: Device aspect ratio\n        h, w = np.array(image).shape[:2]\n        aspect = w / h\n        device_aspects = [(16/9, 'standard'), (1125/2436, 'iphone_x'),\n                         (1080/1920, 'android_fhd'), (1440/2960, 'samsung_s8')]\n\n        for target_aspect, device_name in device_aspects:\n            if abs(aspect - target_aspect) < 0.01:\n                signals.append((f'device_aspect_{device_name}', 0.6))\n                break\n\n        # SIGNAL 6: Perfect sharpness\n        sharpness = self.compute_sharpness(image)\n        if sharpness > 2000:\n            signals.append(('perfect_sharpness', 0.5))\n\n        if not signals:\n            return False, 0.0\n\n        max_confidence = max(conf for _, conf in signals)\n        return max_confidence > 0.6, max_confidence\n\n    def detect_ui_elements(self, image):\n        \"\"\"Detect status bars, buttons, icons.\"\"\"\n        h, w = np.array(image).shape[:2]\n        top_strip = np.array(image)[:int(h * 0.05), :]\n        top_variance = np.var(top_strip)\n\n        if top_variance < 100:\n            return [{'type': 'status_bar', 'confidence': 0.8}]\n        return []\n\n    def compute_text_coverage(self, image):\n        \"\"\"Compute % of image covered by text.\"\"\"\n        import pytesseract\n        data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n\n        total_area = image.width * image.height\n        text_area = sum(\n            data['width'][i] * data['height'][i]\n            for i, conf in enumerate(data['conf']) if conf > 0\n        )\n        return text_area / total_area\n\n    def detect_perfect_rectangles(self, image):\n        \"\"\"Detect pixel-perfect rectangles (UI buttons).\"\"\"\n        gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n        edges = cv2.Canny(gray, 50, 150)\n        contours, _ = cv2.findContours(edges, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n        perfect_rects = 0\n        for contour in contours:\n            epsilon = 0.01 * cv2.arcLength(contour, True)\n            approx = cv2.approxPolyDP(contour, epsilon, True)\n            if len(approx) == 4:\n                perfect_rects += 1\n        return perfect_rects\n\n    def compute_sharpness(self, image):\n        gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n        return cv2.Laplacian(gray, cv2.CV_64F).var()\n```\n\n---\n\n## Burst Selection Weights\n\n| Criterion | Weight | Description |\n|-----------|--------|-------------|\n| Sharpness | 30% | Laplacian variance |\n| Face Quality | 35% | Eyes open, smiling, face sharpness |\n| Aesthetics | 20% | NIMA score |\n| Position | 10% | Middle frames bonus |\n| Exposure | 5% | Not over/underexposed |\n\n## Screenshot Detection Signals\n\n| Signal | Confidence | Description |\n|--------|------------|-------------|\n| UI elements | 0.85 | Status bars, buttons |\n| Perfect rectangles | 0.75 | UI buttons (4 corners, 90° angles) |\n| High text | 0.70 | >25% text coverage |\n| No camera EXIF | 0.60 | Missing Make/Model/Lens |\n| Device aspect | 0.60 | Exact phone screen ratio |\n| Perfect sharpness | 0.50 | >2000 Laplacian variance |\n"
        },
        {
          "name": "face-clustering.md",
          "type": "file",
          "path": "photo-content-recognition-curation-expert/references/face-clustering.md",
          "size": 7414,
          "content": "# Face Recognition & Clustering Reference\n\n## Overview\n\nGroup photos by person without user labeling using face detection, embedding extraction, and clustering.\n\n## Face Detection & Embedding Extraction\n\n```python\nfrom facenet_pytorch import MTCNN, InceptionResnetV1\nimport torch\n\nclass FaceEmbeddingExtractor:\n    \"\"\"Extract face embeddings using FaceNet (512-dim vectors).\"\"\"\n\n    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        self.device = device\n\n        # MTCNN for face detection\n        self.mtcnn = MTCNN(\n            image_size=160,\n            margin=0,\n            min_face_size=20,\n            device=self.device\n        )\n\n        # InceptionResnetV1 for embeddings\n        self.resnet = InceptionResnetV1(pretrained='vggface2').eval().to(self.device)\n\n    def extract_faces(self, image):\n        \"\"\"\n        Detect and extract face embeddings.\n        Returns: List of (face_crop, embedding, bounding_box) tuples\n        \"\"\"\n        boxes, probs = self.mtcnn.detect(image)\n\n        if boxes is None:\n            return []\n\n        faces = []\n        for box, prob in zip(boxes, probs):\n            if prob < 0.9:\n                continue\n\n            face_crop = self.mtcnn.extract(image, [box], save_path=None)[0]\n            face_tensor = face_crop.unsqueeze(0).to(self.device)\n\n            with torch.no_grad():\n                embedding = self.resnet(face_tensor).cpu().numpy().flatten()\n\n            faces.append({\n                'crop': face_crop,\n                'embedding': embedding,\n                'bbox': box,\n                'confidence': prob\n            })\n\n        return faces\n```\n\n---\n\n## Apple-Style Two-Pass Agglomerative Clustering\n\n**Strategy (Apple Photos 2021-2025):**\n1. Extract face + upper body embeddings\n2. Two-pass agglomerative clustering\n3. Conservative first pass (high precision)\n4. HAC second pass (increase recall)\n5. Incremental updates for new photos\n\n```python\nfrom sklearn.cluster import AgglomerativeClustering\nfrom scipy.spatial.distance import cosine\nimport numpy as np\n\nclass ApplePhotosFaceClustering:\n    \"\"\"\n    Two-pass agglomerative clustering inspired by Apple Photos.\n    Based on: \"Recognizing People in Photos Through Private On-Device ML\" (Apple ML, 2021)\n    \"\"\"\n\n    def __init__(self):\n        self.distance_threshold_pass1 = 0.4  # Conservative (high precision)\n        self.distance_threshold_pass2 = 0.6  # Relaxed (increase recall)\n\n    def cluster_faces(self, face_embeddings, photo_ids):\n        \"\"\"Cluster face embeddings into person clusters.\"\"\"\n        if len(face_embeddings) < 2:\n            return {0: list(range(len(face_embeddings)))}\n\n        embeddings = np.array(face_embeddings)\n\n        # PASS 1: Conservative clustering\n        clustering_pass1 = AgglomerativeClustering(\n            n_clusters=None,\n            distance_threshold=self.distance_threshold_pass1,\n            linkage='average',\n            metric='cosine'\n        )\n        labels_pass1 = clustering_pass1.fit_predict(embeddings)\n\n        # Compute cluster centroids from pass 1\n        unique_labels = np.unique(labels_pass1)\n        cluster_centroids = []\n        cluster_members = {}\n\n        for label in unique_labels:\n            mask = labels_pass1 == label\n            cluster_emb = embeddings[mask]\n            centroid = np.median(cluster_emb, axis=0)\n            cluster_centroids.append(centroid)\n            cluster_members[label] = np.where(mask)[0].tolist()\n\n        # PASS 2: Merge similar clusters\n        if len(cluster_centroids) > 1:\n            clustering_pass2 = AgglomerativeClustering(\n                n_clusters=None,\n                distance_threshold=self.distance_threshold_pass2,\n                linkage='average',\n                metric='cosine'\n            )\n            centroid_labels = clustering_pass2.fit_predict(np.array(cluster_centroids))\n\n            final_clusters = {}\n            for old_label, new_label in enumerate(centroid_labels):\n                if new_label not in final_clusters:\n                    final_clusters[new_label] = []\n                final_clusters[new_label].extend(cluster_members[old_label])\n        else:\n            final_clusters = cluster_members\n\n        return final_clusters\n\n    def incremental_update(self, existing_clusters, new_faces, new_embeddings):\n        \"\"\"Incrementally add new faces to existing clusters.\"\"\"\n        updated_clusters = existing_clusters.copy()\n        unassigned_faces = []\n\n        for face_idx, embedding in enumerate(new_embeddings):\n            min_distance = float('inf')\n            closest_cluster = None\n\n            for cluster_id, face_indices in existing_clusters.items():\n                cluster_embeddings = [face_embeddings[i] for i in face_indices]\n                cluster_median = np.median(cluster_embeddings, axis=0)\n                distance = cosine(embedding, cluster_median)\n\n                if distance < min_distance:\n                    min_distance = distance\n                    closest_cluster = cluster_id\n\n            if min_distance < self.distance_threshold_pass2:\n                updated_clusters[closest_cluster].append(face_idx)\n            else:\n                unassigned_faces.append(face_idx)\n\n        # Create new clusters for unassigned\n        if unassigned_faces:\n            next_cluster_id = max(updated_clusters.keys()) + 1\n            for face_idx in unassigned_faces:\n                updated_clusters[next_cluster_id] = [face_idx]\n                next_cluster_id += 1\n\n        return updated_clusters\n```\n\n---\n\n## HDBSCAN Alternative (More Robust to Noise)\n\n**Advantage:** Doesn't require distance threshold, automatically finds optimal clustering.\n\n```python\nimport hdbscan\n\nclass HDBSCANFaceClustering:\n    \"\"\"\n    HDBSCAN for face clustering.\n    More robust than agglomerative, doesn't need threshold tuning.\n    \"\"\"\n\n    def __init__(self, min_cluster_size=3, min_samples=1):\n        self.min_cluster_size = min_cluster_size\n        self.min_samples = min_samples\n\n    def cluster_faces(self, face_embeddings):\n        \"\"\"Cluster faces using HDBSCAN.\"\"\"\n        if len(face_embeddings) < self.min_cluster_size:\n            return np.zeros(len(face_embeddings), dtype=int)\n\n        embeddings = np.array(face_embeddings)\n\n        clusterer = hdbscan.HDBSCAN(\n            min_cluster_size=self.min_cluster_size,\n            min_samples=self.min_samples,\n            metric='cosine',\n            cluster_selection_method='eom'\n        )\n\n        return clusterer.fit_predict(embeddings)\n```\n\n---\n\n## Method Comparison\n\n| Method | Pros | Cons | Use When |\n|--------|------|------|----------|\n| **Agglomerative** | Fast, deterministic, Apple-proven | Needs threshold tuning | Have tuned thresholds |\n| **HDBSCAN** | Automatic, robust to noise | Slower, non-deterministic | Unknown data distribution |\n\n## Parameters\n\n**Face Detection:**\n- `min_face_size`: 20px (detect small faces)\n- `confidence_threshold`: 0.9 (high confidence only)\n\n**Clustering:**\n- `distance_threshold_pass1`: 0.4 (conservative)\n- `distance_threshold_pass2`: 0.6 (relaxed for recall)\n- `min_cluster_size`: 3 (minimum photos of same person)\n\n## References\n\n1. \"Recognizing People in Photos Through Private On-Device Machine Learning\" (Apple ML Research, 2021)\n2. FaceNet: A Unified Embedding for Face Recognition and Clustering\n3. HDBSCAN: Hierarchical Density-Based Spatial Clustering (2013-2025)\n"
        },
        {
          "name": "perceptual-hashing.md",
          "type": "file",
          "path": "photo-content-recognition-curation-expert/references/perceptual-hashing.md",
          "size": 8838,
          "content": "# Perceptual Hashing Implementation Reference\n\n## Overview\n\nPerceptual hashing generates similar hash values for visually similar images, enabling near-duplicate detection.\n\n## DINOHash (2025 State-of-the-Art)\n\n**Breakthrough:** Adversarially fine-tuned self-supervised DINOv2 features.\n\n**Advantages:**\n- Higher bit-accuracy under heavy crops\n- Robust to compression artifacts\n- Resilient to adversarial attacks\n- Outperforms classical DCT-DWT schemes and NeuralHash\n\n```python\nimport torch\nfrom transformers import AutoModel, AutoImageProcessor\n\nclass DINOHasher:\n    \"\"\"\n    DINOHash: State-of-the-art perceptual hashing using DINOv2.\n    Based on: \"DINOHash: Adversarially Fine-Tuned DINOv2 Features\" (2025)\n    \"\"\"\n\n    def __init__(self):\n        self.model = AutoModel.from_pretrained('facebook/dinov2-base')\n        self.processor = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n        self.model.eval()\n        self.hash_bits = 128\n\n    def compute_hash(self, image):\n        \"\"\"Compute perceptual hash from image.\"\"\"\n        inputs = self.processor(images=image, return_tensors=\"pt\")\n\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n            features = outputs.last_hidden_state[:, 0]  # CLS token\n\n        features_reduced = self.project_to_hash_space(features)\n        hash_binary = (features_reduced > 0).cpu().numpy().astype(np.uint8)\n        return hash_binary.flatten()\n\n    def project_to_hash_space(self, features):\n        \"\"\"Project high-dimensional features to hash space.\"\"\"\n        if not hasattr(self, 'projection_matrix'):\n            self.projection_matrix = torch.randn(\n                features.shape[-1], self.hash_bits\n            ) / np.sqrt(features.shape[-1])\n        return features @ self.projection_matrix\n\n    def hamming_distance(self, hash1, hash2):\n        \"\"\"Compute Hamming distance between two hashes.\"\"\"\n        return np.sum(hash1 != hash2)\n\n    def are_duplicates(self, hash1, hash2, threshold=5):\n        \"\"\"Check if two hashes represent near-duplicates.\"\"\"\n        return self.hamming_distance(hash1, hash2) <= threshold\n```\n\n---\n\n## Classical Perceptual Hashing\n\n### dHash (Difference Hash) - Fastest\n\n```python\nfrom PIL import Image\nimport numpy as np\n\ndef compute_dhash(image, hash_size=8):\n    \"\"\"\n    Compute dHash (Difference Hash).\n    Fast, good for exact duplicates and minor edits.\n    \"\"\"\n    image = image.convert('L')\n    image = image.resize((hash_size + 1, hash_size), Image.LANCZOS)\n    pixels = np.array(image)\n\n    diff = pixels[:, 1:] > pixels[:, :-1]\n\n    hash_value = 0\n    for bit in diff.flatten():\n        hash_value = (hash_value << 1) | int(bit)\n    return hash_value\n\n\ndef dhash_hamming_distance(hash1, hash2):\n    \"\"\"Hamming distance between two dHashes.\"\"\"\n    return bin(hash1 ^ hash2).count('1')\n```\n\n### pHash (Perceptual Hash) - More Robust\n\n```python\nimport cv2\nfrom scipy.fftpack import dct\n\ndef compute_phash(image, hash_size=8):\n    \"\"\"\n    Compute pHash using DCT.\n    Better for near-duplicates with brightness/contrast changes.\n    \"\"\"\n    image = image.convert('L')\n    image = image.resize((hash_size * 4, hash_size * 4), Image.LANCZOS)\n    pixels = np.array(image, dtype=np.float32)\n\n    dct_coeff = dct(dct(pixels.T).T)\n    dct_low = dct_coeff[:hash_size, :hash_size]\n    median = np.median(dct_low)\n    hash_binary = dct_low > median\n\n    hash_value = 0\n    for bit in hash_binary.flatten():\n        hash_value = (hash_value << 1) | int(bit)\n    return hash_value\n```\n\n---\n\n## Hybrid Duplicate Detection Pipeline\n\n**Strategy:** Use fast classical hashing for filtering, deep learning for refinement.\n\n```python\nclass HybridDuplicateDetector:\n    \"\"\"\n    Hybrid near-duplicate detection pipeline.\n    Stage 1: Fast pHash filtering (eliminates obvious non-duplicates)\n    Stage 2: DINOHash refinement (accurate near-duplicate detection)\n    Stage 3: Siamese ViT verification (final confirmation)\n    \"\"\"\n\n    def __init__(self):\n        self.phash_index = {}\n        self.dinohash_index = {}\n        self.dino_hasher = DINOHasher()\n\n    def add_photo(self, photo_id, image):\n        \"\"\"Add photo to index.\"\"\"\n        self.phash_index[photo_id] = compute_phash(image)\n        self.dinohash_index[photo_id] = self.dino_hasher.compute_hash(image)\n\n    def find_duplicates(self, aggressive=False):\n        \"\"\"Find all near-duplicate groups.\"\"\"\n        # Stage 1: Fast pHash pre-filtering\n        phash_candidates = []\n        photo_ids = list(self.phash_index.keys())\n\n        for i in range(len(photo_ids)):\n            for j in range(i + 1, len(photo_ids)):\n                id1, id2 = photo_ids[i], photo_ids[j]\n                distance = bin(self.phash_index[id1] ^ self.phash_index[id2]).count('1')\n                if distance <= (10 if aggressive else 5):\n                    phash_candidates.append((id1, id2, distance))\n\n        # Stage 2: DINOHash refinement\n        dino_duplicates = []\n        for id1, id2, phash_dist in phash_candidates:\n            dino_distance = self.dino_hasher.hamming_distance(\n                self.dinohash_index[id1], self.dinohash_index[id2]\n            )\n            if dino_distance <= (10 if aggressive else 5):\n                dino_duplicates.append((id1, id2, dino_distance))\n\n        return self.cluster_duplicates(dino_duplicates)\n\n    def cluster_duplicates(self, duplicate_pairs):\n        \"\"\"Cluster duplicate pairs into groups using union-find.\"\"\"\n        parent = {}\n\n        def find(x):\n            if x not in parent:\n                parent[x] = x\n            if parent[x] != x:\n                parent[x] = find(parent[x])\n            return parent[x]\n\n        def union(x, y):\n            root_x, root_y = find(x), find(y)\n            if root_x != root_y:\n                parent[root_x] = root_y\n\n        for id1, id2, _ in duplicate_pairs:\n            union(id1, id2)\n\n        groups = {}\n        for photo_id in set(id for pair in duplicate_pairs for id in pair[:2]):\n            root = find(photo_id)\n            groups.setdefault(root, []).append(photo_id)\n\n        return list(groups.values())\n```\n\n---\n\n## BK-Tree for Efficient Search (100K+ Photos)\n\n```python\nclass BKTree:\n    \"\"\"\n    Burkhard-Keller tree for efficient Hamming distance search.\n    Enables O(log N) average-case search for perceptual hashes.\n    \"\"\"\n\n    class Node:\n        def __init__(self, hash_value, photo_id):\n            self.hash = hash_value\n            self.photo_id = photo_id\n            self.children = {}\n\n    def __init__(self):\n        self.root = None\n\n    def insert(self, photo_id, hash_value):\n        \"\"\"Insert photo hash into tree.\"\"\"\n        if self.root is None:\n            self.root = self.Node(hash_value, photo_id)\n        else:\n            self._insert_recursive(self.root, photo_id, hash_value)\n\n    def _insert_recursive(self, node, photo_id, hash_value):\n        distance = self.hamming_distance(node.hash, hash_value)\n        if distance in node.children:\n            self._insert_recursive(node.children[distance], photo_id, hash_value)\n        else:\n            node.children[distance] = self.Node(hash_value, photo_id)\n\n    def search(self, query_hash, threshold):\n        \"\"\"Find all photos within Hamming distance threshold.\"\"\"\n        if self.root is None:\n            return []\n        return self._search_recursive(self.root, query_hash, threshold)\n\n    def _search_recursive(self, node, query_hash, threshold):\n        results = []\n        distance = self.hamming_distance(node.hash, query_hash)\n\n        if distance <= threshold:\n            results.append((node.photo_id, distance))\n\n        for child_dist in range(max(0, distance - threshold),\n                                distance + threshold + 1):\n            if child_dist in node.children:\n                results.extend(\n                    self._search_recursive(node.children[child_dist],\n                                         query_hash, threshold)\n                )\n        return results\n\n    @staticmethod\n    def hamming_distance(hash1, hash2):\n        if isinstance(hash1, np.ndarray):\n            return np.sum(hash1 != hash2)\n        return bin(hash1 ^ hash2).count('1')\n```\n\n---\n\n## Performance\n\n**O(N²) for pHash comparison, but with early termination. For 10K photos:**\n- Stage 1 (pHash): ~5 seconds\n- Stage 2 (DINOHash on candidates): ~2 seconds\n- Total: ~7 seconds for full duplicate detection\n\n## Method Comparison\n\n| Method | Speed | Robustness | Use Case |\n|--------|-------|------------|----------|\n| dHash | Fastest | Low | Exact duplicates |\n| pHash | Fast | Medium | Brightness/contrast changes |\n| DINOHash | Slower | High | Heavy crops, compression |\n| Hybrid | Medium | Very High | Production systems |\n\n## References\n\n1. \"DINOHash: Adversarially Fine-Tuned DINOv2 Features\" (2025)\n2. Neal Krawetz: dHash development\n3. DCT-based pHash algorithms\n"
        },
        {
          "name": "photo-indexing.md",
          "type": "file",
          "path": "photo-content-recognition-curation-expert/references/photo-indexing.md",
          "size": 8678,
          "content": "# Photo Indexing Pipeline Reference\n\n## Quick Indexing Before First Collage\n\n**Goal:** Efficiently index 10K+ photos before creating first collage.\n\n**Strategy:** Pipeline with caching, batching, and GPU acceleration.\n\n```python\nclass QuickPhotoIndexer:\n    \"\"\"\n    Fast photo indexing pipeline for large libraries.\n\n    Extracts:\n    - Perceptual hashes (de-duplication)\n    - Face embeddings (people clustering)\n    - CLIP embeddings (semantic search)\n    - Color palettes\n    - Aesthetic scores\n\n    Optimized for 10K photos in < 5 minutes.\n    \"\"\"\n\n    def __init__(self, cache_dir='./photo_cache'):\n        self.cache_dir = Path(cache_dir)\n        self.cache_dir.mkdir(exist_ok=True)\n\n        self.dino_hasher = DINOHasher()\n        self.face_extractor = FaceEmbeddingExtractor()\n        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n        self.aesthetic_scorer = NIMAPredictor()\n\n    def index_photo_library(self, photo_paths, batch_size=32):\n        \"\"\"Index entire photo library.\"\"\"\n        index = PhotoIndex()\n\n        # Check cache\n        cached_index = self.load_cache()\n        if cached_index:\n            print(f\"Loaded {len(cached_index)} photos from cache\")\n            index = cached_index\n\n        # Find new photos\n        new_photos = [p for p in photo_paths if p not in index.photos]\n\n        if not new_photos:\n            return index\n\n        print(f\"Indexing {len(new_photos)} new photos...\")\n\n        # Process in batches\n        for batch_start in range(0, len(new_photos), batch_size):\n            batch_paths = new_photos[batch_start:batch_start + batch_size]\n            batch_images = [Image.open(p).convert('RGB') for p in batch_paths]\n\n            # BATCHED FEATURE EXTRACTION\n            hashes = [self.dino_hasher.compute_hash(img) for img in batch_images]\n            clip_embeddings = self.extract_clip_batch(batch_images)\n            faces_batch = [self.face_extractor.extract_faces(img) for img in batch_images]\n            palettes = [extract_palette(img) for img in batch_images]\n            aesthetic_scores = self.aesthetic_scorer.predict_batch(batch_images)\n\n            # Store in index\n            for i, photo_path in enumerate(batch_paths):\n                index.add_photo(\n                    photo_id=str(photo_path),\n                    perceptual_hash=hashes[i],\n                    clip_embedding=clip_embeddings[i],\n                    faces=faces_batch[i],\n                    color_palette=palettes[i],\n                    aesthetic_score=aesthetic_scores[i]\n                )\n\n            print(f\"Indexed {batch_start + len(batch_paths)}/{len(new_photos)}\")\n\n        self.save_cache(index)\n\n        # Post-processing\n        print(\"Clustering faces...\")\n        index.cluster_faces()\n\n        print(\"Detecting duplicates...\")\n        index.detect_duplicates()\n\n        print(\"Detecting events...\")\n        index.detect_events()\n\n        return index\n\n    def extract_clip_batch(self, images):\n        \"\"\"Extract CLIP embeddings in batch (GPU-accelerated).\"\"\"\n        from transformers import CLIPProcessor\n\n        processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n        inputs = processor(images=images, return_tensors=\"pt\", padding=True)\n        inputs = {k: v.to(self.dino_hasher.device) for k, v in inputs.items()}\n\n        with torch.no_grad():\n            embeddings = self.clip_model.get_image_features(**inputs)\n\n        return embeddings.cpu().numpy()\n\n    def save_cache(self, index):\n        cache_path = self.cache_dir / 'photo_index.pkl'\n        with open(cache_path, 'wb') as f:\n            pickle.dump(index, f)\n\n    def load_cache(self):\n        cache_path = self.cache_dir / 'photo_index.pkl'\n        if cache_path.exists():\n            with open(cache_path, 'rb') as f:\n                return pickle.load(f)\n        return None\n```\n\n---\n\n## PhotoIndex Container\n\n```python\n@dataclass\nclass PhotoIndex:\n    \"\"\"Container for all photo features.\"\"\"\n\n    photos: Dict[str, Dict] = field(default_factory=dict)\n    face_clusters: Dict = field(default_factory=dict)\n    duplicate_groups: List = field(default_factory=list)\n    events: List = field(default_factory=list)\n\n    def add_photo(self, photo_id, **features):\n        self.photos[photo_id] = features\n\n    def cluster_faces(self):\n        \"\"\"Cluster all faces using HDBSCAN.\"\"\"\n        all_faces = []\n        for photo_id, features in self.photos.items():\n            for face in features.get('faces', []):\n                all_faces.append({\n                    'photo_id': photo_id,\n                    'embedding': face['embedding']\n                })\n\n        if len(all_faces) < 3:\n            return\n\n        embeddings = [f['embedding'] for f in all_faces]\n        clusterer = HDBSCANFaceClustering(min_cluster_size=3)\n        labels = clusterer.cluster_faces(embeddings)\n\n        for face, label in zip(all_faces, labels):\n            if label == -1:\n                continue\n            self.face_clusters.setdefault(label, []).append(face)\n\n    def detect_duplicates(self):\n        \"\"\"Detect duplicate photo groups.\"\"\"\n        detector = HybridDuplicateDetector()\n\n        for photo_id, features in self.photos.items():\n            detector.phash_index[photo_id] = features['perceptual_hash']\n            detector.dinohash_index[photo_id] = features['perceptual_hash']\n\n        self.duplicate_groups = detector.find_duplicates()\n\n    def detect_events(self):\n        \"\"\"Detect temporal events (requires timestamps + GPS).\"\"\"\n        # Use ST-DBSCAN from event-detection-temporal-intelligence-expert\n        pass\n```\n\n---\n\n## Complete Curation Pipeline\n\n```python\ndef curate_photos_for_collage(photo_library_path, target_count=100):\n    \"\"\"\n    Complete curation pipeline.\n\n    Steps:\n    1. Index all photos (quick indexing)\n    2. Filter inappropriate (NSFW, screenshots, mundane)\n    3. De-duplicate (keep best from each group)\n    4. Cluster by person (prioritize important people)\n    5. Detect events (prioritize significant events)\n    6. Select diverse set\n    \"\"\"\n    # 1. QUICK INDEXING\n    indexer = QuickPhotoIndexer()\n    photo_paths = list(Path(photo_library_path).glob('**/*.jpg'))\n    index = indexer.index_photo_library(photo_paths)\n\n    # 2. FILTERING\n    filtered_photos = []\n    for photo_id, features in index.photos.items():\n        if features.get('is_nsfw', False):\n            continue\n        if features.get('is_screenshot', False):\n            continue\n        if features['aesthetic_score'] < 0.3:\n            continue\n        filtered_photos.append(photo_id)\n\n    # 3. DE-DUPLICATION\n    duplicates_removed = set()\n    for dup_group in index.duplicate_groups:\n        if len(dup_group) < 2:\n            continue\n        best = max(dup_group, key=lambda pid: index.photos[pid]['aesthetic_score'])\n        for pid in dup_group:\n            if pid != best:\n                duplicates_removed.add(pid)\n\n    filtered_photos = [p for p in filtered_photos if p not in duplicates_removed]\n\n    # 4. PRIORITIZE IMPORTANT PEOPLE\n    person_importance = {}\n    for cluster_id, faces in index.face_clusters.items():\n        importance = min(1.0, len(faces) / 100)\n        person_importance[cluster_id] = importance\n\n    for photo_id in filtered_photos:\n        faces = index.photos[photo_id].get('faces', [])\n        for face in faces:\n            for cluster_id, cluster_faces in index.face_clusters.items():\n                if any(f['photo_id'] == photo_id for f in cluster_faces):\n                    boost = person_importance.get(cluster_id, 0) * 0.2\n                    index.photos[photo_id]['aesthetic_score'] += boost\n\n    # 5. EVENT-AWARE SELECTION\n    # Use event-detection-temporal-intelligence-expert\n\n    # 6. FINAL SELECTION\n    filtered_photos.sort(\n        key=lambda pid: index.photos[pid]['aesthetic_score'],\n        reverse=True\n    )\n\n    return filtered_photos[:target_count]\n```\n\n---\n\n## Performance Benchmarks\n\n**Target Performance (Swift/Metal/Core ML):**\n\n| Operation | 10K Photos |\n|-----------|-----------|\n| Perceptual hashing | < 2 minutes |\n| CLIP embeddings | < 3 minutes (GPU) |\n| Face detection | < 4 minutes |\n| Color palettes | < 1 minute |\n| Aesthetic scoring | < 2 minutes (GPU) |\n| Face clustering | < 30 seconds |\n| Duplicate detection | < 20 seconds |\n| **Total (first run)** | **~13 minutes** |\n| **Incremental updates** | **< 1 minute** |\n\n---\n\n## Integration Points\n\n- **event-detection-temporal-intelligence-expert**: Temporal event clustering\n- **color-theory-palette-harmony-expert**: Color extraction\n- **collage-layout-expert**: Photo selection for collages\n- **clip-aware-embeddings**: Semantic search and similarity\n"
        }
      ]
    },
    {
      "name": "CHANGELOG.md",
      "type": "file",
      "path": "photo-content-recognition-curation-expert/CHANGELOG.md",
      "size": 2805,
      "content": "# Changelog: photo-content-recognition-curation-expert\n\n## [2.0.0] - 2025-11-26\n\n### Major Refactoring\n- **Reduced SKILL.md from 1513 lines to 322 lines** (79% reduction)\n- Extracted detailed implementations to reference files\n- Added proper skill-coach compliant structure\n\n### Added\n- **Frontmatter**: Updated to `allowed-tools` format with integration points\n- **NOT clause**: Clear boundaries with sister skills (event-detection, color-theory, clip-aware-embeddings)\n- **Decision tree**: Quick algorithm selection guide for all recognition tasks\n- **6 Anti-patterns**: Common mistakes specific to photo curation\n  - Euclidean distance for face embeddings (use cosine)\n  - Fixed clustering thresholds\n  - Raw pixel comparison for duplicates\n  - Sequential face detection (use batching)\n  - No confidence filtering\n  - Forcing every photo into clusters\n- **Quick reference tables**: Method comparisons, parameters, performance benchmarks\n- **Integration points**: Links to event-detection, color-theory, collage-layout, clip-aware-embeddings\n\n### Reference Files Created\n- `references/perceptual-hashing.md` - Hash algorithms and duplicate detection\n  - DINOHash (2025 state-of-the-art)\n  - dHash, pHash implementations\n  - Hybrid duplicate detection pipeline\n  - BK-Tree for efficient search (100K+ photos)\n  - Method comparison table\n\n- `references/face-clustering.md` - Face recognition and clustering\n  - FaceEmbeddingExtractor (MTCNN + InceptionResnetV1)\n  - Apple-style two-pass agglomerative clustering\n  - HDBSCAN alternative\n  - Incremental updates for new photos\n  - Method comparison (Agglomerative vs HDBSCAN)\n\n- `references/content-detection.md` - Content analysis\n  - PetRecognizer (YOLO + CLIP)\n  - BurstPhotoSelector with multi-criteria scoring\n  - ScreenshotDetector with multi-signal approach\n  - Scoring weight tables\n\n- `references/photo-indexing.md` - Indexing pipeline\n  - QuickPhotoIndexer with caching\n  - PhotoIndex container class\n  - Complete curation pipeline\n  - Performance benchmarks (10K photos)\n\n### Performance Targets (Documented)\n| Operation | 10K Photos |\n|-----------|-----------|\n| Perceptual hashing | < 2 minutes |\n| CLIP embeddings | < 3 minutes (GPU) |\n| Face detection | < 4 minutes |\n| Face clustering | < 30 seconds |\n| Duplicate detection | < 20 seconds |\n| Full pipeline (first run) | ~13 minutes |\n| Incremental updates | < 1 minute |\n\n### Dependencies\n```\ntorch transformers facenet-pytorch ultralytics hdbscan opencv-python scipy numpy scikit-learn pillow pytesseract\n```\n\n## [1.0.0] - 2025-11 (Initial)\n\n### Initial Implementation\n- DINOHash perceptual hashing\n- Apple-style face clustering\n- HDBSCAN clustering alternative\n- Pet/animal recognition\n- Burst photo selection\n- Screenshot detection\n- Quick indexing pipeline\n- Complete curation workflow\n"
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "photo-content-recognition-curation-expert/SKILL.md",
      "size": 11434,
      "content": "---\nname: photo-content-recognition-curation-expert\ndescription: Expert in photo content recognition, intelligent curation, and quality filtering. Specializes in face/animal/place recognition, perceptual hashing for de-duplication, screenshot/meme detection, burst photo selection, and quick indexing strategies. Activate on \"face recognition\", \"face clustering\", \"perceptual hash\", \"near-duplicate\", \"burst photo\", \"screenshot detection\", \"photo curation\", \"photo indexing\", \"NSFW detection\", \"pet recognition\", \"DINOHash\", \"HDBSCAN faces\". NOT for GPS-based location clustering (use event-detection-temporal-intelligence-expert), color palette extraction (use color-theory-palette-harmony-expert), semantic image-text matching (use clip-aware-embeddings), or video analysis/frame extraction.\nallowed-tools:\n  - Read\n  - Write\n  - Edit\n  - Bash\n  - Grep\n  - Glob\n  - mcp__firecrawl__firecrawl_search\n  - WebFetch\nintegrates_with:\n  - event-detection-temporal-intelligence-expert  # Temporal event clustering\n  - color-theory-palette-harmony-expert           # Color extraction\n  - collage-layout-expert                         # Photo selection for collages\n  - clip-aware-embeddings                         # CLIP embeddings for DeepDBSCAN\n---\n\n# Photo Content Recognition & Curation Expert\n\nExpert in photo content analysis and intelligent curation. Combines classical computer vision with modern deep learning for comprehensive photo analysis.\n\n## When to Use This Skill\n\n✅ **Use for:**\n- Face recognition and clustering (identifying important people)\n- Animal/pet detection and clustering\n- Near-duplicate detection using perceptual hashing (DINOHash, pHash, dHash)\n- Burst photo selection (finding best frame from 10-50 shots)\n- Screenshot vs photo classification\n- Meme/download filtering\n- NSFW content detection\n- Quick indexing for large photo libraries (10K+)\n- Aesthetic quality scoring (NIMA)\n\n❌ **NOT for:**\n- GPS-based location clustering → `event-detection-temporal-intelligence-expert`\n- Color palette extraction → `color-theory-palette-harmony-expert`\n- Semantic image-text matching → `clip-aware-embeddings`\n- Video analysis or frame extraction\n\n## Quick Decision Tree\n\n```\nWhat do you need to recognize/filter?\n│\n├─ Duplicate photos? ─────────────────────────────── Perceptual Hashing\n│   ├─ Exact duplicates? ──────────────────────────── dHash (fastest)\n│   ├─ Brightness/contrast changes? ───────────────── pHash (DCT-based)\n│   ├─ Heavy crops/compression? ───────────────────── DINOHash (2025 SOTA)\n│   └─ Production system? ─────────────────────────── Hybrid (pHash → DINOHash)\n│\n├─ People in photos? ─────────────────────────────── Face Clustering\n│   ├─ Known thresholds? ──────────────────────────── Apple-style Agglomerative\n│   └─ Unknown data distribution? ─────────────────── HDBSCAN\n│\n├─ Pets/Animals? ─────────────────────────────────── Pet Recognition\n│   ├─ Detection? ─────────────────────────────────── YOLOv8\n│   └─ Individual clustering? ─────────────────────── CLIP + HDBSCAN\n│\n├─ Best from burst? ──────────────────────────────── Burst Selection\n│   └─ Score: sharpness + face quality + aesthetics\n│\n└─ Filter junk? ──────────────────────────────────── Content Detection\n    ├─ Screenshots? ───────────────────────────────── Multi-signal classifier\n    └─ NSFW? ──────────────────────────────────────── Safety classifier\n```\n\n---\n\n## Core Concepts\n\n### 1. Perceptual Hashing for Near-Duplicate Detection\n\n**Problem:** Camera bursts, re-saved images, and minor edits create near-duplicates.\n\n**Solution:** Perceptual hashes generate similar values for visually similar images.\n\n**Method Comparison:**\n\n| Method | Speed | Robustness | Best For |\n|--------|-------|------------|----------|\n| dHash | Fastest | Low | Exact duplicates |\n| pHash | Fast | Medium | Brightness/contrast changes |\n| DINOHash | Slower | High | Heavy crops, compression |\n| Hybrid | Medium | Very High | Production systems |\n\n**Hybrid Pipeline (2025 Best Practice):**\n1. **Stage 1:** Fast pHash filtering (eliminates obvious non-duplicates)\n2. **Stage 2:** DINOHash refinement (accurate detection)\n3. **Stage 3:** Optional Siamese ViT verification\n\n**Hamming Distance Thresholds:**\n- Conservative: ≤5 bits different = duplicates\n- Aggressive: ≤10 bits different = duplicates\n\n→ **Deep dive**: `references/perceptual-hashing.md`\n\n---\n\n### 2. Face Recognition & Clustering\n\n**Goal:** Group photos by person without user labeling.\n\n**Apple Photos Strategy (2021-2025):**\n1. Extract face + upper body embeddings (FaceNet, 512-dim)\n2. Two-pass agglomerative clustering\n3. Conservative first pass (threshold=0.4, high precision)\n4. HAC second pass (threshold=0.6, increase recall)\n5. Incremental updates for new photos\n\n**HDBSCAN Alternative:**\n- No threshold tuning required\n- Robust to noise\n- Better for unknown data distributions\n\n**Parameters:**\n\n| Setting | Agglomerative | HDBSCAN |\n|---------|---------------|---------|\n| Pass 1 threshold | 0.4 (cosine) | - |\n| Pass 2 threshold | 0.6 (cosine) | - |\n| Min cluster size | - | 3 photos |\n| Metric | cosine | cosine |\n\n→ **Deep dive**: `references/face-clustering.md`\n\n---\n\n### 3. Burst Photo Selection\n\n**Problem:** Burst mode creates 10-50 nearly identical photos.\n\n**Multi-Criteria Scoring:**\n\n| Criterion | Weight | Measurement |\n|-----------|--------|-------------|\n| Sharpness | 30% | Laplacian variance |\n| Face Quality | 35% | Eyes open, smiling, face sharpness |\n| Aesthetics | 20% | NIMA score |\n| Position | 10% | Middle frames bonus |\n| Exposure | 5% | Histogram clipping check |\n\n**Burst Detection:** Photos within 0.5 seconds of each other.\n\n→ **Deep dive**: `references/content-detection.md`\n\n---\n\n### 4. Screenshot Detection\n\n**Multi-Signal Approach:**\n\n| Signal | Confidence | Description |\n|--------|------------|-------------|\n| UI elements | 0.85 | Status bars, buttons detected |\n| Perfect rectangles | 0.75 | &gt;5 UI buttons (90° angles) |\n| High text | 0.70 | &gt;25% text coverage (OCR) |\n| No camera EXIF | 0.60 | Missing Make/Model/Lens |\n| Device aspect | 0.60 | Exact phone screen ratio |\n| Perfect sharpness | 0.50 | &gt;2000 Laplacian variance |\n\n**Decision:** Confidence &gt;0.6 = screenshot\n\n→ **Deep dive**: `references/content-detection.md`\n\n---\n\n### 5. Quick Indexing Pipeline\n\n**Goal:** Index 10K+ photos efficiently with caching.\n\n**Features Extracted:**\n- Perceptual hashes (de-duplication)\n- Face embeddings (people clustering)\n- CLIP embeddings (semantic search)\n- Color palettes\n- Aesthetic scores\n\n**Performance (10K photos, M1 MacBook Pro):**\n\n| Operation | Time |\n|-----------|------|\n| Perceptual hashing | 2 min |\n| CLIP embeddings | 3 min (GPU) |\n| Face detection | 4 min |\n| Color palettes | 1 min |\n| Aesthetic scoring | 2 min (GPU) |\n| Clustering + dedup | 1 min |\n| **Total (first run)** | **~13 min** |\n| **Incremental** | **&lt;1 min** |\n\n→ **Deep dive**: `references/photo-indexing.md`\n\n---\n\n## Common Anti-Patterns\n\n### Anti-Pattern: Euclidean Distance for Face Embeddings\n\n**What it looks like:**\n```python\ndistance = np.linalg.norm(embedding1 - embedding2)  # WRONG\n```\n\n**Why it's wrong:** Face embeddings are normalized; cosine similarity is the correct metric.\n\n**What to do instead:**\n```python\nfrom scipy.spatial.distance import cosine\ndistance = cosine(embedding1, embedding2)  # Correct\n```\n\n### Anti-Pattern: Fixed Clustering Thresholds\n\n**What it looks like:** Using same distance threshold for all face clusters.\n\n**Why it's wrong:** Different people have varying intra-class variance (twins vs. diverse ages).\n\n**What to do instead:** Use HDBSCAN for automatic threshold discovery, or two-pass clustering with conservative + relaxed passes.\n\n### Anti-Pattern: Raw Pixel Comparison for Duplicates\n\n**What it looks like:**\n```python\nis_duplicate = np.allclose(img1, img2)  # WRONG\n```\n\n**Why it's wrong:** Re-saved JPEGs, crops, brightness changes create pixel differences.\n\n**What to do instead:** Perceptual hashing (pHash or DINOHash) with Hamming distance.\n\n### Anti-Pattern: Sequential Face Detection\n\n**What it looks like:** Processing faces one photo at a time without batching.\n\n**Why it's wrong:** GPU underutilization, 10x slower than batched.\n\n**What to do instead:** Batch process images (batch_size=32) with GPU acceleration.\n\n### Anti-Pattern: No Confidence Filtering\n\n**What it looks like:**\n```python\nfor face in all_detected_faces:\n    cluster(face)  # No filtering\n```\n\n**Why it's wrong:** Low-confidence detections create noise clusters (hands, objects).\n\n**What to do instead:** Filter by confidence (threshold 0.9 for faces).\n\n### Anti-Pattern: Forcing Every Photo into Clusters\n\n**What it looks like:** Assigning noise points to nearest cluster.\n\n**Why it's wrong:** Solo appearances shouldn't pollute person clusters.\n\n**What to do instead:** HDBSCAN/DBSCAN naturally identifies noise (label=-1). Keep noise separate.\n\n---\n\n## Quick Start\n\n```python\nfrom photo_curation import PhotoCurationPipeline\n\npipeline = PhotoCurationPipeline()\n\n# Index photo library\nindex = pipeline.index_library('/path/to/photos')\n\n# De-duplicate\nduplicates = index.find_duplicates()\nprint(f\"Found {len(duplicates)} duplicate groups\")\n\n# Cluster faces\nface_clusters = index.cluster_faces()\nprint(f\"Found {len(face_clusters)} people\")\n\n# Select best from bursts\nbest_photos = pipeline.select_best_from_bursts(index)\n\n# Filter screenshots\nreal_photos = pipeline.filter_screenshots(index)\n\n# Curate for collage\ncollage_photos = pipeline.curate_for_collage(index, target_count=100)\n```\n\n---\n\n## Python Dependencies\n\n```\ntorch transformers facenet-pytorch ultralytics hdbscan opencv-python scipy numpy scikit-learn pillow pytesseract\n```\n\n---\n\n## Integration Points\n\n- **event-detection-temporal-intelligence-expert**: Provides temporal event clustering for event-aware curation\n- **color-theory-palette-harmony-expert**: Extracts color palettes for visual diversity\n- **collage-layout-expert**: Receives curated photos for assembly\n- **clip-aware-embeddings**: Provides CLIP embeddings for semantic search and DeepDBSCAN\n\n---\n\n## References\n\n1. **DINOHash (2025)**: \"Adversarially Fine-Tuned DINOv2 Features for Perceptual Hashing\"\n2. **Apple Photos (2021)**: \"Recognizing People in Photos Through Private On-Device ML\"\n3. **HDBSCAN**: \"Hierarchical Density-Based Spatial Clustering\" (2013-2025)\n4. **Perceptual Hashing**: dHash (Neal Krawetz), DCT-based pHash\n\n---\n\n**Version**: 2.0.0\n**Last Updated**: November 2025\n"
    }
  ]
}