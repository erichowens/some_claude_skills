{
  "name": "code-necromancer",
  "type": "folder",
  "path": "code-necromancer",
  "children": [
    {
      "name": "references",
      "type": "folder",
      "path": "code-necromancer/references",
      "children": [
        {
          "name": "archaeology-guide.md",
          "type": "file",
          "path": "code-necromancer/references/archaeology-guide.md",
          "size": 7520,
          "content": "# Code Archaeology Guide\n\nA deep dive into techniques for understanding legacy codebases.\n\n## Philosophy\n\n**\"Understand before you touch.\"**\n\nThe cardinal rule of code archaeology is to fully map and understand a system before making any changes. Premature modification leads to:\n- Breaking unknown dependencies\n- Introducing regressions in untested code\n- Missing the actual architecture (vs. assumed architecture)\n- Wasted effort on deprecated components\n\n## The Archaeological Method\n\n### 1. Surface Survey\n\nStart with the broadest possible view:\n\n```bash\n# Get organization overview\ngh repo list ORG --limit 1000 --json name,description,primaryLanguage,pushedAt,isArchived\n\n# Count repos by language\ngh repo list ORG --json primaryLanguage -q '.[].primaryLanguage.name' | sort | uniq -c\n\n# Find most recently active\ngh repo list ORG --json name,pushedAt --jq 'sort_by(.pushedAt) | reverse | .[0:5]'\n\n# Find oldest\ngh repo list ORG --json name,createdAt --jq 'sort_by(.createdAt) | .[0:5]'\n```\n\n### 2. Stratification\n\nCategorize repos into layers:\n\n**Layer 1: User-Facing**\n- Web frontends\n- Mobile apps\n- CLI tools\n- Signs: \"react\", \"vue\", \"angular\", \"flutter\", \"swift\"\n\n**Layer 2: API Gateway**\n- Main API servers\n- GraphQL endpoints\n- Signs: \"express\", \"fastapi\", \"gin\", \"/api/\", \"graphql\"\n\n**Layer 3: Services**\n- Background workers\n- Microservices\n- Signs: \"worker\", \"service\", \"processor\", \"queue\"\n\n**Layer 4: Data**\n- Database migrations\n- Data pipelines\n- Signs: \"migrations\", \"etl\", \"pipeline\", \"warehouse\"\n\n**Layer 5: Infrastructure**\n- Deployment configs\n- Terraform/CDK\n- Signs: \"infra\", \"terraform\", \"cdk\", \"k8s\", \"deploy\"\n\n**Layer 6: Libraries**\n- Shared code\n- Internal packages\n- Signs: \"@org/\", \"common\", \"shared\", \"utils\", \"lib\"\n\n### 3. Excavation\n\nDeep dive into each significant repo:\n\n#### README Analysis\n```bash\n# Check for README\nls -la README*\n\n# Extract key sections\ngrep -A 10 \"^## \" README.md\n```\n\n#### Package Manifest Analysis\n```bash\n# Node.js - Find all package.json files\nfind . -name \"package.json\" -not -path \"*/node_modules/*\"\n\n# Extract dependencies\njq '.dependencies, .devDependencies' package.json\n\n# Find internal dependencies\njq '.dependencies | keys[] | select(startswith(\"@org\"))' package.json\n\n# Python - requirements\ncat requirements*.txt\n\n# Go - modules\ncat go.mod\n```\n\n#### Configuration Discovery\n```bash\n# Find all config files\nfind . -name \"*.config.*\" -o -name \".env*\" -o -name \"*.yaml\" -o -name \"*.yml\"\n\n# Extract environment variables from code\ngrep -r \"process.env\\.\" --include=\"*.js\" --include=\"*.ts\" | \\\n  sed 's/.*process.env.\\([A-Z_]*\\).*/\\1/' | sort -u\n\n# Python env vars\ngrep -r \"os.environ\\|os.getenv\" --include=\"*.py\" | \\\n  grep -oE '\"[A-Z_]+\"' | sort -u\n```\n\n#### Database Schema Discovery\n```bash\n# Find migration files\nfind . -name \"*migration*\" -o -name \"*schema*\" -type f\n\n# Look for ORM models\ngrep -r \"class.*Model\\|@Entity\\|db.Model\" --include=\"*.py\" --include=\"*.ts\" --include=\"*.js\"\n\n# Find SQL files\nfind . -name \"*.sql\"\n```\n\n#### API Endpoint Discovery\n```bash\n# Express routes\ngrep -r \"app\\.\\(get\\|post\\|put\\|delete\\|patch\\)\" --include=\"*.js\" --include=\"*.ts\"\n\n# FastAPI/Flask routes\ngrep -r \"@app\\.\\(get\\|post\\|put\\|delete\\|route\\)\" --include=\"*.py\"\n\n# Find OpenAPI/Swagger specs\nfind . -name \"swagger*\" -o -name \"openapi*\"\n```\n\n### 4. Dating (Timeline Reconstruction)\n\n```bash\n# First commit ever\ngit log --reverse --format=\"%H %ai %s\" | head -1\n\n# Last commit\ngit log -1 --format=\"%H %ai %s\"\n\n# Commits per month over project lifetime\ngit log --format=\"%ai\" | cut -d'-' -f1,2 | sort | uniq -c\n\n# Most active contributors\ngit shortlog -sn --all\n\n# When was each file last modified?\ngit ls-tree -r --name-only HEAD | while read f; do\n  echo \"$(git log -1 --format=\"%ai\" -- \"$f\") $f\"\ndone | sort -r\n```\n\n### 5. Cross-Referencing\n\nFind how repos connect:\n\n```bash\n# Find internal package references\ngrep -r \"@org/\" --include=\"package.json\" | grep -v node_modules\n\n# Find API calls between services\ngrep -r \"http://\\|https://\\|localhost:\" --include=\"*.js\" --include=\"*.ts\" --include=\"*.py\"\n\n# Find shared database names\ngrep -r \"DATABASE_URL\\|DB_NAME\\|MONGO_URI\" --include=\"*.env*\" --include=\"*.yaml\"\n```\n\n## Pattern Recognition\n\n### Common Architecture Patterns\n\n**Monolith Signs:**\n- Single repo with /api, /frontend, /workers directories\n- Single package.json with many scripts\n- Single database connection\n\n**Microservices Signs:**\n- Multiple repos with similar structure\n- Service discovery config (consul, eureka)\n- Message queues (RabbitMQ, Kafka, SQS)\n- API gateway repo\n\n**Monorepo Signs:**\n- Lerna/Nx/Turborepo config\n- packages/ or apps/ directory\n- Shared workspace dependencies\n\n**JAMstack Signs:**\n- Static site generators (Gatsby, Next, Hugo)\n- Netlify/Vercel configs\n- Headless CMS references\n\n### Common Problems to Look For\n\n**Dependency Hell:**\n- Multiple versions of same package\n- Circular dependencies between repos\n- Pinned versions that are EOL\n\n**Configuration Sprawl:**\n- Environment variables everywhere\n- No .env.example files\n- Hardcoded values in code\n\n**Documentation Rot:**\n- README refers to non-existent files\n- Installation instructions don't work\n- API docs don't match code\n\n**Test Desert:**\n- No test files\n- Empty test directories\n- Tests that don't run\n\n## Tools of the Trade\n\n### Essential CLI Tools\n\n```bash\n# Code statistics\ncloc .                    # Count lines of code\ntokei .                   # Faster alternative\n\n# Dependency analysis\nnpm ls --all              # Node dependency tree\npipdeptree                # Python dependency tree\ngo mod graph              # Go dependency graph\n\n# Security scanning\nnpm audit                 # Node vulnerabilities\nsafety check              # Python vulnerabilities\ngovulncheck ./...         # Go vulnerabilities\n\n# Git archaeology\ngit-fame                  # Contributor statistics\ngit log --graph --oneline # Visual history\n```\n\n### Visualization Tools\n\n- **Mermaid**: For architecture diagrams\n- **D2**: For more complex diagrams\n- **Graphviz**: For dependency graphs\n- **PlantUML**: For UML diagrams\n\n### Analysis Scripts\n\nSee `scripts/` directory for automated analysis tools.\n\n## Documentation Templates\n\n### Repo Profile Template\n\n```markdown\n# [Repo Name]\n\n## Purpose\n[One sentence description]\n\n## Category\n[core|support|library|deprecated|unknown]\n\n## Tech Stack\n- Language:\n- Framework:\n- Database:\n- Runtime:\n\n## Dependencies\n- Internal: [@org/lib1, @org/lib2]\n- External critical: [express, mongoose, ...]\n\n## Environment\n- Required vars: [LIST]\n- Config files: [LIST]\n\n## Status\n- Last active: [DATE]\n- Maturity: [1-5]\n- Test coverage: [%]\n\n## Notes\n[Anything notable discovered]\n```\n\n## Anti-Patterns to Avoid\n\n1. **Premature Optimization**: Don't start fixing things before understanding the whole\n2. **Tunnel Vision**: Don't focus too long on one interesting repo\n3. **Assumption Bias**: Don't assume you know what something does—verify\n4. **Documentation Trust**: Don't trust old docs—verify against code\n5. **Recency Bias**: Old repos may be more important than recent ones\n\n## Checklist: Is Archaeology Complete?\n\n- [ ] All repos identified and cataloged\n- [ ] Primary language/framework per repo known\n- [ ] Inter-repo dependencies mapped\n- [ ] External service dependencies listed\n- [ ] Database schemas identified\n- [ ] API surfaces documented\n- [ ] Environment requirements known\n- [ ] Core vs peripheral repos identified\n- [ ] Development timeline understood\n- [ ] Missing pieces documented\n- [ ] Architecture diagram created\n"
        },
        {
          "name": "dependency-patterns.md",
          "type": "file",
          "path": "code-necromancer/references/dependency-patterns.md",
          "size": 5537,
          "content": "# Dependency Detection Patterns\n\nPatterns for identifying and analyzing dependencies across ecosystems.\n\n---\n\n## Dependency File Detection\n\n### By Ecosystem\n\n| Language | Primary File | Lock File | Additional |\n|----------|--------------|-----------|------------|\n| Node.js | package.json | package-lock.json, yarn.lock, pnpm-lock.yaml | .npmrc |\n| Python | requirements.txt | Pipfile.lock, poetry.lock | setup.py, pyproject.toml |\n| Ruby | Gemfile | Gemfile.lock | .ruby-version |\n| Go | go.mod | go.sum | vendor/ |\n| Rust | Cargo.toml | Cargo.lock | - |\n| Java | pom.xml | - | build.gradle |\n| .NET | *.csproj | packages.lock.json | nuget.config |\n| PHP | composer.json | composer.lock | - |\n\n---\n\n## Parsing Dependency Files\n\n### Node.js package.json\n```javascript\nconst pkg = require('./package.json');\n\nconst allDeps = {\n  runtime: Object.keys(pkg.dependencies || {}),\n  dev: Object.keys(pkg.devDependencies || {}),\n  peer: Object.keys(pkg.peerDependencies || {}),\n  optional: Object.keys(pkg.optionalDependencies || {}),\n};\n\n// Detect monorepo\nconst isMonorepo = pkg.workspaces !== undefined;\n\n// Detect package manager\nconst packageManager = pkg.packageManager?.split('@')[0] || 'npm';\n```\n\n### Python Requirements\n```python\nimport re\n\ndef parse_requirements(file_path):\n    deps = []\n    with open(file_path) as f:\n        for line in f:\n            line = line.strip()\n            if line and not line.startswith('#') and not line.startswith('-'):\n                # Handle version specifiers\n                match = re.match(r'^([a-zA-Z0-9_-]+)(.*)$', line)\n                if match:\n                    deps.append({\n                        'name': match.group(1),\n                        'constraint': match.group(2) or '*'\n                    })\n    return deps\n```\n\n---\n\n## Common Dependency Patterns\n\n### Framework Detection\n\n```bash\n# Node.js frameworks\ngrep -q '\"express\"' package.json && echo \"Express\"\ngrep -q '\"fastify\"' package.json && echo \"Fastify\"\ngrep -q '\"next\"' package.json && echo \"Next.js\"\ngrep -q '\"nuxt\"' package.json && echo \"Nuxt\"\ngrep -q '\"@nestjs/core\"' package.json && echo \"NestJS\"\n\n# Python frameworks\ngrep -q 'django' requirements.txt && echo \"Django\"\ngrep -q 'flask' requirements.txt && echo \"Flask\"\ngrep -q 'fastapi' requirements.txt && echo \"FastAPI\"\ngrep -q 'celery' requirements.txt && echo \"Celery (async tasks)\"\n\n# Frontend frameworks\ngrep -q '\"react\"' package.json && echo \"React\"\ngrep -q '\"vue\"' package.json && echo \"Vue\"\ngrep -q '\"@angular/core\"' package.json && echo \"Angular\"\ngrep -q '\"svelte\"' package.json && echo \"Svelte\"\n```\n\n### Database Client Detection\n\n```bash\n# SQL\ngrep -qE 'pg|postgres|sequelize' package.json && echo \"PostgreSQL\"\ngrep -qE 'mysql2?|knex' package.json && echo \"MySQL\"\ngrep -qE 'sqlite3|better-sqlite3' package.json && echo \"SQLite\"\n\n# NoSQL\ngrep -q 'mongodb\\|mongoose' package.json && echo \"MongoDB\"\ngrep -q 'redis\\|ioredis' package.json && echo \"Redis\"\ngrep -q 'elasticsearch' package.json && echo \"Elasticsearch\"\n\n# ORMs\ngrep -q 'prisma' package.json && echo \"Prisma ORM\"\ngrep -q 'typeorm' package.json && echo \"TypeORM\"\ngrep -q 'sequelize' package.json && echo \"Sequelize\"\n```\n\n### Cloud Service SDKs\n\n```bash\n# AWS\ngrep -q '@aws-sdk\\|aws-sdk' package.json && echo \"AWS SDK\"\ngrep -q 'boto3' requirements.txt && echo \"AWS (Python)\"\n\n# Google Cloud\ngrep -q '@google-cloud' package.json && echo \"Google Cloud\"\ngrep -q 'google-cloud' requirements.txt && echo \"Google Cloud (Python)\"\n\n# Azure\ngrep -q '@azure' package.json && echo \"Azure\"\ngrep -q 'azure-' requirements.txt && echo \"Azure (Python)\"\n```\n\n---\n\n## Security Vulnerability Patterns\n\n### High-Risk Indicators\n\n```bash\n# Wildcard versions (Node.js)\ngrep '\"*\"' package.json  # Any version\ngrep '\">=' package.json  # No upper bound\n\n# Very old packages\nnpm outdated --json | jq '.[] | select(.current != .wanted)'\n\n# Known vulnerable patterns\ngrep -q 'request\"' package.json && echo \"⚠️ 'request' is deprecated\"\ngrep -q 'node-uuid' package.json && echo \"⚠️ Use 'uuid' instead\"\ngrep -q 'moment\"' package.json && echo \"⚠️ Consider 'date-fns' or 'dayjs'\"\n```\n\n### Audit Commands\n\n```bash\n# Node.js\nnpm audit --json | jq '.vulnerabilities | keys[]'\n\n# Python\npip-audit --format=json\n\n# Ruby\nbundle audit check\n\n# Go\ngo list -m -json all | nancy sleuth\n```\n\n---\n\n## Dependency Graph Generation\n\n### Mermaid Output\n\n```javascript\nfunction generateDependencyMermaid(repos) {\n  const lines = ['graph LR'];\n\n  repos.forEach(repo => {\n    const deps = repo.dependencies || {};\n    Object.keys(deps).forEach(dep => {\n      // Check if dep is an internal repo\n      if (repos.some(r => r.name === dep)) {\n        lines.push(`  ${repo.name}-->${dep}`);\n      }\n    });\n  });\n\n  return lines.join('\\n');\n}\n```\n\n### Output Example\n```mermaid\ngraph LR\n  api-gateway-->auth-service\n  api-gateway-->user-service\n  user-service-->shared-utils\n  auth-service-->shared-utils\n  web-frontend-->api-gateway\n```\n\n---\n\n## Upgrade Path Analysis\n\n### Breaking Change Detection\n\n| Ecosystem | Breaking Change Indicator |\n|-----------|--------------------------|\n| npm | Major version bump (semver) |\n| Python | Check CHANGELOG, deprecation warnings |\n| Go | go.mod requires statement |\n\n### Safe Upgrade Process\n\n1. **Audit current state**: `npm audit` / `pip-audit`\n2. **Check breaking changes**: Read CHANGELOGs\n3. **Update patch versions first**: Low risk\n4. **Update minor versions**: Medium risk\n5. **Update major versions last**: High risk, plan carefully\n6. **Run tests after each step**: Catch regressions early\n"
        },
        {
          "name": "framework-detection.md",
          "type": "file",
          "path": "code-necromancer/references/framework-detection.md",
          "size": 6079,
          "content": "# Framework Detection Patterns\n\nTechniques for identifying frameworks, libraries, and tech stacks in legacy code.\n\n---\n\n## Quick Detection Matrix\n\n### Backend Frameworks\n\n| Framework | Config Files | Key Imports | Directory Structure |\n|-----------|--------------|-------------|---------------------|\n| Express | - | `require('express')` | routes/, middleware/ |\n| NestJS | nest-cli.json | `@nestjs/core` | src/modules/, src/*.controller.ts |\n| Django | manage.py, settings.py | `from django` | apps/, templates/, static/ |\n| Flask | - | `from flask` | app.py, templates/ |\n| FastAPI | - | `from fastapi` | routers/, models/ |\n| Rails | Gemfile, config/routes.rb | - | app/controllers/, app/models/ |\n| Laravel | composer.json, artisan | - | app/Http/Controllers/, routes/ |\n| Spring | pom.xml, build.gradle | `@SpringBootApplication` | src/main/java/, src/main/resources/ |\n\n### Frontend Frameworks\n\n| Framework | Config Files | Key Indicators | Build Output |\n|-----------|--------------|----------------|--------------|\n| React | - | `import React`, `useState`, JSX | build/ |\n| Next.js | next.config.js | `next/` imports | .next/ |\n| Vue | vue.config.js | `.vue` files, `createApp` | dist/ |\n| Nuxt | nuxt.config.js | `pages/`, `composables/` | .nuxt/ |\n| Angular | angular.json | `@Component`, `@NgModule` | dist/ |\n| Svelte | svelte.config.js | `.svelte` files | build/ |\n\n---\n\n## Detection Scripts\n\n### Universal Framework Detector\n\n```bash\n#!/bin/bash\n# detect-framework.sh\n\ndetect_framework() {\n  local dir=\"${1:-.}\"\n\n  # Node.js\n  if [ -f \"$dir/package.json\" ]; then\n    if grep -q '\"next\"' \"$dir/package.json\"; then echo \"Next.js\"; fi\n    if grep -q '\"nuxt\"' \"$dir/package.json\"; then echo \"Nuxt\"; fi\n    if grep -q '\"@nestjs/core\"' \"$dir/package.json\"; then echo \"NestJS\"; fi\n    if grep -q '\"express\"' \"$dir/package.json\"; then echo \"Express\"; fi\n    if grep -q '\"fastify\"' \"$dir/package.json\"; then echo \"Fastify\"; fi\n    if grep -q '\"react\"' \"$dir/package.json\"; then echo \"React\"; fi\n    if grep -q '\"vue\"' \"$dir/package.json\"; then echo \"Vue\"; fi\n    if grep -q '\"@angular/core\"' \"$dir/package.json\"; then echo \"Angular\"; fi\n    if grep -q '\"svelte\"' \"$dir/package.json\"; then echo \"Svelte\"; fi\n  fi\n\n  # Python\n  if [ -f \"$dir/requirements.txt\" ] || [ -f \"$dir/pyproject.toml\" ]; then\n    if grep -qi 'django' \"$dir/requirements.txt\" 2>/dev/null; then echo \"Django\"; fi\n    if grep -qi 'flask' \"$dir/requirements.txt\" 2>/dev/null; then echo \"Flask\"; fi\n    if grep -qi 'fastapi' \"$dir/requirements.txt\" 2>/dev/null; then echo \"FastAPI\"; fi\n  fi\n\n  # Ruby\n  if [ -f \"$dir/Gemfile\" ]; then\n    if grep -q 'rails' \"$dir/Gemfile\"; then echo \"Rails\"; fi\n    if grep -q 'sinatra' \"$dir/Gemfile\"; then echo \"Sinatra\"; fi\n  fi\n\n  # PHP\n  if [ -f \"$dir/composer.json\" ]; then\n    if grep -q 'laravel' \"$dir/composer.json\"; then echo \"Laravel\"; fi\n    if grep -q 'symfony' \"$dir/composer.json\"; then echo \"Symfony\"; fi\n  fi\n\n  # Java\n  if [ -f \"$dir/pom.xml\" ]; then\n    if grep -q 'spring-boot' \"$dir/pom.xml\"; then echo \"Spring Boot\"; fi\n  fi\n\n  # Go\n  if [ -f \"$dir/go.mod\" ]; then\n    if grep -q 'gin-gonic' \"$dir/go.mod\"; then echo \"Gin\"; fi\n    if grep -q 'echo' \"$dir/go.mod\"; then echo \"Echo\"; fi\n    if grep -q 'fiber' \"$dir/go.mod\"; then echo \"Fiber\"; fi\n  fi\n}\n```\n\n---\n\n## Version Detection\n\n### Node.js / npm\n```bash\n# Get exact versions from lockfile\njq '.packages | to_entries[] | select(.key | contains(\"express\")) | .value.version' package-lock.json\n\n# From package.json (may be range)\njq '.dependencies.express' package.json\n```\n\n### Python\n```bash\n# From frozen requirements\ngrep -i 'django' requirements.txt  # django==4.2.1\n\n# From installed packages\npip show django | grep Version\n```\n\n### Runtime Version Detection\n```bash\n# Node.js version requirements\njq '.engines.node' package.json  # \">=18.0.0\"\n\n# Python version\ngrep 'python_requires' setup.py\ngrep 'requires-python' pyproject.toml\n\n# .nvmrc / .python-version files\ncat .nvmrc .python-version .ruby-version 2>/dev/null\n```\n\n---\n\n## Build System Detection\n\n| Build Tool | Config File | Ecosystem |\n|------------|-------------|-----------|\n| webpack | webpack.config.js | Node.js |\n| Vite | vite.config.js | Node.js |\n| esbuild | esbuild.config.js | Node.js |\n| Rollup | rollup.config.js | Node.js |\n| Parcel | package.json (source field) | Node.js |\n| Gradle | build.gradle | JVM |\n| Maven | pom.xml | JVM |\n| Make | Makefile | Universal |\n| CMake | CMakeLists.txt | C/C++ |\n| Cargo | Cargo.toml | Rust |\n\n---\n\n## ORM / Database Layer Detection\n\n```bash\n# Sequelize (Node.js)\nfind . -name \"*.js\" -exec grep -l \"sequelize\" {} \\;\nls models/*.js  # Sequelize convention\n\n# TypeORM\nfind . -name \"*.entity.ts\"  # TypeORM entity files\ngrep -r \"@Entity\" --include=\"*.ts\"\n\n# Prisma\nls prisma/schema.prisma\n\n# Django ORM\nfind . -name \"models.py\"\ngrep -r \"models.Model\" --include=\"*.py\"\n\n# SQLAlchemy\ngrep -r \"from sqlalchemy\" --include=\"*.py\"\n\n# ActiveRecord (Rails)\nls app/models/*.rb\n```\n\n---\n\n## Test Framework Detection\n\n| Framework | Config File | Test Pattern |\n|-----------|-------------|--------------|\n| Jest | jest.config.js | *.test.js, *.spec.js |\n| Mocha | .mocharc.js | test/*.js |\n| Vitest | vite.config.js (test) | *.test.ts |\n| pytest | pytest.ini, pyproject.toml | test_*.py |\n| unittest | - | test*.py |\n| RSpec | .rspec | *_spec.rb |\n| JUnit | - | *Test.java |\n| Go test | - | *_test.go |\n\n---\n\n## CI/CD Detection\n\n| Platform | Config File |\n|----------|-------------|\n| GitHub Actions | .github/workflows/*.yml |\n| GitLab CI | .gitlab-ci.yml |\n| CircleCI | .circleci/config.yml |\n| Travis CI | .travis.yml |\n| Jenkins | Jenkinsfile |\n| Azure Pipelines | azure-pipelines.yml |\n| Bitbucket Pipelines | bitbucket-pipelines.yml |\n\n---\n\n## Container / Infrastructure Detection\n\n```bash\n# Docker\nls Dockerfile docker-compose*.yml\n\n# Kubernetes\nfind . -name \"*.yaml\" -path \"*k8s*\" -o -name \"*.yaml\" -path \"*kubernetes*\"\nls helm/\n\n# Terraform\nfind . -name \"*.tf\"\n\n# Serverless\nls serverless.yml serverless.ts\n\n# Cloud Formation\nfind . -name \"*cloudformation*.yml\" -o -name \"*cfn*.yml\"\n```\n"
        },
        {
          "name": "infrastructure-mapping.md",
          "type": "file",
          "path": "code-necromancer/references/infrastructure-mapping.md",
          "size": 5796,
          "content": "# Infrastructure Mapping\n\nTechniques for discovering cloud resources, services, and deployment topology.\n\n---\n\n## Cloud Resource Discovery\n\n### AWS\n\n```bash\n# List all resources (requires AWS CLI + credentials)\naws resourcegroupstaggingapi get-resources --output json > aws-resources.json\n\n# Common services audit\naws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,State.Name,Tags]'\naws rds describe-db-instances --query 'DBInstances[*].[DBInstanceIdentifier,Engine,DBInstanceStatus]'\naws s3 ls\naws lambda list-functions --query 'Functions[*].[FunctionName,Runtime,LastModified]'\naws ecs list-clusters\naws eks list-clusters\n\n# IAM users/roles (security audit)\naws iam list-users\naws iam list-roles --query 'Roles[*].RoleName'\n```\n\n### GCP\n\n```bash\n# List all projects\ngcloud projects list\n\n# Compute resources\ngcloud compute instances list\ngcloud container clusters list\ngcloud run services list\ngcloud functions list\n\n# Storage\ngcloud storage ls\n\n# SQL\ngcloud sql instances list\n```\n\n### Azure\n\n```bash\n# List all resources\naz resource list --output table\n\n# Specific services\naz vm list\naz webapp list\naz sql server list\naz storage account list\naz aks list\n```\n\n---\n\n## Infrastructure-as-Code Analysis\n\n### Terraform State Parsing\n\n```bash\n# List all resources in state\nterraform state list\n\n# Show specific resource\nterraform state show aws_instance.web\n\n# Export as JSON\nterraform show -json > tf-state.json\n\n# Parse for dependencies\njq '.values.root_module.resources[] | {type: .type, name: .name}' tf-state.json\n```\n\n### CloudFormation Analysis\n\n```bash\n# List stacks\naws cloudformation list-stacks --stack-status-filter CREATE_COMPLETE UPDATE_COMPLETE\n\n# Get stack resources\naws cloudformation list-stack-resources --stack-name MyStack\n\n# Export template\naws cloudformation get-template --stack-name MyStack > template.json\n```\n\n---\n\n## Docker Compose Topology\n\n```yaml\n# Example docker-compose.yml analysis\n# Services = boxes, depends_on = arrows\n\n# Extract service map:\nservices:\n  api:\n    depends_on: [db, redis]\n  worker:\n    depends_on: [db, redis, rabbitmq]\n  web:\n    depends_on: [api]\n```\n\n### Generate Mermaid from Docker Compose\n\n```python\nimport yaml\n\ndef compose_to_mermaid(compose_file):\n    with open(compose_file) as f:\n        data = yaml.safe_load(f)\n\n    lines = ['graph TD']\n    services = data.get('services', {})\n\n    for name, config in services.items():\n        deps = config.get('depends_on', [])\n        if isinstance(deps, dict):\n            deps = list(deps.keys())\n\n        for dep in deps:\n            lines.append(f'  {name}-->{dep}')\n\n    return '\\n'.join(lines)\n```\n\n---\n\n## Network Topology Discovery\n\n### From Code\n\n```bash\n# Find hardcoded URLs/IPs\ngrep -rn 'http://\\|https://\\|localhost\\|127\\.0\\.0\\.1' --include=\"*.js\" --include=\"*.py\"\n\n# Find port references\ngrep -rn ':[0-9]\\{4,5\\}' --include=\"*.js\" --include=\"*.py\" --include=\"*.yml\"\n\n# DNS references\ngrep -rn '\\.com\\|\\.io\\|\\.internal' --include=\"*.js\" --include=\"*.py\" --include=\"*.env*\"\n```\n\n### From Configuration\n\n```bash\n# Docker Compose ports\ngrep -A5 'ports:' docker-compose.yml\n\n# Kubernetes services\ngrep -A10 'kind: Service' k8s/*.yml\n\n# Environment files\ngrep -i 'HOST\\|URL\\|ENDPOINT' .env.example\n```\n\n---\n\n## Service Dependency Map Template\n\n```mermaid\ngraph TB\n    subgraph \"External\"\n        EXT_API[Third-party API]\n        EXT_AUTH[Auth Provider]\n        CDN[CDN]\n    end\n\n    subgraph \"Frontend\"\n        WEB[Web App]\n        MOBILE[Mobile App]\n    end\n\n    subgraph \"Backend\"\n        API[API Gateway]\n        AUTH[Auth Service]\n        CORE[Core Service]\n        WORKER[Background Worker]\n    end\n\n    subgraph \"Data\"\n        PG[(PostgreSQL)]\n        REDIS[(Redis)]\n        S3[(S3)]\n        MQ[Message Queue]\n    end\n\n    WEB --> CDN\n    WEB --> API\n    MOBILE --> API\n\n    API --> AUTH\n    API --> CORE\n    AUTH --> PG\n    AUTH --> REDIS\n    AUTH --> EXT_AUTH\n\n    CORE --> PG\n    CORE --> REDIS\n    CORE --> S3\n    CORE --> MQ\n    CORE --> EXT_API\n\n    MQ --> WORKER\n    WORKER --> PG\n```\n\n---\n\n## Resource Inventory Template\n\n```json\n{\n  \"metadata\": {\n    \"project\": \"legacy-app\",\n    \"discovered\": \"2024-01-15\",\n    \"clouds\": [\"aws\"]\n  },\n  \"compute\": [\n    {\n      \"type\": \"ec2\",\n      \"id\": \"i-0abc123\",\n      \"name\": \"api-server\",\n      \"state\": \"running\",\n      \"size\": \"t3.medium\",\n      \"region\": \"us-east-1\"\n    }\n  ],\n  \"databases\": [\n    {\n      \"type\": \"rds\",\n      \"engine\": \"postgres\",\n      \"id\": \"prod-db\",\n      \"size\": \"db.t3.medium\",\n      \"storage_gb\": 100\n    }\n  ],\n  \"storage\": [\n    {\n      \"type\": \"s3\",\n      \"name\": \"app-uploads\",\n      \"region\": \"us-east-1\",\n      \"public\": false\n    }\n  ],\n  \"networking\": {\n    \"vpc_id\": \"vpc-abc123\",\n    \"load_balancers\": [\"app-lb\"],\n    \"dns\": [\"api.example.com\"]\n  },\n  \"secrets\": {\n    \"manager\": \"aws-secrets-manager\",\n    \"keys\": [\"db-password\", \"api-key-stripe\"]\n  },\n  \"estimated_monthly_cost\": \"$250\"\n}\n```\n\n---\n\n## Health Check Discovery\n\n```bash\n# Find health endpoints in code\ngrep -rn 'health\\|ready\\|alive' --include=\"*.js\" --include=\"*.py\" --include=\"*.yml\"\n\n# Common patterns\n/health\n/healthz\n/ready\n/readiness\n/liveness\n/api/health\n/api/v1/health\n\n# Kubernetes probes\ngrep -A5 'livenessProbe\\|readinessProbe' k8s/*.yml\n```\n\n---\n\n## Cost Analysis\n\n### Quick AWS Cost Check\n\n```bash\n# Get last month's cost by service\naws ce get-cost-and-usage \\\n  --time-period Start=2024-01-01,End=2024-01-31 \\\n  --granularity MONTHLY \\\n  --metrics UnblendedCost \\\n  --group-by Type=DIMENSION,Key=SERVICE \\\n  --query 'ResultsByTime[0].Groups[*].[Keys[0],Metrics.UnblendedCost.Amount]' \\\n  --output table\n```\n\n### Resource Tagging Audit\n\n```bash\n# Find untagged resources\naws resourcegroupstaggingapi get-resources \\\n  --query 'ResourceTagMappingList[?length(Tags)==`0`].ResourceARN'\n```\n"
        },
        {
          "name": "integration-test-patterns.md",
          "type": "file",
          "path": "code-necromancer/references/integration-test-patterns.md",
          "size": 8067,
          "content": "# Integration Test Patterns\n\nResurrection tests to verify legacy systems are working.\n\n---\n\n## Test Categories\n\n### 1. Service Startup Tests\nVerify each service can start without crashing.\n\n```javascript\n// Node.js service startup test\ndescribe('Service Startup', () => {\n  let server;\n\n  beforeAll(async () => {\n    // Set test environment\n    process.env.NODE_ENV = 'test';\n    process.env.DATABASE_URL = 'postgresql://localhost:5432/test_db';\n\n    // Import and start\n    const app = require('../src/app');\n    server = app.listen(0);  // Random port\n  });\n\n  afterAll(async () => {\n    await server.close();\n  });\n\n  test('server starts without error', () => {\n    expect(server.listening).toBe(true);\n  });\n\n  test('health endpoint responds', async () => {\n    const response = await fetch(`http://localhost:${server.address().port}/health`);\n    expect(response.status).toBe(200);\n  });\n});\n```\n\n### 2. Database Connection Tests\n\n```javascript\ndescribe('Database Connection', () => {\n  test('can connect to database', async () => {\n    const { Pool } = require('pg');\n    const pool = new Pool({ connectionString: process.env.DATABASE_URL });\n\n    const result = await pool.query('SELECT NOW()');\n    expect(result.rows).toHaveLength(1);\n\n    await pool.end();\n  });\n\n  test('migrations are applied', async () => {\n    const result = await pool.query(`\n      SELECT table_name FROM information_schema.tables\n      WHERE table_schema = 'public'\n    `);\n\n    expect(result.rows.map(r => r.table_name)).toContain('users');\n    expect(result.rows.map(r => r.table_name)).toContain('orders');\n  });\n});\n```\n\n### 3. Inter-Service Communication Tests\n\n```javascript\ndescribe('Service Communication', () => {\n  test('API can reach Auth service', async () => {\n    const response = await fetch(`${process.env.AUTH_SERVICE_URL}/health`);\n    expect(response.ok).toBe(true);\n  });\n\n  test('API can authenticate with Auth service', async () => {\n    const response = await fetch(`${process.env.AUTH_SERVICE_URL}/verify`, {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({ token: 'test-token' })\n    });\n\n    // Even if invalid token, should get structured response\n    expect(response.status).toBeLessThan(500);\n  });\n\n  test('Worker can connect to message queue', async () => {\n    const amqp = require('amqplib');\n    const conn = await amqp.connect(process.env.RABBITMQ_URL);\n    const channel = await conn.createChannel();\n\n    // Verify queue exists\n    const queue = await channel.checkQueue('tasks');\n    expect(queue).toBeDefined();\n\n    await conn.close();\n  });\n});\n```\n\n---\n\n## Critical Path Tests\n\n### User Authentication Flow\n\n```javascript\ndescribe('Authentication Flow', () => {\n  test('can register new user', async () => {\n    const response = await api.post('/auth/register', {\n      email: `test-${Date.now()}@example.com`,\n      password: 'SecurePassword123!'\n    });\n\n    expect(response.status).toBe(201);\n    expect(response.data.token).toBeDefined();\n  });\n\n  test('can login existing user', async () => {\n    const response = await api.post('/auth/login', {\n      email: 'existing@example.com',\n      password: 'KnownPassword123!'\n    });\n\n    expect(response.status).toBe(200);\n    expect(response.data.token).toBeDefined();\n  });\n\n  test('can access protected route with token', async () => {\n    const loginResponse = await api.post('/auth/login', {\n      email: 'existing@example.com',\n      password: 'KnownPassword123!'\n    });\n\n    const token = loginResponse.data.token;\n\n    const response = await api.get('/api/me', {\n      headers: { Authorization: `Bearer ${token}` }\n    });\n\n    expect(response.status).toBe(200);\n    expect(response.data.email).toBe('existing@example.com');\n  });\n});\n```\n\n### E-commerce Order Flow\n\n```javascript\ndescribe('Order Flow', () => {\n  let authToken;\n\n  beforeAll(async () => {\n    const login = await api.post('/auth/login', {\n      email: 'test@example.com',\n      password: 'password'\n    });\n    authToken = login.data.token;\n  });\n\n  test('can view products', async () => {\n    const response = await api.get('/api/products');\n    expect(response.status).toBe(200);\n    expect(response.data.length).toBeGreaterThan(0);\n  });\n\n  test('can add to cart', async () => {\n    const response = await api.post('/api/cart', {\n      productId: 1,\n      quantity: 2\n    }, {\n      headers: { Authorization: `Bearer ${authToken}` }\n    });\n\n    expect(response.status).toBe(200);\n  });\n\n  test('can checkout', async () => {\n    const response = await api.post('/api/checkout', {\n      paymentMethod: 'test',\n      shippingAddress: {\n        street: '123 Test St',\n        city: 'Test City',\n        zip: '12345'\n      }\n    }, {\n      headers: { Authorization: `Bearer ${authToken}` }\n    });\n\n    expect(response.status).toBe(201);\n    expect(response.data.orderId).toBeDefined();\n  });\n});\n```\n\n---\n\n## External Service Tests\n\n### Third-Party API Connectivity\n\n```javascript\ndescribe('External Services', () => {\n  test('Stripe API is reachable', async () => {\n    const stripe = require('stripe')(process.env.STRIPE_SECRET_KEY);\n\n    // Simple API call to verify credentials\n    const balance = await stripe.balance.retrieve();\n    expect(balance.object).toBe('balance');\n  });\n\n  test('SendGrid API is reachable', async () => {\n    const sgMail = require('@sendgrid/mail');\n    sgMail.setApiKey(process.env.SENDGRID_API_KEY);\n\n    // Don't actually send, just verify client\n    expect(sgMail).toBeDefined();\n  });\n\n  test('AWS S3 bucket is accessible', async () => {\n    const { S3Client, HeadBucketCommand } = require('@aws-sdk/client-s3');\n    const s3 = new S3Client({ region: process.env.AWS_REGION });\n\n    const command = new HeadBucketCommand({ Bucket: process.env.S3_BUCKET });\n    const response = await s3.send(command);\n\n    expect(response.$metadata.httpStatusCode).toBe(200);\n  });\n});\n```\n\n---\n\n## Smoke Test Checklist\n\n```markdown\n## Resurrection Smoke Tests\n\n### Infrastructure\n- [ ] All databases reachable\n- [ ] All caches (Redis) reachable\n- [ ] Message queues accessible\n- [ ] S3/storage buckets accessible\n- [ ] DNS resolves correctly\n\n### Services\n- [ ] API gateway starts\n- [ ] Auth service starts\n- [ ] Core service starts\n- [ ] Background workers start\n- [ ] All health endpoints respond\n\n### Communication\n- [ ] Frontend can reach API\n- [ ] API can reach auth service\n- [ ] API can reach core service\n- [ ] Services can reach databases\n- [ ] Workers can receive from queue\n\n### Critical Paths\n- [ ] User can register\n- [ ] User can login\n- [ ] User can view main page\n- [ ] User can perform primary action\n- [ ] Admin can access admin panel\n\n### External\n- [ ] Payment provider reachable\n- [ ] Email service reachable\n- [ ] CDN working\n- [ ] OAuth providers working\n```\n\n---\n\n## Docker Compose Test Environment\n\n```yaml\nversion: '3.8'\n\nservices:\n  test-db:\n    image: postgres:14\n    environment:\n      POSTGRES_DB: test_db\n      POSTGRES_USER: test\n      POSTGRES_PASSWORD: test\n    ports:\n      - \"5433:5432\"\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U test\"]\n      interval: 5s\n      timeout: 5s\n      retries: 5\n\n  test-redis:\n    image: redis:7\n    ports:\n      - \"6380:6379\"\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 5s\n      timeout: 5s\n      retries: 5\n\n  test-rabbitmq:\n    image: rabbitmq:3-management\n    ports:\n      - \"5673:5672\"\n      - \"15673:15672\"\n    healthcheck:\n      test: [\"CMD\", \"rabbitmq-diagnostics\", \"check_running\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n\n  integration-tests:\n    build:\n      context: .\n      dockerfile: Dockerfile.test\n    depends_on:\n      test-db:\n        condition: service_healthy\n      test-redis:\n        condition: service_healthy\n      test-rabbitmq:\n        condition: service_healthy\n    environment:\n      DATABASE_URL: postgresql://test:test@test-db:5432/test_db\n      REDIS_URL: redis://test-redis:6379\n      RABBITMQ_URL: amqp://guest:guest@test-rabbitmq:5672\n    command: npm run test:integration\n```\n"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "folder",
      "path": "code-necromancer/scripts",
      "children": [
        {
          "name": "analyze-repo.sh",
          "type": "file",
          "path": "code-necromancer/scripts/analyze-repo.sh",
          "size": 12411,
          "content": "#!/bin/bash\n\n# Code Necromancer - Single Repository Analyzer\n# Deep analysis of a single repository\n\nset -e\n\n# Colors\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nYELLOW='\\033[1;33m'\nBLUE='\\033[0;34m'\nCYAN='\\033[0;36m'\nNC='\\033[0m'\n\nusage() {\n    echo \"Usage: $0 <repo-path> [output-file]\"\n    echo \"\"\n    echo \"Performs deep analysis of a repository\"\n    echo \"\"\n    echo \"Arguments:\"\n    echo \"  repo-path   Path to cloned repository\"\n    echo \"  output-file Output JSON file (default: repo-analysis.json)\"\n}\n\nanalyze_repo() {\n    local REPO_PATH=$1\n    local OUTPUT_FILE=${2:-\"repo-analysis.json\"}\n\n    if [ ! -d \"$REPO_PATH\" ]; then\n        echo -e \"${RED}Error: Directory not found: $REPO_PATH${NC}\"\n        exit 1\n    fi\n\n    cd \"$REPO_PATH\"\n    local REPO_NAME=$(basename \"$REPO_PATH\")\n\n    echo -e \"${BLUE}═══════════════════════════════════════════════════════════${NC}\"\n    echo -e \"${BLUE}  Analyzing: $REPO_NAME${NC}\"\n    echo -e \"${BLUE}═══════════════════════════════════════════════════════════${NC}\"\n    echo \"\"\n\n    # Initialize JSON output\n    echo \"{\" > \"$OUTPUT_FILE\"\n    echo \"  \\\"name\\\": \\\"$REPO_NAME\\\",\" >> \"$OUTPUT_FILE\"\n    echo \"  \\\"analyzed_at\\\": \\\"$(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\\\",\" >> \"$OUTPUT_FILE\"\n\n    # Git info\n    echo -e \"${CYAN}[1/10] Git History...${NC}\"\n    if [ -d \".git\" ]; then\n        FIRST_COMMIT=$(git log --reverse --format=\"%ai\" 2>/dev/null | head -1 || echo \"unknown\")\n        LAST_COMMIT=$(git log -1 --format=\"%ai\" 2>/dev/null || echo \"unknown\")\n        TOTAL_COMMITS=$(git rev-list --count HEAD 2>/dev/null || echo \"0\")\n        CONTRIBUTORS=$(git shortlog -sn --all 2>/dev/null | wc -l | tr -d ' ')\n        BRANCHES=$(git branch -a 2>/dev/null | wc -l | tr -d ' ')\n\n        echo \"  \\\"git\\\": {\" >> \"$OUTPUT_FILE\"\n        echo \"    \\\"first_commit\\\": \\\"$FIRST_COMMIT\\\",\" >> \"$OUTPUT_FILE\"\n        echo \"    \\\"last_commit\\\": \\\"$LAST_COMMIT\\\",\" >> \"$OUTPUT_FILE\"\n        echo \"    \\\"total_commits\\\": $TOTAL_COMMITS,\" >> \"$OUTPUT_FILE\"\n        echo \"    \\\"contributors\\\": $CONTRIBUTORS,\" >> \"$OUTPUT_FILE\"\n        echo \"    \\\"branches\\\": $BRANCHES\" >> \"$OUTPUT_FILE\"\n        echo \"  },\" >> \"$OUTPUT_FILE\"\n\n        echo \"    First commit: $FIRST_COMMIT\"\n        echo \"    Last commit: $LAST_COMMIT\"\n        echo \"    Total commits: $TOTAL_COMMITS\"\n    else\n        echo \"  \\\"git\\\": null,\" >> \"$OUTPUT_FILE\"\n        echo \"    Not a git repository\"\n    fi\n\n    # File structure\n    echo -e \"${CYAN}[2/10] File Structure...${NC}\"\n    TOTAL_FILES=$(find . -type f -not -path \"*/node_modules/*\" -not -path \"*/.git/*\" -not -path \"*/venv/*\" -not -path \"*/__pycache__/*\" 2>/dev/null | wc -l | tr -d ' ')\n    echo \"  \\\"total_files\\\": $TOTAL_FILES,\" >> \"$OUTPUT_FILE\"\n    echo \"    Files (excl. node_modules, .git): $TOTAL_FILES\"\n\n    # Language detection\n    echo -e \"${CYAN}[3/10] Languages...${NC}\"\n    echo \"  \\\"languages\\\": {\" >> \"$OUTPUT_FILE\"\n    FIRST=true\n    for ext in js ts jsx tsx py go java rb rs php; do\n        COUNT=$(find . -name \"*.$ext\" -not -path \"*/node_modules/*\" -not -path \"*/.git/*\" 2>/dev/null | wc -l | tr -d ' ')\n        if [ \"$COUNT\" -gt 0 ]; then\n            if [ \"$FIRST\" = true ]; then\n                FIRST=false\n            else\n                echo \",\" >> \"$OUTPUT_FILE\"\n            fi\n            echo -n \"    \\\"$ext\\\": $COUNT\" >> \"$OUTPUT_FILE\"\n            echo \"    $ext: $COUNT files\"\n        fi\n    done\n    echo \"\" >> \"$OUTPUT_FILE\"\n    echo \"  },\" >> \"$OUTPUT_FILE\"\n\n    # Framework detection\n    echo -e \"${CYAN}[4/10] Framework Detection...${NC}\"\n    FRAMEWORKS=\"\"\n\n    # Node.js / JavaScript\n    if [ -f \"package.json\" ]; then\n        if grep -q '\"react\"' package.json 2>/dev/null; then FRAMEWORKS=\"$FRAMEWORKS React\"; fi\n        if grep -q '\"vue\"' package.json 2>/dev/null; then FRAMEWORKS=\"$FRAMEWORKS Vue\"; fi\n        if grep -q '\"angular\"' package.json 2>/dev/null; then FRAMEWORKS=\"$FRAMEWORKS Angular\"; fi\n        if grep -q '\"express\"' package.json 2>/dev/null; then FRAMEWORKS=\"$FRAMEWORKS Express\"; fi\n        if grep -q '\"next\"' package.json 2>/dev/null; then FRAMEWORKS=\"$FRAMEWORKS Next.js\"; fi\n        if grep -q '\"nest\"' package.json 2>/dev/null; then FRAMEWORKS=\"$FRAMEWORKS NestJS\"; fi\n        if grep -q '\"electron\"' package.json 2>/dev/null; then FRAMEWORKS=\"$FRAMEWORKS Electron\"; fi\n    fi\n\n    # Python\n    if [ -f \"requirements.txt\" ] || [ -f \"setup.py\" ] || [ -f \"pyproject.toml\" ]; then\n        if grep -qE \"django|Django\" requirements.txt setup.py pyproject.toml 2>/dev/null; then FRAMEWORKS=\"$FRAMEWORKS Django\"; fi\n        if grep -qE \"flask|Flask\" requirements.txt setup.py pyproject.toml 2>/dev/null; then FRAMEWORKS=\"$FRAMEWORKS Flask\"; fi\n        if grep -qE \"fastapi|FastAPI\" requirements.txt setup.py pyproject.toml 2>/dev/null; then FRAMEWORKS=\"$FRAMEWORKS FastAPI\"; fi\n    fi\n\n    # Go\n    if [ -f \"go.mod\" ]; then\n        if grep -q \"gin-gonic\" go.mod 2>/dev/null; then FRAMEWORKS=\"$FRAMEWORKS Gin\"; fi\n        if grep -q \"echo\" go.mod 2>/dev/null; then FRAMEWORKS=\"$FRAMEWORKS Echo\"; fi\n    fi\n\n    FRAMEWORKS=$(echo \"$FRAMEWORKS\" | xargs)  # Trim\n    echo \"  \\\"frameworks\\\": \\\"$FRAMEWORKS\\\",\" >> \"$OUTPUT_FILE\"\n    echo \"    Detected: ${FRAMEWORKS:-None detected}\"\n\n    # Configuration files\n    echo -e \"${CYAN}[5/10] Configuration Files...${NC}\"\n    echo \"  \\\"config_files\\\": [\" >> \"$OUTPUT_FILE\"\n    FIRST=true\n    for cf in package.json tsconfig.json webpack.config.js vite.config.js rollup.config.js \\\n              requirements.txt setup.py pyproject.toml setup.cfg \\\n              go.mod Cargo.toml Gemfile pom.xml build.gradle \\\n              Dockerfile docker-compose.yml docker-compose.yaml \\\n              .env.example .env.sample \\\n              .github/workflows .travis.yml .circleci jenkins* \\\n              terraform*.tf serverless.yml sam.yaml; do\n        if [ -e \"$cf\" ]; then\n            if [ \"$FIRST\" = true ]; then\n                FIRST=false\n            else\n                echo \",\" >> \"$OUTPUT_FILE\"\n            fi\n            echo -n \"    \\\"$cf\\\"\" >> \"$OUTPUT_FILE\"\n            echo \"    Found: $cf\"\n        fi\n    done\n    echo \"\" >> \"$OUTPUT_FILE\"\n    echo \"  ],\" >> \"$OUTPUT_FILE\"\n\n    # Environment variables\n    echo -e \"${CYAN}[6/10] Environment Variables...${NC}\"\n    echo \"  \\\"env_vars\\\": [\" >> \"$OUTPUT_FILE\"\n\n    # Extract from various sources\n    ENV_VARS=\"\"\n\n    # From .env.example\n    if [ -f \".env.example\" ]; then\n        ENV_VARS=\"$ENV_VARS $(grep -E \"^[A-Z_]+=\" .env.example 2>/dev/null | cut -d= -f1 || true)\"\n    fi\n\n    # From JS/TS files\n    ENV_VARS=\"$ENV_VARS $(grep -rh \"process\\.env\\.\" --include=\"*.js\" --include=\"*.ts\" 2>/dev/null | \\\n        grep -oE \"process\\.env\\.[A-Z_]+\" | sed 's/process.env.//' | sort -u || true)\"\n\n    # From Python files\n    ENV_VARS=\"$ENV_VARS $(grep -rhE \"os\\.environ|os\\.getenv\" --include=\"*.py\" 2>/dev/null | \\\n        grep -oE '\"[A-Z_]+\"' | tr -d '\"' | sort -u || true)\"\n\n    # Deduplicate and format\n    ENV_VARS=$(echo \"$ENV_VARS\" | tr ' ' '\\n' | sort -u | grep -v \"^$\")\n\n    FIRST=true\n    for var in $ENV_VARS; do\n        if [ \"$FIRST\" = true ]; then\n            FIRST=false\n        else\n            echo \",\" >> \"$OUTPUT_FILE\"\n        fi\n        echo -n \"    \\\"$var\\\"\" >> \"$OUTPUT_FILE\"\n    done\n    echo \"\" >> \"$OUTPUT_FILE\"\n    echo \"  ],\" >> \"$OUTPUT_FILE\"\n\n    ENV_COUNT=$(echo \"$ENV_VARS\" | wc -w | tr -d ' ')\n    echo \"    Found $ENV_COUNT unique environment variables\"\n\n    # Database detection\n    echo -e \"${CYAN}[7/10] Database Detection...${NC}\"\n    DATABASES=\"\"\n\n    # Check package.json\n    if [ -f \"package.json\" ]; then\n        if grep -qE \"pg|postgres\" package.json 2>/dev/null; then DATABASES=\"$DATABASES PostgreSQL\"; fi\n        if grep -q \"mysql\" package.json 2>/dev/null; then DATABASES=\"$DATABASES MySQL\"; fi\n        if grep -q \"mongodb\\|mongoose\" package.json 2>/dev/null; then DATABASES=\"$DATABASES MongoDB\"; fi\n        if grep -q \"redis\" package.json 2>/dev/null; then DATABASES=\"$DATABASES Redis\"; fi\n        if grep -q \"sqlite\" package.json 2>/dev/null; then DATABASES=\"$DATABASES SQLite\"; fi\n    fi\n\n    # Check requirements.txt\n    if [ -f \"requirements.txt\" ]; then\n        if grep -qE \"psycopg|asyncpg\" requirements.txt 2>/dev/null; then DATABASES=\"$DATABASES PostgreSQL\"; fi\n        if grep -q \"pymysql\\|mysqlclient\" requirements.txt 2>/dev/null; then DATABASES=\"$DATABASES MySQL\"; fi\n        if grep -q \"pymongo\" requirements.txt 2>/dev/null; then DATABASES=\"$DATABASES MongoDB\"; fi\n        if grep -q \"redis\" requirements.txt 2>/dev/null; then DATABASES=\"$DATABASES Redis\"; fi\n    fi\n\n    DATABASES=$(echo \"$DATABASES\" | tr ' ' '\\n' | sort -u | xargs)\n    echo \"  \\\"databases\\\": \\\"$DATABASES\\\",\" >> \"$OUTPUT_FILE\"\n    echo \"    Detected: ${DATABASES:-None detected}\"\n\n    # Tests\n    echo -e \"${CYAN}[8/10] Test Detection...${NC}\"\n    TEST_FILES=$(find . -name \"*test*.js\" -o -name \"*test*.ts\" -o -name \"*test*.py\" -o -name \"*_test.go\" \\\n        -not -path \"*/node_modules/*\" -not -path \"*/.git/*\" 2>/dev/null | wc -l | tr -d ' ')\n    SPEC_FILES=$(find . -name \"*.spec.*\" -not -path \"*/node_modules/*\" 2>/dev/null | wc -l | tr -d ' ')\n\n    HAS_JEST=$(grep -q '\"jest\"' package.json 2>/dev/null && echo \"true\" || echo \"false\")\n    HAS_MOCHA=$(grep -q '\"mocha\"' package.json 2>/dev/null && echo \"true\" || echo \"false\")\n    HAS_PYTEST=$(grep -q \"pytest\" requirements.txt 2>/dev/null && echo \"true\" || echo \"false\")\n\n    echo \"  \\\"tests\\\": {\" >> \"$OUTPUT_FILE\"\n    echo \"    \\\"test_files\\\": $TEST_FILES,\" >> \"$OUTPUT_FILE\"\n    echo \"    \\\"spec_files\\\": $SPEC_FILES,\" >> \"$OUTPUT_FILE\"\n    echo \"    \\\"jest\\\": $HAS_JEST,\" >> \"$OUTPUT_FILE\"\n    echo \"    \\\"mocha\\\": $HAS_MOCHA,\" >> \"$OUTPUT_FILE\"\n    echo \"    \\\"pytest\\\": $HAS_PYTEST\" >> \"$OUTPUT_FILE\"\n    echo \"  },\" >> \"$OUTPUT_FILE\"\n    echo \"    Test files: $TEST_FILES, Spec files: $SPEC_FILES\"\n\n    # Documentation\n    echo -e \"${CYAN}[9/10] Documentation...${NC}\"\n    HAS_README=$([ -f \"README.md\" ] || [ -f \"README.rst\" ] || [ -f \"README\" ] && echo \"true\" || echo \"false\")\n    HAS_DOCS=$([ -d \"docs\" ] && echo \"true\" || echo \"false\")\n    HAS_CHANGELOG=$([ -f \"CHANGELOG.md\" ] || [ -f \"CHANGELOG\" ] && echo \"true\" || echo \"false\")\n    HAS_CONTRIBUTING=$([ -f \"CONTRIBUTING.md\" ] && echo \"true\" || echo \"false\")\n\n    echo \"  \\\"documentation\\\": {\" >> \"$OUTPUT_FILE\"\n    echo \"    \\\"readme\\\": $HAS_README,\" >> \"$OUTPUT_FILE\"\n    echo \"    \\\"docs_folder\\\": $HAS_DOCS,\" >> \"$OUTPUT_FILE\"\n    echo \"    \\\"changelog\\\": $HAS_CHANGELOG,\" >> \"$OUTPUT_FILE\"\n    echo \"    \\\"contributing\\\": $HAS_CONTRIBUTING\" >> \"$OUTPUT_FILE\"\n    echo \"  },\" >> \"$OUTPUT_FILE\"\n    echo \"    README: $HAS_README, /docs: $HAS_DOCS, CHANGELOG: $HAS_CHANGELOG\"\n\n    # CI/CD\n    echo -e \"${CYAN}[10/10] CI/CD...${NC}\"\n    HAS_GH_ACTIONS=$([ -d \".github/workflows\" ] && echo \"true\" || echo \"false\")\n    HAS_TRAVIS=$([ -f \".travis.yml\" ] && echo \"true\" || echo \"false\")\n    HAS_CIRCLE=$([ -d \".circleci\" ] && echo \"true\" || echo \"false\")\n    HAS_DOCKER=$([ -f \"Dockerfile\" ] && echo \"true\" || echo \"false\")\n    HAS_COMPOSE=$([ -f \"docker-compose.yml\" ] || [ -f \"docker-compose.yaml\" ] && echo \"true\" || echo \"false\")\n\n    echo \"  \\\"ci_cd\\\": {\" >> \"$OUTPUT_FILE\"\n    echo \"    \\\"github_actions\\\": $HAS_GH_ACTIONS,\" >> \"$OUTPUT_FILE\"\n    echo \"    \\\"travis\\\": $HAS_TRAVIS,\" >> \"$OUTPUT_FILE\"\n    echo \"    \\\"circleci\\\": $HAS_CIRCLE,\" >> \"$OUTPUT_FILE\"\n    echo \"    \\\"dockerfile\\\": $HAS_DOCKER,\" >> \"$OUTPUT_FILE\"\n    echo \"    \\\"docker_compose\\\": $HAS_COMPOSE\" >> \"$OUTPUT_FILE\"\n    echo \"  }\" >> \"$OUTPUT_FILE\"\n    echo \"}\" >> \"$OUTPUT_FILE\"\n\n    echo \"    GitHub Actions: $HAS_GH_ACTIONS, Docker: $HAS_DOCKER\"\n\n    echo \"\"\n    echo -e \"${GREEN}═══════════════════════════════════════════════════════════${NC}\"\n    echo -e \"${GREEN}  Analysis Complete!${NC}\"\n    echo -e \"${GREEN}═══════════════════════════════════════════════════════════${NC}\"\n    echo \"\"\n    echo \"Output: $OUTPUT_FILE\"\n}\n\n# Main\nif [ $# -lt 1 ]; then\n    usage\n    exit 1\nfi\n\nanalyze_repo \"$1\" \"${2:-repo-analysis.json}\"\n"
        },
        {
          "name": "scan-repos.sh",
          "type": "file",
          "path": "code-necromancer/scripts/scan-repos.sh",
          "size": 6183,
          "content": "#!/bin/bash\n\n# Code Necromancer - Repository Scanner\n# Scans a GitHub organization and creates initial inventory\n\nset -e\n\n# Colors for output\nRED='\\033[0;31m'\nGREEN='\\033[0;32m'\nYELLOW='\\033[1;33m'\nBLUE='\\033[0;34m'\nNC='\\033[0m' # No Color\n\n# Check for required tools\ncheck_requirements() {\n    echo -e \"${BLUE}Checking requirements...${NC}\"\n\n    if ! command -v gh &> /dev/null; then\n        echo -e \"${RED}Error: GitHub CLI (gh) is not installed${NC}\"\n        echo \"Install with: brew install gh\"\n        exit 1\n    fi\n\n    if ! command -v jq &> /dev/null; then\n        echo -e \"${RED}Error: jq is not installed${NC}\"\n        echo \"Install with: brew install jq\"\n        exit 1\n    fi\n\n    # Check gh auth status\n    if ! gh auth status &> /dev/null; then\n        echo -e \"${RED}Error: Not authenticated with GitHub CLI${NC}\"\n        echo \"Run: gh auth login\"\n        exit 1\n    fi\n\n    echo -e \"${GREEN}All requirements met${NC}\"\n}\n\n# Usage\nusage() {\n    echo \"Usage: $0 <org-name> [output-dir]\"\n    echo \"\"\n    echo \"Scans a GitHub organization and creates inventory files\"\n    echo \"\"\n    echo \"Arguments:\"\n    echo \"  org-name    GitHub organization name\"\n    echo \"  output-dir  Directory for output files (default: ./archaeology-output)\"\n    echo \"\"\n    echo \"Example:\"\n    echo \"  $0 dolphin-ai ./dolphin-archaeology\"\n}\n\n# Main scan function\nscan_organization() {\n    local ORG=$1\n    local OUTPUT_DIR=$2\n\n    echo -e \"${BLUE}═══════════════════════════════════════════════════════════${NC}\"\n    echo -e \"${BLUE}  Code Necromancer - Repository Scanner${NC}\"\n    echo -e \"${BLUE}═══════════════════════════════════════════════════════════${NC}\"\n    echo \"\"\n    echo -e \"Organization: ${GREEN}$ORG${NC}\"\n    echo -e \"Output: ${GREEN}$OUTPUT_DIR${NC}\"\n    echo \"\"\n\n    mkdir -p \"$OUTPUT_DIR\"\n\n    # Phase 1: List all repos\n    echo -e \"${YELLOW}Phase 1: Listing repositories...${NC}\"\n\n    gh repo list \"$ORG\" --limit 1000 --json \\\n        name,description,primaryLanguage,pushedAt,createdAt,isArchived,isFork,url,sshUrl,diskUsage,defaultBranchRef \\\n        > \"$OUTPUT_DIR/repos-raw.json\"\n\n    REPO_COUNT=$(jq 'length' \"$OUTPUT_DIR/repos-raw.json\")\n    echo -e \"  Found ${GREEN}$REPO_COUNT${NC} repositories\"\n\n    # Phase 2: Basic categorization\n    echo -e \"${YELLOW}Phase 2: Categorizing by language...${NC}\"\n\n    jq -r '.[].primaryLanguage.name // \"Unknown\"' \"$OUTPUT_DIR/repos-raw.json\" | \\\n        sort | uniq -c | sort -rn > \"$OUTPUT_DIR/languages.txt\"\n\n    echo \"  Language distribution:\"\n    cat \"$OUTPUT_DIR/languages.txt\" | while read count lang; do\n        echo \"    $lang: $count\"\n    done\n\n    # Phase 3: Activity analysis\n    echo -e \"${YELLOW}Phase 3: Analyzing activity...${NC}\"\n\n    # Most recently active\n    jq -r 'sort_by(.pushedAt) | reverse | .[0:5] | .[] | \"\\(.pushedAt)\\t\\(.name)\"' \\\n        \"$OUTPUT_DIR/repos-raw.json\" > \"$OUTPUT_DIR/recent-activity.txt\"\n\n    echo \"  Most recently active:\"\n    cat \"$OUTPUT_DIR/recent-activity.txt\" | while read line; do\n        echo \"    $line\"\n    done\n\n    # Oldest repos\n    jq -r 'sort_by(.createdAt) | .[0:5] | .[] | \"\\(.createdAt)\\t\\(.name)\"' \\\n        \"$OUTPUT_DIR/repos-raw.json\" > \"$OUTPUT_DIR/oldest-repos.txt\"\n\n    # Archived repos\n    ARCHIVED=$(jq '[.[] | select(.isArchived == true)] | length' \"$OUTPUT_DIR/repos-raw.json\")\n    echo -e \"  Archived repositories: ${YELLOW}$ARCHIVED${NC}\"\n\n    # Forked repos\n    FORKED=$(jq '[.[] | select(.isFork == true)] | length' \"$OUTPUT_DIR/repos-raw.json\")\n    echo -e \"  Forked repositories: ${YELLOW}$FORKED${NC}\"\n\n    # Phase 4: Generate summary\n    echo -e \"${YELLOW}Phase 4: Generating summary...${NC}\"\n\n    LAST_ACTIVITY=$(jq -r 'sort_by(.pushedAt) | reverse | .[0].pushedAt' \"$OUTPUT_DIR/repos-raw.json\")\n    FIRST_REPO=$(jq -r 'sort_by(.createdAt) | .[0].createdAt' \"$OUTPUT_DIR/repos-raw.json\")\n    TOTAL_SIZE=$(jq '[.[].diskUsage] | add' \"$OUTPUT_DIR/repos-raw.json\")\n\n    cat > \"$OUTPUT_DIR/summary.md\" << EOF\n# Organization Scan Summary: $ORG\n\n**Scan Date**: $(date -u +\"%Y-%m-%dT%H:%M:%SZ\")\n**Scanner**: Code Necromancer\n\n## Overview\n\n| Metric | Value |\n|--------|-------|\n| Total Repositories | $REPO_COUNT |\n| Archived | $ARCHIVED |\n| Forked | $FORKED |\n| Total Size | ${TOTAL_SIZE:-0} KB |\n| First Repository | $FIRST_REPO |\n| Last Activity | $LAST_ACTIVITY |\n\n## Language Distribution\n\n\\`\\`\\`\n$(cat \"$OUTPUT_DIR/languages.txt\")\n\\`\\`\\`\n\n## Most Recently Active\n\n| Date | Repository |\n|------|------------|\n$(cat \"$OUTPUT_DIR/recent-activity.txt\" | awk -F'\\t' '{print \"| \"$1\" | \"$2\" |\"}')\n\n## Repository List\n\n| Name | Language | Last Push | Archived |\n|------|----------|-----------|----------|\n$(jq -r '.[] | \"| \\(.name) | \\(.primaryLanguage.name // \"?\") | \\(.pushedAt | split(\"T\")[0]) | \\(.isArchived) |\"' \"$OUTPUT_DIR/repos-raw.json\")\n\n## Next Steps\n\n1. Review the repository list above\n2. Identify which repos are \"core\" vs \"peripheral\"\n3. Run detailed analysis on priority repos\n4. Create dependency graph between repos\nEOF\n\n    echo -e \"${GREEN}═══════════════════════════════════════════════════════════${NC}\"\n    echo -e \"${GREEN}  Scan Complete!${NC}\"\n    echo -e \"${GREEN}═══════════════════════════════════════════════════════════${NC}\"\n    echo \"\"\n    echo \"Output files:\"\n    echo \"  - $OUTPUT_DIR/summary.md (start here)\"\n    echo \"  - $OUTPUT_DIR/repos-raw.json (full data)\"\n    echo \"  - $OUTPUT_DIR/languages.txt\"\n    echo \"  - $OUTPUT_DIR/recent-activity.txt\"\n    echo \"  - $OUTPUT_DIR/oldest-repos.txt\"\n    echo \"\"\n    echo \"Next: Review summary.md and run analyze-repo.sh on key repositories\"\n}\n\n# Main\nif [ $# -lt 1 ]; then\n    usage\n    exit 1\nfi\n\nORG=$1\nOUTPUT_DIR=${2:-\"./archaeology-output\"}\n\ncheck_requirements\nscan_organization \"$ORG\" \"$OUTPUT_DIR\"\n"
        }
      ]
    },
    {
      "name": "templates",
      "type": "folder",
      "path": "code-necromancer/templates",
      "children": [
        {
          "name": "archaeology-report.md",
          "type": "file",
          "path": "code-necromancer/templates/archaeology-report.md",
          "size": 6391,
          "content": "# Archaeology Report: [PROJECT_NAME]\n\n**Organization**: [ORG_NAME]\n**Scan Date**: [DATE]\n**Analyst**: [NAME]\n\n---\n\n## Executive Summary\n\n[2-3 paragraph summary of findings: what the system is, how it's structured, and what state it's in]\n\n---\n\n## System Overview\n\n### What Is This?\n\n[Describe the product/system in plain English. What does it do? Who uses it?]\n\n### Architecture at a Glance\n\n```mermaid\ngraph TB\n    %% Replace with actual architecture\n    subgraph \"User-Facing\"\n        A[Component 1]\n    end\n    subgraph \"Backend\"\n        B[Component 2]\n    end\n    A --> B\n```\n\n### Key Statistics\n\n| Metric | Value |\n|--------|-------|\n| Total Repositories | X |\n| Primary Languages | Lang1, Lang2 |\n| Total Lines of Code | ~X,XXX |\n| Last Active | YYYY-MM-DD |\n| Dormancy Period | X years, X months |\n\n---\n\n## Repository Inventory\n\n### Core Systems\n\nThese repositories form the heart of the application:\n\n| Repo | Purpose | Tech Stack | Status | Maturity |\n|------|---------|------------|--------|----------|\n| repo-1 | Main API | Node.js/Express | Dormant | Medium |\n| repo-2 | Web Frontend | React | Dormant | High |\n\n### Supporting Services\n\nThese repos provide supporting functionality:\n\n| Repo | Purpose | Tech Stack | Status | Maturity |\n|------|---------|------------|--------|----------|\n| repo-3 | Auth Service | Python/Flask | Dormant | Low |\n\n### Libraries & Shared Code\n\nInternal packages and shared utilities:\n\n| Repo | Purpose | Used By |\n|------|---------|---------|\n| shared-lib | Common utilities | repo-1, repo-2 |\n\n### Deprecated / Abandoned\n\nThese appear to be deprecated or abandoned:\n\n| Repo | Last Activity | Notes |\n|------|---------------|-------|\n| old-repo | 2018-03-15 | Superseded by repo-2 |\n\n### Unknown / Unclear\n\nPurpose unclear, needs investigation:\n\n| Repo | Notes |\n|------|-------|\n| mystery-repo | No README, sparse commits |\n\n---\n\n## Dependency Graph\n\n### Inter-Repository Dependencies\n\n```mermaid\ngraph LR\n    %% Replace with actual dependencies\n    A[frontend] --> B[api]\n    B --> C[auth-service]\n    B --> D[database]\n    C --> D\n```\n\n### External Service Dependencies\n\n| Service | Used By | Purpose | Status |\n|---------|---------|---------|--------|\n| AWS S3 | repo-1 | File storage | Unknown |\n| PostgreSQL | repo-1, repo-3 | Primary database | Unknown |\n| Redis | repo-1 | Caching/sessions | Unknown |\n| Stripe | repo-1 | Payments | Unknown |\n\n---\n\n## Technology Stack\n\n### Languages\n\n| Language | Repos | Percentage |\n|----------|-------|------------|\n| TypeScript | 8 | 40% |\n| Python | 5 | 25% |\n| Go | 3 | 15% |\n| Other | 4 | 20% |\n\n### Frameworks\n\n| Framework | Version Found | Current Version | Gap |\n|-----------|---------------|-----------------|-----|\n| React | 16.8.0 | 18.x | 2 major |\n| Express | 4.17.0 | 4.18.x | Minor |\n| Flask | 1.1.0 | 3.x | 2 major |\n\n### Runtime Versions\n\n| Runtime | Version Found | Current LTS | Support Status |\n|---------|---------------|-------------|----------------|\n| Node.js | 12.x | 20.x | EOL |\n| Python | 3.7 | 3.12 | EOL |\n\n---\n\n## Maturity Assessment\n\n### Scoring Criteria\n\n- **Documentation** (1-5): README quality, inline comments, API docs\n- **Test Coverage** (1-5): Presence and quality of tests\n- **Code Quality** (1-5): Linting, typing, architecture\n- **Activity** (1-5): Recent commits, active development\n- **Maintainability** (1-5): Complexity, dependency health\n\n### Repository Scores\n\n| Repo | Doc | Tests | Quality | Activity | Maintain | Overall |\n|------|-----|-------|---------|----------|----------|---------|\n| repo-1 | 3 | 2 | 4 | 1 | 3 | 2.6 |\n| repo-2 | 4 | 3 | 4 | 1 | 4 | 3.2 |\n\n### Overall Assessment\n\n**System Maturity**: [LOW / MEDIUM / HIGH]\n\n[Explanation of overall system state]\n\n---\n\n## Missing Pieces\n\n### Identified Gaps\n\n1. **[Gap 1]**: [Description of what's missing and impact]\n2. **[Gap 2]**: [Description]\n\n### Orphaned Components\n\n[Components that seem disconnected from the main system]\n\n### Documentation Gaps\n\n- [ ] No API documentation found\n- [ ] No deployment guide\n- [ ] No architecture decision records\n\n---\n\n## Configuration & Environment\n\n### Environment Variables Required\n\n| Variable | Used By | Purpose | Type |\n|----------|---------|---------|------|\n| DATABASE_URL | repo-1, repo-3 | Database connection | Secret |\n| JWT_SECRET | repo-1, repo-3 | Auth tokens | Secret |\n| AWS_ACCESS_KEY | repo-1 | S3 access | Secret |\n| NODE_ENV | repo-1, repo-2 | Environment flag | Config |\n\n### Configuration Files\n\n| File | Found In | Purpose |\n|------|----------|---------|\n| .env.example | repo-1 | Environment template |\n| config.yaml | repo-3 | Service configuration |\n\n---\n\n## Infrastructure\n\n### Cloud Resources (Detected/Suspected)\n\n- AWS Region: [if detected]\n- Services: S3, RDS, EC2, etc.\n- Databases: PostgreSQL, Redis, etc.\n\n### CI/CD\n\n| Repo | CI System | Status |\n|------|-----------|--------|\n| repo-1 | GitHub Actions | Likely broken |\n| repo-2 | Travis CI | Unknown |\n\n---\n\n## Development History\n\n### Timeline\n\n```\n2017 ─────────────────────────────────────────────────── 2024\n     │                                                    │\n     ├─ [2017-06] Initial commit (repo-1)                │\n     ├─ [2018-03] Frontend added (repo-2)                │\n     ├─ [2019-01] Auth service added (repo-3)            │\n     ├─ [2020-08] Last significant feature               │\n     └─ [2021-03] Last commit                            │\n                                                          │\n                      [3+ years dormancy]                 │\n```\n\n### Key Contributors (Historical)\n\n| Contributor | Commits | Primary Repos |\n|-------------|---------|---------------|\n| dev1@company.com | 500 | repo-1, repo-2 |\n| dev2@company.com | 300 | repo-3 |\n\n---\n\n## Recommendations\n\n### Immediate Actions\n\n1. **[Action 1]**: [Specific recommendation]\n2. **[Action 2]**: [Specific recommendation]\n\n### Before Resurrection\n\n1. [Pre-requisite 1]\n2. [Pre-requisite 2]\n\n### Key Risks\n\n1. **[Risk 1]**: [Description and mitigation]\n2. **[Risk 2]**: [Description and mitigation]\n\n---\n\n## Appendices\n\n### A. Full Repository List\n\n[Complete list with all metadata]\n\n### B. Dependency Trees\n\n[Detailed dependency information per repo]\n\n### C. API Surface Area\n\n[All detected endpoints and their status]\n"
        },
        {
          "name": "repo-inventory.json",
          "type": "file",
          "path": "code-necromancer/templates/repo-inventory.json",
          "size": 2682,
          "content": "{\n  \"$schema\": \"repo-inventory-schema\",\n  \"organization\": \"ORGANIZATION_NAME\",\n  \"scan_date\": \"YYYY-MM-DD\",\n  \"total_repos\": 0,\n  \"repos\": [\n    {\n      \"name\": \"repo-name\",\n      \"description\": \"Repository description\",\n      \"url\": \"https://github.com/org/repo\",\n      \"primary_language\": \"TypeScript\",\n      \"languages\": {\n        \"TypeScript\": 80,\n        \"JavaScript\": 15,\n        \"CSS\": 5\n      },\n      \"framework\": {\n        \"name\": \"React\",\n        \"version\": \"16.8.0\",\n        \"detected_from\": \"package.json\"\n      },\n      \"runtime\": {\n        \"name\": \"Node.js\",\n        \"version\": \"12.x\",\n        \"detected_from\": \".nvmrc\"\n      },\n      \"last_commit\": {\n        \"date\": \"2021-03-15T10:30:00Z\",\n        \"author\": \"developer@company.com\",\n        \"message\": \"Last commit message\"\n      },\n      \"commit_stats\": {\n        \"total_commits\": 1500,\n        \"contributors\": 8,\n        \"first_commit\": \"2018-01-15T00:00:00Z\",\n        \"commits_last_year\": 0,\n        \"commits_last_6_months\": 0\n      },\n      \"size\": {\n        \"files\": 500,\n        \"lines_of_code\": 45000,\n        \"disk_mb\": 25\n      },\n      \"has\": {\n        \"readme\": true,\n        \"dockerfile\": true,\n        \"docker_compose\": true,\n        \"ci_cd\": \"github_actions\",\n        \"tests\": true,\n        \"env_example\": false\n      },\n      \"dependencies\": {\n        \"count\": 45,\n        \"outdated\": 30,\n        \"vulnerable\": 5,\n        \"internal_deps\": [\"@org/shared-lib\", \"@org/auth-client\"]\n      },\n      \"env_vars\": [\n        \"DATABASE_URL\",\n        \"REDIS_URL\",\n        \"JWT_SECRET\",\n        \"AWS_ACCESS_KEY_ID\"\n      ],\n      \"api_endpoints\": {\n        \"count\": 25,\n        \"documented\": false,\n        \"openapi_spec\": false\n      },\n      \"databases\": [\"PostgreSQL\", \"Redis\"],\n      \"external_services\": [\"AWS S3\", \"Stripe\", \"SendGrid\"],\n      \"category\": \"core|support|deprecated|unknown\",\n      \"maturity\": {\n        \"score\": 3,\n        \"max\": 5,\n        \"factors\": {\n          \"documentation\": 2,\n          \"test_coverage\": 3,\n          \"code_quality\": 4,\n          \"activity\": 1,\n          \"maintainability\": 3\n        }\n      },\n      \"notes\": \"Free-form notes about this repo\",\n      \"depends_on\": [\"repo-2\", \"repo-3\"],\n      \"depended_by\": [\"repo-4\"]\n    }\n  ],\n  \"summary\": {\n    \"by_language\": {\n      \"TypeScript\": 8,\n      \"Python\": 5,\n      \"Go\": 3,\n      \"Other\": 4\n    },\n    \"by_category\": {\n      \"core\": 5,\n      \"support\": 8,\n      \"deprecated\": 4,\n      \"unknown\": 3\n    },\n    \"by_maturity\": {\n      \"high\": 3,\n      \"medium\": 10,\n      \"low\": 7\n    },\n    \"oldest_repo\": \"repo-name\",\n    \"newest_repo\": \"repo-name\",\n    \"most_active\": \"repo-name\",\n    \"least_active\": \"repo-name\"\n  }\n}\n"
        },
        {
          "name": "resurrection-plan.md",
          "type": "file",
          "path": "code-necromancer/templates/resurrection-plan.md",
          "size": 5626,
          "content": "# Resurrection Plan: [PROJECT_NAME]\n\n**Based on Archaeology Report**: [DATE]\n**Plan Created**: [DATE]\n\n---\n\n## Executive Summary\n\n[Brief summary of what needs to happen to get the system running]\n\n---\n\n## Resurrection Readiness Score\n\n| Component | Ready | Blockers |\n|-----------|-------|----------|\n| repo-1 | 40% | 3 critical |\n| repo-2 | 60% | 1 critical |\n| repo-3 | 20% | 5 critical |\n| **Overall** | **35%** | **9 critical** |\n\n---\n\n## Critical Blockers\n\n### Blocker 1: [Title]\n\n**Severity**: Critical / High / Medium\n**Component**: repo-1\n**Category**: Dependency / Infrastructure / Configuration / Code\n\n**Description**:\n[What the blocker is]\n\n**Impact**:\n[What won't work without fixing this]\n\n**Resolution**:\n```bash\n# Steps to resolve\nnpm install new-package@version\n```\n\n**Estimated Effort**: [X hours/days]\n**Dependencies**: [Other blockers that must be resolved first]\n\n---\n\n### Blocker 2: [Title]\n\n[Repeat for each blocker]\n\n---\n\n## Dependency Audit\n\n### Critical Updates Required\n\n| Package | Current | Required | Breaking Changes |\n|---------|---------|----------|------------------|\n| react | 16.8.0 | 18.x | Yes - Concurrent mode |\n| node | 12.x | 18.x+ | Yes - ESM, APIs |\n\n### Security Vulnerabilities\n\n| Severity | Count | Action |\n|----------|-------|--------|\n| Critical | 5 | Immediate |\n| High | 12 | Before production |\n| Medium | 23 | Soon |\n| Low | 45 | When convenient |\n\n### Deprecated Packages\n\n| Package | Status | Replacement |\n|---------|--------|-------------|\n| request | Deprecated | axios or fetch |\n| moment | Maintenance | dayjs or date-fns |\n\n---\n\n## Environment Setup\n\n### Required Environment Variables\n\n| Variable | Required For | How to Obtain | Status |\n|----------|--------------|---------------|--------|\n| DATABASE_URL | repo-1, repo-3 | Create new DB | Pending |\n| JWT_SECRET | repo-1, repo-3 | Generate new | Pending |\n| AWS_ACCESS_KEY | repo-1 | AWS Console | Unknown |\n| STRIPE_API_KEY | repo-1 | Stripe Dashboard | Unknown |\n\n### Secrets Inventory\n\n| Secret Type | Count | Status |\n|-------------|-------|--------|\n| API Keys | 5 | Need renewal |\n| Certificates | 2 | Expired |\n| OAuth Credentials | 3 | Unknown |\n\n---\n\n## Infrastructure Requirements\n\n### Cloud Resources Needed\n\n| Resource | Purpose | Status | Action |\n|----------|---------|--------|--------|\n| PostgreSQL DB | Primary data | Unknown | Verify or create |\n| Redis | Caching | Unknown | Verify or create |\n| S3 Bucket | File storage | Unknown | Verify or create |\n\n### Local Development Setup\n\n```bash\n# Prerequisites\n- Docker Desktop\n- Node.js 18+\n- Python 3.11+\n- PostgreSQL 15+\n- Redis 7+\n\n# Setup steps\n1. Clone all repos\n2. Copy .env.example files\n3. Start infrastructure (docker-compose)\n4. Install dependencies\n5. Run migrations\n6. Seed data (if available)\n7. Start services\n```\n\n---\n\n## Resurrection Order\n\nBased on dependencies, resurrect in this order:\n\n```\nPhase 1: Infrastructure\n├── Set up local databases\n├── Configure environment variables\n└── Verify cloud access\n\nPhase 2: Foundation\n├── repo-shared-lib (no dependencies)\n├── repo-database (migrations)\n└── repo-auth (minimal deps)\n\nPhase 3: Core Services\n├── repo-api (depends on auth, db)\n└── repo-workers (depends on api)\n\nPhase 4: User Facing\n├── repo-frontend (depends on api)\n└── repo-mobile (depends on api)\n\nPhase 5: Integration\n├── End-to-end testing\n└── Full system verification\n```\n\n---\n\n## Integration Tests\n\n### Resurrection Verification Tests\n\nThese tests verify each phase of resurrection:\n\n#### Phase 1 Tests: Infrastructure\n- [ ] Can connect to PostgreSQL\n- [ ] Can connect to Redis\n- [ ] Can access S3 bucket\n- [ ] Environment variables loaded correctly\n\n#### Phase 2 Tests: Foundation\n- [ ] Shared library imports work\n- [ ] Database migrations run successfully\n- [ ] Auth service starts\n- [ ] Auth service responds to health check\n\n#### Phase 3 Tests: Core Services\n- [ ] API service starts\n- [ ] API service connects to database\n- [ ] API service connects to auth\n- [ ] Worker service starts\n- [ ] Worker service processes test job\n\n#### Phase 4 Tests: User Facing\n- [ ] Frontend builds successfully\n- [ ] Frontend loads in browser\n- [ ] Frontend can authenticate\n- [ ] Basic user flow works\n\n#### Phase 5 Tests: Integration\n- [ ] Full user signup flow\n- [ ] Full user login flow\n- [ ] Core feature #1 works\n- [ ] Core feature #2 works\n\n---\n\n## Risk Assessment\n\n### High Risk Items\n\n1. **[Risk]**: [Description]\n   - Mitigation: [Plan]\n   - Fallback: [Alternative if mitigation fails]\n\n### Data Concerns\n\n- [ ] Old database needs migration\n- [ ] User data needs to be preserved\n- [ ] PII handling compliance\n\n---\n\n## Timeline Estimate\n\n| Phase | Tasks | Effort Range |\n|-------|-------|--------------|\n| Phase 1: Infrastructure | 5 | Low-Medium |\n| Phase 2: Foundation | 8 | Medium |\n| Phase 3: Core | 12 | Medium-High |\n| Phase 4: User Facing | 6 | Medium |\n| Phase 5: Integration | 4 | Medium |\n| **Total** | **35** | **Medium-High** |\n\n*Note: Estimates assume minimal unexpected blockers*\n\n---\n\n## Success Criteria\n\nThe system is considered \"resurrected\" when:\n\n- [ ] All services start without errors\n- [ ] Services can communicate with each other\n- [ ] Database connections work\n- [ ] Authentication works end-to-end\n- [ ] At least one core user flow works\n- [ ] Integration tests pass\n- [ ] A user can [primary use case]\n\n---\n\n## Next Steps\n\n1. [ ] Obtain/renew required credentials\n2. [ ] Set up development infrastructure\n3. [ ] Begin Phase 1 resurrection\n4. [ ] Document any additional blockers found\n5. [ ] Update this plan as needed\n"
        }
      ]
    },
    {
      "name": "CHANGELOG.md",
      "type": "file",
      "path": "code-necromancer/CHANGELOG.md",
      "size": 1169,
      "content": "# Changelog\n\nAll notable changes to the code-necromancer skill.\n\n## [2.0.0] - 2024-12-14\n\n### Changed\n- **BREAKING**: Restructured skill following progressive disclosure pattern\n- Reduced SKILL.md from 356 lines to ~180 lines for faster loading\n- Extracted detailed content to reference files\n\n### Added\n- `references/archaeology-guide.md` - Deep code archaeology techniques\n- `references/dependency-patterns.md` - Dependency detection across ecosystems\n- `references/framework-detection.md` - Framework/stack identification\n- `references/infrastructure-mapping.md` - Cloud resource discovery\n- `references/integration-test-patterns.md` - Resurrection test patterns\n\n### Improved\n- Clearer phase separation (Archaeology → Resurrection → Rejuvenation)\n- Expanded anti-patterns section (5 → 6 items)\n- Better output deliverables documentation\n- Streamlined process descriptions\n\n## [1.0.0] - 2024-11-26\n\n### Added\n- Initial skill creation\n- Three-phase framework (Archaeology, Resurrection, Rejuvenation)\n- Repo inventory and dependency graph templates\n- GitHub organization scanning commands\n- Architecture diagram generation patterns\n- Success metrics checklists\n"
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "code-necromancer/SKILL.md",
      "size": 6108,
      "content": "---\nname: code-necromancer\nversion: 2.0.0\ndescription: Systematic framework for resurrecting and modernizing legacy codebases through archaeology, resurrection, and rejuvenation phases. Activate on \"legacy code\", \"inherited codebase\", \"no documentation\", \"technical debt\", \"resurrect\", \"modernize\". NOT for greenfield projects or well-documented active codebases.\nallowed-tools: Read,Write,Edit,Bash,WebFetch,Grep,Glob\n---\n\n# Code Necromancer\n\n**Tagline**: Raise dead codebases from the grave\n\nSystematic framework for understanding, resurrecting, and modernizing legacy codebases.\n\n## When to Activate\n\n✅ **Use when:**\n- Inheriting a codebase with 5+ repos and no documentation\n- Resurrecting a product dormant for 2+ years\n- Joining a company with significant technical debt and tribal knowledge loss\n- Performing due diligence on acquired codebases\n- Modernizing legacy systems without breaking existing functionality\n\n❌ **NOT for:**\n- Greenfield projects (start fresh instead)\n- Well-documented active codebases\n- Simple bug fixes in maintained systems\n\n## The Three Phases\n\n### Phase 1: ARCHAEOLOGY\n**Objective**: Create a complete map before touching anything.\n\n| Output | Description |\n|--------|-------------|\n| `repo-inventory.json` | All repos with metadata, languages, activity |\n| `dependency-graph.mmd` | Inter-repo and external dependencies |\n| `architecture-diagram.mmd` | Visual system topology |\n| `tech-stack-matrix.md` | Language/framework versions per repo |\n| `maturity-assessment.md` | Code quality, test coverage, docs quality |\n| `missing-pieces.md` | Gaps, orphaned repos, broken integrations |\n\n**Process**: Inventory → Deep Scan → Cross-Reference → Visualize → Assess\n\n→ See `references/archaeology-guide.md` for detailed techniques.\n\n### Phase 2: RESURRECTION\n**Objective**: Get the system running in development.\n\n| Output | Description |\n|--------|-------------|\n| `dependency-audit.md` | Outdated packages, vulnerabilities, breaking changes |\n| `environment-variables.md` | All required env vars with defaults |\n| `secrets-needed.md` | API keys, certs, OAuth credentials |\n| `infrastructure-status.md` | Cloud resources, what exists vs deleted |\n| `resurrection-blockers.md` | Critical issues preventing launch |\n| `integration-tests/` | Tests verifying components work and communicate |\n\n**Process**: Audit Dependencies → Map Environment → Check Infrastructure → Write Tests → Document Blockers\n\n→ See `references/integration-test-patterns.md` for resurrection test patterns.\n\n### Phase 3: REJUVENATION\n**Objective**: Modernize while maintaining feature parity.\n\n| Output | Description |\n|--------|-------------|\n| `security-recommendations.md` | Vulnerability fixes, compliance |\n| `modernization-roadmap.md` | Prioritized upgrades with effort estimates |\n| `architecture-improvements.md` | Scalability, performance, maintainability |\n\n**Process**: Security First → Infrastructure (containerize, CI/CD) → Code Quality → Architecture\n\n## Key Commands\n\n```bash\n# List all repos in org\ngh repo list ORG --limit 1000 --json name,primaryLanguage,pushedAt\n\n# Dependency analysis\nnpm audit && npm outdated      # Node.js\npip list --outdated && safety check  # Python\ngo mod graph                    # Go\n\n# Find env vars in code\ngrep -rn 'process\\.env\\|os\\.environ' --include=\"*.js\" --include=\"*.py\"\n```\n\n→ See `references/framework-detection.md` for framework/stack identification.\n→ See `references/infrastructure-mapping.md` for cloud resource discovery.\n→ See `references/dependency-patterns.md` for dependency detection.\n\n## Anti-Patterns to Avoid\n\n### 1. Premature Resurrection\n**What it looks like**: Running `npm install` before reading any code\n**Why it's wrong**: You'll fix the same bug 5 times; dependencies have changed\n**Fix**: Complete archaeology first; understand before touching\n\n### 2. Scope Creep\n**What it looks like**: \"Let's also refactor while we're here\"\n**Why it's wrong**: Scope explosion; never actually resurrect\n**Fix**: Strict phase separation; refactoring is Phase 3\n\n### 3. Big Bang Updates\n**What it looks like**: Update all dependencies in one commit\n**Why it's wrong**: Something breaks, no idea what\n**Fix**: Update incrementally; test after each\n\n### 4. Ignoring Tests\n**What it looks like**: \"It runs, ship it\"\n**Why it's wrong**: Regression city; no baseline for changes\n**Fix**: Write resurrection tests as you go; they prove progress\n\n### 5. Undocumented Changes\n**What it looks like**: \"I fixed it but forgot what I changed\"\n**Why it's wrong**: Tribal knowledge returns; next person is you in 6 months\n**Fix**: Document everything you learn and change\n\n### 6. Trusting Old Documentation\n**What it looks like**: Following README from 2019\n**Why it's wrong**: APIs change, services get deprecated\n**Fix**: Verify every instruction; documentation lies\n\n## Success Metrics\n\n### Archaeology Complete When:\n- [ ] All repos cataloged with metadata\n- [ ] Dependency graph visualized\n- [ ] Architecture diagram created\n- [ ] Core vs peripheral repos identified\n- [ ] Missing pieces documented\n\n### Resurrection Complete When:\n- [ ] All services start locally\n- [ ] Services can communicate with each other\n- [ ] Integration tests pass\n- [ ] At least one full user flow works\n\n### Rejuvenation Complete When:\n- [ ] No critical security vulnerabilities\n- [ ] All dependencies reasonably current\n- [ ] CI/CD pipeline working\n- [ ] Documentation current\n- [ ] Team can develop new features\n\n## References\n\n→ `references/archaeology-guide.md` - Deep code archaeology techniques\n→ `references/dependency-patterns.md` - Dependency detection across ecosystems\n→ `references/framework-detection.md` - Framework/stack identification\n→ `references/infrastructure-mapping.md` - Cloud resource discovery\n→ `references/integration-test-patterns.md` - Resurrection test patterns\n\n## Templates\n\n→ `templates/repo-inventory.json` - Repository catalog\n→ `templates/archaeology-report.md` - Phase 1 output\n→ `templates/resurrection-plan.md` - Phase 2 output\n→ `templates/rejuvenation-roadmap.md` - Phase 3 output\n"
    }
  ]
}