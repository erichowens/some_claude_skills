{
  "name": "collage-layout-expert",
  "type": "folder",
  "path": "collage-layout-expert",
  "children": [
    {
      "name": "references",
      "type": "folder",
      "path": "collage-layout-expert/references",
      "children": [
        {
          "name": "advanced-techniques.md",
          "type": "file",
          "path": "collage-layout-expert/references/advanced-techniques.md",
          "size": 9979,
          "content": "# Advanced Collage Techniques\n\n## Cross-Photo Interactions\n\n**Concept**: Photos \"talk\" to each other across boundaries.\n\n### Types of Interactions\n\n1. **Gesture-Response Pairs**:\n   ```\n   Photo A (left): Person waving to the right →\n   Photo B (right): Person waving to the left ←\n   Result: Two people greeting each other\n   ```\n\n2. **Pointing Interactions**:\n   ```\n   Photo A: Person pointing right →\n   Photo B: Interesting object/scene\n   Result: Person pointing at the object\n   ```\n\n3. **Gaze Direction**:\n   ```\n   Photo A: Person looking right →\n   Photo B: Beautiful landscape\n   Result: Person admiring the view\n   ```\n\n4. **Passing Objects**:\n   ```\n   Photo A (top): Hands reaching down ↓\n   Photo B (bottom): Hands reaching up ↑\n   Result: Handing something between photos\n   ```\n\n### Implementation\n\n```python\nclass InteractionDetector:\n    def __init__(self):\n        self.pose_estimator = load_pose_model()\n        self.action_classifier = load_action_model()\n\n    def find_interaction_pairs(self, photo1, photo2, edge_pair):\n        \"\"\"Find natural interactions across photo boundary.\"\"\"\n        people1 = self.detect_people(photo1)\n        people2 = self.detect_people(photo2)\n\n        interactions = []\n        for p1 in people1:\n            if not self.is_near_edge(p1, edge_pair[0]):\n                continue\n\n            gesture1 = self.detect_gesture(photo1, p1.bbox)\n\n            for p2 in people2:\n                if not self.is_near_edge(p2, edge_pair[1]):\n                    continue\n\n                gesture2 = self.detect_gesture(photo2, p2.bbox)\n                score = self.score_interaction(gesture1, gesture2)\n\n                if score > 0.5:\n                    interactions.append({\n                        'person1': p1,\n                        'person2': p2,\n                        'type': self.classify_interaction(gesture1, gesture2),\n                        'score': score\n                    })\n\n        return interactions\n\n    def score_interaction(self, gesture1, gesture2):\n        \"\"\"Natural interaction pairs.\"\"\"\n        NATURAL_PAIRS = {\n            ('waving', 'waving'): 0.9,\n            ('waving', 'looking'): 0.8,\n            ('pointing', 'looking'): 0.85,\n            ('reaching', 'reaching'): 0.7,\n            ('throwing', 'catching'): 0.95,\n            ('looking_right', 'looking_left'): 0.7,\n        }\n\n        key = (gesture1['gesture'], gesture2['gesture'])\n        base_score = NATURAL_PAIRS.get(key, 0.3)\n\n        if self.directions_align(gesture1, gesture2):\n            base_score += 0.1\n\n        return min(1.0, base_score)\n```\n\n---\n\n## Negative Space Awareness\n\n**The Insight**: Empty space is as important as filled space.\n\n```python\nclass NegativeSpaceAnalyzer:\n    def analyze_negative_space(self, photo, subject_mask):\n        \"\"\"Analyze quality and distribution of negative space.\"\"\"\n        h, w = photo.shape[:2]\n        negative_mask = 1 - subject_mask\n\n        breathing_room = {\n            'top': negative_mask[:h//3, :].mean(),\n            'bottom': negative_mask[2*h//3:, :].mean(),\n            'left': negative_mask[:, :w//3].mean(),\n            'right': negative_mask[:, 2*w//3:].mean(),\n            'overall': negative_mask.mean()\n        }\n\n        background = photo * negative_mask[..., None]\n        bg_variance = np.var(background)\n        quality = 1.0 - min(1.0, bg_variance / 1000.0)\n\n        return {\n            'distribution': breathing_room,\n            'quality': quality,\n            'total_ratio': breathing_room['overall']\n        }\n\n    def match_negative_space(self, analysis1, analysis2):\n        \"\"\"Find complementary negative space patterns.\"\"\"\n        # Subject on left + Subject on right = good pair\n        if (analysis1['distribution']['left'] < 0.3 and\n            analysis2['distribution']['right'] < 0.3):\n            return 'left_right_pair', 0.9\n\n        if (analysis1['distribution']['bottom'] < 0.3 and\n            analysis2['distribution']['top'] < 0.3):\n            return 'top_bottom_pair', 0.9\n\n        if (analysis1['distribution']['right'] > 0.6 and\n            analysis2['distribution']['right'] > 0.6):\n            return 'right_stack', 0.7\n\n        return None, 0.0\n```\n\n**Use Case**:\n```\nPhoto A: Person on left, empty beach on right\nPhoto B: Sunset on right, empty ocean on left\n\nComposite: Person (from A) on left + Sunset (from B) on right\nResult: Person appears to be watching the sunset\n```\n\n---\n\n## Multi-Layer Compositing\n\n**Concept**: Create depth through foreground/midground/background layers.\n\n```python\nclass LayeredCollage:\n    def create_layered_composition(self, photos):\n        \"\"\"Build composition with depth layers.\"\"\"\n\n        background_photos = self.select_backgrounds(photos)\n        midground_photos = self.select_midgrounds(photos)\n        foreground_photos = self.select_foregrounds(photos)\n\n        layers = {\n            'background': self.create_background_layer(background_photos),\n            'midground': self.create_midground_layer(midground_photos),\n            'foreground': self.create_foreground_layer(foreground_photos)\n        }\n\n        canvas = self.composite_layers(layers)\n        return canvas\n\n    def select_backgrounds(self, photos):\n        \"\"\"Select photos suitable for background layer.\"\"\"\n        candidates = []\n\n        for photo in photos:\n            score = 0.0\n\n            if self.contains_sky(photo):\n                score += 0.5\n            if self.is_landscape_oriented(photo):\n                score += 0.3\n\n            depth = self.estimate_depth(photo)\n            if depth.mean() > 0.7:\n                score += 0.2\n\n            if score > 0.5:\n                candidates.append((photo, score))\n\n        return [p for p, s in sorted(candidates, key=lambda x: -x[1])]\n```\n\n---\n\n## Narrative Sequences\n\n**Concept**: Tell a story across the collage.\n\n```python\nclass NarrativeCollageBuilder:\n    def build_story_collage(self, photos, story_type='journey'):\n        \"\"\"Build collage that tells a story.\"\"\"\n\n        if story_type == 'journey':\n            # Start → Travel → Arrive → Experience → Depart\n            segments = self.segment_by_story_arc(photos)\n            layout = self.create_flow_layout(segments)\n\n        elif story_type == 'day_in_life':\n            # Morning → Midday → Evening → Night\n            segments = self.segment_by_time_of_day(photos)\n            layout = self.create_temporal_gradient_layout(segments)\n\n        elif story_type == 'emotion_arc':\n            # Calm → Excitement → Joy → Reflection\n            segments = self.segment_by_emotion(photos)\n            layout = self.create_emotional_flow_layout(segments)\n\n        return layout\n\n    def segment_by_story_arc(self, photos):\n        \"\"\"Cluster photos into narrative segments.\"\"\"\n        features = []\n        for photo in photos:\n            feat = np.concatenate([\n                photo.clip_embedding,\n                self.encode_location(photo.gps),\n                self.encode_time(photo.timestamp)\n            ])\n            features.append(feat)\n\n        segments = self.hierarchical_cluster(features, n_clusters=5)\n\n        segments = sorted(segments,\n                         key=lambda s: np.mean([p.timestamp for p in s]))\n\n        return {\n            'beginning': segments[0],\n            'rising': segments[1],\n            'climax': segments[2],\n            'falling': segments[3],\n            'end': segments[4]\n        }\n```\n\n---\n\n## Simulated Annealing for Photo Swapping\n\n**When to Use**: User explicitly wants to explore alternative arrangements, or initial assembly has suboptimal global aesthetics.\n\n**What It Does**: Randomly swaps photos in the existing collage and accepts swaps that improve the global energy function.\n\n```python\ndef refine_with_simulated_annealing(canvas, max_iters=10000):\n    \"\"\"\n    Refine existing collage by swapping photos.\n    NOTE: This is a refinement, NOT the primary assembly algorithm.\n    \"\"\"\n    T = 10.0\n    T_min = 0.01\n    cooling_rate = 0.95\n\n    current_energy = compute_total_energy(canvas)\n    best_canvas = canvas.copy()\n    best_energy = current_energy\n\n    for iteration in range(max_iters):\n        canvas_new = canvas.copy()\n        i, j = random.sample(range(len(canvas.shards)), 2)\n        canvas_new.swap_shards(i, j)\n\n        new_energy = compute_total_energy(canvas_new)\n        delta_E = new_energy - current_energy\n\n        if delta_E < 0:\n            canvas = canvas_new\n            current_energy = new_energy\n        else:\n            acceptance_prob = np.exp(-delta_E / T)\n            if np.random.random() < acceptance_prob:\n                canvas = canvas_new\n                current_energy = new_energy\n\n        if current_energy < best_energy:\n            best_canvas = canvas.copy()\n            best_energy = current_energy\n\n        T = max(T_min, T * cooling_rate)\n\n    return best_canvas\n```\n\n**Performance**:\n- Time: 5-15 seconds for 50 photos\n- Quality gain: 5-10% improvement in global aesthetics\n- Diminishing returns after 1000-2000 iterations\n\n**When NOT to Use**:\n- Interactive editing (too slow)\n- Initial assembly (use greedy edge growth)\n- User wants predictable results (stochastic)\n\n---\n\n## Genetic Algorithms for Layout Evolution\n\n**Concept**: Maintain population of collages, breed and mutate to explore layout space.\n\n**Operations**:\n- **Crossover**: Swap regions between two parent collages\n- **Mutation**: Random perturbations (rotate, scale, move shards)\n- **Selection**: Keep top-scoring collages, discard poor ones\n\n**Performance**: Even slower than simulated annealing, typically for offline rendering.\n\n---\n\n## Constraint Satisfaction Problem (CSP) Formulation\n\n**Concept**: Define collage assembly as constraint satisfaction problem.\n\n**Constraints**:\n- Edge compatibility > threshold\n- No overlaps (or controlled overlaps for Hockney)\n- Minimum global aesthetics score\n- Semantic coherence within range\n\nListed as alternative strategy, not recommended for MVP.\n"
        },
        {
          "name": "algorithms.md",
          "type": "file",
          "path": "collage-layout-expert/references/algorithms.md",
          "size": 8914,
          "content": "# Core Collage Algorithms\n\nMathematical and computational techniques for collage composition.\n\n---\n\n## Edge-Based Assembly (Hockney/Joiners)\n\n```python\ndef edge_compatibility(edge1, edge2):\n    \"\"\"Score how well two edges can connect (0-1).\"\"\"\n    return (\n        0.30 * line_continuation_score +\n        0.15 * curve_flow_score +\n        0.25 * color_harmony_score +\n        0.20 * semantic_coherence +  # CLIP similarity\n        0.10 * complexity_balance\n    )\n```\n\n### Edge Extraction\n```python\ndef extract_edges(image, edge_position='right'):\n    \"\"\"\n    Extract edge strip from image for compatibility scoring.\n    \"\"\"\n    edge_width = 20  # pixels\n\n    if edge_position == 'right':\n        return image[:, -edge_width:]\n    elif edge_position == 'left':\n        return image[:, :edge_width]\n    elif edge_position == 'top':\n        return image[:edge_width, :]\n    elif edge_position == 'bottom':\n        return image[-edge_width:, :]\n```\n\n### Line Continuation Score\n```python\ndef line_continuation_score(edge1, edge2):\n    \"\"\"\n    Score how well lines continue across edge boundary.\n    Uses Hough line detection.\n    \"\"\"\n    # Detect lines in both edges\n    lines1 = cv2.HoughLinesP(edge1, 1, np.pi/180, 50)\n    lines2 = cv2.HoughLinesP(edge2, 1, np.pi/180, 50)\n\n    if lines1 is None or lines2 is None:\n        return 0.5  # Neutral score\n\n    # Find lines that approach the boundary\n    boundary_lines1 = [l for l in lines1 if approaches_boundary(l, 'right')]\n    boundary_lines2 = [l for l in lines2 if approaches_boundary(l, 'left')]\n\n    # Score angle continuity\n    score = 0\n    for l1 in boundary_lines1:\n        for l2 in boundary_lines2:\n            angle_diff = abs(get_angle(l1) - get_angle(l2))\n            score += max(0, 1 - angle_diff / 45)  # Within 45° = good\n\n    return min(1.0, score / max(len(boundary_lines1), 1))\n```\n\n---\n\n## Poisson Blending (Seamless Transitions)\n\nPreserves gradients from source while matching boundary conditions.\n\n```python\ndef poisson_blend(source, target, mask, center):\n    \"\"\"\n    Seamless clone using OpenCV's implementation.\n    \"\"\"\n    # Ensure mask is binary\n    mask = (mask > 127).astype(np.uint8) * 255\n\n    # NORMAL_CLONE preserves source gradients\n    # MIXED_CLONE preserves stronger gradient from either\n    result = cv2.seamlessClone(\n        source, target, mask, center,\n        cv2.NORMAL_CLONE  # or cv2.MIXED_CLONE\n    )\n\n    return result\n```\n\n### When to Use Each Mode\n- **NORMAL_CLONE**: Standard seamless blending, preserves source fully\n- **MIXED_CLONE**: Preserves dominant gradients (good for textured backgrounds)\n- **MONOCHROME_TRANSFER**: Transfers lighting only, not color\n\n### Performance Notes\n- GPU-parallelizable with Jacobi iteration\n- ~20ms for 512×512 on modern GPU\n- ~100ms for 1080p on CPU\n\n---\n\n## Optimal Transport (Color Harmonization)\n\nWasserstein distance measures \"effort\" to transform color distributions.\n\n```python\ndef color_transfer_optimal_transport(source, target):\n    \"\"\"\n    Transfer color distribution from target to source\n    using optimal transport.\n    \"\"\"\n    # Convert to LAB for perceptual uniformity\n    source_lab = cv2.cvtColor(source, cv2.COLOR_BGR2LAB)\n    target_lab = cv2.cvtColor(target, cv2.COLOR_BGR2LAB)\n\n    # Compute means and covariances\n    source_mean, source_std = compute_stats(source_lab)\n    target_mean, target_std = compute_stats(target_lab)\n\n    # Affine transformation\n    result = (source_lab - source_mean) * (target_std / source_std) + target_mean\n\n    return cv2.cvtColor(result.astype(np.uint8), cv2.COLOR_LAB2BGR)\n\ndef compute_stats(lab_image):\n    \"\"\"Compute per-channel mean and std.\"\"\"\n    mean = np.mean(lab_image, axis=(0, 1))\n    std = np.std(lab_image, axis=(0, 1))\n    std = np.where(std == 0, 1, std)  # Avoid division by zero\n    return mean, std\n```\n\n### Sinkhorn Algorithm (Full Optimal Transport)\n```python\ndef sinkhorn_color_transfer(source, target, reg=0.01, iterations=100):\n    \"\"\"\n    More accurate but slower color transfer using Sinkhorn.\n    \"\"\"\n    import ot  # POT library\n\n    # Flatten and sample colors\n    source_colors = source.reshape(-1, 3).astype(float)\n    target_colors = target.reshape(-1, 3).astype(float)\n\n    # Sample for speed (full image too slow)\n    n_samples = 1000\n    source_sample = source_colors[np.random.choice(len(source_colors), n_samples)]\n    target_sample = target_colors[np.random.choice(len(target_colors), n_samples)]\n\n    # Compute cost matrix (Euclidean distance in LAB)\n    M = ot.dist(source_sample, target_sample, metric='euclidean')\n\n    # Sinkhorn transport\n    T = ot.sinkhorn(\n        np.ones(n_samples) / n_samples,\n        np.ones(n_samples) / n_samples,\n        M, reg\n    )\n\n    # Apply transport (simplified - full implementation more complex)\n    return source  # Placeholder\n```\n\n---\n\n## Force-Directed Layout (Organic Scatter)\n\n```python\ndef force_directed_layout(images, canvas_size, iterations=100):\n    \"\"\"\n    Organic layout using physics simulation.\n    \"\"\"\n    # Initialize random positions\n    for img in images:\n        img.position = np.random.rand(2) * canvas_size\n        img.velocity = np.zeros(2)\n\n    canvas_center = np.array(canvas_size) / 2\n\n    for _ in range(iterations):\n        for img in images:\n            force = np.zeros(2)\n\n            # Repulsion from other images\n            for other in images:\n                if img != other:\n                    diff = img.position - other.position\n                    dist = np.linalg.norm(diff)\n                    if dist < 1:\n                        dist = 1\n                    # Inverse square repulsion\n                    force += diff / (dist ** 2) * 100\n\n            # Attraction to center (prevent drift)\n            center_diff = canvas_center - img.position\n            force += center_diff * 0.01\n\n            # Boundary repulsion\n            for i in range(2):\n                if img.position[i] < 50:\n                    force[i] += 10\n                if img.position[i] > canvas_size[i] - 50:\n                    force[i] -= 10\n\n            # Apply force with damping\n            img.velocity = img.velocity * 0.9 + force * 0.1\n            img.position += img.velocity\n\n    return images\n```\n\n### Collision Avoidance\n```python\ndef check_overlap(img1, img2):\n    \"\"\"Check if two positioned images overlap.\"\"\"\n    r1 = img1.get_rect()  # (x, y, w, h)\n    r2 = img2.get_rect()\n\n    return (r1.x < r2.x + r2.w and\n            r1.x + r1.w > r2.x and\n            r1.y < r2.y + r2.h and\n            r1.y + r1.h > r2.y)\n\ndef resolve_overlap(img1, img2):\n    \"\"\"Push overlapping images apart.\"\"\"\n    diff = img1.position - img2.position\n    dist = np.linalg.norm(diff)\n    if dist < 1:\n        diff = np.random.rand(2) - 0.5\n        dist = np.linalg.norm(diff)\n\n    # Move each image half the overlap distance\n    overlap = get_overlap_distance(img1, img2)\n    push = diff / dist * overlap / 2\n\n    img1.position += push\n    img2.position -= push\n```\n\n---\n\n## Bin Packing (Tight Grid Layout)\n\n```python\ndef guillotine_pack(images, canvas_width):\n    \"\"\"\n    Pack images using guillotine algorithm.\n    Returns positions for each image.\n    \"\"\"\n    # Sort by height (tallest first)\n    sorted_images = sorted(images, key=lambda x: -x.height)\n\n    # Initialize free rectangles\n    free_rects = [(0, 0, canvas_width, float('inf'))]\n    positions = []\n\n    for img in sorted_images:\n        # Find best fit\n        best_rect = None\n        best_score = float('inf')\n\n        for rect in free_rects:\n            if rect[2] >= img.width and rect[3] >= img.height:\n                score = rect[2] * rect[3]  # Area\n                if score < best_score:\n                    best_score = score\n                    best_rect = rect\n\n        if best_rect:\n            # Place image\n            positions.append((img, best_rect[0], best_rect[1]))\n\n            # Split remaining space (guillotine cut)\n            free_rects.remove(best_rect)\n            # Right split\n            if best_rect[2] - img.width > 0:\n                free_rects.append((\n                    best_rect[0] + img.width,\n                    best_rect[1],\n                    best_rect[2] - img.width,\n                    img.height\n                ))\n            # Bottom split\n            if best_rect[3] - img.height > 0:\n                free_rects.append((\n                    best_rect[0],\n                    best_rect[1] + img.height,\n                    best_rect[2],\n                    best_rect[3] - img.height\n                ))\n\n    return positions\n```\n\n---\n\n## Performance Benchmarks\n\n| Operation | Mac M2 | iPhone 15 Pro |\n|-----------|--------|---------------|\n| Grid layout (20 photos) | <50ms | <100ms |\n| Photo mosaic (10k tiles) | 2s | 5s |\n| Force-directed (50 images, 100 iter) | 200ms | 500ms |\n| Poisson blending (512×512) | 20ms | 50ms |\n| Hockney assembly (10 photos) | 0.5s | 2s |\n| Color transfer (1080p) | 100ms | 300ms |\n"
        },
        {
          "name": "art-historical-styles.md",
          "type": "file",
          "path": "collage-layout-expert/references/art-historical-styles.md",
          "size": 5328,
          "content": "# Art-Historical Collage Styles\n\nDetailed implementations for historically-inspired collage techniques.\n\n---\n\n## David Hockney's Joiners (1982-1985)\n\nHockney created photographs with **\"perspectival sophistication of Cubist paintings\"**:\n\n```python\nHOCKNEY_JOINER_STYLE = {\n    'overlap': 0.1,              # 5-15% overlap between photos\n    'rotation_variance': 2.0,    # ±2° rotation per photo\n    'perspective_shift': True,   # Multiple viewpoints\n    'grid_irregularity': 0.15,   # 10-15% positional offset\n    'border_style': 'polaroid',  # White borders (optional)\n    'allow_gaps': True,          # Intentional negative space\n}\n```\n\n**Key Innovations**:\n- Multiple perspectives simultaneously (vs. single camera viewpoint)\n- Temporal dimension (same scene, different moments)\n- Viewer's eye \"constructs\" the scene (active participation)\n- Embraces imperfection (overlaps, gaps, misalignments)\n\n### Implementation Notes\n\nThe magic of Hockney joiners comes from:\n1. **Intentional misalignment** - Don't perfect-stitch\n2. **Visible seams** - Borders are part of the aesthetic\n3. **Multiple focal lengths** - Shift attention through zoom variation\n4. **Temporal narrative** - Capture same scene at different moments\n\n---\n\n## Dadaist Photomontage (Hannah Höch, 1920s)\n\n```python\nDADAIST_STYLE = {\n    'layout': 'chaotic',\n    'semantic_mismatch': True,  # Intentionally incongruous elements\n    'sharp_cutouts': True,      # No feathering\n    'scale_absurdity': True,    # Giant heads, tiny bodies\n    'political_commentary': True,\n}\n```\n\n### Key Characteristics\n- **Sharp edges**: No smooth transitions\n- **Jarring juxtapositions**: Unrelated elements collide\n- **Scale distortion**: Size relationships defied\n- **Found imagery**: Newspapers, magazines, advertisements\n- **Political/social critique**: Visual commentary\n\n### Implementation Approach\n```python\ndef dadaist_cutout(image, subject_mask):\n    \"\"\"\n    Create Dadaist-style sharp cutout\n    \"\"\"\n    # Hard edge, no feathering\n    mask = subject_mask.astype(np.uint8) * 255\n\n    # Optional: Add slight paper-tear effect to edges\n    if random.random() > 0.7:\n        kernel = np.ones((2, 2), np.uint8)\n        mask = cv2.erode(mask, kernel, iterations=1)\n\n    return cv2.bitwise_and(image, image, mask=mask)\n```\n\n---\n\n## Pop Art Combines (Rauschenberg, 1950s-60s)\n\n```python\nRAUSCHENBERG_STYLE = {\n    'layout': 'layered',\n    'blend_modes': ['multiply', 'screen', 'overlay'],\n    'found_imagery': True,      # Newspaper, ads, photos\n    'paint_integration': True,  # Mix photo + paint texture\n    'silkscreen_effect': True,\n}\n```\n\n### Key Characteristics\n- **Layered composition**: Images stacked, semi-transparent\n- **Mixed media**: Photography + painting + found objects\n- **Urban detritus**: Street imagery, commercial artifacts\n- **Color registration errors**: Intentional offset for silkscreen look\n\n### Silkscreen Effect\n```python\ndef silkscreen_effect(image, color_shift=(5, 3)):\n    \"\"\"\n    Create Warhol/Rauschenberg silkscreen look\n    \"\"\"\n    # Split channels\n    b, g, r = cv2.split(image)\n\n    # Offset each channel slightly\n    rows, cols = b.shape\n    M_r = np.float32([[1, 0, color_shift[0]], [0, 1, color_shift[1]]])\n    M_b = np.float32([[1, 0, -color_shift[0]], [0, 1, -color_shift[1]]])\n\n    r = cv2.warpAffine(r, M_r, (cols, rows))\n    b = cv2.warpAffine(b, M_b, (cols, rows))\n\n    # Recombine\n    return cv2.merge([b, g, r])\n```\n\n---\n\n## Surrealist Assemblage\n\n```python\nSURREALIST_STYLE = {\n    'dreamlike_transitions': True,\n    'impossible_juxtaposition': True,\n    'seamless_blend': True,     # Unlike Dada's sharp cuts\n    'perspective_manipulation': True,\n}\n```\n\n### Key Characteristics\n- **Dreamlike logic**: Elements connected by unconscious association\n- **Seamless integration**: Smooth blending (unlike Dada)\n- **Scale manipulation**: Objects in impossible sizes\n- **Perspective warping**: Shared vanishing points across disparate elements\n\n### Implementation Approach\n```python\ndef surrealist_blend(foreground, background, mask):\n    \"\"\"\n    Seamless surrealist integration using Poisson blending\n    \"\"\"\n    # Find center of foreground object\n    M = cv2.moments(mask)\n    cx = int(M['m10'] / M['m00'])\n    cy = int(M['m01'] / M['m00'])\n\n    # Seamless clone\n    result = cv2.seamlessClone(\n        foreground,\n        background,\n        mask,\n        (cx, cy),\n        cv2.NORMAL_CLONE\n    )\n\n    return result\n```\n\n---\n\n## Constructivist Montage (Rodchenko, 1920s)\n\n```python\nCONSTRUCTIVIST_STYLE = {\n    'layout': 'dynamic_diagonal',\n    'typography_integration': True,\n    'bold_geometry': True,\n    'limited_palette': ['red', 'black', 'white'],\n    'propaganda_aesthetic': True,\n}\n```\n\n### Key Characteristics\n- **Strong diagonals**: Dynamic composition\n- **Bold typography**: Text as design element\n- **Limited color palette**: Red, black, white dominant\n- **Geometric shapes**: Circles, triangles, bars as framing\n- **Worker imagery**: Industrial, heroic subjects\n\n---\n\n## Choosing a Historical Style\n\n| If you want... | Use... |\n|----------------|--------|\n| Multiple perspectives of one scene | Hockney Joiner |\n| Political/social commentary with sharp contrasts | Dadaist |\n| Layered, painterly texture | Rauschenberg Pop |\n| Dreamlike seamless fantasy | Surrealist |\n| Bold propaganda poster feel | Constructivist |\n"
        },
        {
          "name": "collage-types.md",
          "type": "file",
          "path": "collage-layout-expert/references/collage-types.md",
          "size": 5009,
          "content": "# Collage Types & Techniques\n\nDetailed code examples and configurations for each collage style.\n\n---\n\n## 1. Grid Collages\n\n**Use for**: Instagram profiles, product showcases, team photos, systematic displays.\n\n```python\nGRID_STYLES = {\n    'uniform': {\n        'rows': 3, 'cols': 3,\n        'gap': 4,  # pixels\n        'aspect': '1:1',\n    },\n    'masonry': {\n        'columns': 3,\n        'gap': 8,\n        'variable_height': True,  # Pinterest-style\n    },\n    'mixed_grid': {\n        'hero_size': 2,  # 2x2 for main image\n        'small_count': 5,\n        'layout': 'L_shape',  # or 'corner', 'split'\n    },\n}\n```\n\n**Key considerations**:\n- Consistent color temperature across images\n- Visual flow (Z-pattern or F-pattern for reading)\n- One hero image as anchor; others support\n\n---\n\n## 2. Photo Mosaics\n\n**Use for**: Tribute images, corporate displays, artistic recreations.\n\n```python\ndef create_photo_mosaic(target_image, tile_images, tile_size=32):\n    \"\"\"\n    Each tile_image replaces a region of target_image\n    based on average color matching.\n    \"\"\"\n    # 1. Compute average color of each tile\n    tile_colors = [avg_color(img) for img in tile_images]\n\n    # 2. Build k-d tree for fast lookup\n    color_tree = KDTree(tile_colors)\n\n    # 3. For each grid cell in target\n    for y in range(0, target.height, tile_size):\n        for x in range(0, target.width, tile_size):\n            region_color = avg_color(target[y:y+tile_size, x:x+tile_size])\n            best_tile_idx = color_tree.query(region_color)\n            place_tile(tile_images[best_tile_idx], x, y)\n```\n\n**Expert tips**:\n- Tile size 20-40px for viewing distance balance\n- Use LAB color space for perceptual matching\n- Avoid repetition: track tile usage, penalize reuse\n- Consider edge detection for structural preservation\n\n---\n\n## 3. Scrapbook & Digital Journal\n\n**Use for**: Personal memories, travel journals, baby books, wedding albums.\n\n```python\nSCRAPBOOK_ELEMENTS = {\n    'photos': {'rotation_variance': (-5, 5), 'drop_shadow': True},\n    'frames': ['polaroid', 'vintage', 'tape_corners', 'washi_tape'],\n    'text': {'fonts': ['handwritten', 'typewriter', 'label_maker']},\n    'embellishments': ['stickers', 'stamps', 'doodles', 'tickets'],\n    'backgrounds': ['paper_texture', 'cork_board', 'fabric'],\n}\n```\n\n**Layer order** (back to front):\n1. Background texture/paper\n2. Decorative elements (washi tape, ribbons)\n3. Photos with frames/borders\n4. Text blocks and labels\n5. Small embellishments (stickers, stamps)\n\n---\n\n## 4. Magazine & Editorial Layouts\n\n**Use for**: Professional publications, marketing materials, portfolios.\n\n```python\nEDITORIAL_GRIDS = {\n    '3_column': {'cols': 3, 'gutter': 20, 'margin': 40},\n    '12_column': {'cols': 12, 'gutter': 16, 'margin': 48},  # Flexible\n    'modular': {'rows': 6, 'cols': 6, 'baseline': 24},\n}\n\n# Text-image relationships\nWRAP_STYLES = ['square', 'tight', 'through', 'top_bottom']\n```\n\n**Typography integration**:\n- Headlines: contrast with imagery, never compete\n- Body text: respect image boundaries, maintain gutter\n- Pull quotes: can overlap images with proper contrast\n- Captions: anchor to relevant image\n\n---\n\n## 5. Vision Boards & Mood Boards\n\n**Use for**: Design inspiration, goal visualization, brand development.\n\n```python\nMOOD_BOARD_LAYOUT = {\n    'style': 'organic_cluster',  # or 'grid', 'radial', 'timeline'\n    'overlap': 0.15,\n    'rotation_range': (-8, 8),\n    'scale_variation': (0.7, 1.3),\n    'anchor_image': 'largest',  # Central focal point\n    'color_coherence': 0.8,  # How matched colors should be\n}\n```\n\n**Curation principles**:\n- 60/30/10 rule: dominant/secondary/accent\n- Mix scales: wide shots + details + textures\n- Include non-photo elements: swatches, type samples, textures\n\n---\n\n## 6. Memory Walls & Polaroid Layouts\n\n**Use for**: Nostalgic displays, event walls, family galleries.\n\n```python\nPOLAROID_STYLE = {\n    'border': {'top': 8, 'sides': 8, 'bottom': 24},  # Classic Polaroid\n    'caption_font': 'permanent_marker',\n    'scatter': {\n        'rotation': (-15, 15),\n        'overlap_allowed': True,\n        'pin_style': 'pushpin',  # or 'tape', 'clip', 'magnet'\n    },\n}\n```\n\n**Arrangement algorithms**:\n- **Force-directed**: Images repel like particles, settle naturally\n- **Gravity clustering**: Images fall toward anchor points\n- **Chronological spiral**: Time-based arrangement outward\n\n---\n\n## 7. Social Media Collages\n\n**Use for**: Instagram stories, carousel covers, Pinterest pins.\n\n```python\nSOCIAL_TEMPLATES = {\n    'instagram_story': {'width': 1080, 'height': 1920, 'safe_zone': 100},\n    'instagram_post': {'width': 1080, 'height': 1080},\n    'instagram_carousel': {'count': 10, 'continuity': True},  # Seamless swipe\n    'pinterest_pin': {'width': 1000, 'height': 1500},\n    'twitter_card': {'width': 1200, 'height': 628},\n}\n```\n\n**Platform-specific tips**:\n- Instagram: Avoid text in top/bottom 15% (UI overlap)\n- Carousel: Create visual continuity across swipes\n- Pinterest: Vertical images, text overlay in top third\n"
        },
        {
          "name": "edge-assembly.md",
          "type": "file",
          "path": "collage-layout-expert/references/edge-assembly.md",
          "size": 11807,
          "content": "# Edge-Based Assembly Strategy\n\n## Core Concept: \"Edge-First\" Composition\n\n**The Insight**: Photos connect at their edges, not by timestamp or random placement.\n\n## Edge Descriptor\n\n```python\n@dataclass\nclass EdgeDescriptor:\n    photo_id: UUID\n    side: str  # 'top', 'bottom', 'left', 'right'\n    region: np.ndarray  # 10% strip along edge\n\n    # Geometric features\n    lines: List[Line]              # Lines intersecting this edge\n    curves: List[Curve]            # Curves at edge\n    dominant_angle: float          # -90° to 90°\n    complexity: float              # 0-1 (busy vs. clean)\n\n    # Color features\n    colors: ColorPalette           # 3-5 dominant colors in LAB\n    gradient_direction: str        # 'lighter', 'darker', 'neutral'\n    temperature: str               # 'warm', 'cool', 'neutral'\n\n    # Semantic features\n    clip_embedding: np.ndarray     # 512-dim CLIP of edge region\n    detected_objects: List[str]    # ['sky', 'water', 'person_partial']\n\n    # Match preferences\n    blendability: float            # 0-1 (how well can this edge blend?)\n    wants_continuation: bool       # Is something cut off?\n```\n\n---\n\n## Edge Compatibility Scoring\n\n```python\ndef edge_compatibility(edge1, edge2):\n    \"\"\"\n    Score how well two edges can connect (0-1, higher = better).\n    \"\"\"\n    scores = {}\n\n    # GEOMETRIC: Lines/curves flow across boundary\n    scores['line_continuation'] = (\n        angle_alignment(edge1.lines, edge2.lines) * 0.4 +\n        position_alignment(edge1.lines, edge2.lines) * 0.3 +\n        multiple_line_bonus(edge1.lines, edge2.lines) * 0.3\n    )\n\n    scores['curve_flow'] = (\n        tangent_match(edge1.curves, edge2.curves) * 0.5 +\n        curvature_naturalness(edge1.curves, edge2.curves) * 0.5\n    )\n\n    # COLOR: Harmonious or complementary\n    scores['color_harmony'] = compute_color_harmony(\n        edge1.colors, edge2.colors, mode='edge_regions'\n    )\n\n    # SEMANTIC: Related content (CLIP similarity)\n    scores['semantic_coherence'] = cosine_similarity(\n        edge1.clip_embedding, edge2.clip_embedding\n    )\n\n    # BALANCE: Complexity contrast\n    complexity_diff = abs(edge1.complexity - edge2.complexity)\n    scores['complexity_balance'] = 1.0 - min(1.0, complexity_diff / 0.5)\n\n    # Weighted combination\n    return (\n        0.30 * scores['line_continuation'] +\n        0.15 * scores['curve_flow'] +\n        0.25 * scores['color_harmony'] +\n        0.20 * scores['semantic_coherence'] +\n        0.10 * scores['complexity_balance']\n    )\n```\n\n---\n\n## Angle Alignment\n\n```python\ndef angle_alignment(lines1, lines2, tolerance=15.0):\n    \"\"\"\n    Check if dominant angles of two edge regions align.\n    tolerance: degrees (15° is forgiving, 5° is strict)\n    \"\"\"\n    if not lines1 or not lines2:\n        return 0.0\n\n    # Weighted average by line length and strength\n    angle1 = weighted_average_angle(lines1)\n    angle2 = weighted_average_angle(lines2)\n\n    # Angular difference (accounting for ±180° equivalence)\n    diff = abs(angle1 - angle2)\n    diff = min(diff, 180 - diff)  # Handle wraparound\n\n    # Score: 1.0 if perfect, 0.0 if > tolerance\n    return max(0.0, 1.0 - diff / tolerance)\n\ndef weighted_average_angle(lines):\n    \"\"\"Calculate dominant angle weighted by line properties.\"\"\"\n    weights = [line.length * line.strength for line in lines]\n    angles = [line.angle for line in lines]\n    return np.average(angles, weights=weights)\n```\n\n---\n\n## Position Alignment\n\n```python\ndef position_alignment(lines1, lines2, edge_pair):\n    \"\"\"\n    Check if lines align positionally across boundary.\n\n    Example: For right edge of photo A and left edge of photo B,\n             do horizontal lines have matching y-coordinates?\n    \"\"\"\n    edge_type = edge_pair  # ('right', 'left') or ('top', 'bottom')\n\n    if edge_type in [('right', 'left'), ('left', 'right')]:\n        coord_dim = 'y'\n    else:\n        coord_dim = 'x'\n\n    relevant_lines1 = filter_lines_by_orientation(lines1, edge_type[0])\n    relevant_lines2 = filter_lines_by_orientation(lines2, edge_type[1])\n\n    if not relevant_lines1 or not relevant_lines2:\n        return 0.0\n\n    coords1 = [get_boundary_coord(line, edge_type[0], coord_dim) for line in relevant_lines1]\n    coords2 = [get_boundary_coord(line, edge_type[1], coord_dim) for line in relevant_lines2]\n\n    # Find closest pairs and compute alignment score\n    min_distances = []\n    for c1 in coords1:\n        min_dist = min(abs(c1 - c2) for c2 in coords2)\n        min_distances.append(min_dist)\n\n    avg_misalignment = np.mean(min_distances)\n\n    # Score: 1.0 if perfect (<5px), 0.0 if terrible (>50px)\n    return max(0.0, 1.0 - avg_misalignment / 50.0)\n```\n\n---\n\n## Assembly Algorithm: Greedy Edge Growth\n\n```python\ndef assemble_collage_greedy(seed_photo, photo_database, target_size=(10, 10)):\n    \"\"\"\n    Build collage by iteratively adding photos to best-matching edges.\n    \"\"\"\n    # 1. SEED SELECTION\n    canvas = Canvas(target_size)\n    canvas.place_photo(seed_photo, position='center', locked=True)\n\n    # Priority queue of open edges (scored by \"urgency\")\n    open_edges = PriorityQueue()\n    for edge in seed_photo.edges:\n        urgency = compute_edge_urgency(edge)\n        open_edges.push(edge, priority=urgency)\n\n    # 2. ITERATIVE GROWTH\n    while canvas.coverage < 0.8 and not open_edges.empty():\n        current_edge = open_edges.pop()\n\n        # Query k best matches from database\n        candidates = photo_database.find_compatible_edges(\n            query_edge=current_edge,\n            k=20,\n            filters={\n                'aspect_ratio': current_edge.compatible_aspect_ratios,\n                'min_compatibility': 0.4\n            }\n        )\n\n        # Try candidates in order of compatibility\n        placed = False\n        for candidate_photo in candidates:\n            if canvas.would_overlap(candidate_photo):\n                continue\n\n            local_fit = edge_compatibility(current_edge, candidate_photo.opposite_edge)\n            global_aesthetics = canvas.score_global_aesthetics_with(candidate_photo)\n\n            if local_fit > 0.5 and global_aesthetics > 0.6:\n                canvas.place_photo(candidate_photo, adjacent_to=current_edge)\n\n                for new_edge in candidate_photo.new_open_edges:\n                    urgency = compute_edge_urgency(new_edge)\n                    open_edges.push(new_edge, priority=urgency)\n\n                placed = True\n                break\n\n        if not placed:\n            current_edge.relaxed = True\n            open_edges.push(current_edge, priority=0.5)\n\n    # 3. BOUNDARY REFINEMENT\n    canvas.refine_boundaries(\n        crop_for_alignment=True,\n        blend_overlaps=True,\n        inpaint_gaps=True,\n        color_grade_globally=True\n    )\n\n    return canvas.render()\n```\n\n---\n\n## Edge Urgency Heuristic\n\n```python\ndef compute_edge_urgency(edge):\n    \"\"\"\n    Determine which edges should be filled first.\n    Higher urgency = fill sooner\n    \"\"\"\n    urgency = 0.0\n\n    # Strong lines → high urgency (want to continue them)\n    if edge.has_strong_lines():\n        urgency += 0.5\n\n    # Cut-off objects → very high urgency (want completion)\n    if edge.wants_continuation:\n        urgency += 0.7\n\n    # High aesthetic quality → high urgency\n    urgency += edge.photo.aesthetic_score * 0.3\n\n    # Central position → higher urgency (build from center out)\n    distance_from_center = edge.distance_to_canvas_center()\n    urgency += (1.0 - distance_from_center) * 0.2\n\n    return urgency\n```\n\n---\n\n## Practical Optimizations\n\n### 1. Hierarchical Clustering\n\n**Concept**: Group similar photos into clusters, search within clusters first.\n\n```python\nclass PhotoDatabase:\n    def __init__(self, photos):\n        self.clusters = self._cluster_photos_hierarchically(photos)\n\n    def _cluster_photos_hierarchically(self, photos):\n        \"\"\"\n        Group photos into ~50-100 clusters using CLIP embeddings.\n        Benefits: 50x speedup in matching\n        \"\"\"\n        embeddings = np.array([p.clip_embedding for p in photos])\n\n        from sklearn.cluster import AgglomerativeClustering\n        clustering = AgglomerativeClustering(\n            n_clusters=min(100, len(photos) // 100),\n            metric='cosine',\n            linkage='average'\n        )\n        labels = clustering.fit_predict(embeddings)\n\n        clusters = {}\n        for photo, label in zip(photos, labels):\n            clusters.setdefault(label, []).append(photo)\n\n        return clusters\n```\n\n### 2. Multi-Scale Matching\n\n```python\ndef find_matches_multiscale(query_edge, database):\n    \"\"\"\n    Progressive refinement: fast coarse search, slow precise refinement.\n\n    Total: 50ms instead of 500ms for all-full-res\n    \"\"\"\n    # Stage 1: Coarse search on thumbnails\n    candidates_coarse = database.search_thumbnails(\n        query_edge.thumbnail_embedding, k=100\n    )\n\n    # Stage 2: Geometric filtering\n    candidates_filtered = [\n        c for c in candidates_coarse\n        if abs(c.dominant_angle - query_edge.dominant_angle) < 30\n    ]\n\n    # Stage 3: Full-resolution scoring (top 20 only)\n    candidates_scored = []\n    for c in candidates_filtered[:20]:\n        score = edge_compatibility_fullres(query_edge, c)\n        candidates_scored.append((score, c))\n\n    candidates_scored.sort(reverse=True, key=lambda x: x[0])\n    return [c for score, c in candidates_scored[:10]]\n```\n\n### 3. Caching Good Pairs\n\n```python\nclass PairCache:\n    \"\"\"Learn from experience: which edges work well together?\"\"\"\n    def __init__(self):\n        self.successful_pairs = {}\n        self.usage_counts = {}\n\n    def record_success(self, edge1, edge2, score):\n        pair_key = (edge1.id, edge2.id)\n        self.successful_pairs[pair_key] = score\n        self.usage_counts[pair_key] = self.usage_counts.get(pair_key, 0) + 1\n\n    def boost_known_pairs(self, candidates, query_edge):\n        for c in candidates:\n            pair_key = (query_edge.id, c.edge_id)\n            if pair_key in self.successful_pairs:\n                boost = self.successful_pairs[pair_key] * 0.2\n                boost += np.log1p(self.usage_counts[pair_key]) * 0.1\n                c.score += boost\n        return sorted(candidates, key=lambda c: c.score, reverse=True)\n```\n\n### 4. Pruning Generic Edges\n\n```python\ndef is_edge_generic(edge):\n    \"\"\"\n    Generic edges (plain sky, solid colors) don't need expensive matching.\n    \"\"\"\n    if edge.complexity < 0.2 and edge.blendability > 0.8:\n        if len(edge.lines) < 2 and len(edge.colors.colors) <= 2:\n            return True\n    return False\n```\n\n### 5. Backtracking\n\n```python\ndef assemble_with_backtracking(seed, database, target_size):\n    \"\"\"Greedy growth with backtracking for difficult cases.\"\"\"\n    canvas = Canvas(target_size)\n    canvas.place_photo(seed, position='center')\n\n    history = []\n    max_backtracks = 5\n\n    while canvas.coverage < 0.8:\n        edge = canvas.best_open_edge()\n        candidates = database.find_compatible_edges(edge, k=20)\n\n        placed = False\n        for candidate in candidates:\n            if canvas.can_place(candidate):\n                canvas.place_photo(candidate, adjacent_to=edge)\n                history.append((candidate, edge))\n                placed = True\n                break\n\n        if not placed and len(history) > 0 and max_backtracks > 0:\n            canvas.undo(history.pop())\n            canvas.undo(history.pop())\n            max_backtracks -= 1\n            continue\n\n        if not placed:\n            edge.mark_skipped()\n\n    return canvas\n```\n\n---\n\n## Performance Impact\n\n| Optimization | Speedup |\n|--------------|---------|\n| Hierarchical clustering | **50x** |\n| Multi-scale matching | **10x** |\n| Caching | **1.5x** |\n| Pruning | **2-3x** |\n| Backtracking | Quality improvement |\n\n**Combined**: 10-photo collage in **0.5-2 seconds** instead of 50-200 seconds.\n"
        },
        {
          "name": "hockney-technique.md",
          "type": "file",
          "path": "collage-layout-expert/references/hockney-technique.md",
          "size": 4826,
          "content": "# David Hockney's Joiners Technique (1982-1985)\n\n## Historical Context\n\n**Origins (1982)**:\n- Curator Alain Sayag invited Hockney to Centre Pompidou (Paris) photography exhibition\n- Breakthrough: Overcome photography's limitation of single perspective + frozen moment\n- Started with **Polaroid instant prints**, creating grid-like compositions\n- Later evolved to **35mm commercially processed prints** with organic shapes\n\n## Technique Characteristics\n\n### Phase 1 - Grid Joiners (1982)\n\n```\n┌─────┬─────┬─────┐\n│ POL │ POL │ POL │  ← Polaroid grid\n├─────┼─────┼─────┤    Multiple viewpoints\n│ POL │ POL │ POL │    Slight overlaps (~5-15%)\n├─────┼─────┼─────┤    Subtle misalignments\n│ POL │ POL │ POL │    Capturing time + space\n└─────┴─────┴─────┘\n```\n\n### Phase 2 - Organic Joiners (1984-1985)\n\n- Compositions \"took on a shape of their own\"\n- Less rigid structure, more painterly\n- Influenced by Cubist paintings (Picasso, Braque)\n- Intentional rotation variance (±2-3°)\n- Grid irregularity (~10-15% positional variance)\n\n## Artistic Intent\n\n**Hockney's Goal**: Create photographs with **\"perspectival sophistication of Cubist paintings\"**\n\n**Key Innovations**:\n1. Multiple perspectives simultaneously (vs. single camera viewpoint)\n2. Temporal dimension (same scene, different moments)\n3. Viewer's eye \"constructs\" the scene (active participation)\n4. Embraces imperfection (overlaps, gaps, misalignments)\n\n## Computational Implementation\n\n```python\nHOCKNEY_JOINER_STYLE = {\n    'overlap': 0.1,                    # 5-15% overlap between photos\n    'rotation_variance': 2.0,          # ±2° rotation per photo\n    'perspective_shift': True,         # Multiple viewpoints\n    'grid_irregularity': 0.15,         # 10-15% positional offset\n    'border_style': 'polaroid',        # White borders (optional)\n    'allow_gaps': True,                # Intentional negative space\n}\n```\n\n**Modern Interpretation**:\n- Hockney's manual Polaroid placement → Edge-based algorithmic assembly\n- Visual intuition → CLIP semantic matching + geometric compatibility\n- Trial-and-error → Greedy edge growth with intelligent optimizations\n- Days/weeks per piece → Seconds to minutes with GPU acceleration\n\n---\n\n## Art Historical References\n\n### Photographers and Artists to Study\n\n1. **David Hockney** (1937-present)\n   - **Joiners** series (1982-1985)\n   - Cubist-inspired multiple perspectives\n   - Polaroid and 35mm collages\n   - Key works: \"Pearblossom Hwy.\", \"The Scrabble Game\"\n\n2. **Robert Rauschenberg** (1925-2008)\n   - **Combines** (1950s-1960s)\n   - Mixed media: photos + painting + objects\n   - Layering and transparency\n   - Abstract + representational\n\n3. **Hannah Höch** (1889-1978)\n   - **Dada photomontage** (1920s)\n   - Cut-and-paste magazine photos\n   - Juxtapose unrelated subjects\n   - Political/social commentary\n\n4. **John Baldessari** (1931-2020)\n   - Conceptual photography\n   - Colored dots over faces\n   - Text + image combinations\n   - Systematic rules (all red objects, all circles)\n\n5. **Martha Rosler** (1943-present)\n   - Critical photomontage\n   - \"House Beautiful: Bringing the War Home\" series\n   - Political commentary through juxtaposition\n\n### Style Implementations\n\n```python\nARTISTIC_STYLES = {\n    'hockney_joiner': {\n        'layout': 'irregular_grid',\n        'overlap': (0.05, 0.15),\n        'rotation_variance': (-3, 3),\n        'scale_variance': (0.95, 1.05),\n        'perspective_shift': True,\n        'border': 'polaroid',  # White borders\n        'allow_gaps': True,\n    },\n\n    'rauschenberg_combine': {\n        'layout': 'layered',\n        'overlap': (0.2, 0.5),\n        'blend_modes': ['multiply', 'screen', 'overlay'],\n        'texture_overlay': True,\n        'abstract_elements': True,\n    },\n\n    'hoch_photomontage': {\n        'layout': 'chaotic',\n        'semantic_mismatch': True,  # Intentional surrealism\n        'sharp_cutouts': True,\n        'juxtaposition': 'unexpected',\n    },\n\n    'baldessari_conceptual': {\n        'layout': 'systematic',\n        'color_dots_on_faces': True,\n        'thematic_constraints': True,  # e.g., \"all blue objects\"\n        'text_overlay': True,\n    },\n}\n```\n\n### Contemporary Trends (2025)\n\n1. **Maximalist**\n   - Dense, abundant, ornate\n   - 15-30+ photos overlapping\n   - Nature horror vacui (fear of empty space)\n\n2. **Y2K Revival**\n   - Early 2000s aesthetic\n   - Glitchy effects, chromatic aberration\n   - Metallic, holographic elements\n\n3. **Nostalgic Analog**\n   - Film grain, light leaks\n   - Vintage color grading\n   - Polaroid borders, film strip edges\n\n4. **Brutalist**\n   - Raw, unpolished\n   - Exposed grid structures\n   - Monochrome, high contrast\n"
        },
        {
          "name": "implementation-guide.md",
          "type": "file",
          "path": "collage-layout-expert/references/implementation-guide.md",
          "size": 5938,
          "content": "# Practical Implementation Guide\n\n## Metal Shader Pipeline\n\n### 1. Edge Extraction\n\n```metal\nkernel void extract_edge_region(\n    texture2d<float, access::read> image [[texture(0)]],\n    texture2d<float, access::write> edge_region [[texture(1)]],\n    constant EdgeParams& params [[buffer(0)]],\n    uint2 gid [[thread_position_in_grid]]\n) {\n    // Extract 10% strip along specified edge\n    // ...\n}\n```\n\n### 2. Line Detection (EDLines on GPU)\n\n```metal\n// Multi-pass: gradient → edge chains → line fitting\nkernel void compute_gradients(...);\nkernel void extract_edge_chains(...);\nkernel void fit_line_segments(...);\n```\n\n### 3. Color Histogram\n\n```metal\nkernel void build_lab_histogram(\n    texture2d<float, access::read> lab_image [[texture(0)]],\n    device atomic_uint* histogram [[buffer(0)]],\n    uint2 gid [[thread_position_in_grid]]\n) {\n    float3 lab = lab_image.read(gid).rgb;\n\n    // Quantize to bins (8×8×8)\n    uint l_bin = uint(lab.x / 100.0 * 8.0);\n    uint a_bin = uint((lab.y + 128.0) / 256.0 * 8.0);\n    uint b_bin = uint((lab.z + 128.0) / 256.0 * 8.0);\n\n    uint bin_index = l_bin * 64 + a_bin * 8 + b_bin;\n    atomic_fetch_add_explicit(&histogram[bin_index], 1, memory_order_relaxed);\n}\n```\n\n### 4. Poisson Blending\n\n```metal\nkernel void poisson_jacobi_iteration(...);  // 50 iterations\n```\n\n---\n\n## Core ML Integration\n\n### Models Needed\n\n1. **MobileSAM** (segmentation) - 5M params\n2. **CLIP ViT-B/32** (embeddings) - 150M params\n3. **MediaPipe Pose** (gesture detection) - 3M params\n\n### Conversion\n\n```python\nimport coremltools as ct\n\n# Convert PyTorch → Core ML\ntraced_model = torch.jit.trace(model, example_input)\nmlmodel = ct.convert(traced_model, inputs=[...])\nmlmodel.save(\"model.mlpackage\")\n```\n\n---\n\n## Database Indexing\n\n### HNSW for CLIP embeddings\n\n```python\nimport hnswlib\n\n# Initialize index\ndim = 512  # CLIP dimension\nindex = hnswlib.Index(space='cosine', dim=dim)\nindex.init_index(max_elements=10000, ef_construction=200, M=16)\n\n# Add embeddings\nfor i, embedding in enumerate(clip_embeddings):\n    index.add_items(embedding, i)\n\n# Query\nk = 50\nlabels, distances = index.knn_query(query_embedding, k=k)\n```\n\n---\n\n## Performance Targets\n\n| Operation | Mac M2 | iPhone 15 Pro |\n|-----------|--------|---------------|\n| SAM segmentation (1024×1024) | 0.5s | 2s |\n| Edge extraction (100 shards) | 1s | 3s |\n| Line detection (EDLines, per photo) | 10ms | 20ms |\n| k-NN search (10k database) | &lt;10ms | &lt;50ms |\n| Greedy assembly (10-photo collage) | 0.5s | 2s |\n| Poisson blending (100 junctions) | 2s | 6s |\n\n---\n\n## Memory Management\n\n### Texture Compression\n\n```swift\nlet descriptor = MTLTextureDescriptor()\ndescriptor.pixelFormat = .bc7_rgbaUnorm  // 6:1 compression\n```\n\n### Lazy Loading\n\n```swift\n// Store only feature vectors in memory\n// Load textures on-demand from disk\nclass ShardDatabase {\n    var features: [UUID: ShardFeatures]  // In memory\n    var texturePaths: [UUID: URL]        // On disk\n\n    func loadTexture(id: UUID) -> MTLTexture {\n        // Load PNG from disk when needed\n    }\n}\n```\n\n---\n\n## Algorithm Selection Guide\n\n### Line Detection\n\n| Context | Recommended |\n|---------|-------------|\n| Interactive generation | EDLines |\n| Final high-res render | LSD |\n| Teaching / legacy code | Hough |\n| Deep learning pipeline | LETR |\n| Mobile real-time | EDLines |\n\n### Layout Strategy\n\n- **Greedy Edge Growth** (MVP, Phase 4): Primary algorithm\n- **Hierarchical Clustering**: Essential optimization (50x speedup)\n- **Multi-Scale Matching**: Progressive refinement (10x speedup)\n- **Simulated Annealing** (Phase 6): Optional refinement\n- **Hockney Joiner Style**: User explicitly requests\n\n### Color Harmonization\n\n- **Optimal Transport**: Always use (mathematically principled)\n- **Affine Approximation**: Real-time preview (fast)\n- **Full Sinkhorn**: Final render (accurate)\n\n### Blending\n\n- **Poisson**: Seamless photographic junctions\n- **Alpha Feathering**: Simple overlaps, soft edges\n- **Diffusion Inpainting**: Poor-quality junctions (expensive)\n\n---\n\n## Error Handling\n\n```python\ndef place_shard_safe(canvas, shard, position):\n    if canvas.would_overlap(shard, position):\n        raise PlacementError(\"Overlap detected\")\n\n    if canvas.is_out_of_bounds(shard, position):\n        raise PlacementError(\"Out of bounds\")\n\n    compatibility = canvas.check_neighbor_compatibility(shard, position)\n    if compatibility < 0.3:\n        logger.warning(f\"Low compatibility: {compatibility:.2f}\")\n\n    canvas.place(shard, position)\n```\n\n---\n\n## Testing Strategies\n\n```python\ndef test_edge_alignment():\n    \"\"\"Verify lines align across boundaries.\"\"\"\n    photo1 = load_test_photo(\"horizon_left.jpg\")\n    photo2 = load_test_photo(\"horizon_right.jpg\")\n\n    edge1 = extract_edge_descriptor(photo1, 'right')\n    edge2 = extract_edge_descriptor(photo2, 'left')\n\n    assert len(edge1.lines) >= 1\n    assert len(edge2.lines) >= 1\n\n    angle_diff = abs(edge1.dominant_angle - edge2.dominant_angle)\n    assert angle_diff < 5.0\n\ndef test_hockney_style():\n    \"\"\"Verify Hockney characteristics are present.\"\"\"\n    collage = create_collage(photos, style='hockney_joiner')\n\n    positions = [s.position for s in collage.shards]\n    irregularity = compute_grid_irregularity(positions)\n    assert 0.1 < irregularity < 0.2\n\n    rotations = [s.rotation for s in collage.shards]\n    assert np.std(rotations) > 1.0\n\n    overlaps = count_overlaps(collage)\n    assert overlaps > 0\n```\n\n---\n\n## Python Dependencies\n\n```bash\npip install opencv-python numpy scipy scikit-image transformers pot hnswlib\n```\n\n| Package | Purpose |\n|---------|---------|\n| `opencv-python` | Line detection (EDLines, LSD), image processing |\n| `numpy` | Numerical computing, matrix operations |\n| `scipy` | Optimization, spatial algorithms |\n| `scikit-image` | Image processing, Poisson blending |\n| `transformers` | CLIP embeddings |\n| `pot` | Optimal transport (Wasserstein distance) |\n| `hnswlib` | Fast k-NN search |\n"
        },
        {
          "name": "line-detection.md",
          "type": "file",
          "path": "collage-layout-expert/references/line-detection.md",
          "size": 3455,
          "content": "# Line Detection Algorithms (State of the Art)\n\n## Algorithm Comparison (2025)\n\n| Algorithm | Speed vs LSD | Accuracy | Real-time? | Use Case |\n|-----------|-------------|----------|------------|----------|\n| **Hough Transform** | 0.1x | Good | No | Traditional, needs Canny preprocessing |\n| **LSD** | 1x (baseline) | Excellent | Borderline | Baseline for modern methods |\n| **EDLines** | **10-11x** | Excellent | **Yes** | **Recommended for your projects** |\n| **LB-LSD** | 8x | Good | Yes | Length-based optimization |\n| **LETR** (Transformer) | 0.5x | Excellent | No | Deep learning, GPU-heavy |\n\n## EDLines: Optimal Choice for Collage Assembly\n\n**Why EDLines for Edge-Based Collage Assembly**:\n\n1. **Speed**: 10x faster than LSD (critical for interactive generation)\n2. **Accuracy**: Produces precise line segments with false detection control\n3. **No parameter tuning**: Works out-of-box (vs. Hough's many parameters)\n4. **Edge-based**: Aligns perfectly with \"edge-first assembly\" approach\n5. **Real-time**: Suitable for live preview as users adjust parameters\n\n### EDLines Algorithm Overview\n\n```\n1. Edge Detection (Edge Drawing algorithm)\n   - Fast gradient-based edge extraction\n   - Produces clean edge chains (not noisy pixel maps)\n\n2. Line Segment Fitting\n   - Fit line segments to edge chains\n   - Use least-squares fitting with error threshold\n   - Validate line segments (reject false detections)\n\n3. Output\n   - List of line segments: [(x1, y1, x2, y2, angle, length, strength), ...]\n   - Angle in degrees (-90 to 90)\n   - Strength from gradient magnitude\n```\n\n### Performance Benchmarks\n\n- **1024×1024 image**: ~10-15ms on M2 GPU\n- **4K image**: ~40-50ms on M2 GPU\n- **iPhone 15 Pro**: ~20-30ms (1024×1024)\n\n---\n\n## LSD (Line Segment Detector)\n\n**Use when**: You need maximum accuracy over speed (e.g., final high-res render)\n\n**Characteristics**:\n- Gradient grouping approach\n- Built-in false detection control (Helmholtz principle)\n- Parameter-free (adaptive thresholds)\n- Produces sub-pixel accurate line segments\n\n### Implementation\n\n```python\nimport cv2\n\n# OpenCV includes LSD\nlsd = cv2.createLineSegmentDetector(0)  # 0 = LSD_REFINE_NONE\nlines, width, prec, nfa = lsd.detect(gray_image)\n\n# lines: Nx1x4 array of [x1, y1, x2, y2]\n# width: line widths\n# nfa: Number of False Alarms (lower = more confident)\n```\n\n---\n\n## Hough Transform\n\n**Use when**: Detecting specific geometric patterns (circles, ellipses) or teaching/legacy contexts\n\n### Classical Hough\n\n```python\nimport cv2\nimport numpy as np\n\n# 1. Preprocess: Edge detection\nedges = cv2.Canny(gray_image, 50, 150)\n\n# 2. Hough Transform\nlines = cv2.HoughLines(edges, rho=1, theta=np.pi/180, threshold=100)\n\n# 3. Convert from (ρ, θ) to (x1, y1, x2, y2)\nfor rho, theta in lines:\n    a, b = np.cos(theta), np.sin(theta)\n    x0, y0 = a * rho, b * rho\n    x1 = int(x0 + 1000 * (-b))\n    y1 = int(y0 + 1000 * (a))\n    x2 = int(x0 - 1000 * (-b))\n    y2 = int(y0 - 1000 * (a))\n```\n\n### Probabilistic Hough (faster variant)\n\n```python\nlines = cv2.HoughLinesP(\n    edges,\n    rho=1,\n    theta=np.pi/180,\n    threshold=50,\n    minLineLength=30,\n    maxLineGap=10\n)\n# Returns line segments directly: [(x1, y1, x2, y2), ...]\n```\n\n---\n\n## When to Use Each Algorithm\n\n| Context | Recommended |\n|---------|-------------|\n| Interactive generation | EDLines |\n| Final high-res render | LSD |\n| Teaching / legacy code | Hough |\n| Deep learning pipeline | LETR |\n| Mobile real-time | EDLines |\n"
        },
        {
          "name": "mathematical-foundations.md",
          "type": "file",
          "path": "collage-layout-expert/references/mathematical-foundations.md",
          "size": 9655,
          "content": "# Mathematical Foundations\n\n## Optimal Transport for Color Harmonization\n\n**Problem**: Harmonize shard colors with global palette without destroying local structure.\n\n### Wasserstein Distance (Earth Mover's Distance)\n\n```\nW₂(μ, ν)² = inf{γ ∈ Π(μ,ν)} ∫∫ ‖x - y‖² dγ(x,y)\n```\n\nWhere:\n- μ = shard's color distribution (LAB histogram)\n- ν = target/global distribution\n- γ = transport plan (how to move color mass)\n\n### Sinkhorn Algorithm (entropy-regularized)\n\n```python\ndef sinkhorn_optimal_transport(source_hist, target_hist, epsilon=0.1, max_iters=100):\n    \"\"\"\n    Compute optimal transport plan using Sinkhorn iterations.\n    epsilon: regularization strength (smaller = closer to true OT)\n    \"\"\"\n    # Cost matrix: squared distances in LAB space\n    C = compute_cost_matrix_lab(source_hist.bins, target_hist.bins)\n\n    # Kernel matrix\n    K = np.exp(-C / epsilon)\n\n    # Initialize\n    u = np.ones(len(source_hist))\n    v = np.ones(len(target_hist))\n\n    # Iterate (converges exponentially fast)\n    for _ in range(max_iters):\n        u = source_hist.weights / (K @ v)\n        v = target_hist.weights / (K.T @ u)\n\n    # Optimal transport plan\n    gamma = np.diag(u) @ K @ np.diag(v)\n\n    return gamma  # gamma[i,j] = mass to move from bin i to bin j\n```\n\n### Affine Approximation (for real-time)\n\n```python\ndef fit_affine_color_transform(source_hist, target_hist):\n    \"\"\"\n    Approximate optimal transport as affine transform in LAB space.\n    Returns: (M, b) where transformed_color = M @ color + b\n    \"\"\"\n    # 1. Compute OT plan\n    gamma = sinkhorn_optimal_transport(source_hist, target_hist)\n\n    # 2. Sample points from distributions\n    source_samples = source_hist.sample(n=256)\n    target_samples = target_hist.sample(n=256)\n\n    # 3. Weighted least squares\n    X = source_samples  # Nx3 (L, a, b)\n    Y = target_samples  # Mx3\n\n    M = (Y.T @ gamma @ X.T) @ np.linalg.inv(X.T @ gamma.T @ X)\n    b = target_hist.mean() - M @ source_hist.mean()\n\n    return M, b\n```\n\n### Why LAB Space\n\n- **Perceptually uniform**: Euclidean distance ≈ perceived color difference\n- **Separates luminance from chrominance**: L (lightness), a (green-red), b (blue-yellow)\n- **Better blending**: Avoids hue shifts that occur in RGB\n\n---\n\n## Poisson Blending for Seamless Junctions\n\n**Problem**: Blend overlapping halos without visible seams.\n\n### Poisson Equation\n\n```\n∇²f = div(g)  in Ω\nf = T         on ∂Ω\n```\n\nWhere:\n- f = unknown blended image\n- g = guidance field (gradients from source images)\n- Ω = blend region (halo intersection)\n- ∂Ω = boundary (fixed to target values)\n\n### Discrete Form (pixel grid)\n\n```python\n# For each interior pixel (i, j):\n4·f[i,j] - f[i-1,j] - f[i+1,j] - f[i,j-1] - f[i,j+1] = div(g)[i,j]\n```\n\n### Jacobi Iteration Solver\n\n```python\ndef poisson_blend_jacobi(source, target, mask, max_iters=50):\n    \"\"\"\n    Solve Poisson equation using Jacobi iteration.\n    Perfect for GPU parallelization (Metal shader).\n    \"\"\"\n    # Compute guidance field (source gradients)\n    gx = np.gradient(source, axis=1)\n    gy = np.gradient(source, axis=0)\n\n    # Divergence of guidance field\n    div_g = np.gradient(gx, axis=1) + np.gradient(gy, axis=0)\n\n    # Initialize solution with target\n    f = target.copy()\n    f_new = f.copy()\n\n    # Iterate\n    for iteration in range(max_iters):\n        for i in range(1, mask.shape[0] - 1):\n            for j in range(1, mask.shape[1] - 1):\n                if mask[i, j]:  # Interior pixel\n                    f_new[i, j] = 0.25 * (\n                        f[i-1, j] + f[i+1, j] +\n                        f[i, j-1] + f[i, j+1] +\n                        div_g[i, j]\n                    )\n                # else: boundary pixel, keep f_new[i,j] = target[i,j]\n\n        f = f_new.copy()\n\n    return f\n```\n\n### Metal Implementation (GPU acceleration)\n\n```metal\nkernel void poisson_jacobi_step(\n    texture2d<float, access::read> f_current [[texture(0)]],\n    texture2d<float, access::read> divergence [[texture(1)]],\n    texture2d<float, access::write> f_next [[texture(2)]],\n    texture2d<uint, access::read> mask [[texture(3)]],\n    uint2 gid [[thread_position_in_grid]]\n) {\n    if (mask.read(gid).r == 0) {\n        // Boundary: keep original\n        f_next.write(f_current.read(gid), gid);\n        return;\n    }\n\n    // Interior: Jacobi update\n    float left  = f_current.read(gid + uint2(-1,  0)).r;\n    float right = f_current.read(gid + uint2( 1,  0)).r;\n    float down  = f_current.read(gid + uint2( 0, -1)).r;\n    float up    = f_current.read(gid + uint2( 0,  1)).r;\n    float div   = divergence.read(gid).r;\n\n    float f_new = 0.25 * (left + right + down + up + div);\n\n    f_next.write(float4(f_new, 0, 0, 0), gid);\n}\n```\n\n**Performance**: ~20ms for 512×512 image on M2 GPU (50 iterations)\n\n---\n\n## Energy Function for Composition Optimization\n\n### Total Energy\n\n```\nE(C) = α·E_semantic(C) + β·E_geometric(C) + γ·E_aesthetic(C)\n```\n\n### 1. Semantic Energy (CLIP similarity)\n\n```python\ndef compute_semantic_energy(canvas):\n    \"\"\"Reward semantically coherent adjacencies.\"\"\"\n    energy = 0.0\n\n    for (i, j) in canvas.adjacent_pairs():\n        similarity = cosine_similarity(\n            canvas.shards[i].clip_embedding,\n            canvas.shards[j].clip_embedding\n        )\n        energy -= similarity  # Negative: higher similarity → lower energy\n\n    return energy / len(canvas.adjacent_pairs())\n```\n\n### 2. Geometric Energy (boundary compatibility)\n\n```python\ndef compute_geometric_energy(canvas):\n    \"\"\"Penalize geometric incompatibilities at junctions.\"\"\"\n    energy = 0.0\n\n    for (i, j) in canvas.adjacent_pairs():\n        # Tangent angle mismatch\n        angle_diff = abs(canvas.tangent_angle[i] - canvas.tangent_angle[j])\n        angle_diff = min(angle_diff, 180 - angle_diff)\n        energy += (angle_diff / 180.0) ** 2\n\n        # Curvature mismatch\n        curv_diff = abs(canvas.curvature[i] - canvas.curvature[j])\n        energy += curv_diff ** 2\n\n    return energy / len(canvas.adjacent_pairs())\n```\n\n### 3. Aesthetic Energy (composition principles)\n\n```python\ndef compute_aesthetic_energy(canvas):\n    \"\"\"Classical aesthetic principles.\"\"\"\n\n    # Balance: visual weight distribution\n    weights = [compute_visual_weight(s) for s in canvas.shards]\n    quadrants = canvas.divide_into_quadrants()\n    quadrant_weights = [sum(weights[s] for s in q) for q in quadrants]\n    balance = np.var(quadrant_weights)\n\n    # Symmetry\n    symmetry = compute_symmetry(canvas)\n\n    # Density variance\n    density_grid = canvas.compute_density_grid(grid_size=10)\n    density_variance = np.var(density_grid)\n\n    # Rule of thirds\n    thirds_score = compute_rule_of_thirds_score(canvas)\n\n    return (\n        0.3 * balance +\n        0.2 * (1 - symmetry) +\n        0.3 * density_variance +\n        0.2 * (1 - thirds_score)\n    )\n```\n\n### Typical Weight Values\n\n- **α = 1.0**: Semantic coherence is primary\n- **β = 0.5**: Geometry important but secondary\n- **γ = 0.3**: Aesthetics are subtle refinements\n\n### User Modes\n\n- **\"Coherent\"**: α=1.5, β=0.8, γ=0.2 (prioritize meaning)\n- **\"Balanced\"**: α=1.0, β=0.5, γ=0.3 (default)\n- **\"Chaotic\"**: α=0.2, β=0.1, γ=0.7 (prioritize aesthetics, allow surprises)\n\n---\n\n## Aesthetic Principles\n\n### Rule of Thirds\n\n```python\ndef compute_rule_of_thirds_score(canvas):\n    \"\"\"Score how well composition follows rule of thirds.\"\"\"\n    thirds_points = [\n        (1/3, 1/3), (1/3, 2/3),\n        (2/3, 1/3), (2/3, 2/3)\n    ]\n\n    salient_shards = [s for s in canvas.shards if s.salience > 0.7]\n\n    if not salient_shards:\n        return 0.5\n\n    scores = []\n    for shard in salient_shards:\n        center = shard.center_normalized()\n        distances = [\n            np.linalg.norm(np.array(center) - np.array(tp))\n            for tp in thirds_points\n        ]\n        min_distance = min(distances)\n        score = max(0.0, 1.0 - min_distance / 0.5)\n        scores.append(score * shard.salience)\n\n    return np.mean(scores)\n```\n\n### Visual Weight\n\n```python\ndef compute_visual_weight(shard):\n    \"\"\"\n    Visual weight considers:\n    - Area (larger = heavier)\n    - Contrast (higher contrast = heavier)\n    - Color saturation (vibrant = heavier)\n    - Semantic importance (faces = heavier)\n    \"\"\"\n    weight = shard.area / 10000.0\n    weight *= (1 + shard.contrast)\n    weight *= (1 + shard.saturation)\n\n    if shard.contains_face:\n        weight *= 1.5\n\n    return weight\n```\n\n### Balance Score\n\n```python\ndef compute_balance(canvas):\n    \"\"\"Check if visual weight is distributed evenly.\"\"\"\n    quadrants = canvas.divide_into_quadrants()\n    weights = [\n        sum(compute_visual_weight(s) for s in q)\n        for q in quadrants\n    ]\n\n    variance = np.var(weights)\n    return max(0.0, 1.0 - variance / 10.0)\n```\n\n### Golden Ratio\n\n```python\ndef check_golden_ratio(canvas):\n    \"\"\"Bonus if composition exhibits φ ≈ 1.618 proportions.\"\"\"\n    phi = (1 + np.sqrt(5)) / 2  # 1.618...\n\n    aspect_ratio = canvas.width / canvas.height\n    aspect_score = np.exp(-abs(aspect_ratio - phi))\n\n    return aspect_score\n```\n\n### Negative Space Quality\n\n```python\ndef compute_negative_space_quality(canvas):\n    \"\"\"\n    High-quality negative space:\n    - Exists (at least 20% of canvas)\n    - Is simple/clean (low variance)\n    - Is strategically placed\n    \"\"\"\n    coverage = canvas.compute_coverage()\n    negative_ratio = 1.0 - coverage\n\n    if negative_ratio < 0.2:\n        return 0.0  # Too crowded\n    if negative_ratio > 0.6:\n        return 0.0  # Too sparse\n\n    negative_regions = canvas.extract_negative_space()\n    simplicity = 1.0 - np.mean([np.var(r) for r in negative_regions])\n\n    return simplicity\n```\n"
        }
      ]
    },
    {
      "name": "CHANGELOG.md",
      "type": "file",
      "path": "collage-layout-expert/CHANGELOG.md",
      "size": 4510,
      "content": "# Changelog\n\nAll notable changes to the collage-layout-expert skill will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [2.0.0] - 2025-11-26\n\n### Changed\n- **BREAKING**: Refactored from single 1761-line file to modular structure\n- Reduced SKILL.md from 1761 lines to 287 lines (84% reduction)\n- Moved detailed implementations to `/references/` directory\n- Updated frontmatter from custom YAML to standard `allowed-tools` format\n- Simplified description with proper NOT clause and activation keywords\n\n### Added\n- **When to Use This Skill** section with clear scope boundaries\n- **Do NOT use for** section with skill alternatives\n- **MCP Integrations** section (Firecrawl, Stability AI)\n- Created `/references/hockney-technique.md`:\n  - David Hockney's joiners technique (1982-1985)\n  - Historical context and artistic intent\n  - Computational implementation parameters\n  - Art historical references (Rauschenberg, Höch, Baldessari, Rosler)\n  - Style implementations dictionary\n  - Contemporary trends (2025)\n- Created `/references/line-detection.md`:\n  - Algorithm comparison table (EDLines, LSD, Hough, LB-LSD, LETR)\n  - EDLines algorithm overview and performance benchmarks\n  - LSD and Hough implementation examples\n  - When to use each algorithm guide\n- Created `/references/edge-assembly.md`:\n  - EdgeDescriptor dataclass\n  - Edge compatibility scoring function\n  - Angle and position alignment algorithms\n  - Greedy edge growth algorithm\n  - Edge urgency heuristics\n  - Performance optimizations:\n    - Hierarchical clustering (50x speedup)\n    - Multi-scale matching (10x speedup)\n    - Caching good pairs (1.5x speedup)\n    - Pruning generic edges (2-3x speedup)\n    - Backtracking strategy\n- Created `/references/mathematical-foundations.md`:\n  - Optimal transport for color harmonization\n  - Wasserstein distance and Sinkhorn algorithm\n  - Affine approximation for real-time\n  - LAB color space rationale\n  - Poisson blending for seamless junctions\n  - Jacobi iteration solver (Python and Metal)\n  - Energy function formulation\n  - Semantic, geometric, and aesthetic energy components\n  - User mode presets (Coherent, Balanced, Chaotic)\n  - Aesthetic principles (Rule of thirds, visual weight, balance, golden ratio, negative space)\n- Created `/references/advanced-techniques.md`:\n  - Cross-photo interactions (gesture-response, pointing, gaze, passing)\n  - InteractionDetector class\n  - Negative space awareness and matching\n  - Multi-layer compositing\n  - Narrative sequences (journey, day_in_life, emotion_arc)\n  - Simulated annealing for photo swapping\n  - Genetic algorithms concept\n  - CSP formulation concept\n- Created `/references/implementation-guide.md`:\n  - Metal shader pipeline (edge extraction, line detection, histogram, Poisson)\n  - Core ML integration (MobileSAM, CLIP, MediaPipe)\n  - HNSW database indexing\n  - Performance targets table (Mac M2, iPhone 15 Pro)\n  - Memory management strategies\n  - Algorithm selection guide\n  - Error handling patterns\n  - Testing strategies\n  - Python dependencies\n\n### Removed\n- Custom YAML frontmatter format (tools, triggers, integrates_with, python_dependencies)\n- 1500+ lines of detailed implementations (moved to references)\n\n### Improved\n- Progressive disclosure: essential concepts in SKILL.md, full code in references\n- Quick reference tables for algorithms and performance\n- Cross-references to related skills (native-app-designer, clip-aware-embeddings, photo-composition-critic, color-theory-palette-harmony-expert)\n\n## [1.0.0] - 2024-XX-XX\n\n### Added\n- Initial collage-layout-expert skill\n- David Hockney's joiners technique documentation\n- Line detection algorithms (EDLines, LSD, Hough)\n- Edge-based assembly strategy with greedy growth\n- Advanced collage concepts:\n  - Cross-photo interactions\n  - Negative space awareness\n  - Multi-layer compositing\n  - Narrative sequences\n- Mathematical foundations:\n  - Optimal transport (Wasserstein, Sinkhorn)\n  - Poisson blending\n  - Energy function optimization\n- Aesthetic principles from art history\n- Art historical references (Hockney, Rauschenberg, Höch, Baldessari, Rosler)\n- Practical implementation guidance:\n  - Metal shader pipeline\n  - Core ML integration\n  - Database indexing (HNSW)\n- Performance optimization strategies\n- Common patterns and best practices\n- Advanced techniques (simulated annealing, genetic algorithms)\n"
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "collage-layout-expert/SKILL.md",
      "size": 6882,
      "content": "---\nname: collage-layout-expert\nversion: 2.0.0\ndescription: \"Expert in ALL computational collage composition: photo mosaics, grid layouts, scrapbook/journal styles, magazine editorial, vision boards, mood boards, social media collages, memory walls, abstract/generative arrangements, and art-historical techniques (Hockney joiners, Dadaist photomontage, Surrealist assemblage, Rauschenberg combines). Masters edge-based assembly, Poisson blending, optimal transport color harmonization, and aesthetic optimization. Activate on 'collage', 'photo mosaic', 'grid layout', 'scrapbook', 'vision board', 'mood board', 'photo wall', 'magazine layout', 'Hockney', 'joiner', 'photomontage'. NOT for simple image editing (use native-app-designer), generating new images (use Stability AI), single photo enhancement (use photo-composition-critic), or basic image similarity search (use clip-aware-embeddings).\"\nallowed-tools: Read,Write,Edit,Bash,mcp__stability-ai__stability-ai-generate-image,mcp__firecrawl__firecrawl_search,WebFetch\n---\n\n# Collage & Layout Composition Expert\n\nExpert in **ALL forms of computational collage composition** - from Instagram grids to Hockney joiners, from magazine layouts to generative art.\n\n## When to Use This Skill\n\n✅ **Use for:**\n- **Grid Collages**: Instagram profiles, regular layouts, tiled compositions\n- **Photo Mosaics**: Small images forming larger pictures\n- **Hockney-Style Joiners**: Multi-perspective photographic assemblies\n- **Scrapbook/Journal**: Mixed media with text, frames, embellishments\n- **Magazine/Editorial**: Professional layouts with text integration\n- **Vision/Mood Boards**: Inspiration collections, design references\n- **Memory Walls**: Scattered Polaroid-style arrangements\n- **Social Media**: Stories, carousel previews, profile grids\n- **Abstract/Generative**: Algorithmic and procedural arrangements\n- **Art-Historical**: Dadaist, Surrealist, Pop Art styles\n\n❌ **Do NOT use for:**\n- Simple image editing → **native-app-designer**\n- Generating new images → **Stability AI**\n- Single photo quality → **photo-composition-critic**\n- Image similarity search → **clip-aware-embeddings**\n- Color palette extraction → **color-theory-palette-harmony-expert**\n\n## Expert vs Novice Shibboleths\n\n| Topic | Novice | Expert |\n|-------|--------|--------|\n| **Layout** | \"Just arrange randomly\" | Visual weight, balance, golden ratio |\n| **Blending** | Hard edges or simple feather | Poisson blending preserves gradients |\n| **Color** | \"Match colors manually\" | Optimal transport; LAB space advantages |\n| **Composition** | Fills all space | Negative space as design element |\n| **Scale** | Same size for everything | Varies scale for hierarchy |\n| **Mosaic** | \"More tiles = better\" | Tile size vs. recognition tradeoff |\n| **Hockney** | \"Stitch seamlessly\" | Imperfection IS the technique |\n\n## Decision Tree: Choosing a Style\n\n**What's the purpose?**\n- Systematic display → **Grid Collage**\n- Artistic portrait from photos → **Photo Mosaic**\n- Personal memories → **Scrapbook** or **Memory Wall**\n- Design inspiration → **Mood Board**\n- Professional/publication → **Magazine Layout**\n- Social media → **Social Templates**\n- Art project → **Hockney/Dadaist/Surrealist**\n\n**What's the vibe?**\n- Clean, modern → Grid with tight gutters\n- Nostalgic, warm → Polaroid scatter, vintage frames\n- Edgy, disruptive → Dadaist sharp cuts\n- Dreamy, surreal → Seamless Poisson blending\n- Cubist, intellectual → Hockney joiners\n\n## Core Algorithms (Summary)\n\n| Algorithm | Use Case | Performance |\n|-----------|----------|-------------|\n| **Edge-Based Assembly** | Hockney joiners | 0.5s for 10 photos |\n| **Poisson Blending** | Seamless transitions | 20ms (512×512) |\n| **Optimal Transport** | Color harmonization | Real-time w/ affine approx |\n| **Force-Directed** | Organic scatter | 200ms (50 images) |\n| **K-d Tree Matching** | Photo mosaic tiles | 2s for 10k tiles |\n\n→ See `references/algorithms.md` for full implementations.\n\n## Anti-Patterns to Avoid\n\n### 1. Ignoring Visual Weight\n**What it looks like**: All images same size, random placement\n**Why it's wrong**: No focal point, viewer's eye wanders aimlessly\n**Fix**: Establish 60/30/10 hierarchy with one hero image\n\n### 2. Over-Saturating the Canvas\n**What it looks like**: Every pixel filled with image content\n**Why it's wrong**: Visual claustrophobia, no breathing room\n**Fix**: Use negative space intentionally (20-30% white space minimum)\n\n### 3. Linear FFT for Color Matching\n**What it looks like**: Poor perceptual color matches\n**Why it's wrong**: RGB is not perceptually uniform\n**Fix**: Use LAB color space for matching\n\n### 4. Seamless Hockney Joiners\n**What it looks like**: Perfectly stitched panorama\n**Why it's wrong**: Misses the entire point - multiple perspectives\n**Fix**: Embrace ±2° rotation variance, 5-15% overlap, intentional gaps\n\n### 5. Global Poisson Blending\n**What it looks like**: Entire image becomes washed out\n**Why it's wrong**: Destroys local contrast, looks fake\n**Fix**: Apply locally at seams only, preserve source gradients\n\n### 6. Reusing Mosaic Tiles\n**What it looks like**: Obvious repetition patterns in mosaic\n**Why it's wrong**: Human eye detects patterns immediately\n**Fix**: Track tile usage, penalize reuse, use larger tile library\n\n## MCP Integrations\n\n| MCP | Purpose |\n|-----|---------|\n| **Stability AI** | Generate backgrounds, textures, missing elements |\n| **Firecrawl** | Research techniques, algorithm papers, art history |\n| **WebFetch** | Fetch documentation, tutorials, design references |\n\n## Performance Targets\n\n| Operation | Mac M2 | iPhone 15 Pro |\n|-----------|--------|---------------|\n| Grid layout (20 photos) | <50ms | <100ms |\n| Photo mosaic (10k tiles) | 2s | 5s |\n| Force-directed (50 images) | 200ms | 500ms |\n| Poisson blending (512×512) | 20ms | 50ms |\n| Hockney assembly (10 photos) | 0.5s | 2s |\n\n## References\n\n→ `references/collage-types.md` - Grid, mosaic, scrapbook, magazine, social templates\n→ `references/art-historical-styles.md` - Hockney, Dadaist, Surrealist, Rauschenberg\n→ `references/algorithms.md` - Edge assembly, Poisson, optimal transport, force-directed\n→ `references/advanced-techniques.md` - Cross-photo interactions, narrative sequences\n→ `references/implementation-guide.md` - Metal shaders, Core ML, performance\n\n## Integrates With\n\n- **photo-composition-critic** - Assess individual photos before collaging\n- **color-theory-palette-harmony-expert** - Extract/match color palettes\n- **clip-aware-embeddings** - Semantic grouping of images\n- **native-app-designer** - Build collage creation UI\n- **metal-shader-expert** - GPU-accelerated blending/effects\n\n---\n\n**Remember**: Great collages tell stories through arrangement. Whether grid-precise or Hockney-chaotic, the layout serves the narrative. Master both the math and the art.\n"
    }
  ]
}