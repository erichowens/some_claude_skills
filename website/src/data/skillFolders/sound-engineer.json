{
  "name": "sound-engineer",
  "type": "folder",
  "path": "sound-engineer",
  "children": [
    {
      "name": "references",
      "type": "folder",
      "path": "sound-engineer/references",
      "children": [
        {
          "name": "implementations.md",
          "type": "file",
          "path": "sound-engineer/references/implementations.md",
          "size": 9357,
          "content": "# Sound Engineering Implementation Reference\n\nDetailed code implementations for spatial audio, procedural sound, and middleware integration.\n\n## HRTF Spatial Audio Engine\n\n```cpp\n#include <vector>\n#include <complex>\n#include <cmath>\n\nclass SpatialAudioEngine {\nprivate:\n    struct HRTF_IR {\n        std::vector<float> left_ear;   // 512 samples typical\n        std::vector<float> right_ear;\n        float azimuth;    // Horizontal angle (0-360°)\n        float elevation;  // Vertical angle (-90 to +90°)\n    };\n\n    std::vector<HRTF_IR> hrtf_database;\n    int sample_rate = 48000;\n\npublic:\n    void load_hrtf_database(const std::string& path) {\n        // Load MIT KEMAR database (1800 IRs typical)\n    }\n\n    void process_spatial_sound(\n        const std::vector<float>& mono_input,\n        glm::vec3 source_position,\n        glm::vec3 listener_position,\n        glm::vec3 listener_forward,\n        glm::vec3 listener_up,\n        std::vector<float>& output_left,\n        std::vector<float>& output_right)\n    {\n        glm::vec3 to_source = glm::normalize(source_position - listener_position);\n        glm::vec3 listener_right = glm::cross(listener_forward, listener_up);\n\n        float azimuth = std::atan2(\n            glm::dot(to_source, listener_right),\n            glm::dot(to_source, listener_forward)\n        ) * 180.0f / M_PI;\n\n        float elevation = std::asin(glm::dot(to_source, listener_up)) * 180.0f / M_PI;\n\n        HRTF_IR hrtf = find_closest_hrtf(azimuth, elevation);\n        convolve(mono_input, hrtf.left_ear, output_left);\n        convolve(mono_input, hrtf.right_ear, output_right);\n\n        float distance = glm::length(source_position - listener_position);\n        float attenuation = calculate_distance_attenuation(distance);\n\n        for (auto& s : output_left) s *= attenuation;\n        for (auto& s : output_right) s *= attenuation;\n    }\n\nprivate:\n    float calculate_distance_attenuation(float distance) {\n        float reference = 1.0f;\n        if (distance < reference) return 1.0f;\n        return reference / (reference + (distance - reference));\n    }\n};\n```\n\n## Ambisonic Encoder/Decoder\n\n```cpp\nclass AmbisonicEncoder {\npublic:\n    struct AmbisonicSignal {\n        std::vector<float> W;  // Omnidirectional\n        std::vector<float> X;  // Front-back\n        std::vector<float> Y;  // Left-right\n        std::vector<float> Z;  // Up-down\n    };\n\n    AmbisonicSignal encode(const std::vector<float>& mono, glm::vec3 direction) {\n        AmbisonicSignal out;\n        out.W.resize(mono.size());\n        out.X.resize(mono.size());\n        out.Y.resize(mono.size());\n        out.Z.resize(mono.size());\n\n        direction = glm::normalize(direction);\n        float w = 0.707f;\n\n        for (size_t i = 0; i < mono.size(); ++i) {\n            out.W[i] = mono[i] * w;\n            out.X[i] = mono[i] * direction.x;\n            out.Y[i] = mono[i] * direction.y;\n            out.Z[i] = mono[i] * direction.z;\n        }\n        return out;\n    }\n\n    void decode_binaural(const AmbisonicSignal& amb, const glm::quat& head_rotation,\n                         std::vector<float>& out_left, std::vector<float>& out_right) {\n        AmbisonicSignal rotated = rotate_ambisonic(amb, head_rotation);\n        out_left.resize(amb.W.size());\n        out_right.resize(amb.W.size());\n\n        // Virtual speaker positions (cube configuration)\n        std::vector<glm::vec3> speakers = {\n            {1,0,0}, {-1,0,0}, {0,1,0}, {0,-1,0},\n            {0,0,1}, {0,0,-1}, {0.707,0.707,0}, {-0.707,0.707,0}\n        };\n\n        for (size_t i = 0; i < out_left.size(); ++i) {\n            out_left[i] = out_right[i] = 0.0f;\n            for (const auto& sp : speakers) {\n                float sig = decode_direction(rotated, sp, i);\n                float pan = (sp.x + 1.0f) * 0.5f;\n                out_left[i] += sig * (1.0f - pan);\n                out_right[i] += sig * pan;\n            }\n        }\n    }\n};\n```\n\n## Procedural Footstep System\n\n```cpp\nclass ProceduralFootsteps {\npublic:\n    enum class SurfaceType { Concrete, Wood, Grass, Gravel, Metal, Water };\n\n    struct Params {\n        float impact_force;    // 0-1\n        SurfaceType surface;\n        float wetness;         // 0-1\n        float debris_amount;   // 0-1\n    };\n\n    std::vector<float> generate(const Params& p, int sample_rate) {\n        std::vector<float> out(sample_rate / 2, 0.0f);  // 0.5 sec\n\n        // Impact synthesis\n        int impact_dur = int(0.02 * sample_rate);\n        for (int i = 0; i < impact_dur && i < out.size(); ++i) {\n            float t = float(i) / impact_dur;\n            float env = std::exp(-10.0f * t);\n            float noise = random(-1.0f, 1.0f);\n            float freq = get_resonance(p.surface);\n            float tone = std::sin(2.0f * M_PI * freq * t);\n            out[i] = (0.7f * noise + 0.3f * tone) * env * p.impact_force;\n        }\n\n        add_surface_texture(out, p, sample_rate);\n        if (p.debris_amount > 0.1f) add_debris(out, p);\n        apply_surface_eq(out, p.surface);\n\n        return out;\n    }\n\nprivate:\n    float get_resonance(SurfaceType s) {\n        switch(s) {\n            case SurfaceType::Concrete: return 150.0f;\n            case SurfaceType::Wood: return 250.0f;\n            case SurfaceType::Metal: return 500.0f;\n            case SurfaceType::Gravel: return 300.0f;\n            default: return 200.0f;\n        }\n    }\n};\n```\n\n## Wind Synthesizer\n\n```cpp\nclass WindSynthesizer {\npublic:\n    std::vector<float> generate(float speed, float gust, float duration, int sr) {\n        int samples = int(duration * sr);\n        std::vector<float> out(samples);\n        auto pink = generate_pink_noise(samples);\n\n        for (int i = 0; i < samples; ++i) {\n            float t = i / float(sr);\n            float gust_env = 0.5f + 0.5f * std::sin(2.0f * M_PI * (0.2f + gust * 0.5f) * t);\n            out[i] = pink[i] * (speed / 30.0f) * gust_env;\n        }\n\n        apply_bandpass(out, 100.0f, 2000.0f);\n        if (speed > 15.0f) add_whistle(out, speed, sr);\n        return out;\n    }\n\nprivate:\n    std::vector<float> generate_pink_noise(int n) {\n        std::vector<float> out(n);\n        float b0=0, b1=0, b2=0, b3=0, b4=0, b5=0, b6=0;\n        for (int i = 0; i < n; ++i) {\n            float w = random(-1.0f, 1.0f);\n            b0 = 0.99886f * b0 + w * 0.0555179f;\n            b1 = 0.99332f * b1 + w * 0.0750759f;\n            b2 = 0.96900f * b2 + w * 0.1538520f;\n            b3 = 0.86650f * b3 + w * 0.3104856f;\n            b4 = 0.55000f * b4 + w * 0.5329522f;\n            b5 = -0.7616f * b5 - w * 0.0168980f;\n            out[i] = (b0 + b1 + b2 + b3 + b4 + b5 + b6 + w * 0.5362f) * 0.11f;\n            b6 = w * 0.115926f;\n        }\n        return out;\n    }\n};\n```\n\n## Wwise Integration (Unreal)\n\n```cpp\n#include \"AkGameplayStatics.h\"\n\nclass SpatialSoundManager {\npublic:\n    void play_3d(UAkAudioEvent* ev, FVector loc, AActor* owner = nullptr) {\n        UAkGameplayStatics::PostEventAtLocation(ev, loc, FRotator::ZeroRotator,\n            owner ? owner->GetWorld() : nullptr);\n    }\n\n    void set_rtpc(const FString& name, float val, AActor* owner = nullptr) {\n        UAkGameplayStatics::SetRTPCValue(*name, val, 0, owner);\n    }\n\n    void set_switch(const FString& group, const FString& state, AActor* owner) {\n        UAkGameplayStatics::SetSwitch(*group, *state, owner);\n    }\n};\n\n// Character footstep integration\nvoid OnFootDown(AActor* character, FVector location) {\n    FHitResult hit;\n    if (GetWorld()->LineTraceSingleByChannel(hit, location, location - FVector(0,0,100), ECC_Visibility)) {\n        FString surface = DetermineSurface(hit.PhysMaterial);\n        UAkGameplayStatics::SetSwitch(TEXT(\"Surface\"), *surface, character);\n\n        float speed = character->GetVelocity().Size();\n        UAkGameplayStatics::SetRTPCValue(TEXT(\"Impact_Force\"), FMath::Clamp(speed/600.0f, 0.0f, 1.0f), 0, character);\n        UAkGameplayStatics::PostEvent(FootstepEvent, character);\n    }\n}\n```\n\n## Adaptive Music Manager\n\n```cpp\nclass AdaptiveMusicManager {\npublic:\n    enum class Intensity { Ambient, Low, Medium, High, Combat };\n\n    void set_intensity(Intensity level) {\n        const char* states[] = {\"Ambient\", \"Low\", \"Medium\", \"High\", \"Combat\"};\n        UAkGameplayStatics::SetState(TEXT(\"Music_Intensity\"), TEXT(states[int(level)]));\n    }\n\n    void update_from_gameplay(int enemies, float health, bool combat) {\n        Intensity target = Intensity::Ambient;\n        if (!combat) target = Intensity::Ambient;\n        else if (enemies == 0) target = Intensity::Low;\n        else if (enemies < 3 && health > 0.5f) target = Intensity::Medium;\n        else if (enemies < 5 || health > 0.25f) target = Intensity::High;\n        else target = Intensity::Combat;\n        set_intensity(target);\n    }\n};\n```\n\n## Performance Benchmarks\n\n| Operation | CPU Time | Notes |\n|-----------|----------|-------|\n| HRTF Convolution (512-tap) | ~2ms/source | Use FFT overlap-add |\n| Ambisonic encode | ~0.1ms/source | Very efficient |\n| Ambisonic decode (binaural) | ~1ms total | Supports many sources |\n| Procedural footstep | ~1-2ms | vs 500KB per sample |\n| Wind synthesis | ~0.5ms/frame | Real-time streaming |\n| Wwise event post | <0.1ms | Negligible |\n\n## Key References\n\n- MIT KEMAR HRTF Database (public domain)\n- Google Resonance Audio (open source)\n- Wwise/FMOD Documentation (2024)\n- Farnell: \"Designing Sound\" (MIT Press)\n"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "sound-engineer/SKILL.md",
      "size": 11345,
      "content": "---\nname: sound-engineer\ndescription: \"Expert in spatial audio, procedural sound design, game audio middleware, and app UX sound design. Specializes in HRTF/Ambisonics, Wwise/FMOD integration, UI sound design, and adaptive music systems. Activate on 'spatial audio', 'HRTF', 'binaural', 'Wwise', 'FMOD', 'procedural sound', 'footstep system', 'adaptive music', 'UI sounds', 'notification audio', 'sonic branding'. NOT for music composition/production (use DAW), audio post-production for film (linear media), voice cloning/TTS (use voice-audio-engineer), podcast editing (use standard audio editors), or hardware design.\"\nallowed-tools: Read,Write,Edit,Bash,mcp__firecrawl__firecrawl_search,WebFetch,mcp__ElevenLabs__text_to_sound_effects\n---\n\n# Sound Engineer: Spatial Audio, Procedural Sound & App UX Audio\n\nExpert audio engineer for interactive media: games, VR/AR, and mobile apps. Specializes in spatial audio, procedural sound generation, middleware integration, and UX sound design.\n\n## When to Use This Skill\n\n✅ **Use for:**\n- Spatial audio (HRTF, binaural, Ambisonics)\n- Procedural sound (footsteps, wind, environmental)\n- Game audio middleware (Wwise, FMOD)\n- Adaptive/interactive music systems\n- UI/UX sound design (clicks, notifications, feedback)\n- Sonic branding (audio logos, brand sounds)\n- iOS/Android audio session handling\n- Haptic-audio coordination\n- Real-time DSP (reverb, EQ, compression)\n\n❌ **Do NOT use for:**\n- Music composition/production → DAW tools (Logic, Ableton)\n- Voice synthesis/cloning → **voice-audio-engineer**\n- Film audio post-production → linear editing workflows\n- Podcast editing → standard audio editors\n- Hardware microphone setup → specialized domain\n\n## MCP Integrations\n\n| MCP | Purpose |\n|-----|---------|\n| **ElevenLabs** | `text_to_sound_effects` - Generate UI sounds, notifications, impacts |\n| **Firecrawl** | Research Wwise/FMOD docs, DSP algorithms, platform guidelines |\n| **WebFetch** | Fetch Apple/Android audio session documentation |\n\n## Expert vs Novice Shibboleths\n\n| Topic | Novice | Expert |\n|-------|--------|--------|\n| **Spatial audio** | \"Just pan left/right\" | Uses HRTF convolution for true 3D; knows Ambisonics for VR head tracking |\n| **Footsteps** | \"Use 10-20 samples\" | Procedural synthesis: infinite variation, tiny memory, parameter-driven |\n| **Middleware** | \"Just play sounds\" | Uses RTPC for continuous params, Switches for materials, States for music |\n| **Adaptive music** | \"Crossfade tracks\" | Horizontal re-orchestration (layers) + vertical remixing (stems) |\n| **UI sounds** | \"Any click sound works\" | Designs for brand consistency, accessibility, haptic coordination |\n| **iOS audio** | \"AVAudioPlayer works\" | Knows AVAudioSession categories, interruption handling, route changes |\n| **Distance rolloff** | Linear attenuation | Inverse square with reference distance; logarithmic for realism |\n| **CPU budget** | \"Audio is cheap\" | Knows 5-10% budget; HRTF convolution is expensive (2ms/source) |\n\n## Common Anti-Patterns\n\n### Anti-Pattern: Sample-Based Footsteps at Scale\n**What it looks like**: 20 footstep samples × 6 surfaces × 3 intensities = 360 files (180MB)\n**Why it's wrong**: Memory bloat, repetition audible after 20 minutes of play\n**What to do instead**: Procedural synthesis - impact + texture layers, infinite variation from parameters\n**When samples OK**: Small games, very specific character sounds\n\n### Anti-Pattern: HRTF for Every Sound\n**What it looks like**: Full HRTF convolution on 50 simultaneous sources\n**Why it's wrong**: 50 × 2ms = 100ms CPU time; destroys frame budget\n**What to do instead**: HRTF for 3-5 important sources; Ambisonics for ambient bed; simple panning for distant/unimportant\n\n### Anti-Pattern: Ignoring Audio Sessions (Mobile)\n**What it looks like**: App audio stops when user gets a phone call, never resumes\n**Why it's wrong**: iOS/Android require explicit session management\n**What to do instead**: Implement `AVAudioSession` (iOS) or `AudioFocus` (Android); handle interruptions, route changes\n\n### Anti-Pattern: Hard-Coded Sounds\n**What it looks like**: `PlaySound(\"footstep_concrete_01.wav\")`\n**Why it's wrong**: No variation, no parameter control, can't adapt to context\n**What to do instead**: Use middleware events with Switches/RTPCs; procedural generation for environmental sounds\n\n### Anti-Pattern: Loud UI Sounds\n**What it looks like**: Every button click at -3dB, same volume as gameplay audio\n**Why it's wrong**: UI sounds should be subtle, never fatiguing; violates platform guidelines\n**What to do instead**: UI sounds at -18 to -24dB; use short, high-frequency transients; respect system volume\n\n## Evolution Timeline\n\n### Pre-2010: Fixed Audio\n- Sample playback only\n- Basic stereo panning\n- Limited real-time processing\n\n### 2010-2015: Middleware Era\n- Wwise/FMOD become standard\n- RTPC and State systems mature\n- Basic HRTF support\n\n### 2016-2020: VR Audio Revolution\n- Ambisonics for VR head tracking\n- Spatial audio APIs (Resonance, Steam Audio)\n- Procedural audio gains traction\n\n### 2021-2024: AI & Mobile\n- ElevenLabs/AI sound effect generation\n- Apple Spatial Audio for AirPods\n- Procedural audio standard for AAA\n- Haptic-audio design becomes discipline\n\n### 2025+: Current Best Practices\n- AI-assisted sound design\n- Neural audio codecs\n- Real-time voice transformation\n- Personalized HRTF from photos\n\n## Core Concepts\n\n### Spatial Audio Approaches\n\n| Approach | CPU Cost | Quality | Use Case |\n|----------|----------|---------|----------|\n| **Stereo panning** | ~0.01ms | Basic | Distant sounds, many sources |\n| **HRTF convolution** | ~2ms/source | Excellent | Close/important 3D sounds |\n| **Ambisonics** | ~1ms total | Good | VR, many sources, head tracking |\n| **Binaural (simple)** | ~0.1ms/source | Decent | Budget/mobile spatial |\n\n**HRTF**: Convolves audio with measured ear impulse responses (512-1024 taps). Creates convincing 3D positioning including elevation.\n\n**Ambisonics**: Encodes sound field as spherical harmonics (W,X,Y,Z for 1st order). Rotation-invariant, efficient for many sources.\n\n```cpp\n// Key insight: encode once, rotate cheaply\nAmbisonicSignal encode(mono_input, direction) {\n    return {\n        mono * 0.707f,      // W (omnidirectional)\n        mono * direction.x, // X (front-back)\n        mono * direction.y, // Y (left-right)\n        mono * direction.z  // Z (up-down)\n    };\n}\n```\n\n### Procedural Footsteps\n\n**Why procedural beats samples:**\n- ✅ Infinite variation (no repetition)\n- ✅ Tiny memory (~50KB vs 5-10MB)\n- ✅ Parameter-driven (speed → impact force)\n- ✅ Surface-aware from physics materials\n\n**Core synthesis:**\n1. Impact burst (20ms noise + resonant tone)\n2. Surface texture (gravel = granular, grass = filtered noise)\n3. Debris (scattered micro-impacts)\n4. Surface EQ (metal = bright, grass = muffled)\n\n```cpp\n// Surface resonance frequencies (expert knowledge)\nfloat get_resonance(Surface s) {\n    switch(s) {\n        case Concrete: return 150.0f;  // Low, dull\n        case Wood:     return 250.0f;  // Mid, warm\n        case Metal:    return 500.0f;  // High, ringing\n        case Gravel:   return 300.0f;  // Crunchy mid\n        default:       return 200.0f;\n    }\n}\n```\n\n### Wwise/FMOD Integration\n\n**Key abstractions:**\n- **Events**: Trigger sounds (footstep, explosion, ambient loop)\n- **RTPC**: Continuous parameters (speed 0-100, health 0-1)\n- **Switches**: Discrete choices (surface type, weapon type)\n- **States**: Global context (music intensity, underwater)\n\n```cpp\n// Material-aware footsteps via Wwise\nvoid OnFootDown(FHitResult& hit) {\n    FString surface = DetectSurface(hit.PhysMaterial);\n    float speed = GetVelocity().Size();\n\n    SetSwitch(\"Surface\", surface, this);        // Concrete/Wood/Metal\n    SetRTPCValue(\"Impact_Force\", speed/600.0f); // 0-1 normalized\n    PostEvent(FootstepEvent, this);\n}\n```\n\n### UI/UX Sound Design\n\n**Principles for app sounds:**\n1. **Subtle** - UI sounds at -18 to -24dB\n2. **Short** - 50-200ms for most interactions\n3. **Consistent** - Same family/timbre across app\n4. **Accessible** - Don't rely solely on audio for feedback\n5. **Haptic-paired** - iOS haptics should match audio characteristics\n\n**Sound types:**\n| Category | Examples | Duration | Character |\n|----------|----------|----------|-----------|\n| Tap feedback | Button, toggle | 30-80ms | Soft, high-frequency click |\n| Success | Save, send, complete | 150-300ms | Rising, positive tone |\n| Error | Invalid, failed | 200-400ms | Descending, minor tone |\n| Notification | Alert, reminder | 300-800ms | Distinctive, attention-getting |\n| Transition | Screen change, modal | 100-250ms | Whoosh, subtle movement |\n\n### iOS/Android Audio Sessions\n\n**iOS AVAudioSession categories:**\n- `.ambient` - Mixes with other audio, silenced by ringer\n- `.playback` - Interrupts other audio, ignores ringer\n- `.playAndRecord` - For voice apps\n- `.soloAmbient` - Default, silences other audio\n\n**Critical handlers:**\n- Interruption (phone call)\n- Route change (headphones unplugged)\n- Secondary audio (Siri)\n\n```swift\n// Proper iOS audio session setup\nfunc configureAudioSession() {\n    let session = AVAudioSession.sharedInstance()\n    try? session.setCategory(.playback, mode: .default, options: [.mixWithOthers])\n    try? session.setActive(true)\n\n    NotificationCenter.default.addObserver(\n        self,\n        selector: #selector(handleInterruption),\n        name: AVAudioSession.interruptionNotification,\n        object: nil\n    )\n}\n```\n\n## Performance Targets\n\n| Operation | CPU Time | Notes |\n|-----------|----------|-------|\n| HRTF convolution (512-tap) | ~2ms/source | Use FFT overlap-add |\n| Ambisonic encode | ~0.1ms/source | Very efficient |\n| Ambisonic decode (binaural) | ~1ms total | Supports many sources |\n| Procedural footstep | ~1-2ms | vs 500KB per sample |\n| Wind synthesis | ~0.5ms/frame | Real-time streaming |\n| Wwise event post | <0.1ms | Negligible |\n| iOS audio callback | 5-10ms budget | At 48kHz/512 samples |\n\n**Budget guideline**: Audio should use 5-10% of frame time.\n\n## Quick Reference\n\n### Spatial Audio Decision Tree\n- **VR with head tracking?** → Ambisonics\n- **Few important sources?** → Full HRTF\n- **Many background sources?** → Simple panning + distance rolloff\n- **Mobile with limited CPU?** → Binaural (simple) or panning\n\n### When to Use Procedural Audio\n- Environmental (wind, rain, fire) → Always procedural\n- Footsteps → Procedural for large games, samples for small\n- UI sounds → Generated once, then cached\n- Impacts/explosions → Hybrid (procedural + sample layers)\n\n### Platform Audio Sessions\n- **Game with music**: `.ambient` + `mixWithOthers`\n- **Meditation/focus app**: `.playback` (interrupt music)\n- **Voice chat**: `.playAndRecord`\n- **Video player**: `.playback`\n\n## Integrates With\n\n- **voice-audio-engineer** - Voice synthesis and TTS\n- **vr-avatar-engineer** - VR audio + avatar integration\n- **metal-shader-expert** - GPU audio processing\n- **native-app-designer** - App UI sound integration\n\n---\n\n**For detailed implementations**: See `/references/implementations.md`\n\n**Remember**: Great audio is invisible—players feel it, don't notice it. Focus on supporting the experience, not showing off. Procedural audio saves memory and eliminates repetition. Always respect CPU budgets and platform audio session requirements.\n"
    }
  ]
}