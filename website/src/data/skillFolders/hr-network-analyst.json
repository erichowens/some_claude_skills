{
  "name": "hr-network-analyst",
  "type": "folder",
  "path": "hr-network-analyst",
  "children": [
    {
      "name": "references",
      "type": "folder",
      "path": "hr-network-analyst/references",
      "children": [
        {
          "name": "algorithms.md",
          "type": "file",
          "path": "hr-network-analyst/references/algorithms.md",
          "size": 11086,
          "content": "# Network Analysis Algorithms Reference\n\n## Centrality Measures\n\n### Betweenness Centrality\n\n**Mathematical Definition**:\n```\nCB(v) = Σ σst(v) / σst\n       s≠v≠t\n```\n\nWhere:\n- σst = total number of shortest paths from node s to node t\n- σst(v) = number of those paths passing through v\n\n**Interpretation**: Measures how often a node acts as a bridge along shortest paths.\n\n**NetworkX Implementation**:\n```python\nimport networkx as nx\n\n# Basic betweenness\nbc = nx.betweenness_centrality(G)\n\n# Weighted (edges with 'weight' attribute)\nbc_weighted = nx.betweenness_centrality(G, weight='weight')\n\n# Normalized (default) vs unnormalized\nbc_unnorm = nx.betweenness_centrality(G, normalized=False)\n\n# Approximate (faster for large graphs)\nbc_approx = nx.betweenness_centrality(G, k=100)  # sample k nodes\n```\n\n**Complexity**: O(VE) for unweighted, O(VE + V² log V) for weighted\n\n---\n\n### Degree Centrality\n\n**Mathematical Definition**:\n```\nCD(v) = deg(v) / (n-1)\n```\n\n**Interpretation**: Simple count of connections, normalized.\n\n**NetworkX Implementation**:\n```python\n# Undirected\ndc = nx.degree_centrality(G)\n\n# Directed\nin_dc = nx.in_degree_centrality(G)\nout_dc = nx.out_degree_centrality(G)\n```\n\n---\n\n### Eigenvector Centrality\n\n**Mathematical Definition**:\n```\nxi = (1/λ) Σ Aij xj\n           j\n```\n\nWhere A is the adjacency matrix and λ is the largest eigenvalue.\n\n**Interpretation**: A node is important if connected to other important nodes (recursive).\n\n**NetworkX Implementation**:\n```python\nec = nx.eigenvector_centrality(G)\n\n# With weights\nec_weighted = nx.eigenvector_centrality(G, weight='weight')\n\n# NumPy version (faster)\nec_numpy = nx.eigenvector_centrality_numpy(G)\n```\n\n---\n\n### PageRank\n\n**Mathematical Definition**:\n```\nPR(v) = (1-d)/N + d Σ PR(u)/L(u)\n                   u∈Bin(v)\n```\n\nWhere:\n- d = damping factor (typically 0.85)\n- N = total nodes\n- Bin(v) = nodes linking to v\n- L(u) = outgoing links from u\n\n**Interpretation**: Probability of random walker landing on node.\n\n**NetworkX Implementation**:\n```python\npr = nx.pagerank(G)\n\n# Custom damping\npr = nx.pagerank(G, alpha=0.9)\n\n# With weights\npr = nx.pagerank(G, weight='weight')\n```\n\n---\n\n### Closeness Centrality\n\n**Mathematical Definition**:\n```\nCC(v) = (n-1) / Σ d(v,u)\n               u≠v\n```\n\n**Interpretation**: Inverse of average distance to all other nodes.\n\n**NetworkX Implementation**:\n```python\ncc = nx.closeness_centrality(G)\n\n# For disconnected graphs\ncc = nx.closeness_centrality(G, wf_improved=True)\n```\n\n---\n\n## Structural Holes (Burt)\n\n### Constraint\n\n**Mathematical Definition**:\n```\nCi = Σ cij²\n     j\n\ncij = (pij + Σ piq × pqj)²\n           q\n```\n\nWhere pij = proportion of i's network invested in j.\n\n**Interpretation**: How constrained is a node by its network? Low constraint = spanning structural holes.\n\n**NetworkX Implementation**:\n```python\nconstraint = nx.constraint(G)\n\n# With weights\nconstraint_w = nx.constraint(G, weight='weight')\n\n# For specific nodes\nconstraint_node = nx.constraint(G, nodes=['Alice', 'Bob'])\n```\n\n### Effective Size\n\n**Mathematical Definition**:\n```\nES(i) = Σ [1 - Σ piq × mjq]\n        j    q≠j\n\nmjq = pjq / max(pkq for all k)\n```\n\n**Interpretation**: Redundancy-adjusted network size.\n\n**NetworkX Implementation**:\n```python\neff_size = nx.effective_size(G)\n```\n\n---\n\n## Community Detection\n\n### Louvain Algorithm\n\n**Implementation**:\n```python\nfrom networkx.algorithms.community import louvain_communities\n\ncommunities = louvain_communities(G)\n\n# With resolution parameter\ncommunities = louvain_communities(G, resolution=1.5)\n\n# Get partition as dict\npartition = {}\nfor i, comm in enumerate(communities):\n    for node in comm:\n        partition[node] = i\n```\n\n### Label Propagation\n\n```python\nfrom networkx.algorithms.community import label_propagation_communities\n\ncommunities = label_propagation_communities(G)\n```\n\n### Modularity Score\n\n```python\nfrom networkx.algorithms.community import modularity\n\nQ = modularity(G, communities)\n```\n\n---\n\n## Network Statistics\n\n### Basic Properties\n\n```python\n# Number of nodes and edges\nn = G.number_of_nodes()\nm = G.number_of_edges()\n\n# Density\ndensity = nx.density(G)\n\n# Average clustering coefficient\navg_clustering = nx.average_clustering(G)\n\n# Transitivity (global clustering)\ntransitivity = nx.transitivity(G)\n\n# Average shortest path (for connected graphs)\nif nx.is_connected(G):\n    avg_path = nx.average_shortest_path_length(G)\n\n# Diameter\nif nx.is_connected(G):\n    diameter = nx.diameter(G)\n```\n\n### K-Core Decomposition\n\n```python\n# Find k-core (subgraph where all nodes have degree >= k)\nk_core = nx.k_core(G, k=5)\n\n# Core number of each node\ncore_numbers = nx.core_number(G)\n```\n\n---\n\n## Useful Patterns\n\n### Multi-Layer Network Fusion\n\n```python\ndef fuse_networks(networks, weights):\n    \"\"\"\n    Combine multiple network sources with weights.\n\n    Args:\n        networks: dict of {source_name: networkx.Graph}\n        weights: dict of {source_name: float}\n\n    Returns:\n        Fused network with combined edge weights\n    \"\"\"\n    G_fused = nx.Graph()\n\n    for source, G_source in networks.items():\n        w = weights.get(source, 1.0)\n\n        for u, v, data in G_source.edges(data=True):\n            edge_weight = data.get('weight', 1.0) * w\n\n            if G_fused.has_edge(u, v):\n                G_fused[u][v]['weight'] += edge_weight\n                G_fused[u][v]['sources'].append(source)\n            else:\n                G_fused.add_edge(u, v, weight=edge_weight, sources=[source])\n\n    return G_fused\n```\n\n### Temporal Decay Weighting\n\n```python\nfrom datetime import datetime\nimport math\n\ndef apply_temporal_decay(G, date_attr='date', half_life_days=365):\n    \"\"\"\n    Apply exponential decay to edge weights based on recency.\n    \"\"\"\n    now = datetime.now()\n\n    for u, v, data in G.edges(data=True):\n        if date_attr in data:\n            edge_date = data[date_attr]\n            days_old = (now - edge_date).days\n            decay = math.exp(-math.log(2) * days_old / half_life_days)\n            data['weight'] = data.get('weight', 1.0) * decay\n\n    return G\n```\n\n### Gladwell Classification\n\n```python\ndef classify_gladwell(G, metrics=None):\n    \"\"\"\n    Classify nodes into Gladwell archetypes.\n\n    Returns dict mapping node -> archetype\n    \"\"\"\n    if metrics is None:\n        metrics = {\n            'betweenness': nx.betweenness_centrality(G),\n            'degree': nx.degree_centrality(G),\n            'eigenvector': nx.eigenvector_centrality(G),\n        }\n        try:\n            metrics['constraint'] = nx.constraint(G)\n        except:\n            metrics['constraint'] = {n: 0.5 for n in G.nodes()}\n\n    classifications = {}\n\n    # Compute thresholds (top percentile)\n    bc_threshold = sorted(metrics['betweenness'].values())[-int(len(G)*0.1)]\n    dc_threshold = sorted(metrics['degree'].values())[-int(len(G)*0.1)]\n    ec_threshold = sorted(metrics['eigenvector'].values())[-int(len(G)*0.1)]\n\n    for node in G.nodes():\n        bc = metrics['betweenness'][node]\n        dc = metrics['degree'][node]\n        ec = metrics['eigenvector'][node]\n        constraint = metrics['constraint'].get(node, 0.5)\n\n        if bc >= bc_threshold and dc >= dc_threshold:\n            classifications[node] = 'connector'\n        elif ec >= ec_threshold and dc < dc_threshold:\n            classifications[node] = 'maven'\n        elif dc >= dc_threshold and constraint < 0.3:\n            classifications[node] = 'salesman'\n        else:\n            classifications[node] = 'standard'\n\n    return classifications\n```\n\n---\n\n## Data Source APIs\n\n### Semantic Scholar\n\n```python\nimport requests\n\ndef get_author_collaborators(author_id):\n    \"\"\"Get co-authors from Semantic Scholar.\"\"\"\n    url = f\"https://api.semanticscholar.org/graph/v1/author/{author_id}\"\n    params = {\n        'fields': 'papers.authors'\n    }\n    resp = requests.get(url, params=params)\n    data = resp.json()\n\n    coauthors = set()\n    for paper in data.get('papers', []):\n        for author in paper.get('authors', []):\n            if author['authorId'] != author_id:\n                coauthors.add((author['authorId'], author['name']))\n\n    return coauthors\n```\n\n### GitHub\n\n```python\nimport requests\n\ndef get_repo_contributors(owner, repo, token=None):\n    \"\"\"Get contributors to a GitHub repo.\"\"\"\n    headers = {}\n    if token:\n        headers['Authorization'] = f'token {token}'\n\n    url = f\"https://api.github.com/repos/{owner}/{repo}/contributors\"\n    resp = requests.get(url, headers=headers)\n\n    return [(c['login'], c['contributions']) for c in resp.json()]\n\n\ndef build_github_network(repos, token=None):\n    \"\"\"Build collaboration network from list of repos.\"\"\"\n    G = nx.Graph()\n\n    for owner, repo in repos:\n        contributors = get_repo_contributors(owner, repo, token)\n\n        # Add edges between all contributors to same repo\n        for i, (user1, contrib1) in enumerate(contributors):\n            for user2, contrib2 in contributors[i+1:]:\n                if G.has_edge(user1, user2):\n                    G[user1][user2]['weight'] += 1\n                    G[user1][user2]['repos'].append(f\"{owner}/{repo}\")\n                else:\n                    G.add_edge(user1, user2, weight=1, repos=[f\"{owner}/{repo}\"])\n\n    return G\n```\n\n---\n\n## Visualization\n\n### Interactive HTML Network\n\n```python\nfrom pyvis.network import Network\n\ndef visualize_network(G, metrics, output='network.html'):\n    \"\"\"Create interactive HTML visualization.\"\"\"\n    net = Network(height='800px', width='100%', bgcolor='#1a1a2e')\n\n    # Color map for Gladwell types\n    colors = {\n        'connector': '#e94560',\n        'maven': '#0f3460',\n        'salesman': '#16c79a',\n        'standard': '#666666'\n    }\n\n    classifications = classify_gladwell(G, metrics)\n\n    for node in G.nodes():\n        bc = metrics['betweenness'][node]\n        size = 10 + 100 * bc\n        color = colors[classifications[node]]\n        title = f\"{node}<br>Type: {classifications[node]}<br>BC: {bc:.4f}\"\n\n        net.add_node(node, size=size, color=color, title=title)\n\n    for u, v, data in G.edges(data=True):\n        weight = data.get('weight', 1)\n        net.add_edge(u, v, value=weight)\n\n    net.show_buttons(filter_=['physics'])\n    net.save_graph(output)\n```\n\n### Static Matplotlib\n\n```python\nimport matplotlib.pyplot as plt\n\ndef plot_network_static(G, metrics, figsize=(12, 12)):\n    \"\"\"Create static network visualization.\"\"\"\n    fig, ax = plt.subplots(figsize=figsize)\n\n    # Layout\n    pos = nx.spring_layout(G, k=2, iterations=50)\n\n    # Node sizes by betweenness\n    node_sizes = [1000 * metrics['betweenness'][n] + 50 for n in G.nodes()]\n\n    # Node colors by eigenvector centrality\n    node_colors = [metrics['eigenvector'][n] for n in G.nodes()]\n\n    nx.draw_networkx(\n        G, pos, ax=ax,\n        node_size=node_sizes,\n        node_color=node_colors,\n        cmap=plt.cm.viridis,\n        with_labels=True,\n        font_size=8,\n        alpha=0.8\n    )\n\n    plt.colorbar(plt.cm.ScalarMappable(cmap=plt.cm.viridis),\n                 label='Eigenvector Centrality', ax=ax)\n\n    plt.tight_layout()\n    return fig\n```\n"
        },
        {
          "name": "graph-databases.md",
          "type": "file",
          "path": "hr-network-analyst/references/graph-databases.md",
          "size": 15063,
          "content": "# Graph Database Analysis Reference\n\n## Neo4j\n\nNeo4j is the most popular graph database for professional network analysis. It uses Cypher query language and has built-in graph data science algorithms.\n\n### Setup\n\n```cypher\n// Create constraint for unique person IDs\nCREATE CONSTRAINT person_id IF NOT EXISTS\nFOR (p:Person) REQUIRE p.id IS UNIQUE;\n\n// Create index for faster lookups\nCREATE INDEX person_name IF NOT EXISTS\nFOR (p:Person) ON (p.name);\n```\n\n### Data Model for Professional Networks\n\n```cypher\n// Node types\n(:Person {id, name, email, role, company, linkedin_url})\n(:Company {id, name, industry, size})\n(:Conference {id, name, year, location})\n(:Publication {id, title, year, venue, doi})\n(:Project {id, name, repo_url, tech_stack})\n\n// Relationship types\n(:Person)-[:WORKS_AT {since, role}]->(:Company)\n(:Person)-[:SPOKE_AT {talk_title, track}]->(:Conference)\n(:Person)-[:COAUTHORED {position}]->(:Publication)\n(:Person)-[:CONTRIBUTED_TO {commits, role}]->(:Project)\n(:Person)-[:KNOWS {strength, source, since}]->(:Person)\n(:Person)-[:COLLABORATED_WITH {project, duration}]->(:Person)\n```\n\n### Loading Data\n\n```cypher\n// Load from CSV\nLOAD CSV WITH HEADERS FROM 'file:///people.csv' AS row\nMERGE (p:Person {id: row.id})\nSET p.name = row.name,\n    p.email = row.email,\n    p.role = row.role;\n\n// Load edges\nLOAD CSV WITH HEADERS FROM 'file:///connections.csv' AS row\nMATCH (a:Person {id: row.source})\nMATCH (b:Person {id: row.target})\nMERGE (a)-[r:KNOWS]->(b)\nSET r.strength = toFloat(row.strength),\n    r.source = row.data_source;\n```\n\n### Centrality Algorithms (Graph Data Science Library)\n\n```cypher\n// First, create a graph projection\nCALL gds.graph.project(\n  'professional-network',\n  'Person',\n  {\n    KNOWS: {\n      orientation: 'UNDIRECTED',\n      properties: ['strength']\n    }\n  }\n);\n\n// Betweenness Centrality\nCALL gds.betweenness.stream('professional-network')\nYIELD nodeId, score\nRETURN gds.util.asNode(nodeId).name AS name, score\nORDER BY score DESC\nLIMIT 20;\n\n// Write back to nodes\nCALL gds.betweenness.write('professional-network', {\n  writeProperty: 'betweenness'\n});\n\n// PageRank\nCALL gds.pageRank.stream('professional-network', {\n  dampingFactor: 0.85,\n  maxIterations: 20\n})\nYIELD nodeId, score\nRETURN gds.util.asNode(nodeId).name AS name, score\nORDER BY score DESC\nLIMIT 20;\n\n// Eigenvector Centrality\nCALL gds.eigenvector.stream('professional-network', {\n  maxIterations: 100\n})\nYIELD nodeId, score\nRETURN gds.util.asNode(nodeId).name AS name, score\nORDER BY score DESC;\n\n// Degree Centrality\nCALL gds.degree.stream('professional-network')\nYIELD nodeId, score\nRETURN gds.util.asNode(nodeId).name AS name, score\nORDER BY score DESC;\n\n// Closeness Centrality\nCALL gds.closeness.stream('professional-network')\nYIELD nodeId, score\nRETURN gds.util.asNode(nodeId).name AS name, score\nORDER BY score DESC;\n```\n\n### Community Detection\n\n```cypher\n// Louvain community detection\nCALL gds.louvain.stream('professional-network')\nYIELD nodeId, communityId\nRETURN gds.util.asNode(nodeId).name AS name, communityId\nORDER BY communityId;\n\n// Write communities to nodes\nCALL gds.louvain.write('professional-network', {\n  writeProperty: 'community'\n});\n\n// Label Propagation\nCALL gds.labelPropagation.stream('professional-network')\nYIELD nodeId, communityId\nRETURN communityId, collect(gds.util.asNode(nodeId).name) AS members\nORDER BY size(members) DESC;\n\n// Weakly Connected Components\nCALL gds.wcc.stream('professional-network')\nYIELD nodeId, componentId\nRETURN componentId, count(*) AS size\nORDER BY size DESC;\n```\n\n### Finding Bridges and Superconnectors\n\n```cypher\n// Find people who bridge communities\nMATCH (p:Person)\nWHERE p.betweenness > 0.1\nRETURN p.name, p.betweenness, p.community\nORDER BY p.betweenness DESC;\n\n// Find people connected to multiple communities\nMATCH (p:Person)-[:KNOWS]-(other:Person)\nWITH p, collect(DISTINCT other.community) AS connected_communities\nWHERE size(connected_communities) >= 3\nRETURN p.name, connected_communities, size(connected_communities) AS bridge_score\nORDER BY bridge_score DESC;\n\n// Find structural hole spanners\nMATCH (p:Person)-[:KNOWS]-(a:Person)\nMATCH (p)-[:KNOWS]-(b:Person)\nWHERE a.community <> b.community AND NOT (a)-[:KNOWS]-(b)\nWITH p, count(DISTINCT [a.community, b.community]) AS holes_spanned\nWHERE holes_spanned > 5\nRETURN p.name, holes_spanned\nORDER BY holes_spanned DESC;\n```\n\n### Gladwell Classification Query\n\n```cypher\n// Classify nodes by Gladwell archetype\nMATCH (p:Person)\nWITH p,\n     percentileDisc(p.betweenness, 0.9) OVER () AS bc_threshold,\n     percentileDisc(p.degree, 0.9) OVER () AS dc_threshold,\n     percentileDisc(p.eigenvector, 0.9) OVER () AS ec_threshold\nRETURN p.name,\n       CASE\n         WHEN p.betweenness >= bc_threshold AND p.degree >= dc_threshold\n           THEN 'connector'\n         WHEN p.eigenvector >= ec_threshold AND p.degree < dc_threshold\n           THEN 'maven'\n         WHEN p.degree >= dc_threshold\n           THEN 'salesman'\n         ELSE 'standard'\n       END AS gladwell_type,\n       p.betweenness, p.degree, p.eigenvector\nORDER BY p.betweenness DESC;\n```\n\n### Path Finding\n\n```cypher\n// Shortest path between two people\nMATCH path = shortestPath(\n  (a:Person {name: 'Alice'})-[:KNOWS*]-(b:Person {name: 'Bob'})\n)\nRETURN path, length(path) AS degrees_of_separation;\n\n// All shortest paths\nMATCH paths = allShortestPaths(\n  (a:Person {name: 'Alice'})-[:KNOWS*]-(b:Person {name: 'Bob'})\n)\nRETURN paths;\n\n// Find connectors who can introduce you\nMATCH (me:Person {name: 'Alice'})\nMATCH (target:Person {name: 'Bob'})\nMATCH path = shortestPath((me)-[:KNOWS*2..4]-(target))\nWITH nodes(path) AS path_nodes\nUNWIND range(1, size(path_nodes)-2) AS i\nWITH path_nodes[i] AS connector\nRETURN connector.name, connector.betweenness\nORDER BY connector.betweenness DESC;\n```\n\n### Multi-Source Network Fusion\n\n```cypher\n// Create weighted relationships from multiple sources\nMATCH (a:Person)-[r:KNOWS]-(b:Person)\nWITH a, b, collect(r) AS rels\nSET a.connection_weight = reduce(w = 0.0, r IN rels |\n  w + CASE r.source\n    WHEN 'coauthorship' THEN 1.0\n    WHEN 'conference' THEN 0.8\n    WHEN 'linkedin' THEN 0.5\n    WHEN 'github' THEN 0.6\n    ELSE 0.3\n  END\n);\n```\n\n---\n\n## Amazon Neptune\n\nNeptune is AWS's managed graph database, compatible with Gremlin and SPARQL.\n\n### Gremlin Queries\n\n```groovy\n// Betweenness-like analysis (Gremlin doesn't have native betweenness)\n// Count paths through each node\ng.V().hasLabel('Person')\n  .project('name', 'pathsThrough')\n  .by('name')\n  .by(\n    __.as('p')\n    .both('KNOWS').as('start')\n    .repeat(__.both('KNOWS').simplePath())\n    .until(__.loops().is(3))\n    .path()\n    .filter(__.unfold().is('p'))\n    .count()\n  )\n  .order().by('pathsThrough', desc)\n  .limit(20)\n\n// Degree centrality\ng.V().hasLabel('Person')\n  .project('name', 'degree')\n  .by('name')\n  .by(__.both('KNOWS').count())\n  .order().by('degree', desc)\n  .limit(20)\n\n// Find bridges between communities\ng.V().hasLabel('Person')\n  .where(\n    __.both('KNOWS').values('community').dedup().count().is(gte(3))\n  )\n  .project('name', 'communities')\n  .by('name')\n  .by(__.both('KNOWS').values('community').dedup().fold())\n```\n\n---\n\n## TigerGraph\n\nTigerGraph is optimized for deep-link analytics on large graphs.\n\n### GSQL Queries\n\n```sql\n-- Create schema\nCREATE VERTEX Person (\n  PRIMARY_ID id STRING,\n  name STRING,\n  email STRING,\n  role STRING\n)\n\nCREATE DIRECTED EDGE KNOWS (\n  FROM Person,\n  TO Person,\n  strength FLOAT,\n  source STRING\n)\n\n-- Betweenness Centrality\nCREATE QUERY betweenness_centrality() FOR GRAPH professional_network {\n  MapAccum<VERTEX<Person>, FLOAT> @@bc_scores;\n\n  Start = {Person.*};\n\n  // Run shortest paths from each node\n  FOREACH src IN Start DO\n    paths = SELECT t\n      FROM Start:s -(KNOWS:e)- Person:t\n      WHERE s == src\n      ACCUM @@bc_scores += (t -> 1.0);\n  END;\n\n  PRINT @@bc_scores;\n}\n\n-- PageRank\nCREATE QUERY pagerank(FLOAT damping = 0.85, INT max_iter = 20)\nFOR GRAPH professional_network {\n  MaxAccum<FLOAT> @pr_score = 1.0;\n  SumAccum<FLOAT> @new_score;\n\n  Start = {Person.*};\n  INT num_vertices = Start.size();\n\n  FOREACH i IN RANGE[1, max_iter] DO\n    Start = SELECT s\n      FROM Start:s -(KNOWS:e)- Person:t\n      ACCUM t.@new_score += s.@pr_score / s.outdegree(\"KNOWS\")\n      POST-ACCUM\n        s.@pr_score = (1 - damping) / num_vertices + damping * s.@new_score,\n        s.@new_score = 0;\n  END;\n\n  PRINT Start[Start.@pr_score];\n}\n\n-- Find superconnectors\nCREATE QUERY find_superconnectors(INT top_k = 20)\nFOR GRAPH professional_network {\n  SumAccum<INT> @degree;\n  MaxAccum<FLOAT> @betweenness;\n\n  Start = {Person.*};\n\n  // Calculate degree\n  connected = SELECT s\n    FROM Start:s -(KNOWS:e)- Person:t\n    ACCUM s.@degree += 1;\n\n  // Return top by combined score\n  Result = SELECT s FROM Start:s\n    ORDER BY s.@degree DESC\n    LIMIT top_k;\n\n  PRINT Result;\n}\n```\n\n---\n\n## ArangoDB\n\nArangoDB is a multi-model database with graph capabilities.\n\n### AQL Queries\n\n```aql\n// Betweenness Centrality (using Pregel)\nWITH \"professional_network\"\nLET result = PREGEL_RUN(\"betweenness\", \"professional_network\", {\n  maxIterations: 100,\n  resultField: \"betweenness\"\n})\nFOR doc IN Person\n  SORT doc.betweenness DESC\n  LIMIT 20\n  RETURN {name: doc.name, betweenness: doc.betweenness}\n\n// PageRank\nWITH \"professional_network\"\nLET result = PREGEL_RUN(\"pagerank\", \"professional_network\", {\n  maxIterations: 100,\n  dampingFactor: 0.85,\n  resultField: \"pagerank\"\n})\nFOR doc IN Person\n  SORT doc.pagerank DESC\n  LIMIT 20\n  RETURN {name: doc.name, pagerank: doc.pagerank}\n\n// Shortest path\nFOR v, e IN OUTBOUND SHORTEST_PATH\n  'Person/alice' TO 'Person/bob'\n  GRAPH 'professional_network'\n  RETURN v.name\n\n// K-hop neighbors\nFOR v, e, p IN 1..3 ANY 'Person/alice'\n  GRAPH 'professional_network'\n  RETURN DISTINCT v.name\n\n// Find bridges\nFOR person IN Person\n  LET neighbors = (\n    FOR v IN 1..1 ANY person GRAPH 'professional_network'\n      RETURN DISTINCT v.community\n  )\n  FILTER LENGTH(neighbors) >= 3\n  SORT LENGTH(neighbors) DESC\n  RETURN {\n    name: person.name,\n    communities_bridged: neighbors,\n    bridge_score: LENGTH(neighbors)\n  }\n```\n\n---\n\n## DGraph\n\nDGraph is a horizontally scalable graph database with GraphQL support.\n\n### DQL Queries\n\n```graphql\n# Schema\ntype Person {\n  id: ID!\n  name: String! @index(term)\n  email: String @index(exact)\n  knows: [Person] @reverse\n  betweenness: Float\n  pagerank: Float\n}\n\n# Query high-centrality people\n{\n  superconnectors(func: ge(betweenness, 0.1), orderdesc: betweenness, first: 20) {\n    name\n    betweenness\n    pagerank\n    knows {\n      name\n    }\n  }\n}\n\n# Shortest path\n{\n  path as shortest(from: 0x1, to: 0x2) {\n    name\n  }\n}\n\n# Find all paths up to depth 3\n{\n  var(func: eq(name, \"Alice\")) {\n    knows @recurse(depth: 3) {\n      uid\n      name\n    }\n  }\n}\n```\n\n---\n\n## Python Integration Patterns\n\n### Neo4j with Python\n\n```python\nfrom neo4j import GraphDatabase\nimport pandas as pd\n\nclass ProfessionalNetworkAnalyzer:\n    def __init__(self, uri, user, password):\n        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n\n    def close(self):\n        self.driver.close()\n\n    def get_superconnectors(self, limit=20):\n        with self.driver.session() as session:\n            result = session.run(\"\"\"\n                CALL gds.betweenness.stream('professional-network')\n                YIELD nodeId, score\n                RETURN gds.util.asNode(nodeId).name AS name, score\n                ORDER BY score DESC\n                LIMIT $limit\n            \"\"\", limit=limit)\n            return pd.DataFrame([dict(r) for r in result])\n\n    def find_path_to_target(self, source_name, target_name):\n        with self.driver.session() as session:\n            result = session.run(\"\"\"\n                MATCH path = shortestPath(\n                  (a:Person {name: $source})-[:KNOWS*]-(b:Person {name: $target})\n                )\n                RETURN [n IN nodes(path) | n.name] AS path,\n                       length(path) AS degrees\n            \"\"\", source=source_name, target=target_name)\n            record = result.single()\n            if record:\n                return record['path'], record['degrees']\n            return None, None\n\n    def classify_by_gladwell(self):\n        with self.driver.session() as session:\n            result = session.run(\"\"\"\n                MATCH (p:Person)\n                WHERE p.betweenness IS NOT NULL\n                WITH p,\n                     percentileDisc(p.betweenness, 0.9) OVER () AS bc_thresh,\n                     percentileDisc(p.degree, 0.9) OVER () AS dc_thresh,\n                     percentileDisc(p.eigenvector, 0.9) OVER () AS ec_thresh\n                RETURN p.name AS name,\n                       CASE\n                         WHEN p.betweenness >= bc_thresh AND p.degree >= dc_thresh\n                           THEN 'connector'\n                         WHEN p.eigenvector >= ec_thresh AND p.degree < dc_thresh\n                           THEN 'maven'\n                         WHEN p.degree >= dc_thresh\n                           THEN 'salesman'\n                         ELSE 'standard'\n                       END AS archetype\n            \"\"\")\n            return pd.DataFrame([dict(r) for r in result])\n\n\n# Usage\nanalyzer = ProfessionalNetworkAnalyzer(\n    \"bolt://localhost:7687\",\n    \"neo4j\",\n    \"password\"\n)\n\nsuperconnectors = analyzer.get_superconnectors()\nprint(superconnectors.head(10))\n\npath, degrees = analyzer.find_path_to_target(\"Alice\", \"Bob\")\nprint(f\"Path: {' -> '.join(path)} ({degrees} degrees)\")\n\nclassifications = analyzer.classify_by_gladwell()\nprint(classifications[classifications['archetype'] == 'connector'])\n\nanalyzer.close()\n```\n\n### Bulk Loading Pattern\n\n```python\nfrom neo4j import GraphDatabase\n\ndef bulk_load_network(driver, nodes_df, edges_df, batch_size=5000):\n    \"\"\"Efficiently load network data into Neo4j.\"\"\"\n\n    with driver.session() as session:\n        # Load nodes in batches\n        for i in range(0, len(nodes_df), batch_size):\n            batch = nodes_df.iloc[i:i+batch_size].to_dict('records')\n            session.run(\"\"\"\n                UNWIND $batch AS row\n                MERGE (p:Person {id: row.id})\n                SET p.name = row.name,\n                    p.email = row.email,\n                    p.role = row.role\n            \"\"\", batch=batch)\n            print(f\"Loaded {min(i+batch_size, len(nodes_df))} nodes\")\n\n        # Load edges in batches\n        for i in range(0, len(edges_df), batch_size):\n            batch = edges_df.iloc[i:i+batch_size].to_dict('records')\n            session.run(\"\"\"\n                UNWIND $batch AS row\n                MATCH (a:Person {id: row.source})\n                MATCH (b:Person {id: row.target})\n                MERGE (a)-[r:KNOWS]->(b)\n                SET r.strength = row.strength,\n                    r.source = row.data_source\n            \"\"\", batch=batch)\n            print(f\"Loaded {min(i+batch_size, len(edges_df))} edges\")\n\n\n# Usage\ndriver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\nbulk_load_network(driver, people_df, connections_df)\ndriver.close()\n```\n"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "hr-network-analyst/SKILL.md",
      "size": 22535,
      "content": "---\nname: hr-network-analyst\ndescription: Hypermodern HR data scientist specializing in professional network graph analysis to identify Gladwellian superconnectors, mavens, and influence brokers. Uses betweenness centrality, structural holes theory, and multi-source network reconstruction from LinkedIn, conferences, publications, and community signals. Use when users need to find key people, map professional influence, optimize networking strategy, or understand \"who knows who\" in any domain.\nallowed-tools: Read,Write,Edit,WebSearch,WebFetch,mcp__firecrawl__firecrawl_search,mcp__firecrawl__firecrawl_scrape,mcp__brave-search__brave_web_search,mcp__SequentialThinking__sequentialthinking\n---\n\n# HR Network Analyst\n\nA hypermodern HR data scientist who applies graph theory and network science to professional relationship mapping. Identifies the hidden superconnectors, influence brokers, and knowledge mavens that drive professional ecosystems.\n\n## Quick Start\n\n**Minimal example to find superconnectors in a domain:**\n\n```\nUser: \"Who are the key connectors in the AI safety research community?\"\n\nAnalyst:\n1. Define network boundary: AI safety researchers, 2020-2024\n2. Identify data sources:\n   - Co-authorship networks (Semantic Scholar, arXiv)\n   - Conference organizers/speakers (NeurIPS, ICML safety workshops)\n   - Cross-institutional collaboration patterns\n   - Twitter/X discussion networks\n3. Compute centrality metrics:\n   - Betweenness: Who bridges different research clusters?\n   - Eigenvector: Who's connected to other influential people?\n   - Structural holes: Who spans otherwise disconnected groups?\n4. Classify by Gladwellian archetype:\n   - Connectors: High betweenness, bridge multiple institutions\n   - Mavens: Publishing volume + citation flow + teaching roles\n   - Brokers: Sit between academia, labs, and policy\n5. Output ranked list with network position rationale\n```\n\n**Key principle**: The most valuable people aren't always the most famous—they're the ones who connect otherwise disconnected worlds.\n\n## Core Mission\n\nApply network science to professional ecosystems to answer:\n- **Who should I know?** (optimal networking targets)\n- **Who knows everyone?** (superconnectors for referrals)\n- **Who bridges worlds?** (cross-domain brokers)\n- **How does influence flow?** (information/opportunity pathways)\n- **Where are the structural holes?** (untapped connection opportunities)\n\n## Theoretical Foundations\n\n### Gladwellian Archetypes (The Tipping Point)\n\n#### Connectors\n**Definition**: People who know an extraordinary number of people across diverse social worlds.\n\n**Network Signature**:\n- Very high degree centrality (many connections)\n- High betweenness centrality (bridge between clusters)\n- Diverse cluster membership (not siloed in one group)\n- Power-law distribution: rare but disproportionately connected\n\n**Identification Signals**:\n- Appears on multiple conference speaker lists across domains\n- Co-authored with people from 5+ different institutions\n- LinkedIn connections span 10+ distinct industries\n- Referenced by people who don't otherwise interact\n\n**HR Value**:\n- Best source for referrals across domains\n- Can accelerate hiring in new markets\n- Bridge between technical and business networks\n\n#### Mavens\n**Definition**: Information specialists who accumulate knowledge and love sharing it.\n\n**Network Signature**:\n- High in-degree (people seek them out)\n- Central in knowledge-sharing networks (Slack, forums)\n- High PageRank (authoritative in their domain)\n- Create content that others reference\n\n**Identification Signals**:\n- Prolific writers/speakers on specific topics\n- Run newsletters, podcasts, or educational content\n- Frequently tagged in \"who should I follow for X?\" threads\n- High engagement-to-follower ratio\n\n**HR Value**:\n- Know who's good at what (talent intelligence)\n- Can validate candidate quality in specialized domains\n- Influence how domain practitioners think\n\n#### Salesmen\n**Definition**: Persuaders with natural ability to get others to agree.\n\n**Network Signature**:\n- High influence propagation (their views spread)\n- Strong reciprocal relationships\n- Often central in deal-making networks\n- Bridge between decision-makers\n\n**Identification Signals**:\n- Track record of successful introductions\n- Referenced in \"how I got my job\" stories\n- Active in investor/founder/hiring manager circles\n- High response rate to outreach\n\n**HR Value**:\n- Close candidates who are on the fence\n- Navigate complex hiring negotiations\n- Connect to decision-makers\n\n### Network Centrality Metrics\n\n#### Betweenness Centrality\n**Formula**: BC(v) = Σ (σst(v) / σst) for all s,t pairs\n**Meaning**: How often a node lies on shortest paths between other nodes\n**HR Interpretation**: \"Gatekeeper\" or \"Bridge\" - controls information flow\n\n```python\nimport networkx as nx\nbc = nx.betweenness_centrality(G)\ntop_brokers = sorted(bc.items(), key=lambda x: x[1], reverse=True)[:10]\n```\n\n**When high betweenness matters**:\n- Finding people who can introduce you to otherwise unreachable networks\n- Identifying informal power centers in organizations\n- Locating bottlenecks in knowledge flow\n\n#### Degree Centrality\n**Formula**: DC(v) = degree(v) / (n-1)\n**Meaning**: Raw count of connections, normalized\n**HR Interpretation**: \"Popular\" - knows many people directly\n\n**When degree matters**:\n- Maximizing referral reach\n- Event organizing (who to invite for network effects)\n- Initial reconnaissance of a new domain\n\n#### Eigenvector Centrality\n**Formula**: Recursive: centrality depends on centrality of neighbors\n**Meaning**: Connected to other well-connected people\n**HR Interpretation**: \"Influential\" - quality over quantity\n\n**When eigenvector matters**:\n- Finding people with access to power\n- Identifying rising stars (high eigenvector, modest degree)\n- Understanding influence hierarchies\n\n#### Closeness Centrality\n**Formula**: CC(v) = (n-1) / Σ d(v,u)\n**Meaning**: Average shortest path to all other nodes\n**HR Interpretation**: \"Accessible\" - can reach anyone quickly\n\n**When closeness matters**:\n- Finding people who can quickly spread information\n- Identifying central positions for new hires\n- Communication efficiency analysis\n\n#### PageRank\n**Formula**: Iterative probability of random walk landing on node\n**Meaning**: Weighted by quality of incoming connections\n**HR Interpretation**: \"Authoritative\" - endorsed by important others\n\n**When PageRank matters**:\n- Identifying thought leaders vs. merely prolific\n- Weighting referrals by referrer quality\n- Academic/publication network analysis\n\n### Structural Holes Theory (Burt)\n\n**Core Insight**: Advantage comes from bridging otherwise disconnected groups, not from connections within dense clusters.\n\n**Key Metrics**:\n- **Constraint**: How much a node's network is concentrated in one group\n- **Effective Size**: Redundancy-adjusted network size\n- **Hierarchy**: Concentration of constraint across contacts\n\n```python\n# NetworkX structural holes\nconstraint = nx.constraint(G)\nlow_constraint = {k: v for k, v in constraint.items() if v < 0.5}\n# These are your broker opportunities\n```\n\n**HR Applications**:\n- **Recruiting**: Candidates who bridge groups bring diverse information\n- **Team Composition**: Mix of connectors and specialists\n- **Networking Strategy**: Target structural holes, not cluster centers\n\n## Data Sources & Network Construction\n\n### Primary Data Sources\n\n#### LinkedIn Analysis\n**What to extract**:\n- Connection overlaps (mutual connections → edge)\n- Shared experiences (same company/school → edge weight)\n- Endorsement patterns (directional edges)\n- Group memberships (bipartite projection)\n- Comment/reaction networks on posts\n\n**Ethical considerations**:\n- Respect rate limits and ToS\n- Use public data only\n- Don't scrape private profiles\n- Aggregate patterns, don't expose individuals\n\n**Network construction**:\n```python\n# Bipartite projection from group membership\n# People connected if they share groups\nfrom networkx import bipartite\npeople_projection = bipartite.projected_graph(B, people_nodes)\n```\n\n#### Conference & Event Networks\n**What to extract**:\n- Co-speaking at same event → strong edge\n- Same session/track → medium edge\n- Same conference → weak edge\n- Panel/roundtable co-participation → very strong edge\n- Organizer relationships\n\n**High-value conferences by domain**:\n- Tech: Strange Loop, QCon, domain-specific (RustConf, ReactConf)\n- AI/ML: NeurIPS, ICML, ICLR, domain workshops\n- Data: Strata, dbt Coalesce, DataEngConf\n- Design: Config, Clarity, Figma events\n- Leadership: First Round CEO Summit, a]16z events\n\n#### Publication & Co-authorship Networks\n**Sources**:\n- Semantic Scholar API (open, good coverage)\n- Google Scholar (requires scraping, comprehensive)\n- arXiv (preprints, fast-moving fields)\n- DBLP (computer science focused)\n- PubMed (life sciences)\n\n**Edge weighting**:\n- Co-authorship count (repeated collaboration = trust)\n- Citation flows (who cites whom)\n- Position on author list (first/last = more weight)\n\n**Pattern identification**:\n```python\n# Find people who collaborate across institutions\ncross_institutional = [\n    (a1, a2) for (a1, a2) in G.edges()\n    if get_institution(a1) != get_institution(a2)\n]\n```\n\n#### GitHub & Open Source Networks\n**What to extract**:\n- Repository collaboration (co-contributors)\n- Review relationships (who reviews whose PRs)\n- Organizational membership\n- Sponsorship networks\n- Issue/discussion participation\n\n**Signal quality**:\n- Sustained collaboration > one-off contribution\n- Cross-project collaboration = broader network\n- Maintainer relationships = trust indicators\n\n#### Twitter/X Network Analysis\n**What to extract**:\n- Follow graphs (who follows whom)\n- Mutual follows (symmetric relationship)\n- Quote-tweet and reply networks (engagement)\n- List memberships (curated associations)\n\n**Practical approach**:\n- Public lists as starting seeds\n- Snowball sampling from known figures\n- Identify conversation clusters\n\n#### Reddit & Community Networks\n**What to extract**:\n- Cross-subreddit posting (bridges communities)\n- Comment thread interactions\n- Moderator networks\n- Frequently referenced usernames\n\n### Network Reconstruction Strategies\n\n#### Multi-Layer Network Fusion\n**Approach**: Combine signals from multiple sources\n\n```python\n# Weight edges by source reliability\nedge_weights = {\n    'coauthor': 1.0,      # Strongest signal\n    'conference_copanel': 0.8,\n    'linkedin_connection': 0.5,\n    'github_corepo': 0.6,\n    'twitter_mutual': 0.3,\n}\n\n# Merge into unified graph\nG_unified = nx.Graph()\nfor source, weight in edge_weights.items():\n    for u, v in source_graphs[source].edges():\n        if G_unified.has_edge(u, v):\n            G_unified[u][v]['weight'] += weight\n        else:\n            G_unified.add_edge(u, v, weight=weight)\n```\n\n#### Entity Resolution\n**Challenge**: Same person appears differently across sources\n- \"Jane Smith\" on LinkedIn\n- \"J. Smith\" on papers\n- \"@janesmith\" on Twitter\n- \"jsmith\" on GitHub\n\n**Approaches**:\n- Email as unique identifier (when available)\n- ORCID for researchers\n- LinkedIn URL as canonical\n- Fuzzy matching with verification\n\n#### Temporal Network Analysis\n**Why it matters**: Networks evolve. Yesterday's connector may be today's isolate.\n\n**Considerations**:\n- Recency-weight edges (recent collaboration > old)\n- Track rising stars (centrality trajectory)\n- Identify fading connections\n- Seasonal patterns (conference cycles)\n\n## Analysis Workflows\n\n### Workflow 1: Find Superconnectors for Referrals\n\n**Use case**: \"I need to hire 5 ML engineers. Who should I ask?\"\n\n**Process**:\n1. Define target domain (ML engineering, specific stack/domain)\n2. Seed network with known domain members\n3. Expand via LinkedIn connections, GitHub repos, conferences\n4. Compute betweenness + degree centrality\n5. Filter by: still active, accessible, relevant to your needs\n6. Rank by referral potential\n\n**Output format**:\n```typescript\ninterface SuperconnectorProfile {\n  name: string;\n  currentRole: string;\n\n  networkMetrics: {\n    betweennessCentrality: number;  // 0-1, higher = more bridging\n    degreeCentrality: number;        // 0-1, higher = more connections\n    clusterMemberships: string[];    // Which communities they bridge\n  };\n\n  gladwellType: 'connector' | 'maven' | 'salesman' | 'hybrid';\n\n  accessibilityScore: number;  // How likely to respond\n  relevanceScore: number;      // Match to your specific need\n\n  approachStrategy: string;    // How to reach out\n  mutualConnections: string[]; // Who can introduce you\n}\n```\n\n### Workflow 2: Map Influence in a Domain\n\n**Use case**: \"How does influence flow in climate tech?\"\n\n**Process**:\n1. Define domain boundaries (climate tech = which companies, investors, researchers?)\n2. Multi-source network construction\n3. Community detection (Louvain, label propagation)\n4. Compute influence metrics per community\n5. Identify cross-community brokers\n6. Map information flow patterns\n\n**Output format**:\n```typescript\ninterface DomainInfluenceMap {\n  communities: {\n    name: string;\n    description: string;\n    size: number;\n    keyFigures: string[];\n    dominantMetric: 'academic' | 'commercial' | 'policy' | 'investment';\n  }[];\n\n  brokers: {\n    name: string;\n    bridgesCommunities: [string, string][];\n    influenceVector: number[];  // Per-community influence\n  }[];\n\n  informationFlows: {\n    source: string;\n    target: string;\n    flowType: 'ideas' | 'talent' | 'capital' | 'deals';\n    strength: number;\n  }[];\n}\n```\n\n### Workflow 3: Optimize Personal Networking Strategy\n\n**Use case**: \"I want to break into VC. Who should I connect with?\"\n\n**Process**:\n1. Map user's current network (LinkedIn export, provided list)\n2. Map target domain network (VC ecosystem)\n3. Find shortest paths from current network to target\n4. Identify structural holes user could fill\n5. Prioritize connections by:\n   - Reachability (can you actually connect?)\n   - Bridge potential (do they connect to people you don't know?)\n   - Domain match (are they in your target area?)\n\n**Output format**:\n```typescript\ninterface NetworkingStrategy {\n  currentPosition: {\n    communities: string[];\n    strengthAreas: string[];\n    structuralHoles: string[];  // Opportunities to bridge\n  };\n\n  targetConnections: {\n    priority: 'high' | 'medium' | 'low';\n    name: string;\n    reason: string;  // Why this person\n    pathway: string[];  // Chain of introductions\n    approachAngle: string;  // What value you offer\n  }[];\n\n  communityTargets: {\n    community: string;\n    entryPoints: string[];  // Accessible members\n    value: string;  // Why this community matters\n  }[];\n}\n```\n\n### Workflow 4: Organizational Network Analysis (ONA)\n\n**Use case**: \"Map how information flows in our 500-person company\"\n\n**Process**:\n1. Data collection (surveys, Slack/email metadata, meeting patterns)\n2. Construct communication/collaboration graph\n3. Identify informal vs formal structure\n4. Find hidden influencers and bottlenecks\n5. Detect silos and integration opportunities\n\n**Output format**:\n```typescript\ninterface ONAReport {\n  formalVsInformal: {\n    person: string;\n    formalRole: string;\n    informalRole: 'connector' | 'bottleneck' | 'isolate' | 'bridge';\n    gap: string;  // Difference between formal and informal power\n  }[];\n\n  silos: {\n    teams: string[];\n    bridgePeople: string[];\n    integrationOpportunity: string;\n  }[];\n\n  bottlenecks: {\n    person: string;\n    constrainedFlows: string[];\n    risk: string;\n    mitigation: string;\n  }[];\n\n  hiddenInfluencers: {\n    person: string;\n    influenceType: string;\n    formalRecognition: boolean;\n    recommendation: string;\n  }[];\n}\n```\n\n## Implementation Tools\n\n### Python Ecosystem\n\n```python\n# Core graph libraries\nimport networkx as nx           # General purpose, easy to use\nimport graph_tool.all as gt     # Fast, good visualization\nimport igraph                   # Fast community detection\n\n# Data processing\nimport pandas as pd\nfrom scipy import sparse\n\n# Visualization\nimport matplotlib.pyplot as plt\nfrom pyvis.network import Network  # Interactive HTML visualization\n\n# Entity resolution\nfrom fuzzywuzzy import fuzz\nimport recordlinkage\n```\n\n### Common Patterns\n\n```python\n# Load and analyze a professional network\ndef analyze_professional_network(edges_df):\n    \"\"\"\n    edges_df: DataFrame with columns [source, target, weight, source_type]\n    \"\"\"\n    G = nx.from_pandas_edgelist(\n        edges_df,\n        source='source',\n        target='target',\n        edge_attr=['weight', 'source_type']\n    )\n\n    # Compute all centrality metrics\n    metrics = {\n        'betweenness': nx.betweenness_centrality(G, weight='weight'),\n        'degree': nx.degree_centrality(G),\n        'eigenvector': nx.eigenvector_centrality(G, weight='weight'),\n        'pagerank': nx.pagerank(G, weight='weight'),\n        'closeness': nx.closeness_centrality(G),\n    }\n\n    # Structural holes\n    constraint = nx.constraint(G, weight='weight')\n\n    # Community detection\n    communities = nx.community.louvain_communities(G)\n\n    # Classify nodes\n    def classify_gladwell(node):\n        bc = metrics['betweenness'][node]\n        dc = metrics['degree'][node]\n        ec = metrics['eigenvector'][node]\n\n        if bc > 0.1 and dc > 0.1:\n            return 'connector'\n        elif ec > 0.1 and dc < 0.05:\n            return 'maven'\n        elif dc > 0.05 and constraint.get(node, 1) < 0.3:\n            return 'salesman'\n        else:\n            return 'standard'\n\n    return {\n        'metrics': metrics,\n        'constraint': constraint,\n        'communities': communities,\n        'classifications': {n: classify_gladwell(n) for n in G.nodes()}\n    }\n```\n\n### Visualization Approaches\n\n```python\n# Interactive network visualization\ndef visualize_network_html(G, metrics, output_path='network.html'):\n    net = Network(height='800px', width='100%', bgcolor='#222222')\n\n    # Size by betweenness, color by community\n    for node in G.nodes():\n        size = 10 + 50 * metrics['betweenness'][node]\n        net.add_node(node, size=size, title=f\"BC: {metrics['betweenness'][node]:.3f}\")\n\n    for edge in G.edges():\n        net.add_edge(edge[0], edge[1])\n\n    net.show(output_path)\n```\n\n## Ethical Guidelines\n\n### What's Acceptable\n- Analyzing public data (conference speakers, publications, public LinkedIn)\n- Aggregate pattern analysis (trends, not individuals)\n- Opt-in organizational analysis (employee surveys)\n- Academic research with proper IRB\n\n### What's NOT Acceptable\n- Scraping private profiles without consent\n- Building surveillance systems disguised as networking tools\n- Selling individual data without consent\n- Discrimination based on network position\n- Stalking or harassment enablement\n\n### Best Practices\n- Anonymize individual results when possible\n- Focus on patterns, not personal details\n- Provide value to the people being analyzed\n- Be transparent about data collection\n- Allow opt-out from organizational analysis\n\n## When NOT to Use\n\nThis skill is NOT appropriate for:\n- **Surveillance**: Tracking individuals without consent\n- **Discrimination**: Using network position to exclude\n- **Manipulation**: Engineering social influence for harm\n- **Privacy violation**: Accessing non-public data\n- **Speculation without data**: Guessing network structure\n\n## Common Anti-Patterns\n\n### Anti-Pattern: Degree Obsession\n**What it looks like**: Only looking at who has most connections\n**Why it's wrong**: High degree often means noise; connectors differ from popular\n**What to do instead**: Use betweenness for bridging, eigenvector for influence quality\n\n### Anti-Pattern: Static Network Assumption\n**What it looks like**: Treating 5-year-old connections as current\n**Why it's wrong**: Networks evolve; old edges may be dead\n**What to do instead**: Recency-weight edges, verify currency of relationships\n\n### Anti-Pattern: Single-Source Reliance\n**What it looks like**: Using only LinkedIn data\n**Why it's wrong**: Missing professional relationships not on LinkedIn\n**What to do instead**: Multi-source fusion with source-appropriate weighting\n\n### Anti-Pattern: Ignoring Context\n**What it looks like**: High betweenness = valuable, regardless of domain\n**Why it's wrong**: Bridging irrelevant communities isn't useful\n**What to do instead**: Constrain analysis to relevant domain boundaries\n\n## Troubleshooting\n\n### Issue: Can't find enough data\n**Cause**: Domain is small or private\n**Fix**: Use snowball sampling from known seeds. Consider surveys. Look for adjacent public communities.\n\n### Issue: Too many false edges\n**Cause**: Over-weighting weak signals (e.g., same conference attendance)\n**Fix**: Require multiple independent signals for edge. Increase edge weight threshold.\n\n### Issue: Network too large to analyze\n**Cause**: Unconstrained boundary definition\n**Fix**: Apply k-core filtering, focus on high-weight edges, subsample by community.\n\n### Issue: Entity resolution failures\n**Cause**: Same person, different names across sources\n**Fix**: Use unique identifiers (ORCID, email), manual verification for high-centrality nodes.\n\n## Integration with Other Skills\n\nWorks well with:\n- **Career Biographer**: Network position informs career narrative framing\n- **Competitive Cartographer**: Map competitive landscape through relationship lens\n- **Research Analyst**: Deep dive on identified key figures\n- **CV Creator**: Leverage network insights for positioning\n\n## Example Analysis: AI Safety Superconnectors\n\n**Domain**: AI Safety research community, 2020-2024\n\n**Data Sources Used**:\n- arXiv preprints with \"AI safety\" keywords\n- NeurIPS/ICML workshop organizers\n- 80,000 Hours job board postings\n- AI safety-focused Twitter lists\n\n**Top Connectors Identified** (fictional example):\n1. **Dr. A** - Bridges academic labs and policy orgs\n   - Betweenness: 0.23 (very high)\n   - Communities: Berkeley CHAI, UK AI Safety Institute, Anthropic\n   - Type: Connector + Maven hybrid\n\n2. **B** - Connects technical and governance worlds\n   - Betweenness: 0.18\n   - Communities: DeepMind, CAIS, FHI\n   - Type: Broker/Salesman\n\n3. **C** - Information hub for field entrants\n   - PageRank: 0.09 (high in-degree)\n   - Creates educational content, runs reading groups\n   - Type: Maven\n\n**Structural Holes Identified**:\n- Gap between compute governance researchers and ML safety researchers\n- Bridge opportunity: Someone spanning both could be valuable hire\n\n**Networking Recommendations**:\n- Entry point to field: Dr. A's reading group\n- For policy roles: B can make introductions\n- For research roles: C's newsletter community\n"
    }
  ]
}