{
  "name": "hr-network-analyst",
  "type": "folder",
  "path": "hr-network-analyst",
  "children": [
    {
      "name": "references",
      "type": "folder",
      "path": "hr-network-analyst/references",
      "children": [
        {
          "name": "algorithms.md",
          "type": "file",
          "path": "hr-network-analyst/references/algorithms.md",
          "size": 11086,
          "content": "# Network Analysis Algorithms Reference\n\n## Centrality Measures\n\n### Betweenness Centrality\n\n**Mathematical Definition**:\n```\nCB(v) = Σ σst(v) / σst\n       s≠v≠t\n```\n\nWhere:\n- σst = total number of shortest paths from node s to node t\n- σst(v) = number of those paths passing through v\n\n**Interpretation**: Measures how often a node acts as a bridge along shortest paths.\n\n**NetworkX Implementation**:\n```python\nimport networkx as nx\n\n# Basic betweenness\nbc = nx.betweenness_centrality(G)\n\n# Weighted (edges with 'weight' attribute)\nbc_weighted = nx.betweenness_centrality(G, weight='weight')\n\n# Normalized (default) vs unnormalized\nbc_unnorm = nx.betweenness_centrality(G, normalized=False)\n\n# Approximate (faster for large graphs)\nbc_approx = nx.betweenness_centrality(G, k=100)  # sample k nodes\n```\n\n**Complexity**: O(VE) for unweighted, O(VE + V² log V) for weighted\n\n---\n\n### Degree Centrality\n\n**Mathematical Definition**:\n```\nCD(v) = deg(v) / (n-1)\n```\n\n**Interpretation**: Simple count of connections, normalized.\n\n**NetworkX Implementation**:\n```python\n# Undirected\ndc = nx.degree_centrality(G)\n\n# Directed\nin_dc = nx.in_degree_centrality(G)\nout_dc = nx.out_degree_centrality(G)\n```\n\n---\n\n### Eigenvector Centrality\n\n**Mathematical Definition**:\n```\nxi = (1/λ) Σ Aij xj\n           j\n```\n\nWhere A is the adjacency matrix and λ is the largest eigenvalue.\n\n**Interpretation**: A node is important if connected to other important nodes (recursive).\n\n**NetworkX Implementation**:\n```python\nec = nx.eigenvector_centrality(G)\n\n# With weights\nec_weighted = nx.eigenvector_centrality(G, weight='weight')\n\n# NumPy version (faster)\nec_numpy = nx.eigenvector_centrality_numpy(G)\n```\n\n---\n\n### PageRank\n\n**Mathematical Definition**:\n```\nPR(v) = (1-d)/N + d Σ PR(u)/L(u)\n                   u∈Bin(v)\n```\n\nWhere:\n- d = damping factor (typically 0.85)\n- N = total nodes\n- Bin(v) = nodes linking to v\n- L(u) = outgoing links from u\n\n**Interpretation**: Probability of random walker landing on node.\n\n**NetworkX Implementation**:\n```python\npr = nx.pagerank(G)\n\n# Custom damping\npr = nx.pagerank(G, alpha=0.9)\n\n# With weights\npr = nx.pagerank(G, weight='weight')\n```\n\n---\n\n### Closeness Centrality\n\n**Mathematical Definition**:\n```\nCC(v) = (n-1) / Σ d(v,u)\n               u≠v\n```\n\n**Interpretation**: Inverse of average distance to all other nodes.\n\n**NetworkX Implementation**:\n```python\ncc = nx.closeness_centrality(G)\n\n# For disconnected graphs\ncc = nx.closeness_centrality(G, wf_improved=True)\n```\n\n---\n\n## Structural Holes (Burt)\n\n### Constraint\n\n**Mathematical Definition**:\n```\nCi = Σ cij²\n     j\n\ncij = (pij + Σ piq × pqj)²\n           q\n```\n\nWhere pij = proportion of i's network invested in j.\n\n**Interpretation**: How constrained is a node by its network? Low constraint = spanning structural holes.\n\n**NetworkX Implementation**:\n```python\nconstraint = nx.constraint(G)\n\n# With weights\nconstraint_w = nx.constraint(G, weight='weight')\n\n# For specific nodes\nconstraint_node = nx.constraint(G, nodes=['Alice', 'Bob'])\n```\n\n### Effective Size\n\n**Mathematical Definition**:\n```\nES(i) = Σ [1 - Σ piq × mjq]\n        j    q≠j\n\nmjq = pjq / max(pkq for all k)\n```\n\n**Interpretation**: Redundancy-adjusted network size.\n\n**NetworkX Implementation**:\n```python\neff_size = nx.effective_size(G)\n```\n\n---\n\n## Community Detection\n\n### Louvain Algorithm\n\n**Implementation**:\n```python\nfrom networkx.algorithms.community import louvain_communities\n\ncommunities = louvain_communities(G)\n\n# With resolution parameter\ncommunities = louvain_communities(G, resolution=1.5)\n\n# Get partition as dict\npartition = {}\nfor i, comm in enumerate(communities):\n    for node in comm:\n        partition[node] = i\n```\n\n### Label Propagation\n\n```python\nfrom networkx.algorithms.community import label_propagation_communities\n\ncommunities = label_propagation_communities(G)\n```\n\n### Modularity Score\n\n```python\nfrom networkx.algorithms.community import modularity\n\nQ = modularity(G, communities)\n```\n\n---\n\n## Network Statistics\n\n### Basic Properties\n\n```python\n# Number of nodes and edges\nn = G.number_of_nodes()\nm = G.number_of_edges()\n\n# Density\ndensity = nx.density(G)\n\n# Average clustering coefficient\navg_clustering = nx.average_clustering(G)\n\n# Transitivity (global clustering)\ntransitivity = nx.transitivity(G)\n\n# Average shortest path (for connected graphs)\nif nx.is_connected(G):\n    avg_path = nx.average_shortest_path_length(G)\n\n# Diameter\nif nx.is_connected(G):\n    diameter = nx.diameter(G)\n```\n\n### K-Core Decomposition\n\n```python\n# Find k-core (subgraph where all nodes have degree >= k)\nk_core = nx.k_core(G, k=5)\n\n# Core number of each node\ncore_numbers = nx.core_number(G)\n```\n\n---\n\n## Useful Patterns\n\n### Multi-Layer Network Fusion\n\n```python\ndef fuse_networks(networks, weights):\n    \"\"\"\n    Combine multiple network sources with weights.\n\n    Args:\n        networks: dict of {source_name: networkx.Graph}\n        weights: dict of {source_name: float}\n\n    Returns:\n        Fused network with combined edge weights\n    \"\"\"\n    G_fused = nx.Graph()\n\n    for source, G_source in networks.items():\n        w = weights.get(source, 1.0)\n\n        for u, v, data in G_source.edges(data=True):\n            edge_weight = data.get('weight', 1.0) * w\n\n            if G_fused.has_edge(u, v):\n                G_fused[u][v]['weight'] += edge_weight\n                G_fused[u][v]['sources'].append(source)\n            else:\n                G_fused.add_edge(u, v, weight=edge_weight, sources=[source])\n\n    return G_fused\n```\n\n### Temporal Decay Weighting\n\n```python\nfrom datetime import datetime\nimport math\n\ndef apply_temporal_decay(G, date_attr='date', half_life_days=365):\n    \"\"\"\n    Apply exponential decay to edge weights based on recency.\n    \"\"\"\n    now = datetime.now()\n\n    for u, v, data in G.edges(data=True):\n        if date_attr in data:\n            edge_date = data[date_attr]\n            days_old = (now - edge_date).days\n            decay = math.exp(-math.log(2) * days_old / half_life_days)\n            data['weight'] = data.get('weight', 1.0) * decay\n\n    return G\n```\n\n### Gladwell Classification\n\n```python\ndef classify_gladwell(G, metrics=None):\n    \"\"\"\n    Classify nodes into Gladwell archetypes.\n\n    Returns dict mapping node -> archetype\n    \"\"\"\n    if metrics is None:\n        metrics = {\n            'betweenness': nx.betweenness_centrality(G),\n            'degree': nx.degree_centrality(G),\n            'eigenvector': nx.eigenvector_centrality(G),\n        }\n        try:\n            metrics['constraint'] = nx.constraint(G)\n        except:\n            metrics['constraint'] = {n: 0.5 for n in G.nodes()}\n\n    classifications = {}\n\n    # Compute thresholds (top percentile)\n    bc_threshold = sorted(metrics['betweenness'].values())[-int(len(G)*0.1)]\n    dc_threshold = sorted(metrics['degree'].values())[-int(len(G)*0.1)]\n    ec_threshold = sorted(metrics['eigenvector'].values())[-int(len(G)*0.1)]\n\n    for node in G.nodes():\n        bc = metrics['betweenness'][node]\n        dc = metrics['degree'][node]\n        ec = metrics['eigenvector'][node]\n        constraint = metrics['constraint'].get(node, 0.5)\n\n        if bc >= bc_threshold and dc >= dc_threshold:\n            classifications[node] = 'connector'\n        elif ec >= ec_threshold and dc < dc_threshold:\n            classifications[node] = 'maven'\n        elif dc >= dc_threshold and constraint < 0.3:\n            classifications[node] = 'salesman'\n        else:\n            classifications[node] = 'standard'\n\n    return classifications\n```\n\n---\n\n## Data Source APIs\n\n### Semantic Scholar\n\n```python\nimport requests\n\ndef get_author_collaborators(author_id):\n    \"\"\"Get co-authors from Semantic Scholar.\"\"\"\n    url = f\"https://api.semanticscholar.org/graph/v1/author/{author_id}\"\n    params = {\n        'fields': 'papers.authors'\n    }\n    resp = requests.get(url, params=params)\n    data = resp.json()\n\n    coauthors = set()\n    for paper in data.get('papers', []):\n        for author in paper.get('authors', []):\n            if author['authorId'] != author_id:\n                coauthors.add((author['authorId'], author['name']))\n\n    return coauthors\n```\n\n### GitHub\n\n```python\nimport requests\n\ndef get_repo_contributors(owner, repo, token=None):\n    \"\"\"Get contributors to a GitHub repo.\"\"\"\n    headers = {}\n    if token:\n        headers['Authorization'] = f'token {token}'\n\n    url = f\"https://api.github.com/repos/{owner}/{repo}/contributors\"\n    resp = requests.get(url, headers=headers)\n\n    return [(c['login'], c['contributions']) for c in resp.json()]\n\n\ndef build_github_network(repos, token=None):\n    \"\"\"Build collaboration network from list of repos.\"\"\"\n    G = nx.Graph()\n\n    for owner, repo in repos:\n        contributors = get_repo_contributors(owner, repo, token)\n\n        # Add edges between all contributors to same repo\n        for i, (user1, contrib1) in enumerate(contributors):\n            for user2, contrib2 in contributors[i+1:]:\n                if G.has_edge(user1, user2):\n                    G[user1][user2]['weight'] += 1\n                    G[user1][user2]['repos'].append(f\"{owner}/{repo}\")\n                else:\n                    G.add_edge(user1, user2, weight=1, repos=[f\"{owner}/{repo}\"])\n\n    return G\n```\n\n---\n\n## Visualization\n\n### Interactive HTML Network\n\n```python\nfrom pyvis.network import Network\n\ndef visualize_network(G, metrics, output='network.html'):\n    \"\"\"Create interactive HTML visualization.\"\"\"\n    net = Network(height='800px', width='100%', bgcolor='#1a1a2e')\n\n    # Color map for Gladwell types\n    colors = {\n        'connector': '#e94560',\n        'maven': '#0f3460',\n        'salesman': '#16c79a',\n        'standard': '#666666'\n    }\n\n    classifications = classify_gladwell(G, metrics)\n\n    for node in G.nodes():\n        bc = metrics['betweenness'][node]\n        size = 10 + 100 * bc\n        color = colors[classifications[node]]\n        title = f\"{node}<br>Type: {classifications[node]}<br>BC: {bc:.4f}\"\n\n        net.add_node(node, size=size, color=color, title=title)\n\n    for u, v, data in G.edges(data=True):\n        weight = data.get('weight', 1)\n        net.add_edge(u, v, value=weight)\n\n    net.show_buttons(filter_=['physics'])\n    net.save_graph(output)\n```\n\n### Static Matplotlib\n\n```python\nimport matplotlib.pyplot as plt\n\ndef plot_network_static(G, metrics, figsize=(12, 12)):\n    \"\"\"Create static network visualization.\"\"\"\n    fig, ax = plt.subplots(figsize=figsize)\n\n    # Layout\n    pos = nx.spring_layout(G, k=2, iterations=50)\n\n    # Node sizes by betweenness\n    node_sizes = [1000 * metrics['betweenness'][n] + 50 for n in G.nodes()]\n\n    # Node colors by eigenvector centrality\n    node_colors = [metrics['eigenvector'][n] for n in G.nodes()]\n\n    nx.draw_networkx(\n        G, pos, ax=ax,\n        node_size=node_sizes,\n        node_color=node_colors,\n        cmap=plt.cm.viridis,\n        with_labels=True,\n        font_size=8,\n        alpha=0.8\n    )\n\n    plt.colorbar(plt.cm.ScalarMappable(cmap=plt.cm.viridis),\n                 label='Eigenvector Centrality', ax=ax)\n\n    plt.tight_layout()\n    return fig\n```\n"
        },
        {
          "name": "data-sources-implementation.md",
          "type": "file",
          "path": "hr-network-analyst/references/data-sources-implementation.md",
          "size": 4092,
          "content": "# Data Sources & Implementation Reference\n\nNetwork construction from multiple professional data sources.\n\n## Primary Data Sources\n\n### LinkedIn Analysis\n**Extract**: Connection overlaps, shared experiences, endorsement patterns, group memberships, comment networks\n\n**Ethical considerations**: Respect rate limits/ToS, public data only, aggregate patterns\n\n```python\nfrom networkx import bipartite\npeople_projection = bipartite.projected_graph(B, people_nodes)\n```\n\n### Conference & Event Networks\n**Edge weights**:\n- Co-speaking at same event → strong\n- Same session/track → medium\n- Same conference → weak\n- Panel co-participation → very strong\n\n**High-value by domain**:\n- Tech: Strange Loop, QCon, domain-specific (RustConf)\n- AI/ML: NeurIPS, ICML, ICLR workshops\n- Data: Strata, dbt Coalesce\n\n### Publication & Co-authorship\n**Sources**: Semantic Scholar (open), Google Scholar, arXiv, DBLP, PubMed\n\n**Edge weighting**:\n- Co-authorship count (repeated = trust)\n- Citation flows\n- Author list position (first/last = more weight)\n\n### GitHub & Open Source\n**Extract**: Repo collaboration, review relationships, org membership, sponsorship, issues\n\n**Quality signals**:\n- Sustained > one-off contribution\n- Cross-project = broader network\n- Maintainer = trust indicator\n\n### Twitter/X Analysis\n**Extract**: Follow graphs, mutual follows, quote-tweet/reply networks, list memberships\n\n### Reddit & Community\n**Extract**: Cross-subreddit posting (bridges), comment interactions, moderator networks\n\n## Multi-Layer Network Fusion\n\n```python\nedge_weights = {\n    'coauthor': 1.0,           # Strongest\n    'conference_copanel': 0.8,\n    'linkedin_connection': 0.5,\n    'github_corepo': 0.6,\n    'twitter_mutual': 0.3,\n}\n\nG_unified = nx.Graph()\nfor source, weight in edge_weights.items():\n    for u, v in source_graphs[source].edges():\n        if G_unified.has_edge(u, v):\n            G_unified[u][v]['weight'] += weight\n        else:\n            G_unified.add_edge(u, v, weight=weight)\n```\n\n## Entity Resolution\n\n**Challenge**: Same person across sources\n- \"Jane Smith\" (LinkedIn)\n- \"J. Smith\" (papers)\n- \"@janesmith\" (Twitter)\n- \"jsmith\" (GitHub)\n\n**Approaches**:\n- Email as unique identifier\n- ORCID for researchers\n- LinkedIn URL as canonical\n- Fuzzy matching with verification\n\n## Analysis Implementation\n\n```python\nimport networkx as nx\nimport pandas as pd\nfrom pyvis.network import Network\n\ndef analyze_professional_network(edges_df):\n    G = nx.from_pandas_edgelist(edges_df, 'source', 'target', ['weight'])\n\n    metrics = {\n        'betweenness': nx.betweenness_centrality(G, weight='weight'),\n        'degree': nx.degree_centrality(G),\n        'eigenvector': nx.eigenvector_centrality(G, weight='weight'),\n        'pagerank': nx.pagerank(G, weight='weight'),\n    }\n\n    constraint = nx.constraint(G, weight='weight')\n    communities = nx.community.louvain_communities(G)\n\n    def classify_gladwell(node):\n        bc = metrics['betweenness'][node]\n        dc = metrics['degree'][node]\n        ec = metrics['eigenvector'][node]\n\n        if bc > 0.1 and dc > 0.1:\n            return 'connector'\n        elif ec > 0.1 and dc < 0.05:\n            return 'maven'\n        elif dc > 0.05 and constraint.get(node, 1) < 0.3:\n            return 'salesman'\n        return 'standard'\n\n    return {\n        'metrics': metrics,\n        'constraint': constraint,\n        'communities': communities,\n        'classifications': {n: classify_gladwell(n) for n in G.nodes()}\n    }\n```\n\n## Visualization\n\n```python\ndef visualize_network_html(G, metrics, output_path='network.html'):\n    net = Network(height='800px', width='100%', bgcolor='#222222')\n\n    for node in G.nodes():\n        size = 10 + 50 * metrics['betweenness'][node]\n        net.add_node(node, size=size, title=f\"BC: {metrics['betweenness'][node]:.3f}\")\n\n    for edge in G.edges():\n        net.add_edge(edge[0], edge[1])\n\n    net.show(output_path)\n```\n\n## Temporal Considerations\n\n- Recency-weight edges (recent > old)\n- Track rising stars (centrality trajectory)\n- Identify fading connections\n- Seasonal patterns (conference cycles)\n"
        },
        {
          "name": "data-sources.md",
          "type": "file",
          "path": "hr-network-analyst/references/data-sources.md",
          "size": 25534,
          "content": "# Professional Network Data Acquisition Guide\n\n## Executive Summary\n\nThis document provides a comprehensive guide to acquiring professional network data for graph analysis, with a focus on identifying superconnectors, influence brokers, and knowledge mavens. We cover official APIs, third-party data providers, scraping tools, and alternative network reconstruction strategies—along with legal considerations and practical implementation code.\n\n**Key Insight**: For network analysis purposes, reconstructing professional networks from public sources (publications, conferences, GitHub) often yields higher-quality relationship data than LinkedIn scraping, which captures connections rather than actual collaboration.\n\n---\n\n## Table of Contents\n\n1. [Data Source Hierarchy](#data-source-hierarchy)\n2. [Official LinkedIn Routes](#official-linkedin-routes)\n3. [Third-Party Data Providers](#third-party-data-providers)\n4. [Scraping Tools](#scraping-tools)\n5. [Alternative Network Reconstruction](#alternative-network-reconstruction)\n6. [Legal Considerations](#legal-considerations)\n7. [Implementation Guide](#implementation-guide)\n8. [Recommended Strategy](#recommended-strategy)\n\n---\n\n## Data Source Hierarchy\n\n### Quality vs. Accessibility Matrix\n\n| Data Source | Relationship Quality | Accessibility | Cost | Legal Risk |\n|-------------|---------------------|---------------|------|------------|\n| Co-authorship (papers) | ★★★★★ | High | Free | None |\n| GitHub collaboration | ★★★★☆ | High | Free | None |\n| Conference co-speaking | ★★★★☆ | Medium | Free | None |\n| LinkedIn connections | ★★☆☆☆ | Low | $$$ | Medium |\n| Email/calendar data | ★★★★★ | Very Low | N/A | High |\n\n**Why LinkedIn connections are low quality for network analysis:**\n- People accept connections from strangers\n- No indication of relationship strength\n- No collaboration signal\n- Includes recruiters, salespeople, random requests\n\n**Why co-authorship is gold:**\n- Months of collaboration required\n- Trust signal (putting your name on shared work)\n- Repeated co-authorship = strong relationship\n- Position on author list indicates role\n\n---\n\n## Official LinkedIn Routes\n\n### 1. Personal Data Export\n\n**Access**: Settings → Data Privacy → Get a copy of your data\n\n**What you get**:\n- `Connections.csv`: Name, company, position, connected date\n- `Messages.csv`: Message history\n- `Invitations.csv`: Sent/received invitations\n\n**Limitations**:\n- Only YOUR 1st-degree network\n- No 2nd-degree visibility\n- No relationship strength indicators\n\n**Use case**: Analyzing your own network position, finding paths to targets\n\n```python\nimport pandas as pd\n\n# Load your LinkedIn export\nconnections = pd.read_csv('Connections.csv', skiprows=3)\n\n# Basic analysis\nprint(f\"Total connections: {len(connections)}\")\nprint(f\"Companies represented: {connections['Company'].nunique()}\")\n\n# Find potential bridges (people at companies you have few connections to)\ncompany_counts = connections['Company'].value_counts()\nrare_companies = company_counts[company_counts <= 2].index\nbridges = connections[connections['Company'].isin(rare_companies)]\n```\n\n### 2. LinkedIn Sales Navigator\n\n**Pricing**: $99.99/mo (Core) to $179.99/mo (Advanced)\n\n**Capabilities**:\n- Advanced search filters (industry, company size, seniority)\n- Lead lists and saved searches\n- InMail credits\n- Account mapping (org charts)\n\n**Export options**:\n- Lead lists to CSV (limited fields)\n- CRM integrations (Salesforce, HubSpot)\n\n**For network analysis**:\n- Can identify target individuals\n- No graph structure data\n- Best combined with enrichment tools\n\n### 3. LinkedIn APIs (Enterprise)\n\n**Available APIs**:\n- Marketing API (ad targeting, company pages)\n- Talent Solutions API (recruiting)\n- Learning API (course completions)\n- Consumer API (deprecated for most uses)\n\n**Access requirements**:\n- Enterprise partnership agreement\n- Significant annual spend ($50K+)\n- Compliance review\n\n**Reality**: Not accessible for most network analysis use cases.\n\n---\n\n## Third-Party Data Providers\n\n### Tier 1: Full-Service B2B Data\n\n#### Apollo.io\n\n**Data**: 275M+ contacts, 73M+ companies\n\n**Pricing**:\n- Free: 50 credits/month\n- Basic: $49/mo (900 credits)\n- Professional: $99/mo (unlimited emails)\n\n**Best for**: Lead generation, email finding, basic enrichment\n\n**API Example**:\n```python\nimport requests\n\nAPOLLO_API_KEY = 'your_key'\n\ndef search_people(domain, title_keywords):\n    \"\"\"Search Apollo for people at a company.\"\"\"\n    response = requests.post(\n        'https://api.apollo.io/v1/mixed_people/search',\n        headers={'X-Api-Key': APOLLO_API_KEY},\n        json={\n            'q_organization_domains': domain,\n            'person_titles': title_keywords,\n            'page': 1,\n            'per_page': 25\n        }\n    )\n    return response.json()['people']\n\n# Find ML engineers at top AI labs\nfor domain in ['anthropic.com', 'openai.com', 'deepmind.com']:\n    people = search_people(domain, ['Machine Learning', 'Research'])\n    print(f\"{domain}: {len(people)} people found\")\n```\n\n#### ZoomInfo\n\n**Data**: 100M+ business profiles, org charts, intent data\n\n**Pricing**: Enterprise (contact sales, typically $15K+/year)\n\n**Best for**: Enterprise recruiting, account-based marketing\n\n**Network analysis value**: Org charts can reveal internal influence structures\n\n#### Clearbit (now Breeze by HubSpot)\n\n**Data**: Company + person enrichment\n\n**Pricing**: Per-lookup ($0.05-0.20 per enrichment)\n\n**API Example**:\n```python\nimport clearbit\n\nclearbit.key = 'your_key'\n\n# Enrich a person by email\nperson = clearbit.Person.find(email='elon@tesla.com', stream=True)\n\nprint(f\"Name: {person['name']['fullName']}\")\nprint(f\"Role: {person['employment']['title']}\")\nprint(f\"Company: {person['employment']['name']}\")\nprint(f\"LinkedIn: {person['linkedin']['handle']}\")\n```\n\n### Tier 2: LinkedIn-Specific APIs\n\n#### Proxycurl\n\n**What it does**: API wrapper for LinkedIn profile data\n\n**Pricing**:\n- $0.01 per profile lookup\n- $0.003 per company lookup\n- Bulk discounts available\n\n**Data returned**:\n- Full profile (experience, education, skills)\n- Company data\n- Job postings\n\n**API Example**:\n```python\nimport requests\n\nPROXYCURL_API_KEY = 'your_key'\n\ndef get_linkedin_profile(linkedin_url):\n    \"\"\"Fetch full LinkedIn profile data.\"\"\"\n    response = requests.get(\n        'https://nubela.co/proxycurl/api/v2/linkedin',\n        params={'url': linkedin_url},\n        headers={'Authorization': f'Bearer {PROXYCURL_API_KEY}'}\n    )\n    return response.json()\n\n# Get profile data\nprofile = get_linkedin_profile('https://linkedin.com/in/satlokomern')\n\n# Extract for network analysis\nperson = {\n    'name': profile['full_name'],\n    'current_company': profile['experiences'][0]['company'] if profile['experiences'] else None,\n    'past_companies': [exp['company'] for exp in profile['experiences']],\n    'education': [edu['school'] for edu in profile['education']],\n    'connections': profile.get('connections')  # Often not available\n}\n```\n\n#### People Data Labs\n\n**What it does**: Bulk access to 1.5B+ person records\n\n**Pricing**: API credits, starting ~$0.01/record\n\n**Best for**: Large-scale network reconstruction\n\n**Key advantage**: Employment history allows you to infer \"worked together\" relationships\n\n```python\nimport requests\n\nPDL_API_KEY = 'your_key'\n\ndef find_coworkers(company, year_range):\n    \"\"\"Find people who worked at a company during a time period.\"\"\"\n    response = requests.get(\n        'https://api.peopledatalabs.com/v5/person/search',\n        headers={'X-Api-Key': PDL_API_KEY},\n        params={\n            'query': f\"experience.company.name:{company} AND experience.start_date:[{year_range[0]} TO {year_range[1]}]\",\n            'size': 100\n        }\n    )\n    return response.json()['data']\n\n# Find people who worked at Stripe 2018-2022\nstripe_alumni = find_coworkers('Stripe', ('2018', '2022'))\n\n# People with overlapping tenure likely know each other\n# This is MUCH better than LinkedIn connections for inferring real relationships\n```\n\n---\n\n## Scraping Tools\n\n### Browser Automation Tools\n\nThese tools automate YOUR logged-in LinkedIn session:\n\n#### Phantombuster\n\n**Pricing**: Free tier (2 hours/day) to $900/mo (enterprise)\n\n**Capabilities**:\n- Profile visitor (triggers profile views)\n- Profile scraper (export profile data)\n- Search export (save search results)\n- Connection automation\n\n**Rate limits**: ~80 profiles/day safely\n\n**Example workflow**:\n```\n1. Sales Navigator search → Save to CSV\n2. Phantombuster profile scraper → Enrich with full data\n3. Export to Google Sheets or Airtable\n4. NetworkX analysis\n```\n\n#### Evaboot\n\n**Pricing**: $49-99/mo\n\n**Specialization**: Sales Navigator export specifically\n\n**What it does**: One-click export of Sales Navigator searches\n\n#### Dux-Soup / Octopus CRM / Waalaxy\n\n**Pricing**: $15-100/mo range\n\n**Capabilities**: Similar browser automation, varying features\n\n### Scraping Risks & Mitigation\n\n**Account ban risk factors**:\n- Too many profile views (&gt;100/day)\n- Automated patterns (regular intervals)\n- Scraping from non-premium account\n- Using headless browsers\n\n**Mitigation strategies**:\n```python\nimport random\nimport time\n\ndef human_like_delay():\n    \"\"\"Randomized delays to avoid detection.\"\"\"\n    base_delay = random.uniform(30, 90)  # 30-90 seconds\n    jitter = random.gauss(0, 10)  # Normal distribution jitter\n    return max(15, base_delay + jitter)\n\ndef scrape_with_delays(profile_urls):\n    \"\"\"Scrape with human-like patterns.\"\"\"\n    results = []\n    for i, url in enumerate(profile_urls):\n        # Don't scrape more than ~50/day\n        if i >= 50:\n            print(\"Daily limit reached\")\n            break\n\n        result = scrape_profile(url)\n        results.append(result)\n\n        delay = human_like_delay()\n        print(f\"Waiting {delay:.1f}s before next request...\")\n        time.sleep(delay)\n\n        # Take breaks\n        if i > 0 and i % 10 == 0:\n            long_break = random.uniform(300, 600)  # 5-10 minute break\n            print(f\"Taking {long_break/60:.1f} minute break...\")\n            time.sleep(long_break)\n\n    return results\n```\n\n---\n\n## Alternative Network Reconstruction\n\n### This is the recommended approach for finding superconnectors.\n\n### 1. Publication Co-authorship Networks\n\n**Why this is gold**: Co-authorship requires months of collaboration and trust.\n\n#### Semantic Scholar API (Free, Excellent)\n\n```python\nimport requests\nfrom collections import defaultdict\nimport networkx as nx\n\nclass SemanticScholarNetwork:\n    BASE_URL = 'https://api.semanticscholar.org/graph/v1'\n\n    def __init__(self):\n        self.G = nx.Graph()\n        self.author_cache = {}\n\n    def search_papers(self, query, limit=100):\n        \"\"\"Search for papers by topic.\"\"\"\n        response = requests.get(\n            f'{self.BASE_URL}/paper/search',\n            params={\n                'query': query,\n                'limit': limit,\n                'fields': 'title,authors,year,citationCount'\n            }\n        )\n        return response.json().get('data', [])\n\n    def get_author_papers(self, author_id):\n        \"\"\"Get all papers by an author.\"\"\"\n        response = requests.get(\n            f'{self.BASE_URL}/author/{author_id}',\n            params={'fields': 'papers.authors,papers.title,papers.year'}\n        )\n        return response.json()\n\n    def build_coauthorship_network(self, papers):\n        \"\"\"Build network from list of papers.\"\"\"\n        for paper in papers:\n            authors = paper.get('authors', [])\n            author_ids = [a['authorId'] for a in authors if a.get('authorId')]\n\n            # Add nodes\n            for author in authors:\n                if author.get('authorId'):\n                    self.G.add_node(\n                        author['authorId'],\n                        name=author.get('name', 'Unknown')\n                    )\n\n            # Add edges between all co-authors\n            for i, a1 in enumerate(author_ids):\n                for a2 in author_ids[i+1:]:\n                    if self.G.has_edge(a1, a2):\n                        self.G[a1][a2]['weight'] += 1\n                        self.G[a1][a2]['papers'].append(paper.get('title'))\n                    else:\n                        self.G.add_edge(a1, a2, weight=1, papers=[paper.get('title')])\n\n        return self.G\n\n    def find_superconnectors(self, top_n=20):\n        \"\"\"Find authors with highest betweenness centrality.\"\"\"\n        bc = nx.betweenness_centrality(self.G, weight='weight')\n        sorted_bc = sorted(bc.items(), key=lambda x: x[1], reverse=True)\n\n        results = []\n        for author_id, score in sorted_bc[:top_n]:\n            results.append({\n                'author_id': author_id,\n                'name': self.G.nodes[author_id].get('name'),\n                'betweenness': score,\n                'degree': self.G.degree(author_id),\n                'collaborators': list(self.G.neighbors(author_id))\n            })\n\n        return results\n\n\n# Usage Example: Find AI Safety superconnectors\nnetwork = SemanticScholarNetwork()\n\n# Search for AI safety papers\npapers = network.search_papers('AI safety alignment', limit=500)\nprint(f\"Found {len(papers)} papers\")\n\n# Build co-authorship network\nG = network.build_coauthorship_network(papers)\nprint(f\"Network: {G.number_of_nodes()} authors, {G.number_of_edges()} collaborations\")\n\n# Find superconnectors\nsuperconnectors = network.find_superconnectors(top_n=10)\nfor sc in superconnectors:\n    print(f\"{sc['name']}: BC={sc['betweenness']:.4f}, {sc['degree']} collaborators\")\n```\n\n#### arXiv API (Free)\n\n```python\nimport arxiv\nfrom collections import defaultdict\n\ndef build_arxiv_network(query, max_results=500):\n    \"\"\"Build co-authorship network from arXiv papers.\"\"\"\n\n    search = arxiv.Search(\n        query=query,\n        max_results=max_results,\n        sort_by=arxiv.SortCriterion.SubmittedDate\n    )\n\n    G = nx.Graph()\n\n    for paper in search.results():\n        authors = [a.name for a in paper.authors]\n\n        # Add nodes\n        for author in authors:\n            if author not in G:\n                G.add_node(author, papers=1)\n            else:\n                G.nodes[author]['papers'] += 1\n\n        # Add edges\n        for i, a1 in enumerate(authors):\n            for a2 in authors[i+1:]:\n                if G.has_edge(a1, a2):\n                    G[a1][a2]['weight'] += 1\n                else:\n                    G.add_edge(a1, a2, weight=1)\n\n    return G\n\n# Example: LLM research network\nG = build_arxiv_network('cat:cs.CL AND (large language model OR LLM)', max_results=1000)\n```\n\n### 2. GitHub Collaboration Networks\n\n```python\nimport requests\nfrom collections import defaultdict\n\nclass GitHubNetwork:\n    def __init__(self, token):\n        self.token = token\n        self.headers = {'Authorization': f'token {token}'}\n        self.G = nx.Graph()\n\n    def get_repo_contributors(self, owner, repo):\n        \"\"\"Get all contributors to a repository.\"\"\"\n        contributors = []\n        page = 1\n\n        while True:\n            response = requests.get(\n                f'https://api.github.com/repos/{owner}/{repo}/contributors',\n                headers=self.headers,\n                params={'page': page, 'per_page': 100}\n            )\n\n            if response.status_code != 200:\n                break\n\n            data = response.json()\n            if not data:\n                break\n\n            contributors.extend(data)\n            page += 1\n\n        return contributors\n\n    def get_pr_reviewers(self, owner, repo, limit=100):\n        \"\"\"Get PR review relationships (high trust signal).\"\"\"\n        response = requests.get(\n            f'https://api.github.com/repos/{owner}/{repo}/pulls',\n            headers=self.headers,\n            params={'state': 'all', 'per_page': limit}\n        )\n\n        reviews = defaultdict(lambda: defaultdict(int))\n\n        for pr in response.json():\n            author = pr['user']['login']\n\n            # Get reviewers for this PR\n            review_response = requests.get(\n                pr['url'] + '/reviews',\n                headers=self.headers\n            )\n\n            for review in review_response.json():\n                reviewer = review['user']['login']\n                if reviewer != author:\n                    reviews[author][reviewer] += 1\n\n        return reviews\n\n    def build_network_from_repos(self, repos):\n        \"\"\"Build collaboration network from list of repos.\"\"\"\n        for owner, repo in repos:\n            contributors = self.get_repo_contributors(owner, repo)\n\n            # Add nodes\n            for c in contributors:\n                login = c['login']\n                if login not in self.G:\n                    self.G.add_node(login, contributions=c['contributions'])\n                else:\n                    self.G.nodes[login]['contributions'] += c['contributions']\n\n            # Add edges between contributors (same repo = collaboration)\n            logins = [c['login'] for c in contributors]\n            for i, u1 in enumerate(logins):\n                for u2 in logins[i+1:]:\n                    if self.G.has_edge(u1, u2):\n                        self.G[u1][u2]['weight'] += 1\n                        self.G[u1][u2]['repos'].append(f\"{owner}/{repo}\")\n                    else:\n                        self.G.add_edge(u1, u2, weight=1, repos=[f\"{owner}/{repo}\"])\n\n        return self.G\n\n\n# Example: Build ML framework contributor network\ngithub = GitHubNetwork('your_token')\n\nml_repos = [\n    ('pytorch', 'pytorch'),\n    ('tensorflow', 'tensorflow'),\n    ('huggingface', 'transformers'),\n    ('langchain-ai', 'langchain'),\n    ('anthropics', 'anthropic-sdk-python'),\n]\n\nG = github.build_network_from_repos(ml_repos)\nprint(f\"Network: {G.number_of_nodes()} developers, {G.number_of_edges()} collaborations\")\n```\n\n### 3. Conference Speaker Networks\n\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\n\nclass ConferenceNetwork:\n    def __init__(self):\n        self.G = nx.Graph()\n        self.speakers = defaultdict(list)  # speaker -> conferences\n\n    def scrape_neurips_speakers(self, year):\n        \"\"\"Scrape NeurIPS speaker list.\"\"\"\n        # Note: Actual implementation depends on conference website structure\n        # This is a template\n        url = f'https://neurips.cc/{year}/Schedule'\n        response = requests.get(url)\n        soup = BeautifulSoup(response.text, 'html.parser')\n\n        sessions = []\n        # Parse session data...\n        return sessions\n\n    def add_conference_edges(self, conference_name, sessions):\n        \"\"\"Add edges for co-presenters at same session.\"\"\"\n        for session in sessions:\n            speakers = session.get('speakers', [])\n\n            # Add nodes\n            for speaker in speakers:\n                if speaker not in self.G:\n                    self.G.add_node(speaker, conferences=[conference_name])\n                else:\n                    if conference_name not in self.G.nodes[speaker]['conferences']:\n                        self.G.nodes[speaker]['conferences'].append(conference_name)\n\n            # Same panel/session = strong edge\n            for i, s1 in enumerate(speakers):\n                for s2 in speakers[i+1:]:\n                    if self.G.has_edge(s1, s2):\n                        self.G[s1][s2]['weight'] += 2  # Co-panel is strong signal\n                    else:\n                        self.G.add_edge(s1, s2, weight=2, type='co_panel')\n\n            # Same conference = weak edge (add later in batch)\n\n    def add_same_conference_edges(self):\n        \"\"\"Add weak edges between all speakers at same conference.\"\"\"\n        conference_speakers = defaultdict(list)\n\n        for node in self.G.nodes():\n            for conf in self.G.nodes[node].get('conferences', []):\n                conference_speakers[conf].append(node)\n\n        for conf, speakers in conference_speakers.items():\n            for i, s1 in enumerate(speakers):\n                for s2 in speakers[i+1:]:\n                    if not self.G.has_edge(s1, s2):\n                        self.G.add_edge(s1, s2, weight=0.5, type='same_conference')\n```\n\n### 4. Multi-Source Fusion\n\n```python\ndef fuse_professional_networks(networks, weights):\n    \"\"\"\n    Combine multiple network sources with configurable weights.\n\n    Args:\n        networks: dict of {source_name: nx.Graph}\n        weights: dict of {source_name: float}\n\n    Returns:\n        Unified nx.Graph with combined edge weights\n    \"\"\"\n    G_unified = nx.Graph()\n\n    # Entity resolution: normalize names across sources\n    name_mapping = {}  # Maps variations to canonical name\n\n    for source, G_source in networks.items():\n        source_weight = weights.get(source, 1.0)\n\n        for u, v, data in G_source.edges(data=True):\n            # Normalize names\n            u_canonical = name_mapping.get(u, u)\n            v_canonical = name_mapping.get(v, v)\n\n            edge_weight = data.get('weight', 1.0) * source_weight\n\n            if G_unified.has_edge(u_canonical, v_canonical):\n                G_unified[u_canonical][v_canonical]['weight'] += edge_weight\n                G_unified[u_canonical][v_canonical]['sources'].append(source)\n            else:\n                G_unified.add_edge(\n                    u_canonical, v_canonical,\n                    weight=edge_weight,\n                    sources=[source]\n                )\n\n    return G_unified\n\n\n# Example usage\nnetworks = {\n    'semantic_scholar': coauthorship_network,\n    'github': github_network,\n    'conferences': conference_network,\n}\n\nweights = {\n    'semantic_scholar': 1.0,  # Strongest signal\n    'github': 0.8,            # Strong signal\n    'conferences': 0.6,       # Medium signal\n}\n\nunified = fuse_professional_networks(networks, weights)\n\n# Now analyze the unified network\nbc = nx.betweenness_centrality(unified, weight='weight')\nsuperconnectors = sorted(bc.items(), key=lambda x: x[1], reverse=True)[:20]\n```\n\n---\n\n## Legal Considerations\n\n### LinkedIn Terms of Service\n\n**Prohibited activities** (Section 8.2):\n- Scraping or copying profiles\n- Using bots or automated tools\n- Circumventing access restrictions\n\n**Consequences**:\n- Account suspension/termination\n- Legal action (rare but possible)\n\n### hiQ Labs v. LinkedIn (2022)\n\n**Key ruling**: Scraping *publicly available* LinkedIn data does not violate the Computer Fraud and Abuse Act (CFAA).\n\n**What this means**:\n- Criminal liability unlikely for public data scraping\n- LinkedIn can still enforce ToS (ban accounts)\n- Civil liability less clear\n\n**Limitations**:\n- Does not apply to logged-in scraping\n- Does not override other laws (GDPR, CCPA)\n- LinkedIn may use technical measures\n\n### GDPR Considerations (EU)\n\nIf processing EU residents' data:\n- Need legal basis (legitimate interest, consent)\n- Data minimization required\n- Right to erasure must be honored\n- Document your processing activities\n\n### Best Practices for Compliance\n\n1. **Prefer public APIs** (Semantic Scholar, GitHub, arXiv)\n2. **Use official exports** for your own data\n3. **Buy from legitimate providers** who handle compliance\n4. **Document business purpose** for any scraping\n5. **Don't store unnecessary PII**\n6. **Honor opt-out requests**\n\n---\n\n## Recommended Strategy\n\n### For Finding Superconnectors\n\n**Don't start with LinkedIn.** Start with better data:\n\n```\nWeek 1: Define Target Domain\n├── List key conferences\n├── Identify top journals/venues\n├── Find major open source projects\n└── Note industry thought leaders (seed nodes)\n\nWeek 2: Build Publication Network\n├── Semantic Scholar API for papers\n├── arXiv for preprints\n├── Google Scholar for citations\n└── Identify high-betweenness authors\n\nWeek 3: Add Collaboration Signals\n├── GitHub contributor networks\n├── Conference speaker lists\n├── Podcast guest appearances\n└── Fuse with publication network\n\nWeek 4: Targeted Enrichment\n├── Proxycurl for top 50 candidates\n├── Apollo for contact info\n├── LinkedIn manual research\n└── Prioritized outreach list\n```\n\n### Cost-Effective Stack\n\n| Purpose | Tool | Monthly Cost |\n|---------|------|--------------|\n| Publication data | Semantic Scholar API | Free |\n| Code collaboration | GitHub API | Free |\n| Conference data | Web scraping | Free |\n| Contact enrichment | Apollo.io | $49 |\n| LinkedIn profiles | Proxycurl | ~$20 (pay per use) |\n| **Total** | | **~$70/month** |\n\n### For Organizational Network Analysis\n\n**Internal data sources** (requires proper authorization):\n- Slack/Teams message patterns\n- Meeting co-attendance (calendar)\n- Email headers (not content)\n- Document collaboration\n- Code review assignments\n\n**Survey-based ONA**:\n- \"Who do you go to for advice?\"\n- \"Who do you collaborate with most?\"\n- \"Who is influential in decisions?\"\n\nThis is the gold standard for ONA—explicit relationship mapping with consent.\n\n---\n\n## Conclusion\n\nFor professional network analysis aimed at finding superconnectors:\n\n1. **LinkedIn is overrated** for this purpose—connections ≠ relationships\n2. **Co-authorship and code collaboration** are stronger signals\n3. **Public data sources** are legally safer and often higher quality\n4. **Targeted enrichment** of top candidates is more cost-effective than bulk scraping\n5. **Multi-source fusion** produces the most accurate network maps\n\nThe best superconnector identification comes from asking: \"Who has actually worked with people across multiple communities?\"—and that question is best answered by publication, code, and conference data rather than LinkedIn connection counts.\n"
        },
        {
          "name": "graph-databases.md",
          "type": "file",
          "path": "hr-network-analyst/references/graph-databases.md",
          "size": 15063,
          "content": "# Graph Database Analysis Reference\n\n## Neo4j\n\nNeo4j is the most popular graph database for professional network analysis. It uses Cypher query language and has built-in graph data science algorithms.\n\n### Setup\n\n```cypher\n// Create constraint for unique person IDs\nCREATE CONSTRAINT person_id IF NOT EXISTS\nFOR (p:Person) REQUIRE p.id IS UNIQUE;\n\n// Create index for faster lookups\nCREATE INDEX person_name IF NOT EXISTS\nFOR (p:Person) ON (p.name);\n```\n\n### Data Model for Professional Networks\n\n```cypher\n// Node types\n(:Person {id, name, email, role, company, linkedin_url})\n(:Company {id, name, industry, size})\n(:Conference {id, name, year, location})\n(:Publication {id, title, year, venue, doi})\n(:Project {id, name, repo_url, tech_stack})\n\n// Relationship types\n(:Person)-[:WORKS_AT {since, role}]->(:Company)\n(:Person)-[:SPOKE_AT {talk_title, track}]->(:Conference)\n(:Person)-[:COAUTHORED {position}]->(:Publication)\n(:Person)-[:CONTRIBUTED_TO {commits, role}]->(:Project)\n(:Person)-[:KNOWS {strength, source, since}]->(:Person)\n(:Person)-[:COLLABORATED_WITH {project, duration}]->(:Person)\n```\n\n### Loading Data\n\n```cypher\n// Load from CSV\nLOAD CSV WITH HEADERS FROM 'file:///people.csv' AS row\nMERGE (p:Person {id: row.id})\nSET p.name = row.name,\n    p.email = row.email,\n    p.role = row.role;\n\n// Load edges\nLOAD CSV WITH HEADERS FROM 'file:///connections.csv' AS row\nMATCH (a:Person {id: row.source})\nMATCH (b:Person {id: row.target})\nMERGE (a)-[r:KNOWS]->(b)\nSET r.strength = toFloat(row.strength),\n    r.source = row.data_source;\n```\n\n### Centrality Algorithms (Graph Data Science Library)\n\n```cypher\n// First, create a graph projection\nCALL gds.graph.project(\n  'professional-network',\n  'Person',\n  {\n    KNOWS: {\n      orientation: 'UNDIRECTED',\n      properties: ['strength']\n    }\n  }\n);\n\n// Betweenness Centrality\nCALL gds.betweenness.stream('professional-network')\nYIELD nodeId, score\nRETURN gds.util.asNode(nodeId).name AS name, score\nORDER BY score DESC\nLIMIT 20;\n\n// Write back to nodes\nCALL gds.betweenness.write('professional-network', {\n  writeProperty: 'betweenness'\n});\n\n// PageRank\nCALL gds.pageRank.stream('professional-network', {\n  dampingFactor: 0.85,\n  maxIterations: 20\n})\nYIELD nodeId, score\nRETURN gds.util.asNode(nodeId).name AS name, score\nORDER BY score DESC\nLIMIT 20;\n\n// Eigenvector Centrality\nCALL gds.eigenvector.stream('professional-network', {\n  maxIterations: 100\n})\nYIELD nodeId, score\nRETURN gds.util.asNode(nodeId).name AS name, score\nORDER BY score DESC;\n\n// Degree Centrality\nCALL gds.degree.stream('professional-network')\nYIELD nodeId, score\nRETURN gds.util.asNode(nodeId).name AS name, score\nORDER BY score DESC;\n\n// Closeness Centrality\nCALL gds.closeness.stream('professional-network')\nYIELD nodeId, score\nRETURN gds.util.asNode(nodeId).name AS name, score\nORDER BY score DESC;\n```\n\n### Community Detection\n\n```cypher\n// Louvain community detection\nCALL gds.louvain.stream('professional-network')\nYIELD nodeId, communityId\nRETURN gds.util.asNode(nodeId).name AS name, communityId\nORDER BY communityId;\n\n// Write communities to nodes\nCALL gds.louvain.write('professional-network', {\n  writeProperty: 'community'\n});\n\n// Label Propagation\nCALL gds.labelPropagation.stream('professional-network')\nYIELD nodeId, communityId\nRETURN communityId, collect(gds.util.asNode(nodeId).name) AS members\nORDER BY size(members) DESC;\n\n// Weakly Connected Components\nCALL gds.wcc.stream('professional-network')\nYIELD nodeId, componentId\nRETURN componentId, count(*) AS size\nORDER BY size DESC;\n```\n\n### Finding Bridges and Superconnectors\n\n```cypher\n// Find people who bridge communities\nMATCH (p:Person)\nWHERE p.betweenness > 0.1\nRETURN p.name, p.betweenness, p.community\nORDER BY p.betweenness DESC;\n\n// Find people connected to multiple communities\nMATCH (p:Person)-[:KNOWS]-(other:Person)\nWITH p, collect(DISTINCT other.community) AS connected_communities\nWHERE size(connected_communities) >= 3\nRETURN p.name, connected_communities, size(connected_communities) AS bridge_score\nORDER BY bridge_score DESC;\n\n// Find structural hole spanners\nMATCH (p:Person)-[:KNOWS]-(a:Person)\nMATCH (p)-[:KNOWS]-(b:Person)\nWHERE a.community <> b.community AND NOT (a)-[:KNOWS]-(b)\nWITH p, count(DISTINCT [a.community, b.community]) AS holes_spanned\nWHERE holes_spanned > 5\nRETURN p.name, holes_spanned\nORDER BY holes_spanned DESC;\n```\n\n### Gladwell Classification Query\n\n```cypher\n// Classify nodes by Gladwell archetype\nMATCH (p:Person)\nWITH p,\n     percentileDisc(p.betweenness, 0.9) OVER () AS bc_threshold,\n     percentileDisc(p.degree, 0.9) OVER () AS dc_threshold,\n     percentileDisc(p.eigenvector, 0.9) OVER () AS ec_threshold\nRETURN p.name,\n       CASE\n         WHEN p.betweenness >= bc_threshold AND p.degree >= dc_threshold\n           THEN 'connector'\n         WHEN p.eigenvector >= ec_threshold AND p.degree < dc_threshold\n           THEN 'maven'\n         WHEN p.degree >= dc_threshold\n           THEN 'salesman'\n         ELSE 'standard'\n       END AS gladwell_type,\n       p.betweenness, p.degree, p.eigenvector\nORDER BY p.betweenness DESC;\n```\n\n### Path Finding\n\n```cypher\n// Shortest path between two people\nMATCH path = shortestPath(\n  (a:Person {name: 'Alice'})-[:KNOWS*]-(b:Person {name: 'Bob'})\n)\nRETURN path, length(path) AS degrees_of_separation;\n\n// All shortest paths\nMATCH paths = allShortestPaths(\n  (a:Person {name: 'Alice'})-[:KNOWS*]-(b:Person {name: 'Bob'})\n)\nRETURN paths;\n\n// Find connectors who can introduce you\nMATCH (me:Person {name: 'Alice'})\nMATCH (target:Person {name: 'Bob'})\nMATCH path = shortestPath((me)-[:KNOWS*2..4]-(target))\nWITH nodes(path) AS path_nodes\nUNWIND range(1, size(path_nodes)-2) AS i\nWITH path_nodes[i] AS connector\nRETURN connector.name, connector.betweenness\nORDER BY connector.betweenness DESC;\n```\n\n### Multi-Source Network Fusion\n\n```cypher\n// Create weighted relationships from multiple sources\nMATCH (a:Person)-[r:KNOWS]-(b:Person)\nWITH a, b, collect(r) AS rels\nSET a.connection_weight = reduce(w = 0.0, r IN rels |\n  w + CASE r.source\n    WHEN 'coauthorship' THEN 1.0\n    WHEN 'conference' THEN 0.8\n    WHEN 'linkedin' THEN 0.5\n    WHEN 'github' THEN 0.6\n    ELSE 0.3\n  END\n);\n```\n\n---\n\n## Amazon Neptune\n\nNeptune is AWS's managed graph database, compatible with Gremlin and SPARQL.\n\n### Gremlin Queries\n\n```groovy\n// Betweenness-like analysis (Gremlin doesn't have native betweenness)\n// Count paths through each node\ng.V().hasLabel('Person')\n  .project('name', 'pathsThrough')\n  .by('name')\n  .by(\n    __.as('p')\n    .both('KNOWS').as('start')\n    .repeat(__.both('KNOWS').simplePath())\n    .until(__.loops().is(3))\n    .path()\n    .filter(__.unfold().is('p'))\n    .count()\n  )\n  .order().by('pathsThrough', desc)\n  .limit(20)\n\n// Degree centrality\ng.V().hasLabel('Person')\n  .project('name', 'degree')\n  .by('name')\n  .by(__.both('KNOWS').count())\n  .order().by('degree', desc)\n  .limit(20)\n\n// Find bridges between communities\ng.V().hasLabel('Person')\n  .where(\n    __.both('KNOWS').values('community').dedup().count().is(gte(3))\n  )\n  .project('name', 'communities')\n  .by('name')\n  .by(__.both('KNOWS').values('community').dedup().fold())\n```\n\n---\n\n## TigerGraph\n\nTigerGraph is optimized for deep-link analytics on large graphs.\n\n### GSQL Queries\n\n```sql\n-- Create schema\nCREATE VERTEX Person (\n  PRIMARY_ID id STRING,\n  name STRING,\n  email STRING,\n  role STRING\n)\n\nCREATE DIRECTED EDGE KNOWS (\n  FROM Person,\n  TO Person,\n  strength FLOAT,\n  source STRING\n)\n\n-- Betweenness Centrality\nCREATE QUERY betweenness_centrality() FOR GRAPH professional_network {\n  MapAccum<VERTEX<Person>, FLOAT> @@bc_scores;\n\n  Start = {Person.*};\n\n  // Run shortest paths from each node\n  FOREACH src IN Start DO\n    paths = SELECT t\n      FROM Start:s -(KNOWS:e)- Person:t\n      WHERE s == src\n      ACCUM @@bc_scores += (t -> 1.0);\n  END;\n\n  PRINT @@bc_scores;\n}\n\n-- PageRank\nCREATE QUERY pagerank(FLOAT damping = 0.85, INT max_iter = 20)\nFOR GRAPH professional_network {\n  MaxAccum<FLOAT> @pr_score = 1.0;\n  SumAccum<FLOAT> @new_score;\n\n  Start = {Person.*};\n  INT num_vertices = Start.size();\n\n  FOREACH i IN RANGE[1, max_iter] DO\n    Start = SELECT s\n      FROM Start:s -(KNOWS:e)- Person:t\n      ACCUM t.@new_score += s.@pr_score / s.outdegree(\"KNOWS\")\n      POST-ACCUM\n        s.@pr_score = (1 - damping) / num_vertices + damping * s.@new_score,\n        s.@new_score = 0;\n  END;\n\n  PRINT Start[Start.@pr_score];\n}\n\n-- Find superconnectors\nCREATE QUERY find_superconnectors(INT top_k = 20)\nFOR GRAPH professional_network {\n  SumAccum<INT> @degree;\n  MaxAccum<FLOAT> @betweenness;\n\n  Start = {Person.*};\n\n  // Calculate degree\n  connected = SELECT s\n    FROM Start:s -(KNOWS:e)- Person:t\n    ACCUM s.@degree += 1;\n\n  // Return top by combined score\n  Result = SELECT s FROM Start:s\n    ORDER BY s.@degree DESC\n    LIMIT top_k;\n\n  PRINT Result;\n}\n```\n\n---\n\n## ArangoDB\n\nArangoDB is a multi-model database with graph capabilities.\n\n### AQL Queries\n\n```aql\n// Betweenness Centrality (using Pregel)\nWITH \"professional_network\"\nLET result = PREGEL_RUN(\"betweenness\", \"professional_network\", {\n  maxIterations: 100,\n  resultField: \"betweenness\"\n})\nFOR doc IN Person\n  SORT doc.betweenness DESC\n  LIMIT 20\n  RETURN {name: doc.name, betweenness: doc.betweenness}\n\n// PageRank\nWITH \"professional_network\"\nLET result = PREGEL_RUN(\"pagerank\", \"professional_network\", {\n  maxIterations: 100,\n  dampingFactor: 0.85,\n  resultField: \"pagerank\"\n})\nFOR doc IN Person\n  SORT doc.pagerank DESC\n  LIMIT 20\n  RETURN {name: doc.name, pagerank: doc.pagerank}\n\n// Shortest path\nFOR v, e IN OUTBOUND SHORTEST_PATH\n  'Person/alice' TO 'Person/bob'\n  GRAPH 'professional_network'\n  RETURN v.name\n\n// K-hop neighbors\nFOR v, e, p IN 1..3 ANY 'Person/alice'\n  GRAPH 'professional_network'\n  RETURN DISTINCT v.name\n\n// Find bridges\nFOR person IN Person\n  LET neighbors = (\n    FOR v IN 1..1 ANY person GRAPH 'professional_network'\n      RETURN DISTINCT v.community\n  )\n  FILTER LENGTH(neighbors) >= 3\n  SORT LENGTH(neighbors) DESC\n  RETURN {\n    name: person.name,\n    communities_bridged: neighbors,\n    bridge_score: LENGTH(neighbors)\n  }\n```\n\n---\n\n## DGraph\n\nDGraph is a horizontally scalable graph database with GraphQL support.\n\n### DQL Queries\n\n```graphql\n# Schema\ntype Person {\n  id: ID!\n  name: String! @index(term)\n  email: String @index(exact)\n  knows: [Person] @reverse\n  betweenness: Float\n  pagerank: Float\n}\n\n# Query high-centrality people\n{\n  superconnectors(func: ge(betweenness, 0.1), orderdesc: betweenness, first: 20) {\n    name\n    betweenness\n    pagerank\n    knows {\n      name\n    }\n  }\n}\n\n# Shortest path\n{\n  path as shortest(from: 0x1, to: 0x2) {\n    name\n  }\n}\n\n# Find all paths up to depth 3\n{\n  var(func: eq(name, \"Alice\")) {\n    knows @recurse(depth: 3) {\n      uid\n      name\n    }\n  }\n}\n```\n\n---\n\n## Python Integration Patterns\n\n### Neo4j with Python\n\n```python\nfrom neo4j import GraphDatabase\nimport pandas as pd\n\nclass ProfessionalNetworkAnalyzer:\n    def __init__(self, uri, user, password):\n        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n\n    def close(self):\n        self.driver.close()\n\n    def get_superconnectors(self, limit=20):\n        with self.driver.session() as session:\n            result = session.run(\"\"\"\n                CALL gds.betweenness.stream('professional-network')\n                YIELD nodeId, score\n                RETURN gds.util.asNode(nodeId).name AS name, score\n                ORDER BY score DESC\n                LIMIT $limit\n            \"\"\", limit=limit)\n            return pd.DataFrame([dict(r) for r in result])\n\n    def find_path_to_target(self, source_name, target_name):\n        with self.driver.session() as session:\n            result = session.run(\"\"\"\n                MATCH path = shortestPath(\n                  (a:Person {name: $source})-[:KNOWS*]-(b:Person {name: $target})\n                )\n                RETURN [n IN nodes(path) | n.name] AS path,\n                       length(path) AS degrees\n            \"\"\", source=source_name, target=target_name)\n            record = result.single()\n            if record:\n                return record['path'], record['degrees']\n            return None, None\n\n    def classify_by_gladwell(self):\n        with self.driver.session() as session:\n            result = session.run(\"\"\"\n                MATCH (p:Person)\n                WHERE p.betweenness IS NOT NULL\n                WITH p,\n                     percentileDisc(p.betweenness, 0.9) OVER () AS bc_thresh,\n                     percentileDisc(p.degree, 0.9) OVER () AS dc_thresh,\n                     percentileDisc(p.eigenvector, 0.9) OVER () AS ec_thresh\n                RETURN p.name AS name,\n                       CASE\n                         WHEN p.betweenness >= bc_thresh AND p.degree >= dc_thresh\n                           THEN 'connector'\n                         WHEN p.eigenvector >= ec_thresh AND p.degree < dc_thresh\n                           THEN 'maven'\n                         WHEN p.degree >= dc_thresh\n                           THEN 'salesman'\n                         ELSE 'standard'\n                       END AS archetype\n            \"\"\")\n            return pd.DataFrame([dict(r) for r in result])\n\n\n# Usage\nanalyzer = ProfessionalNetworkAnalyzer(\n    \"bolt://localhost:7687\",\n    \"neo4j\",\n    \"password\"\n)\n\nsuperconnectors = analyzer.get_superconnectors()\nprint(superconnectors.head(10))\n\npath, degrees = analyzer.find_path_to_target(\"Alice\", \"Bob\")\nprint(f\"Path: {' -> '.join(path)} ({degrees} degrees)\")\n\nclassifications = analyzer.classify_by_gladwell()\nprint(classifications[classifications['archetype'] == 'connector'])\n\nanalyzer.close()\n```\n\n### Bulk Loading Pattern\n\n```python\nfrom neo4j import GraphDatabase\n\ndef bulk_load_network(driver, nodes_df, edges_df, batch_size=5000):\n    \"\"\"Efficiently load network data into Neo4j.\"\"\"\n\n    with driver.session() as session:\n        # Load nodes in batches\n        for i in range(0, len(nodes_df), batch_size):\n            batch = nodes_df.iloc[i:i+batch_size].to_dict('records')\n            session.run(\"\"\"\n                UNWIND $batch AS row\n                MERGE (p:Person {id: row.id})\n                SET p.name = row.name,\n                    p.email = row.email,\n                    p.role = row.role\n            \"\"\", batch=batch)\n            print(f\"Loaded {min(i+batch_size, len(nodes_df))} nodes\")\n\n        # Load edges in batches\n        for i in range(0, len(edges_df), batch_size):\n            batch = edges_df.iloc[i:i+batch_size].to_dict('records')\n            session.run(\"\"\"\n                UNWIND $batch AS row\n                MATCH (a:Person {id: row.source})\n                MATCH (b:Person {id: row.target})\n                MERGE (a)-[r:KNOWS]->(b)\n                SET r.strength = row.strength,\n                    r.source = row.data_source\n            \"\"\", batch=batch)\n            print(f\"Loaded {min(i+batch_size, len(edges_df))} edges\")\n\n\n# Usage\ndriver = GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"))\nbulk_load_network(driver, people_df, connections_df)\ndriver.close()\n```\n"
        },
        {
          "name": "network-theory.md",
          "type": "file",
          "path": "hr-network-analyst/references/network-theory.md",
          "size": 3847,
          "content": "# Network Theory Reference\n\nTheoretical foundations for professional network analysis.\n\n## Gladwellian Archetypes (The Tipping Point)\n\n### Connectors\n\n**Definition**: People who know an extraordinary number across diverse social worlds.\n\n**Network Signature**:\n- Very high degree centrality (many connections)\n- High betweenness centrality (bridge between clusters)\n- Diverse cluster membership (not siloed)\n- Power-law distribution: rare but disproportionately connected\n\n**Identification Signals**:\n- Multiple conference speaker lists across domains\n- Co-authored with 5+ different institutions\n- LinkedIn spans 10+ distinct industries\n- Referenced by people who don't otherwise interact\n\n**HR Value**: Best for referrals across domains, accelerate hiring in new markets\n\n### Mavens\n\n**Definition**: Information specialists who accumulate knowledge and love sharing it.\n\n**Network Signature**:\n- High in-degree (people seek them out)\n- Central in knowledge-sharing networks\n- High PageRank (authoritative)\n- Create content others reference\n\n**Identification Signals**:\n- Prolific writers/speakers on specific topics\n- Run newsletters, podcasts, educational content\n- Tagged in \"who should I follow for X?\" threads\n- High engagement-to-follower ratio\n\n**HR Value**: Know who's good at what, validate candidate quality\n\n### Salesmen\n\n**Definition**: Persuaders with natural ability to get agreement.\n\n**Network Signature**:\n- High influence propagation\n- Strong reciprocal relationships\n- Central in deal-making networks\n- Bridge between decision-makers\n\n**Identification Signals**:\n- Track record of successful introductions\n- Referenced in \"how I got my job\" stories\n- Active in investor/founder/hiring circles\n- High response rate to outreach\n\n**HR Value**: Close candidates on fence, navigate negotiations\n\n## Network Centrality Metrics\n\n### Betweenness Centrality\n**Formula**: BC(v) = Σ (σst(v) / σst) for all s,t pairs\n**Meaning**: How often node lies on shortest paths between others\n**HR Interpretation**: \"Gatekeeper\" - controls information flow\n\n```python\nimport networkx as nx\nbc = nx.betweenness_centrality(G)\n```\n\n**When it matters**: Finding people who can introduce to unreachable networks\n\n### Degree Centrality\n**Formula**: DC(v) = degree(v) / (n-1)\n**Meaning**: Raw count of connections, normalized\n**HR Interpretation**: \"Popular\" - knows many directly\n\n**When it matters**: Maximizing referral reach, event organizing\n\n### Eigenvector Centrality\n**Formula**: Recursive: centrality depends on neighbors' centrality\n**Meaning**: Connected to other well-connected people\n**HR Interpretation**: \"Influential\" - quality over quantity\n\n**When it matters**: Access to power, rising stars, influence hierarchies\n\n### Closeness Centrality\n**Formula**: CC(v) = (n-1) / Σ d(v,u)\n**Meaning**: Average shortest path to all others\n**HR Interpretation**: \"Accessible\" - can reach anyone quickly\n\n**When it matters**: Information spreading, optimal hire positioning\n\n### PageRank\n**Formula**: Iterative probability of random walk\n**Meaning**: Weighted by quality of incoming connections\n**HR Interpretation**: \"Authoritative\" - endorsed by important others\n\n**When it matters**: Thought leaders vs merely prolific\n\n## Structural Holes Theory (Burt)\n\n**Core Insight**: Advantage comes from bridging disconnected groups, not dense cluster connections.\n\n**Key Metrics**:\n- **Constraint**: How concentrated in one group\n- **Effective Size**: Redundancy-adjusted network size\n- **Hierarchy**: Constraint concentration across contacts\n\n```python\nconstraint = nx.constraint(G)\nlow_constraint = {k: v for k, v in constraint.items() if v < 0.5}\n# These are broker opportunities\n```\n\n**HR Applications**:\n- Candidates bridging groups bring diverse information\n- Mix connectors and specialists in teams\n- Target structural holes, not cluster centers\n"
        }
      ]
    },
    {
      "name": "CHANGELOG.md",
      "type": "file",
      "path": "hr-network-analyst/CHANGELOG.md",
      "size": 1135,
      "content": "# Changelog\n\n## [2.0.0] - 2024-01-XX\n\n### Changed\n- **BREAKING**: Restructured from monolithic 694-line file to progressive disclosure architecture\n- Fixed frontmatter format: `tools:` → `allowed-tools:` (comma-separated)\n- Added NOT clause to description for precise activation boundaries\n- Reduced SKILL.md from 694 lines to 148 lines (79% reduction)\n\n### Added\n- `references/network-theory.md` - Betweenness centrality, structural holes, Gladwell archetypes\n- `references/data-sources-implementation.md` - Network construction, data extraction, Python code\n- Anti-patterns section with \"What it looks like / Why wrong / Instead\" format\n- Quick reference for Gladwell archetypes (Connectors, Mavens, Salesmen)\n- Compact analysis workflow table\n\n### Removed\n- Verbose network science explanations (moved to references)\n- Inline Python implementations (moved to references)\n- Redundant metric calculations\n\n### Migration Guide\nReference files are now in `/references/` directory. Import patterns:\n- Network theory background → `references/network-theory.md`\n- Python implementations → `references/data-sources-implementation.md`\n"
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "hr-network-analyst/SKILL.md",
      "size": 6368,
      "content": "---\nname: hr-network-analyst\ndescription: \"Professional network graph analyst identifying Gladwellian superconnectors, mavens, and influence brokers using betweenness centrality, structural holes theory, and multi-source network reconstruction. Activate on 'superconnectors', 'network analysis', 'who knows who', 'professional network', 'influence mapping', 'betweenness centrality'. NOT for surveillance, discrimination, stalking, privacy violation, or speculation without data.\"\nallowed-tools: Read,Write,Edit,WebSearch,WebFetch,mcp__firecrawl__firecrawl_search,mcp__firecrawl__firecrawl_scrape,mcp__brave-search__brave_web_search,mcp__SequentialThinking__sequentialthinking\nintegrates_with:\n  - career-biographer\n  - competitive-cartographer\n  - research-analyst\n  - cv-creator\n---\n\n# HR Network Analyst\n\nApplies graph theory and network science to professional relationship mapping. Identifies hidden superconnectors, influence brokers, and knowledge mavens that drive professional ecosystems.\n\n## Core Questions Answered\n\n- **Who should I know?** (optimal networking targets)\n- **Who knows everyone?** (superconnectors for referrals)\n- **Who bridges worlds?** (cross-domain brokers)\n- **How does influence flow?** (information/opportunity pathways)\n- **Where are structural holes?** (untapped connection opportunities)\n\n## Quick Start\n\n```\nUser: \"Who are the key connectors in AI safety research?\"\n\nProcess:\n1. Define boundary: AI safety researchers, 2020-2024\n2. Identify sources: arXiv, NeurIPS workshops, Twitter clusters\n3. Compute centrality: betweenness (bridges), eigenvector (influence)\n4. Classify by archetype: Connector, Maven, Broker\n5. Output: Ranked list with network position rationale\n```\n\n**Key principle**: Most valuable people aren't always most famous—they connect otherwise disconnected worlds.\n\n## Gladwellian Archetypes (Quick Reference)\n\n| Type | Network Signature | HR Value |\n|------|-------------------|----------|\n| **Connector** | High betweenness + degree, bridges clusters | Best for cross-domain referrals |\n| **Maven** | High in-degree, authoritative, creates content | Know who's good at what |\n| **Salesman** | High influence propagation, deal networks | Close candidates, navigate negotiation |\n\n**Full theory**: See `references/network-theory.md`\n\n## Centrality Metrics (Quick Reference)\n\n| Metric | Meaning | When to Use |\n|--------|---------|-------------|\n| **Betweenness** | Controls information flow | Finding gatekeepers, brokers |\n| **Degree** | Raw connection count | Maximizing referral reach |\n| **Eigenvector** | Quality over quantity | Access to power, rising stars |\n| **PageRank** | Endorsed by important others | Thought leaders |\n| **Closeness** | Can reach anyone quickly | Information spreading |\n\n## Analysis Workflows\n\n### 1. Find Superconnectors for Referrals\n- Define target domain → Seed network → Expand → Compute betweenness + degree → Rank\n\n### 2. Map Domain Influence\n- Define boundaries → Multi-source construction → Community detection → Identify brokers\n\n### 3. Optimize Personal Networking\n- Map current network → Map target domain → Find shortest paths → Identify structural holes\n\n### 4. Organizational Network Analysis (ONA)\n- Collect data (surveys, Slack metadata) → Construct graph → Find informal vs formal structure\n\n**Detailed workflows**: See `references/data-sources-implementation.md`\n\n## Data Sources\n\n| Source | Signal Strength | What to Extract |\n|--------|-----------------|-----------------|\n| Co-authorship | Very strong | Publication collaborations |\n| Conference co-panel | Strong | Speaking relationships |\n| GitHub co-repo | Medium-strong | Code collaboration |\n| LinkedIn connection | Medium | Professional links |\n| Twitter mutual | Weak | Social association |\n\n**Multi-source fusion**: Weight and combine signals for robust network\n\n## When NOT to Use\n\n- **Surveillance**: Tracking individuals without consent\n- **Discrimination**: Using network position to exclude\n- **Manipulation**: Engineering social influence for harm\n- **Privacy violation**: Accessing non-public data\n- **Speculation without data**: Guessing network structure\n\n## Anti-Patterns\n\n### Anti-Pattern: Degree Obsession\n**What it looks like**: Only looking at who has most connections\n**Why wrong**: High degree often = noise; connectors differ from popular\n**Instead**: Use betweenness for bridging, eigenvector for influence quality\n\n### Anti-Pattern: Static Network Assumption\n**What it looks like**: Treating 5-year-old connections as current\n**Why wrong**: Networks evolve; old edges may be dead\n**Instead**: Recency-weight edges, verify currency\n\n### Anti-Pattern: Single-Source Reliance\n**What it looks like**: Using only LinkedIn data\n**Why wrong**: Missing relationships not on LinkedIn\n**Instead**: Multi-source fusion with source-appropriate weighting\n\n### Anti-Pattern: Ignoring Context\n**What it looks like**: High betweenness = valuable, regardless of domain\n**Why wrong**: Bridging irrelevant communities isn't useful\n**Instead**: Constrain analysis to relevant domain boundaries\n\n## Ethical Guidelines\n\n**Acceptable**:\n- Analyzing public data (conference speakers, publications)\n- Aggregate pattern analysis\n- Opt-in organizational analysis\n- Academic research with proper IRB\n\n**NOT Acceptable**:\n- Scraping private profiles without consent\n- Building surveillance systems\n- Selling individual data\n- Discrimination based on network position\n\n## Troubleshooting\n\n| Issue | Cause | Fix |\n|-------|-------|-----|\n| Can't find data | Domain small/private | Snowball sampling, surveys, adjacent communities |\n| False edges | Over-weighting weak signals | Require multiple signals, threshold weights |\n| Too large | Unconstrained boundary | K-core filtering, high-weight only |\n| Entity resolution | Same person, different names | Unique IDs (ORCID), manual verification |\n\n## Reference Files\n\n- `references/algorithms.md` - NetworkX code patterns, centrality formulas, Gladwell classification\n- `references/graph-databases.md` - Neo4j, Neptune, TigerGraph, ArangoDB query examples\n- `references/data-sources.md` - LinkedIn network data acquisition strategies, APIs, scraping, legal considerations\n\n---\n\n**Core insight**: Advantage comes from bridging otherwise disconnected groups, not from connections within dense clusters. — Ron Burt, Structural Holes Theory\n"
    }
  ]
}