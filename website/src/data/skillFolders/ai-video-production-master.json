{
  "name": "ai-video-production-master",
  "type": "folder",
  "path": "ai-video-production-master",
  "children": [
    {
      "name": "docs",
      "type": "folder",
      "path": "ai-video-production-master/docs",
      "children": [
        {
          "name": "contracts",
          "type": "folder",
          "path": "ai-video-production-master/docs/contracts",
          "children": [
            {
              "name": "artist_commission_template.md",
              "type": "file",
              "path": "ai-video-production-master/docs/contracts/artist_commission_template.md",
              "size": 8441,
              "content": "# Artist Commission Agreement\n\n## AI Training Dataset Creation\n\n---\n\n**This Agreement** is entered into as of **[DATE]** between:\n\n**Client:** [YOUR NAME/COMPANY]\n**Address:** [ADDRESS]\n**Email:** [EMAIL]\n\n**Artist:** [ARTIST NAME]\n**Address:** [ADDRESS]\n**Email:** [EMAIL]\n\n---\n\n## 1. SCOPE OF WORK\n\n### 1.1 Deliverables\nArtist agrees to create the following original artwork (\"Work\"):\n\n- **Quantity:** [N] illustrations/artworks\n- **Format:** PNG, minimum resolution 2048×2048 pixels\n- **Style:** [DETAILED STYLE DESCRIPTION]\n- **Subject Matter:** [LIST CATEGORIES AND SPECIFIC SUBJECTS]\n\n### 1.2 Specifications\n- Color mode: RGB\n- Bit depth: 8-bit minimum, 16-bit preferred\n- File naming: [category]_[number].png\n- Delivery method: [Cloud storage link / Direct transfer]\n\n### 1.3 Reference Materials\nClient will provide:\n- Style reference images\n- Subject matter guidelines\n- Color palette (if applicable)\n- Any required brand assets\n\n---\n\n## 2. INTELLECTUAL PROPERTY RIGHTS\n\n### 2.1 Work Made for Hire\nAll Work created under this Agreement shall be considered \"work made for hire\" as defined under the United States Copyright Act (17 U.S.C. § 101). Client shall be considered the author and sole owner of all Work from the moment of creation.\n\n### 2.2 Assignment of Rights\nTo the extent any Work does not qualify as work made for hire under applicable law, Artist hereby irrevocably assigns, transfers, and conveys to Client all right, title, and interest in and to the Work, including but not limited to:\n\n- All copyrights, including the right to reproduce, distribute, display, perform, and create derivative works\n- All trademark rights\n- All patent rights (if applicable)\n- All trade secret rights\n- All moral rights, to the extent permitted by law\n\n### 2.3 AI and Machine Learning Rights\n**IMPORTANT:** Client shall have the unrestricted and perpetual right to use the Work for training artificial intelligence, machine learning, and neural network models, including but not limited to:\n\n- Diffusion models (e.g., Stable Diffusion, FLUX, Wan)\n- Transformer models (e.g., GPT, LLaMA)\n- Generative adversarial networks (GANs)\n- Variational autoencoders (VAEs)\n- Low-Rank Adaptation (LoRA) fine-tuning\n- Any other current or future AI/ML technologies\n\nThis includes creating derivative works generated by such models and using such derivatives for any commercial or non-commercial purpose without limitation or additional compensation.\n\n### 2.4 Commercial Use\nClient may use the Work and any derivatives thereof for any commercial purpose without limitation, including but not limited to:\n\n- Advertising and marketing\n- Merchandise and products\n- Publications (print and digital)\n- Video production and streaming\n- Software and applications\n- Sublicensing to third parties\n\n### 2.5 Artist Retention\nArtist retains only the following limited rights:\n\n- Display Work in personal portfolio (non-commercial purposes only)\n- List Client as a professional reference (with Client's prior written approval)\n- Create similar (but not substantially identical) works for other clients\n\nArtist expressly agrees NOT to:\n- License, sell, or distribute the Work to any third party\n- Use the Work in any commercial context\n- Create merchandise or products from the Work\n- Use the Work to train AI/ML models for any purpose\n- Claim ownership or authorship of the Work\n\n---\n\n## 3. REPRESENTATIONS AND WARRANTIES\n\n### 3.1 Artist Represents and Warrants That:\n\na) **Originality:** All Work is original and does not copy, imitate, or infringe upon any third-party intellectual property rights.\n\nb) **Authority:** Artist has full legal authority and capacity to enter into this Agreement and grant the rights herein.\n\nc) **No AI Generation:** No part of the Work was created using AI/ML generative tools (including but not limited to: Midjourney, DALL-E, Stable Diffusion, or similar tools) unless explicitly approved by Client in writing.\n\nd) **No Encumbrances:** The Work is free and clear of any liens, claims, or encumbrances.\n\ne) **No Violations:** Creation and delivery of the Work does not violate any agreement Artist has with any third party.\n\nf) **Professional Standards:** All Work will meet professional industry standards for quality and technical specifications.\n\n### 3.2 Indemnification\nArtist agrees to indemnify, defend, and hold harmless Client from any claims, damages, losses, or expenses (including reasonable attorney fees) arising from any breach of Artist's representations and warranties.\n\n---\n\n## 4. COMPENSATION\n\n### 4.1 Total Fee\nClient agrees to pay Artist a total of **$[AMOUNT]** USD for completion of all Work.\n\n### 4.2 Payment Schedule\n- **50% Deposit:** $[AMOUNT] due upon signing\n- **50% Final:** $[AMOUNT] due upon final delivery and acceptance\n\n### 4.3 Payment Method\nPayment via: [PayPal / Bank Transfer / Other]\nPayment details: [PROVIDE DETAILS]\n\n### 4.4 Expenses\nUnless otherwise agreed in writing, Artist is responsible for all expenses incurred in creating the Work.\n\n---\n\n## 5. TIMELINE AND DELIVERY\n\n### 5.1 Schedule\n| Milestone | Due Date | Deliverable |\n|-----------|----------|-------------|\n| Project Start | [DATE] | Deposit received, work begins |\n| First Review | [DATE] | First batch ([N] pieces) for approval |\n| Mid-Project | [DATE] | Second batch, revisions to first |\n| Final Delivery | [DATE] | All Work delivered |\n| Acceptance | [DATE + 7 days] | Client acceptance or revision requests |\n\n### 5.2 Revisions\n- Artist will provide up to [2-3] rounds of revisions per piece at no additional cost\n- Additional revisions beyond this scope may incur additional fees\n- Revision requests must be submitted within [7] days of delivery\n\n### 5.3 Late Delivery\nIf Artist fails to deliver Work by the agreed dates without prior written notice:\n- First week late: Written warning\n- Second week late: 10% reduction in final payment\n- Third week late: Client may terminate Agreement\n\n---\n\n## 6. CONFIDENTIALITY\n\n### 6.1 Confidential Information\nArtist agrees to keep confidential:\n- Project details and specifications\n- Client's business information\n- Any materials provided by Client\n- The existence and terms of this Agreement (unless otherwise agreed)\n\n### 6.2 Non-Disclosure\nArtist shall not disclose any Confidential Information to third parties without Client's prior written consent.\n\n### 6.3 Duration\nConfidentiality obligations survive termination of this Agreement for a period of [2] years.\n\n---\n\n## 7. TERMINATION\n\n### 7.1 Termination by Client\nClient may terminate this Agreement at any time with written notice. In such case:\n- Artist retains payment for Work completed and accepted\n- Artist delivers all completed Work to Client\n- All rights in completed Work transfer to Client\n\n### 7.2 Termination by Artist\nArtist may terminate only for material breach by Client after providing [14] days written notice and opportunity to cure.\n\n### 7.3 Effect of Termination\nUpon termination:\n- All rights in delivered Work remain with Client\n- Both parties return or destroy confidential materials\n- Surviving provisions continue in effect\n\n---\n\n## 8. GENERAL PROVISIONS\n\n### 8.1 Governing Law\nThis Agreement shall be governed by the laws of [STATE/COUNTRY].\n\n### 8.2 Entire Agreement\nThis Agreement constitutes the entire agreement between the parties and supersedes all prior discussions, negotiations, and agreements.\n\n### 8.3 Amendments\nNo modification of this Agreement shall be valid unless in writing and signed by both parties.\n\n### 8.4 Severability\nIf any provision is found unenforceable, the remaining provisions shall continue in full force and effect.\n\n### 8.5 Assignment\nArtist may not assign this Agreement without Client's prior written consent. Client may assign this Agreement freely.\n\n### 8.6 Independent Contractor\nArtist is an independent contractor, not an employee of Client.\n\n---\n\n## SIGNATURES\n\n**CLIENT:**\n\nSignature: _________________________\n\nName: [PRINT NAME]\n\nDate: _________________________\n\n\n**ARTIST:**\n\nSignature: _________________________\n\nName: [PRINT NAME]\n\nDate: _________________________\n\n---\n\n## EXHIBIT A: DETAILED SPECIFICATIONS\n\n[Attach detailed brief with style references, subject categories, and technical requirements]\n\n## EXHIBIT B: PAYMENT INFORMATION\n\n[Attach payment details and invoicing information]\n\n---\n\n*This template is provided for informational purposes. Consult a qualified attorney for legal advice specific to your situation and jurisdiction.*\n"
            }
          ]
        },
        {
          "name": "ARTIST_COMMISSIONING_GUIDE.md",
          "type": "file",
          "path": "ai-video-production-master/docs/ARTIST_COMMISSIONING_GUIDE.md",
          "size": 9382,
          "content": "# Artist Commissioning Guide for AI Training Datasets\n\n## Why Commission Original Art?\n\n### Legal Clarity\nThe U.S. Copyright Office's May 2025 report confirmed that using copyrighted works for AI training \"clearly implicates the right of reproduction.\" While some uses qualify as fair use, commissioning original work eliminates ambiguity entirely.\n\n### Strategic Advantages\n1. **Unique Style** - No one else has your exact LoRA\n2. **Legal Shield** - Clear ownership chain\n3. **Quality Control** - Curated dataset = better results\n4. **Ethical Foundation** - Artists compensated fairly\n5. **Commercial Use** - No restrictions on derivative works\n\n---\n\n## Finding the Right Artist\n\n### Platforms by Budget\n\n| Platform | Budget Range | Best For | Response Time |\n|----------|--------------|----------|---------------|\n| **ArtStation** | $500-5000+ | Concept art, illustration | 1-2 weeks |\n| **Fiverr** | $50-500 | Quick turnaround, variety | 2-5 days |\n| **Upwork** | $200-2000 | Long-term collaboration | 1-2 weeks |\n| **DeviantArt** | $100-1000 | Niche styles, fan art | Varies |\n| **Twitter/X** | Varies | Direct artist contact | 3-7 days |\n| **Instagram** | Varies | Visual artists | 3-7 days |\n\n### What to Look For\n\n**Technical Quality:**\n- Consistent style across portfolio\n- High resolution work (2K+)\n- Clean linework or intentional texture\n- Professional presentation\n\n**Style Fit:**\n- Matches your project aesthetic\n- Shows range within consistent style\n- Demonstrates subject matter versatility\n\n**Communication:**\n- Clear pricing information\n- Professional responses\n- Reasonable revision policy\n- Experience with commercial work\n\n---\n\n## Structuring the Commission\n\n### Initial Outreach Template\n\n```\nSubject: Commission Inquiry - [X] Illustrations for AI Training Dataset\n\nHi [Artist Name],\n\nI've been following your work and love your [specific style element].\nI'm looking to commission a set of illustrations for a commercial project.\n\nProject Overview:\n- [20-50] high-resolution illustrations\n- Consistent [your style description] style\n- Various subjects: [list categories]\n- Purpose: Training a custom AI style model (LoRA)\n\nBudget: $[X-Y] for the complete set\nTimeline: [X weeks]\n\nWould you be interested in discussing this further? I can provide\nmore details about the specific subjects and style references.\n\nBest,\n[Your name]\n```\n\n### Detailed Brief Template\n\nOnce interest is confirmed:\n\n```\nPROJECT BRIEF: AI Training Dataset Commission\n\n1. OVERVIEW\n   - Total pieces: [N]\n   - Style: [detailed description with references]\n   - Resolution: 2048x2048 minimum, PNG format\n   - Purpose: Training LoRA for [commercial/personal] video production\n\n2. SUBJECT CATEGORIES (distribute pieces across)\n   - [Category A]: X pieces - [specific subjects]\n   - [Category B]: X pieces - [specific subjects]\n   - [Category C]: X pieces - [specific subjects]\n\n3. STYLE REQUIREMENTS\n   - Color palette: [describe or attach]\n   - Line quality: [clean/sketchy/textured]\n   - Composition: [centered subjects/varied/specific layouts]\n   - Mood: [describe emotional tone]\n   - DO include: [list elements]\n   - DO NOT include: [list exclusions]\n\n4. REFERENCE IMAGES\n   [Attach 5-10 reference images that capture desired style]\n\n5. DELIVERABLES\n   - Individual PNG files at 2048x2048+\n   - Layered source files (PSD/AI) if applicable\n   - Naming convention: [category]_[number].png\n\n6. TIMELINE\n   - Week 1-2: First batch of [N] for approval\n   - Week 3-4: Revisions and second batch\n   - Week 5: Final delivery and revisions\n\n7. USAGE RIGHTS\n   See attached contract for full terms.\n```\n\n---\n\n## The Contract\n\n### Essential Clauses\n\n**1. Work for Hire / IP Assignment**\n```\nAll Work created under this Agreement shall be considered\n\"work made for hire\" under U.S. copyright law. To the extent\nany Work does not qualify as work made for hire, Artist hereby\nirrevocably assigns to Client all right, title, and interest\nin and to the Work, including all copyrights and other\nintellectual property rights therein.\n```\n\n**2. AI/ML Training Rights (CRITICAL)**\n```\nClient shall have the unrestricted right to use the Work for\ntraining artificial intelligence, machine learning, and neural\nnetwork models, including but not limited to: diffusion models,\ntransformers, generative adversarial networks, and any future\ntechnologies. This includes creating derivative works generated\nby such models and using such derivatives for any commercial or\nnon-commercial purpose.\n```\n\n**3. Commercial Use**\n```\nClient may use the Work and any derivatives thereof for any\ncommercial purpose without limitation, including but not limited\nto: advertising, merchandise, publications, digital products,\nvideo production, and sublicensing to third parties.\n```\n\n**4. Artist Retention**\n```\nArtist retains the right to:\n- Display Work in portfolio (non-commercial, credit required)\n- List Client as reference (with prior approval)\n- Create similar (but not identical) works for other clients\n\nArtist does NOT retain:\n- Rights to license, sell, or distribute the Work\n- Rights to create merchandise from the Work\n- Rights to use Work in any commercial context\n```\n\n**5. Representations and Warranties**\n```\nArtist represents and warrants that:\n- The Work is original and does not infringe any third-party rights\n- Artist has full authority to enter this Agreement\n- No part of the Work was created using AI/ML generative tools\n  unless explicitly approved by Client in writing\n- Artist will not use Client's Work to train any AI/ML models\n```\n\n**6. Payment Terms**\n```\nTotal: $[AMOUNT]\n- 50% upon signing: $[AMOUNT/2]\n- 50% upon final delivery: $[AMOUNT/2]\n\nPayment via: [PayPal/Wire/etc.]\n```\n\n### Template Contract\n\nSee `contracts/artist_commission_template.md` for the full template.\n\n---\n\n## Pricing Guidelines\n\n### Cost Per Image by Quality Level\n\n| Level | Artist Experience | Per Image | Dataset of 50 |\n|-------|-------------------|-----------|---------------|\n| **Entry** | Student/Hobbyist | $5-15 | $250-750 |\n| **Emerging** | 1-3 years pro | $15-40 | $750-2000 |\n| **Mid-Level** | 3-7 years pro | $40-100 | $2000-5000 |\n| **Senior** | 7+ years, notable | $100-300 | $5000-15000 |\n| **Elite** | Industry leaders | $300+ | $15000+ |\n\n### Factors Affecting Price\n\n**Increases cost:**\n- Complex backgrounds\n- Multiple characters per piece\n- Detailed environments\n- Rush delivery\n- Exclusive style rights\n- Larger dataset size\n\n**Decreases cost:**\n- Simple compositions\n- Character-only (no background)\n- Longer timeline\n- Ongoing relationship\n- Bulk discount\n\n### Negotiation Tips\n\n1. **Be upfront about AI usage** - Some artists decline AI-related work\n2. **Explain the value** - They're creating a unique asset\n3. **Offer ongoing work** - Future commissions for updates\n4. **Bundle services** - Style guide creation, trigger word suggestions\n5. **Consider royalties** - Alternative to higher upfront cost\n\n---\n\n## Quality Control\n\n### Review Checkpoints\n\n**First Batch (20-30% of pieces):**\n- Style consistency\n- Technical quality (resolution, format)\n- Subject variety\n- Color palette adherence\n\n**Mid-Project Review:**\n- Address any drift from style\n- Confirm remaining subject distribution\n- Discuss any needed adjustments\n\n**Final Delivery:**\n- Complete file check\n- Naming convention verification\n- Backup copies received\n- Contract sign-off\n\n### Common Issues\n\n| Issue | Solution |\n|-------|----------|\n| Style drift | Provide comparison feedback early |\n| Low resolution | Require proof of canvas size |\n| Inconsistent quality | Set minimum standard with examples |\n| Late delivery | Build buffer into timeline |\n| Scope creep | Define boundaries clearly upfront |\n\n---\n\n## Ethical Considerations\n\n### Transparency\n- Be honest about AI training purpose\n- Some artists prefer not to contribute to AI\n- Respect their decision without pressure\n\n### Fair Compensation\n- Pay rates comparable to traditional commercial work\n- Don't exploit the \"it's just for AI\" angle for discounts\n- Consider the long-term value you're receiving\n\n### Credit\n- Offer portfolio credit if artist wants it\n- Don't claim the style as your original creation\n- Acknowledge artistic origin in appropriate contexts\n\n### Data Handling\n- Secure storage of commissioned works\n- Don't redistribute raw dataset\n- Respect any retained rights\n\n---\n\n## Case Study: Building a Style LoRA\n\n### Project: Hand-Drawn PSA Animation Style\n\n**Goal:** Create consistent visual style for educational animations\n\n**Dataset Specification:**\n- 60 illustrations\n- Style: Sketch-like, warm palette, friendly characters\n- Categories:\n  - Character emotions (20 pieces)\n  - Educational diagrams (15 pieces)\n  - Environmental scenes (15 pieces)\n  - Title card designs (10 pieces)\n\n**Artist Selection:**\n- Found on ArtStation\n- Portfolio showed consistent storybook style\n- Previous experience with educational content\n- Responsive and professional\n\n**Budget:** $1,800 ($30/piece average)\n\n**Timeline:**\n- Week 1-2: Style exploration, 5 test pieces\n- Week 3-4: First batch of 25\n- Week 5-6: Second batch of 30\n- Week 7: Final delivery and revisions\n\n**Results:**\n- 63 pieces delivered (3 bonus)\n- LoRA trained at rank 32\n- Achieved 94% style consistency in generations\n- Total project cost including training: ~$1,850\n\n**ROI:**\n- Used in 15+ video productions\n- Cost per video: ~$123\n- Equivalent SaaS cost: ~$750+ per video\n- Break-even: 3 videos\n"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "folder",
      "path": "ai-video-production-master/scripts",
      "children": [
        {
          "name": "cloud_i2v_batch.py",
          "type": "file",
          "path": "ai-video-production-master/scripts/cloud_i2v_batch.py",
          "size": 19497,
          "content": "#!/usr/bin/env python3\n\"\"\"\nCloud I2V Batch Processor\n\nUploads keyframe images to a cloud GPU (Vast.ai),\nruns Wan 2.1 I2V generation, and downloads results.\n\nThis is 10-50x faster than local M4 Max for video generation.\n\nRequires: pip install vastai httpx\n\"\"\"\n\nimport argparse\nimport asyncio\nimport json\nimport os\nimport re\nimport subprocess\nimport sys\nimport time\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Optional\n\nimport httpx\n\n\n@dataclass\nclass CloudInstance:\n    id: str\n    ip: str\n    port: int\n    ssh_port: int\n    hourly_cost: float\n    gpu: str\n\n\ndef run_vastai(args: list[str], capture_json: bool = False) -> dict | str:\n    \"\"\"Run a vastai CLI command.\"\"\"\n    cmd = [\"vastai\"] + args\n    if capture_json:\n        cmd.append(\"--raw\")\n\n    result = subprocess.run(cmd, capture_output=True, text=True)\n\n    if result.returncode != 0:\n        raise RuntimeError(f\"vastai command failed: {result.stderr}\")\n\n    if capture_json:\n        return json.loads(result.stdout)\n    return result.stdout\n\n\ndef search_offers(max_price: float = 0.50, min_gpu_ram: int = 24) -> list[dict]:\n    \"\"\"Search for available GPU instances using vastai CLI.\"\"\"\n    query = f\"gpu_ram >= {min_gpu_ram} num_gpus = 1 rentable = true dph < {max_price} reliability > 0.95\"\n\n    offers = run_vastai([\"search\", \"offers\", query, \"-o\", \"dph+\"], capture_json=True)\n\n    # Filter for good GPUs (both underscore and space formats)\n    good_gpus = [\"RTX 4090\", \"RTX 5090\", \"RTX A6000\", \"A100\", \"H100\", \"A40\",\n                 \"RTX_4090\", \"RTX_5090\", \"RTX_A6000\"]\n    filtered = [o for o in offers if any(g in o.get(\"gpu_name\", \"\") for g in good_gpus)]\n\n    if not filtered:\n        # If no exact matches, return all offers with 24GB+ VRAM\n        return offers[:10]\n\n    return filtered[:10]  # Top 10 cheapest suitable options\n\n\ndef create_instance(offer_id: int, image: str = \"runpod/pytorch:2.1.0-py3.10-cuda11.8.0-devel-ubuntu22.04\") -> int:\n    \"\"\"Create a new instance from an offer.\"\"\"\n    # Use onstart script to install ComfyUI\n    onstart = \"\"\"#!/bin/bash\nset -e\ncd /root\n\n# Ensure pip is available\napt-get update && apt-get install -y python3-pip || true\npip3 install comfy-cli httpx || pip install comfy-cli httpx\ncomfy --skip-prompt install\ncomfy node install ComfyUI-GGUF ComfyUI-VideoHelperSuite\n\n# Download models\nmkdir -p /root/comfy/ComfyUI/models/diffusion_models\nmkdir -p /root/comfy/ComfyUI/models/text_encoders\nmkdir -p /root/comfy/ComfyUI/models/vae\n\n# Wan I2V model (only if not exists)\nif [ ! -f /root/comfy/ComfyUI/models/diffusion_models/wan2.1-i2v-14b-480p-Q5_K_M.gguf ]; then\n    wget -q -O /root/comfy/ComfyUI/models/diffusion_models/wan2.1-i2v-14b-480p-Q5_K_M.gguf \\\n        \"https://huggingface.co/city96/Wan2.1-I2V-14B-480P-GGUF/resolve/main/wan2.1-i2v-14b-480p-Q5_K_M.gguf\"\nfi\n\n# Text encoder\nif [ ! -f /root/comfy/ComfyUI/models/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors ]; then\n    wget -q -O /root/comfy/ComfyUI/models/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors \\\n        \"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors\"\nfi\n\n# VAE\nif [ ! -f /root/comfy/ComfyUI/models/vae/wan_2.1_vae.safetensors ]; then\n    wget -q -O /root/comfy/ComfyUI/models/vae/wan_2.1_vae.safetensors \\\n        \"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors\"\nfi\n\n# Start ComfyUI\ncd /root/comfy/ComfyUI\npython main.py --listen 0.0.0.0 --port 8188 &\n\"\"\"\n\n    # Write onstart script to temp file\n    onstart_file = Path(\"/tmp/vastai_onstart.sh\")\n    onstart_file.write_text(onstart)\n\n    result = run_vastai([\n        \"create\", \"instance\", str(offer_id),\n        \"--image\", image,\n        \"--disk\", \"80\",\n        \"--onstart-cmd\", onstart,\n        \"--direct\",\n    ], capture_json=False)  # CLI doesn't return JSON for create\n\n    # Parse instance ID from success message like \"Started. {'new_contract': 12345, ...}\"\n    # or \"success: created instance 12345\"\n    match = re.search(r\"new_contract['\\\"]?\\s*:\\s*(\\d+)\", result)\n    if match:\n        return int(match.group(1))\n\n    match = re.search(r\"instance\\s+(\\d+)\", result, re.IGNORECASE)\n    if match:\n        return int(match.group(1))\n\n    # Try to find any number that looks like an instance ID\n    match = re.search(r\"(\\d{7,})\", result)\n    if match:\n        return int(match.group(1))\n\n    raise RuntimeError(f\"Failed to parse instance ID from: {result}\")\n\n\ndef get_instance_info(instance_id: int) -> Optional[dict]:\n    \"\"\"Get instance connection info.\"\"\"\n    instances = run_vastai([\"show\", \"instances\"], capture_json=True)\n\n    for inst in instances:\n        if inst.get(\"id\") == instance_id:\n            return inst\n    return None\n\n\ndef wait_for_instance(instance_id: int, timeout: int = 900) -> dict:\n    \"\"\"Wait for instance to be ready.\"\"\"\n    print(f\"Waiting for instance {instance_id} to be ready...\")\n    start = time.time()\n    last_status = None\n\n    while time.time() - start < timeout:\n        info = get_instance_info(instance_id)\n        if info:\n            status = info.get(\"actual_status\", \"\")\n            if status != last_status:\n                print(f\"  Status: {status}\")\n                last_status = status\n\n            if status == \"running\":\n                return info  # Don't wait here - we'll check ComfyUI separately\n\n        time.sleep(10)\n\n    raise TimeoutError(f\"Instance {instance_id} did not become ready in {timeout}s\")\n\n\ndef destroy_instance(instance_id: int):\n    \"\"\"Destroy an instance.\"\"\"\n    try:\n        run_vastai([\"destroy\", \"instance\", str(instance_id)])\n        print(f\"Destroyed instance {instance_id}\")\n    except Exception as e:\n        print(f\"Warning: Failed to destroy instance: {e}\")\n\n\ndef build_i2v_workflow(image_name: str, motion_prompt: str, num_frames: int = 33, steps: int = 6) -> dict:\n    \"\"\"Build ComfyUI workflow for Wan I2V.\"\"\"\n    import random\n    seed = random.randint(0, 2**32 - 1)\n\n    return {\n        \"1\": {\"class_type\": \"LoadImage\", \"inputs\": {\"image\": image_name}},\n        \"2\": {\"class_type\": \"ImageScale\", \"inputs\": {\n            \"image\": [\"1\", 0], \"width\": 832, \"height\": 480,\n            \"upscale_method\": \"lanczos\", \"crop\": \"center\"\n        }},\n        \"3\": {\"class_type\": \"UnetLoaderGGUF\", \"inputs\": {\n            \"unet_name\": \"wan2.1-i2v-14b-480p-Q5_K_M.gguf\"\n        }},\n        \"4\": {\"class_type\": \"CLIPLoader\", \"inputs\": {\n            \"clip_name\": \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\", \"type\": \"wan\"\n        }},\n        \"5\": {\"class_type\": \"VAELoader\", \"inputs\": {\n            \"vae_name\": \"wan_2.1_vae.safetensors\"\n        }},\n        \"6\": {\"class_type\": \"CLIPTextEncode\", \"inputs\": {\n            \"text\": motion_prompt, \"clip\": [\"4\", 0]\n        }},\n        \"7\": {\"class_type\": \"CLIPTextEncode\", \"inputs\": {\n            \"text\": \"blurry, distorted, watermark, static\", \"clip\": [\"4\", 0]\n        }},\n        \"8\": {\"class_type\": \"WanImageToVideo\", \"inputs\": {\n            \"positive\": [\"6\", 0], \"negative\": [\"7\", 0], \"vae\": [\"5\", 0],\n            \"width\": 832, \"height\": 480, \"length\": num_frames,\n            \"batch_size\": 1, \"start_image\": [\"2\", 0],\n        }},\n        \"9\": {\"class_type\": \"ModelSamplingSD3\", \"inputs\": {\n            \"model\": [\"3\", 0], \"shift\": 8.0\n        }},\n        \"10\": {\"class_type\": \"KSampler\", \"inputs\": {\n            \"model\": [\"9\", 0], \"positive\": [\"8\", 0], \"negative\": [\"8\", 1],\n            \"latent_image\": [\"8\", 2], \"seed\": seed, \"steps\": steps,\n            \"cfg\": 5.0, \"sampler_name\": \"uni_pc\", \"scheduler\": \"normal\",\n            \"denoise\": 1.0,\n        }},\n        \"11\": {\"class_type\": \"VAEDecode\", \"inputs\": {\n            \"samples\": [\"10\", 0], \"vae\": [\"5\", 0]\n        }},\n        \"12\": {\"class_type\": \"VHS_VideoCombine\", \"inputs\": {\n            \"frame_rate\": 16, \"loop_count\": 0,\n            \"filename_prefix\": f\"i2v_{Path(image_name).stem}\",\n            \"format\": \"video/h264-mp4\", \"pingpong\": False,\n            \"save_output\": True, \"images\": [\"11\", 0],\n        }},\n    }\n\n\nasync def submit_workflow(base_url: str, workflow: dict) -> str:\n    \"\"\"Submit workflow to ComfyUI and return prompt_id.\"\"\"\n    async with httpx.AsyncClient(timeout=60) as client:\n        resp = await client.post(f\"{base_url}/prompt\", json={\"prompt\": workflow})\n        resp.raise_for_status()\n        return resp.json()[\"prompt_id\"]\n\n\nasync def wait_for_completion(base_url: str, prompt_id: str, timeout: int = 600) -> Optional[str]:\n    \"\"\"Wait for workflow to complete.\"\"\"\n    async with httpx.AsyncClient(timeout=30) as client:\n        start = time.time()\n        while time.time() - start < timeout:\n            await asyncio.sleep(10)\n\n            resp = await client.get(f\"{base_url}/history/{prompt_id}\")\n            if resp.status_code == 200:\n                history = resp.json()\n                if prompt_id in history:\n                    entry = history[prompt_id]\n                    status = entry.get(\"status\", {}).get(\"status_str\", \"\")\n\n                    if status == \"success\":\n                        outputs = entry.get(\"outputs\", {})\n                        for node_id, output in outputs.items():\n                            if \"gifs\" in output:\n                                return output[\"gifs\"][0][\"filename\"]\n                            if \"videos\" in output:\n                                return output[\"videos\"][0][\"filename\"]\n                    elif \"error\" in status.lower():\n                        print(f\"  Workflow error: {entry}\")\n                        return None\n\n        print(f\"  Timeout waiting for {prompt_id}\")\n        return None\n\n\nasync def run_batch(\n    instance_info: dict,\n    images_dir: Path,\n    output_dir: Path,\n    motion_prompt: str,\n    steps: int = 6,\n    frames: int = 33,\n) -> list[Path]:\n    \"\"\"Run batch I2V on cloud instance.\"\"\"\n\n    # Get connection info\n    ssh_host = instance_info.get(\"ssh_host\", \"\").split(\":\")[0]\n    ssh_port = int(instance_info.get(\"ssh_port\", 22))\n    public_ip = instance_info.get(\"public_ipaddr\", ssh_host)\n\n    # ComfyUI port (usually 8188, mapped to a high port)\n    ports = instance_info.get(\"ports\", {})\n    comfy_port = None\n    for port_map in ports.values() if isinstance(ports, dict) else []:\n        if \"8188\" in str(port_map):\n            comfy_port = port_map.get(\"HostPort\", 8188)\n            break\n\n    if not comfy_port:\n        comfy_port = 8188  # Default\n\n    base_url = f\"http://{public_ip}:{comfy_port}\"\n    print(f\"ComfyUI URL: {base_url}\")\n\n    # Find images\n    images = list(images_dir.glob(\"*.png\")) + list(images_dir.glob(\"*.jpg\"))\n    print(f\"Found {len(images)} images to process\")\n\n    # Upload images via SSH\n    print(\"Uploading images...\")\n\n    # Find SSH key for Vast.ai (check for dedicated key first, then default)\n    ssh_key = Path.home() / \".ssh\" / \"id_ed25519_vastai\"\n    if not ssh_key.exists():\n        ssh_key = Path.home() / \".ssh\" / \"id_ed25519\"\n    if not ssh_key.exists():\n        ssh_key = Path.home() / \".ssh\" / \"id_rsa\"\n\n    ssh_opts = f\"-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -i {ssh_key}\"\n    scp_port = f\"-P {ssh_port}\"  # SCP uses capital P for port\n\n    for img in images:\n        result = subprocess.run(\n            f\"scp {ssh_opts} {scp_port} {img} root@{ssh_host}:/root/comfy/ComfyUI/input/\",\n            shell=True, capture_output=True, text=True\n        )\n        if result.returncode != 0:\n            print(f\"  Warning: Failed to upload {img.name}: {result.stderr}\")\n            raise RuntimeError(f\"SCP failed: {result.stderr}\")\n\n    # Process each image\n    output_videos = []\n    for i, img in enumerate(images):\n        print(f\"Processing {i+1}/{len(images)}: {img.name}\")\n\n        workflow = build_i2v_workflow(img.name, motion_prompt, frames, steps)\n\n        try:\n            prompt_id = await submit_workflow(base_url, workflow)\n            print(f\"  Submitted: {prompt_id}\")\n\n            video_file = await wait_for_completion(base_url, prompt_id, timeout=600)\n            if video_file:\n                output_videos.append(video_file)\n                print(f\"  Complete: {video_file}\")\n            else:\n                print(f\"  Failed or timed out\")\n        except Exception as e:\n            print(f\"  Error: {e}\")\n\n    # Download results\n    output_dir.mkdir(parents=True, exist_ok=True)\n    print(f\"\\nDownloading {len(output_videos)} videos...\")\n\n    for video in output_videos:\n        result = subprocess.run(\n            f\"scp {ssh_opts} {scp_port} root@{ssh_host}:/root/comfy/ComfyUI/output/{video} {output_dir}/\",\n            shell=True, capture_output=True, text=True\n        )\n        if result.returncode != 0:\n            print(f\"  Warning: Failed to download {video}: {result.stderr}\")\n\n    return list(output_dir.glob(\"*.mp4\"))\n\n\nasync def main():\n    parser = argparse.ArgumentParser(description=\"Cloud I2V Batch Processor\")\n    parser.add_argument(\"--images\", type=Path, required=True, help=\"Input images directory\")\n    parser.add_argument(\"--output\", type=Path, default=Path(\"i2v_output\"), help=\"Output directory\")\n    parser.add_argument(\"--motion\", default=\"subtle organic motion, gentle breathing, cinematic\")\n    parser.add_argument(\"--steps\", type=int, default=6, help=\"Sampling steps (4-12)\")\n    parser.add_argument(\"--frames\", type=int, default=33, help=\"Output frames (33=~2s at 16fps)\")\n    parser.add_argument(\"--max-price\", type=float, default=0.50, help=\"Max $/hr\")\n    parser.add_argument(\"--dry-run\", action=\"store_true\", help=\"Show offers only\")\n    parser.add_argument(\"--yes\", \"-y\", action=\"store_true\", help=\"Skip confirmations\")\n    parser.add_argument(\"--instance\", type=int, help=\"Use existing instance ID\")\n\n    args = parser.parse_args()\n\n    # Count images\n    images = list(args.images.glob(\"*.png\")) + list(args.images.glob(\"*.jpg\"))\n    if not images:\n        print(f\"No images found in {args.images}\")\n        sys.exit(1)\n\n    print(f\"Found {len(images)} images to process\")\n\n    # Search for offers\n    print(f\"\\nSearching for GPU instances under ${args.max_price}/hr...\")\n    offers = search_offers(args.max_price)\n\n    if not offers:\n        print(\"No suitable GPU instances found. Try increasing --max-price\")\n        sys.exit(1)\n\n    print(f\"\\nTop offers:\")\n    print(f\"{'ID':<12} {'GPU':<20} {'RAM':<8} {'$/hr':<10} {'Location':<20}\")\n    print(\"-\" * 75)\n    for o in offers[:5]:\n        print(f\"{o['id']:<12} {o.get('gpu_name', 'N/A'):<20} {o.get('gpu_ram', 0):<8.0f} ${o.get('dph_total', 0):<9.4f} {o.get('geolocation', 'N/A'):<20}\")\n\n    # Cost estimate\n    best = offers[0]\n    est_time_min = len(images) * 3  # ~3 min per clip on fast GPU\n    est_cost = (est_time_min / 60 + 0.5) * best['dph_total']  # +30 min for setup\n\n    print(f\"\\nEstimated: {est_time_min} min processing + 30 min setup = ${est_cost:.2f}\")\n\n    if args.dry_run:\n        print(\"\\n(Dry run - no instance created)\")\n        return\n\n    # Confirm\n    if not args.yes:\n        response = input(\"\\nProceed with cheapest option? [y/N] \")\n        if response.lower() != \"y\":\n            print(\"Cancelled\")\n            return\n    else:\n        print(\"\\n--yes flag: proceeding automatically\")\n\n    instance_id = args.instance\n    instance_info = None\n\n    try:\n        if not instance_id:\n            # Create instance\n            print(f\"\\nCreating instance from offer {best['id']}...\")\n            instance_id = create_instance(best['id'])\n            print(f\"Created instance: {instance_id}\")\n\n            # Wait for ready\n            instance_info = wait_for_instance(instance_id)\n        else:\n            instance_info = get_instance_info(instance_id)\n            if not instance_info:\n                print(f\"Instance {instance_id} not found\")\n                sys.exit(1)\n\n        ssh_host = instance_info.get(\"ssh_host\", \"\").split(\":\")[0]\n        ssh_port = int(instance_info.get(\"ssh_port\", 22))\n        public_ip = instance_info.get(\"public_ipaddr\", ssh_host)\n\n        print(f\"\\nInstance ready!\")\n        print(f\"  SSH: ssh -p {ssh_port} root@{ssh_host}\")\n\n        # Find SSH key\n        ssh_key = Path.home() / \".ssh\" / \"id_ed25519_vastai\"\n        if not ssh_key.exists():\n            ssh_key = Path.home() / \".ssh\" / \"id_ed25519\"\n        if not ssh_key.exists():\n            ssh_key = Path.home() / \".ssh\" / \"id_rsa\"\n        ssh_opts = f\"-o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ConnectTimeout=10 -i {ssh_key}\"\n\n        # Get ComfyUI port from port mappings\n        ports = instance_info.get(\"ports\", {})\n        comfy_port = 8188\n        for port_key, port_info in (ports.items() if isinstance(ports, dict) else []):\n            if \"8188\" in str(port_key):\n                if isinstance(port_info, list) and port_info:\n                    comfy_port = port_info[0].get(\"HostPort\", 8188)\n                break\n\n        comfy_url = f\"http://{public_ip}:{comfy_port}\"\n\n        # Wait for ComfyUI setup to complete (downloads ~18GB of models)\n        # Check both SSH connectivity AND ComfyUI input directory exists\n        print(f\"  Waiting for setup to complete (this can take 5-10 minutes for model downloads)...\")\n\n        comfy_ready = False\n        for attempt in range(90):  # Wait up to 15 minutes\n            elapsed = attempt * 10\n\n            # First check if SSH works and input directory exists\n            try:\n                ssh_check = subprocess.run(\n                    f\"ssh {ssh_opts} -p {ssh_port} root@{ssh_host} 'test -d /root/comfy/ComfyUI/input && echo OK'\",\n                    shell=True, capture_output=True, text=True, timeout=20\n                )\n            except subprocess.TimeoutExpired:\n                if attempt % 6 == 0:\n                    print(f\"    SSH connection timed out, retrying... ({elapsed}s)\")\n                await asyncio.sleep(10)\n                continue\n\n            if ssh_check.returncode == 0 and \"OK\" in ssh_check.stdout:\n                # SSH works and directory exists, now check ComfyUI API\n                try:\n                    async with httpx.AsyncClient(timeout=10) as client:\n                        resp = await client.get(f\"{comfy_url}/system_stats\")\n                        if resp.status_code == 200:\n                            print(f\"  ComfyUI ready! (took {elapsed}s)\")\n                            comfy_ready = True\n                            break\n                except Exception:\n                    pass\n\n                if attempt % 3 == 0:\n                    print(f\"    ComfyUI directory exists, waiting for API... ({elapsed}s)\")\n            else:\n                if attempt % 6 == 0:\n                    print(f\"    Setup in progress... ({elapsed}s)\")\n\n            await asyncio.sleep(10)\n\n        if not comfy_ready:\n            print(\"  Warning: ComfyUI setup may not be complete, but proceeding...\")\n\n        # Run batch\n        downloaded = await run_batch(\n            instance_info,\n            args.images,\n            args.output,\n            args.motion,\n            args.steps,\n            args.frames,\n        )\n\n        print(f\"\\nComplete! Downloaded {len(downloaded)} videos to {args.output}\")\n\n    finally:\n        if instance_id and not args.instance:\n            if not args.yes:\n                response = input(\"\\nDestroy instance? [Y/n] \")\n                if response.lower() != \"n\":\n                    destroy_instance(instance_id)\n            else:\n                # Auto-destroy with --yes flag\n                destroy_instance(instance_id)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n"
        },
        {
          "name": "cost_calculator.py",
          "type": "file",
          "path": "ai-video-production-master/scripts/cost_calculator.py",
          "size": 10757,
          "content": "#!/usr/bin/env python3\n\"\"\"\nAI Video Production Cost Calculator\n\nCompares costs across different approaches:\n- Full local (M4 Max)\n- Hybrid (local images + cloud I2V)\n- Cloud-only (Vast.ai, RunPod)\n- SaaS platforms (InVideo, Runway)\n\"\"\"\n\nimport argparse\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Optional\n\n\nclass Provider(Enum):\n    LOCAL_M4_MAX = \"local_m4_max\"\n    VASTAI_H100 = \"vastai_h100\"\n    RUNPOD_H100 = \"runpod_h100\"\n    RUNPOD_A100 = \"runpod_a100\"\n    LAMBDA_H100 = \"lambda_h100\"\n    INVIDEO_PLUS = \"invideo_plus\"\n    INVIDEO_MAX = \"invideo_max\"\n    RUNWAY_GEN4 = \"runway_gen4\"\n\n\n@dataclass\nclass ProviderConfig:\n    name: str\n    hourly_cost: float  # $/hr for cloud, monthly for SaaS\n    i2v_time_per_clip: float  # minutes\n    image_time: float  # seconds per image\n    monthly_minutes: Optional[int] = None  # For SaaS platforms\n    is_saas: bool = False\n    is_local: bool = False\n\n\nPROVIDERS = {\n    Provider.LOCAL_M4_MAX: ProviderConfig(\n        name=\"Local M4 Max (128GB)\",\n        hourly_cost=0.0,  # Just electricity\n        i2v_time_per_clip=90,  # 15 min/step × 6 steps\n        image_time=45,  # seconds\n        is_local=True,\n    ),\n    Provider.VASTAI_H100: ProviderConfig(\n        name=\"Vast.ai H100 80GB\",\n        hourly_cost=1.87,\n        i2v_time_per_clip=2,  # minutes\n        image_time=5,  # seconds\n    ),\n    Provider.RUNPOD_H100: ProviderConfig(\n        name=\"RunPod H100 80GB\",\n        hourly_cost=1.99,\n        i2v_time_per_clip=2,\n        image_time=5,\n    ),\n    Provider.RUNPOD_A100: ProviderConfig(\n        name=\"RunPod A100 80GB\",\n        hourly_cost=1.74,\n        i2v_time_per_clip=3,\n        image_time=7,\n    ),\n    Provider.LAMBDA_H100: ProviderConfig(\n        name=\"Lambda Labs H100\",\n        hourly_cost=2.99,\n        i2v_time_per_clip=2,\n        image_time=5,\n    ),\n    Provider.INVIDEO_PLUS: ProviderConfig(\n        name=\"InVideo Plus\",\n        hourly_cost=20.0,  # monthly\n        i2v_time_per_clip=0.5,  # Their pipeline\n        image_time=0,  # Included\n        monthly_minutes=50,\n        is_saas=True,\n    ),\n    Provider.INVIDEO_MAX: ProviderConfig(\n        name=\"InVideo Max\",\n        hourly_cost=48.0,  # monthly\n        i2v_time_per_clip=0.5,\n        image_time=0,\n        monthly_minutes=200,\n        is_saas=True,\n    ),\n    Provider.RUNWAY_GEN4: ProviderConfig(\n        name=\"Runway Gen-4\",\n        hourly_cost=0.05,  # per second of video\n        i2v_time_per_clip=0.5,\n        image_time=0,\n        is_saas=True,\n    ),\n}\n\n\ndef calculate_project_cost(\n    num_shots: int,\n    clip_duration: float,  # seconds\n    provider: Provider,\n    include_images: bool = True,\n) -> dict:\n    \"\"\"Calculate total cost for a video project.\"\"\"\n    config = PROVIDERS[provider]\n\n    result = {\n        \"provider\": config.name,\n        \"num_shots\": num_shots,\n        \"clip_duration\": clip_duration,\n        \"total_video_seconds\": num_shots * clip_duration,\n    }\n\n    if config.is_local:\n        # Local processing - just time cost\n        image_time = (num_shots * config.image_time) / 60  # minutes\n        i2v_time = num_shots * config.i2v_time_per_clip  # minutes\n        total_time = image_time + i2v_time\n\n        result[\"image_generation_min\"] = round(image_time, 1)\n        result[\"i2v_generation_min\"] = round(i2v_time, 1)\n        result[\"total_time_hours\"] = round(total_time / 60, 2)\n        result[\"cost\"] = 0.0\n        result[\"cost_note\"] = \"Free (electricity ~$0.50)\"\n\n    elif config.is_saas:\n        if provider == Provider.RUNWAY_GEN4:\n            # Runway charges per second\n            total_seconds = num_shots * clip_duration\n            result[\"cost\"] = round(total_seconds * config.hourly_cost, 2)\n            result[\"cost_note\"] = f\"${config.hourly_cost}/sec of video\"\n        else:\n            # InVideo-style - monthly subscription\n            total_minutes = (num_shots * clip_duration) / 60\n            if config.monthly_minutes and total_minutes <= config.monthly_minutes:\n                result[\"cost\"] = config.hourly_cost\n                result[\"cost_note\"] = f\"Monthly subscription (includes {config.monthly_minutes} min)\"\n                result[\"minutes_used\"] = round(total_minutes, 1)\n                result[\"minutes_remaining\"] = round(config.monthly_minutes - total_minutes, 1)\n            else:\n                result[\"cost\"] = config.hourly_cost * 2  # Need higher tier\n                result[\"cost_note\"] = \"Exceeds plan limits\"\n\n    else:\n        # Cloud GPU\n        image_time = (num_shots * config.image_time) / 60 if include_images else 0\n        i2v_time = num_shots * config.i2v_time_per_clip\n        total_minutes = image_time + i2v_time\n        total_hours = total_minutes / 60\n\n        # Add 15 min startup overhead\n        total_hours += 0.25\n\n        result[\"image_generation_min\"] = round(image_time, 1)\n        result[\"i2v_generation_min\"] = round(i2v_time, 1)\n        result[\"total_gpu_hours\"] = round(total_hours, 2)\n        result[\"cost\"] = round(total_hours * config.hourly_cost, 2)\n        result[\"cost_note\"] = f\"${config.hourly_cost}/hr\"\n\n    return result\n\n\ndef calculate_hybrid_cost(\n    num_shots: int,\n    clip_duration: float,\n    cloud_provider: Provider = Provider.VASTAI_H100,\n) -> dict:\n    \"\"\"Calculate cost for hybrid approach (local images + cloud I2V).\"\"\"\n    local_config = PROVIDERS[Provider.LOCAL_M4_MAX]\n    cloud_config = PROVIDERS[cloud_provider]\n\n    # Images locally\n    image_time = (num_shots * local_config.image_time) / 60  # minutes\n\n    # I2V on cloud\n    i2v_time = num_shots * cloud_config.i2v_time_per_clip  # minutes\n    i2v_hours = (i2v_time + 15) / 60  # Add startup overhead\n\n    cloud_cost = i2v_hours * cloud_config.hourly_cost\n\n    return {\n        \"approach\": \"Hybrid (Local images + Cloud I2V)\",\n        \"num_shots\": num_shots,\n        \"clip_duration\": clip_duration,\n        \"local_image_time_min\": round(image_time, 1),\n        \"cloud_provider\": cloud_config.name,\n        \"cloud_i2v_time_min\": round(i2v_time, 1),\n        \"cloud_hours\": round(i2v_hours, 2),\n        \"cloud_cost\": round(cloud_cost, 2),\n        \"total_cost\": round(cloud_cost, 2),\n        \"total_time_hours\": round((image_time + i2v_time) / 60, 2),\n    }\n\n\ndef compare_all(num_shots: int, clip_duration: float) -> None:\n    \"\"\"Compare all providers for a given project.\"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"Cost Comparison: {num_shots} shots × {clip_duration}s = {num_shots * clip_duration}s total video\")\n    print(f\"{'='*70}\\n\")\n\n    results = []\n\n    # Cloud/Local providers\n    for provider in [\n        Provider.LOCAL_M4_MAX,\n        Provider.VASTAI_H100,\n        Provider.RUNPOD_H100,\n        Provider.RUNPOD_A100,\n    ]:\n        r = calculate_project_cost(num_shots, clip_duration, provider)\n        results.append(r)\n\n    # Hybrid\n    hybrid = calculate_hybrid_cost(num_shots, clip_duration)\n    results.append({\n        \"provider\": hybrid[\"approach\"],\n        \"cost\": hybrid[\"total_cost\"],\n        \"total_time_hours\": hybrid[\"total_time_hours\"],\n        \"cost_note\": f\"via {hybrid['cloud_provider']}\",\n    })\n\n    # SaaS\n    for provider in [Provider.INVIDEO_MAX, Provider.RUNWAY_GEN4]:\n        r = calculate_project_cost(num_shots, clip_duration, provider)\n        results.append(r)\n\n    # Sort by cost\n    results.sort(key=lambda x: x.get(\"cost\", 0))\n\n    # Display\n    print(f\"{'Provider':<35} {'Cost':>10} {'Time':>12} {'Notes':<25}\")\n    print(\"-\" * 85)\n\n    for r in results:\n        cost_str = f\"${r['cost']:.2f}\" if r['cost'] > 0 else \"FREE\"\n        time_str = f\"{r.get('total_time_hours', 'N/A')} hrs\" if 'total_time_hours' in r else \"Quick\"\n        notes = r.get('cost_note', '')[:25]\n        print(f\"{r['provider']:<35} {cost_str:>10} {time_str:>12} {notes:<25}\")\n\n    print(\"\\n\" + \"=\"*70)\n    print(\"RECOMMENDATION:\")\n\n    # Find best value\n    cheapest = min([r for r in results if r['cost'] > 0], key=lambda x: x['cost'])\n    print(f\"  Best Value: {cheapest['provider']} at ${cheapest['cost']:.2f}\")\n\n    local = next(r for r in results if 'Local' in r['provider'])\n    if local['total_time_hours'] < 4:\n        print(f\"  Best for Quick Projects: Local M4 Max ({local['total_time_hours']} hrs, free)\")\n    else:\n        print(f\"  Local Too Slow: {local['total_time_hours']} hrs - use cloud\")\n\n    print(\"=\"*70)\n\n\ndef monthly_comparison(videos_per_month: int, shots_per_video: int, clip_duration: float) -> None:\n    \"\"\"Compare monthly costs for regular production.\"\"\"\n    print(f\"\\n{'='*70}\")\n    print(f\"Monthly Cost: {videos_per_month} videos × {shots_per_video} shots × {clip_duration}s\")\n    print(f\"{'='*70}\\n\")\n\n    total_shots = videos_per_month * shots_per_video\n    total_seconds = total_shots * clip_duration\n    total_minutes = total_seconds / 60\n\n    print(f\"Total monthly: {total_shots} shots, {total_minutes:.1f} minutes of video\\n\")\n\n    comparisons = [\n        (\"InVideo Plus ($20/mo)\", 20 if total_minutes <= 50 else \"Exceeds limit\"),\n        (\"InVideo Max ($48/mo)\", 48 if total_minutes <= 200 else \"Exceeds limit\"),\n        (\"Runway Gen-4\", round(total_seconds * 0.05, 2)),\n        (\"Hybrid (Vast.ai)\", round(calculate_hybrid_cost(total_shots, clip_duration)[\"total_cost\"], 2)),\n        (\"Full Cloud (Vast.ai)\", round(calculate_project_cost(total_shots, clip_duration, Provider.VASTAI_H100)[\"cost\"], 2)),\n        (\"Full Local (M4 Max)\", \"FREE (but slow)\"),\n    ]\n\n    print(f\"{'Approach':<30} {'Monthly Cost':>15}\")\n    print(\"-\" * 50)\n    for name, cost in comparisons:\n        cost_str = f\"${cost}\" if isinstance(cost, (int, float)) else cost\n        print(f\"{name:<30} {cost_str:>15}\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"AI Video Production Cost Calculator\")\n    parser.add_argument(\"--shots\", type=int, default=10, help=\"Number of shots/clips\")\n    parser.add_argument(\"--duration\", type=float, default=5.0, help=\"Duration per clip (seconds)\")\n    parser.add_argument(\"--monthly\", type=int, help=\"Videos per month (for monthly comparison)\")\n    parser.add_argument(\"--provider\", type=str, help=\"Specific provider to calculate\")\n\n    args = parser.parse_args()\n\n    if args.monthly:\n        monthly_comparison(args.monthly, args.shots, args.duration)\n    elif args.provider:\n        try:\n            provider = Provider[args.provider.upper()]\n            result = calculate_project_cost(args.shots, args.duration, provider)\n            print(f\"\\n{result['provider']}:\")\n            for k, v in result.items():\n                if k != 'provider':\n                    print(f\"  {k}: {v}\")\n        except KeyError:\n            print(f\"Unknown provider: {args.provider}\")\n            print(f\"Available: {[p.name for p in Provider]}\")\n    else:\n        compare_all(args.shots, args.duration)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
          "name": "motion_graphics_generator.py",
          "type": "file",
          "path": "ai-video-production-master/scripts/motion_graphics_generator.py",
          "size": 18130,
          "content": "#!/usr/bin/env python3\n\"\"\"\nMotion Graphics Generator\n\nProgrammatically generate title cards, lower thirds, data visualizations,\nand other synthetic video elements with modern 2025 aesthetics.\n\nStyles:\n- neo_brutalist: Raw, glitchy, utilitarian\n- deep_glow: Intense light blooms, layered neons\n- liquid_motion: Fluid, morphing typography\n- retro_revival: 80s/90s grain and neon\n- glass_morphism: Frosted glass, depth layers\n\"\"\"\n\nimport argparse\nimport json\nimport math\nimport os\nimport subprocess\nimport tempfile\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Optional\n\n\nclass Style(Enum):\n    NEO_BRUTALIST = \"neo_brutalist\"\n    DEEP_GLOW = \"deep_glow\"\n    LIQUID_MOTION = \"liquid_motion\"\n    RETRO_REVIVAL = \"retro_revival\"\n    GLASS_MORPHISM = \"glass_morphism\"\n\n\n@dataclass\nclass ColorPalette:\n    background: str\n    primary: str\n    secondary: str\n    accent: str\n    text: str\n    glow: Optional[str] = None\n\n\nPALETTES = {\n    Style.NEO_BRUTALIST: ColorPalette(\n        background=\"#0a0a0a\",\n        primary=\"#ff3366\",\n        secondary=\"#00ff88\",\n        accent=\"#ffff00\",\n        text=\"#ffffff\",\n    ),\n    Style.DEEP_GLOW: ColorPalette(\n        background=\"#0d0d1a\",\n        primary=\"#7c3aed\",\n        secondary=\"#06b6d4\",\n        accent=\"#f472b6\",\n        text=\"#ffffff\",\n        glow=\"#7c3aed\",\n    ),\n    Style.LIQUID_MOTION: ColorPalette(\n        background=\"#1a1a2e\",\n        primary=\"#4361ee\",\n        secondary=\"#3a0ca3\",\n        accent=\"#f72585\",\n        text=\"#ffffff\",\n        glow=\"#4361ee\",\n    ),\n    Style.RETRO_REVIVAL: ColorPalette(\n        background=\"#1a0a2e\",\n        primary=\"#ff006e\",\n        secondary=\"#8338ec\",\n        accent=\"#ffbe0b\",\n        text=\"#ffffff\",\n        glow=\"#ff006e\",\n    ),\n    Style.GLASS_MORPHISM: ColorPalette(\n        background=\"#0f172a\",\n        primary=\"#38bdf8\",\n        secondary=\"#818cf8\",\n        accent=\"#f472b6\",\n        text=\"#f8fafc\",\n        glow=\"#38bdf8\",\n    ),\n}\n\n\ndef generate_svg_title_card(\n    title: str,\n    subtitle: Optional[str] = None,\n    style: Style = Style.DEEP_GLOW,\n    width: int = 1920,\n    height: int = 1080,\n) -> str:\n    \"\"\"Generate an SVG title card.\"\"\"\n    palette = PALETTES[style]\n\n    # Base SVG\n    svg_parts = [\n        f'<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 {width} {height}\">',\n        '<defs>',\n    ]\n\n    # Add glow filter for styles that use it\n    if palette.glow:\n        svg_parts.append(f'''\n        <filter id=\"glow\" x=\"-50%\" y=\"-50%\" width=\"200%\" height=\"200%\">\n            <feGaussianBlur stdDeviation=\"20\" result=\"coloredBlur\"/>\n            <feMerge>\n                <feMergeNode in=\"coloredBlur\"/>\n                <feMergeNode in=\"coloredBlur\"/>\n                <feMergeNode in=\"SourceGraphic\"/>\n            </feMerge>\n        </filter>\n        ''')\n\n    # Gradient for text\n    svg_parts.append(f'''\n        <linearGradient id=\"textGradient\" x1=\"0%\" y1=\"0%\" x2=\"100%\" y2=\"100%\">\n            <stop offset=\"0%\" style=\"stop-color:{palette.primary}\"/>\n            <stop offset=\"100%\" style=\"stop-color:{palette.secondary}\"/>\n        </linearGradient>\n    </defs>\n    ''')\n\n    # Background\n    svg_parts.append(f'<rect width=\"{width}\" height=\"{height}\" fill=\"{palette.background}\"/>')\n\n    # Style-specific decorations\n    if style == Style.NEO_BRUTALIST:\n        # Grid lines\n        for i in range(0, width, 100):\n            opacity = 0.1 if i % 200 == 0 else 0.05\n            svg_parts.append(f'<line x1=\"{i}\" y1=\"0\" x2=\"{i}\" y2=\"{height}\" stroke=\"{palette.text}\" stroke-opacity=\"{opacity}\"/>')\n        for i in range(0, height, 100):\n            opacity = 0.1 if i % 200 == 0 else 0.05\n            svg_parts.append(f'<line x1=\"0\" y1=\"{i}\" x2=\"{width}\" y2=\"{i}\" stroke=\"{palette.text}\" stroke-opacity=\"{opacity}\"/>')\n        # Accent block\n        svg_parts.append(f'<rect x=\"60\" y=\"{height//2 - 80}\" width=\"20\" height=\"160\" fill=\"{palette.accent}\"/>')\n\n    elif style == Style.DEEP_GLOW:\n        # Ambient glow circles\n        svg_parts.append(f'<circle cx=\"{width//4}\" cy=\"{height//3}\" r=\"300\" fill=\"{palette.primary}\" opacity=\"0.1\"/>')\n        svg_parts.append(f'<circle cx=\"{3*width//4}\" cy=\"{2*height//3}\" r=\"250\" fill=\"{palette.secondary}\" opacity=\"0.1\"/>')\n\n    elif style == Style.GLASS_MORPHISM:\n        # Frosted glass panel\n        svg_parts.append(f'''\n        <rect x=\"{width//4}\" y=\"{height//4}\" width=\"{width//2}\" height=\"{height//2}\"\n              rx=\"20\" fill=\"{palette.background}\" fill-opacity=\"0.5\"\n              stroke=\"{palette.primary}\" stroke-opacity=\"0.3\"/>\n        ''')\n\n    # Title text\n    title_y = height // 2 - (50 if subtitle else 0)\n    font_size = min(120, width // (len(title) * 0.7))\n\n    filter_attr = 'filter=\"url(#glow)\"' if palette.glow else ''\n    svg_parts.append(f'''\n    <text x=\"{width//2}\" y=\"{title_y}\"\n          font-family=\"SF Pro Display, -apple-system, sans-serif\"\n          font-size=\"{font_size}\" font-weight=\"700\"\n          fill=\"url(#textGradient)\" text-anchor=\"middle\" {filter_attr}>\n        {title}\n    </text>\n    ''')\n\n    # Subtitle\n    if subtitle:\n        svg_parts.append(f'''\n        <text x=\"{width//2}\" y=\"{title_y + 80}\"\n              font-family=\"SF Pro Display, -apple-system, sans-serif\"\n              font-size=\"36\" font-weight=\"400\"\n              fill=\"{palette.text}\" fill-opacity=\"0.8\" text-anchor=\"middle\">\n            {subtitle}\n        </text>\n        ''')\n\n    svg_parts.append('</svg>')\n    return '\\n'.join(svg_parts)\n\n\ndef generate_svg_lower_third(\n    name: str,\n    title: str,\n    style: Style = Style.DEEP_GLOW,\n    width: int = 1920,\n    height: int = 1080,\n    position: str = \"left\",  # left, right, center\n) -> str:\n    \"\"\"Generate an SVG lower third overlay.\"\"\"\n    palette = PALETTES[style]\n\n    # Calculate position\n    if position == \"left\":\n        x_offset = 80\n        text_anchor = \"start\"\n    elif position == \"right\":\n        x_offset = width - 80\n        text_anchor = \"end\"\n    else:\n        x_offset = width // 2\n        text_anchor = \"middle\"\n\n    y_base = height - 150\n\n    svg = f'''<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 {width} {height}\">\n    <defs>\n        <linearGradient id=\"barGradient\" x1=\"0%\" y1=\"0%\" x2=\"100%\" y2=\"0%\">\n            <stop offset=\"0%\" style=\"stop-color:{palette.primary}\"/>\n            <stop offset=\"100%\" style=\"stop-color:{palette.secondary}\"/>\n        </linearGradient>\n        <filter id=\"shadow\" x=\"-20%\" y=\"-20%\" width=\"140%\" height=\"140%\">\n            <feDropShadow dx=\"0\" dy=\"4\" stdDeviation=\"8\" flood-color=\"#000\" flood-opacity=\"0.5\"/>\n        </filter>\n    </defs>\n\n    <!-- Background bar -->\n    <rect x=\"{x_offset - 20 if position == 'left' else 0}\" y=\"{y_base - 20}\"\n          width=\"500\" height=\"100\" rx=\"4\"\n          fill=\"{palette.background}\" fill-opacity=\"0.85\" filter=\"url(#shadow)\"/>\n\n    <!-- Accent line -->\n    <rect x=\"{x_offset - 20 if position == 'left' else x_offset - 480}\" y=\"{y_base - 20}\"\n          width=\"4\" height=\"100\" fill=\"url(#barGradient)\"/>\n\n    <!-- Name -->\n    <text x=\"{x_offset}\" y=\"{y_base + 25}\"\n          font-family=\"SF Pro Display, -apple-system, sans-serif\"\n          font-size=\"32\" font-weight=\"600\"\n          fill=\"{palette.text}\" text-anchor=\"{text_anchor}\">\n        {name}\n    </text>\n\n    <!-- Title -->\n    <text x=\"{x_offset}\" y=\"{y_base + 60}\"\n          font-family=\"SF Pro Display, -apple-system, sans-serif\"\n          font-size=\"22\" font-weight=\"400\"\n          fill=\"{palette.text}\" fill-opacity=\"0.7\" text-anchor=\"{text_anchor}\">\n        {title}\n    </text>\n</svg>'''\n    return svg\n\n\ndef generate_svg_data_chart(\n    data: list[dict],  # [{\"label\": \"A\", \"value\": 75}, ...]\n    chart_type: str = \"bar\",  # bar, horizontal_bar, radial\n    title: Optional[str] = None,\n    style: Style = Style.DEEP_GLOW,\n    width: int = 1920,\n    height: int = 1080,\n) -> str:\n    \"\"\"Generate an SVG data visualization.\"\"\"\n    palette = PALETTES[style]\n    max_value = max(d[\"value\"] for d in data)\n\n    svg_parts = [\n        f'<svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 {width} {height}\">',\n        '<defs>',\n        f'''<linearGradient id=\"barFill\" x1=\"0%\" y1=\"100%\" x2=\"0%\" y2=\"0%\">\n            <stop offset=\"0%\" style=\"stop-color:{palette.primary}\"/>\n            <stop offset=\"100%\" style=\"stop-color:{palette.secondary}\"/>\n        </linearGradient>''',\n    ]\n\n    if palette.glow:\n        svg_parts.append(f'''\n        <filter id=\"barGlow\" x=\"-50%\" y=\"-50%\" width=\"200%\" height=\"200%\">\n            <feGaussianBlur stdDeviation=\"8\" result=\"coloredBlur\"/>\n            <feMerge>\n                <feMergeNode in=\"coloredBlur\"/>\n                <feMergeNode in=\"SourceGraphic\"/>\n            </feMerge>\n        </filter>\n        ''')\n\n    svg_parts.append('</defs>')\n    svg_parts.append(f'<rect width=\"{width}\" height=\"{height}\" fill=\"{palette.background}\"/>')\n\n    # Title\n    if title:\n        svg_parts.append(f'''\n        <text x=\"{width//2}\" y=\"80\"\n              font-family=\"SF Pro Display, -apple-system, sans-serif\"\n              font-size=\"48\" font-weight=\"600\"\n              fill=\"{palette.text}\" text-anchor=\"middle\">\n            {title}\n        </text>\n        ''')\n\n    chart_area_top = 150 if title else 80\n    chart_area_height = height - chart_area_top - 120\n    chart_area_width = width - 200\n\n    if chart_type == \"bar\":\n        bar_width = min(80, chart_area_width // len(data) - 20)\n        bar_spacing = chart_area_width // len(data)\n\n        for i, d in enumerate(data):\n            bar_height = (d[\"value\"] / max_value) * chart_area_height\n            x = 100 + i * bar_spacing + (bar_spacing - bar_width) // 2\n            y = chart_area_top + chart_area_height - bar_height\n\n            filter_attr = 'filter=\"url(#barGlow)\"' if palette.glow else ''\n            svg_parts.append(f'''\n            <rect x=\"{x}\" y=\"{y}\" width=\"{bar_width}\" height=\"{bar_height}\"\n                  rx=\"4\" fill=\"url(#barFill)\" {filter_attr}/>\n            <text x=\"{x + bar_width//2}\" y=\"{chart_area_top + chart_area_height + 40}\"\n                  font-family=\"SF Pro Display, -apple-system, sans-serif\"\n                  font-size=\"20\" fill=\"{palette.text}\" fill-opacity=\"0.7\" text-anchor=\"middle\">\n                {d[\"label\"]}\n            </text>\n            <text x=\"{x + bar_width//2}\" y=\"{y - 15}\"\n                  font-family=\"SF Pro Display, -apple-system, sans-serif\"\n                  font-size=\"24\" font-weight=\"600\" fill=\"{palette.text}\" text-anchor=\"middle\">\n                {d[\"value\"]}\n            </text>\n            ''')\n\n    elif chart_type == \"radial\":\n        cx, cy = width // 2, height // 2 + 30\n        radius = min(chart_area_width, chart_area_height) // 3\n\n        for i, d in enumerate(data):\n            angle = (i / len(data)) * 2 * math.pi - math.pi / 2\n            bar_length = (d[\"value\"] / max_value) * radius\n\n            x1 = cx + math.cos(angle) * 50\n            y1 = cy + math.sin(angle) * 50\n            x2 = cx + math.cos(angle) * (50 + bar_length)\n            y2 = cy + math.sin(angle) * (50 + bar_length)\n\n            svg_parts.append(f'''\n            <line x1=\"{x1}\" y1=\"{y1}\" x2=\"{x2}\" y2=\"{y2}\"\n                  stroke=\"url(#barFill)\" stroke-width=\"20\" stroke-linecap=\"round\"/>\n            ''')\n\n            # Label\n            label_x = cx + math.cos(angle) * (radius + 80)\n            label_y = cy + math.sin(angle) * (radius + 80)\n            svg_parts.append(f'''\n            <text x=\"{label_x}\" y=\"{label_y}\"\n                  font-family=\"SF Pro Display, -apple-system, sans-serif\"\n                  font-size=\"18\" fill=\"{palette.text}\" text-anchor=\"middle\" dominant-baseline=\"middle\">\n                {d[\"label\"]}\n            </text>\n            ''')\n\n    svg_parts.append('</svg>')\n    return '\\n'.join(svg_parts)\n\n\ndef svg_to_png(svg_content: str, output_path: Path, width: int = 1920, height: int = 1080) -> Path:\n    \"\"\"Convert SVG to PNG using system tools.\"\"\"\n    with tempfile.NamedTemporaryFile(mode='w', suffix='.svg', delete=False) as f:\n        f.write(svg_content)\n        svg_path = f.name\n\n    try:\n        # Try rsvg-convert first (best quality)\n        result = subprocess.run(\n            ['rsvg-convert', '-w', str(width), '-h', str(height), '-o', str(output_path), svg_path],\n            capture_output=True\n        )\n        if result.returncode == 0:\n            return output_path\n    except FileNotFoundError:\n        pass\n\n    try:\n        # Fall back to ImageMagick\n        subprocess.run(\n            ['convert', '-background', 'none', '-density', '150', svg_path, str(output_path)],\n            check=True, capture_output=True\n        )\n        return output_path\n    except (FileNotFoundError, subprocess.CalledProcessError):\n        pass\n\n    # Last resort: save as SVG\n    svg_output = output_path.with_suffix('.svg')\n    with open(svg_output, 'w') as f:\n        f.write(svg_content)\n    print(f\"Warning: Could not convert to PNG, saved as SVG: {svg_output}\")\n    return svg_output\n\n\ndef generate_animated_title(\n    title: str,\n    subtitle: Optional[str] = None,\n    style: Style = Style.DEEP_GLOW,\n    duration: float = 3.0,\n    fps: int = 30,\n    output_path: Path = Path(\"title_animated.mp4\"),\n) -> Path:\n    \"\"\"Generate an animated title card video using FFmpeg.\"\"\"\n    palette = PALETTES[style]\n    width, height = 1920, 1080\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        tmpdir = Path(tmpdir)\n\n        # Generate frames\n        total_frames = int(duration * fps)\n\n        for frame in range(total_frames):\n            progress = frame / total_frames\n\n            # Animation: fade in (0-30%), hold (30-70%), fade out (70-100%)\n            if progress < 0.3:\n                opacity = progress / 0.3\n                scale = 0.9 + 0.1 * (progress / 0.3)\n            elif progress > 0.7:\n                opacity = 1 - (progress - 0.7) / 0.3\n                scale = 1.0\n            else:\n                opacity = 1.0\n                scale = 1.0\n\n            # Generate SVG with animation state\n            svg = generate_svg_title_card(title, subtitle, style, width, height)\n\n            # Apply opacity via group wrapper\n            svg = svg.replace(\n                '</defs>',\n                f'</defs><g opacity=\"{opacity}\" transform=\"translate({width/2}, {height/2}) scale({scale}) translate({-width/2}, {-height/2})\">'\n            ).replace('</svg>', '</g></svg>')\n\n            frame_path = tmpdir / f\"frame_{frame:04d}.png\"\n            svg_to_png(svg, frame_path, width, height)\n\n        # Combine frames with FFmpeg\n        subprocess.run([\n            'ffmpeg', '-y', '-framerate', str(fps),\n            '-i', str(tmpdir / 'frame_%04d.png'),\n            '-c:v', 'libx264', '-pix_fmt', 'yuv420p',\n            '-preset', 'medium', '-crf', '18',\n            str(output_path)\n        ], check=True, capture_output=True)\n\n    return output_path\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Motion Graphics Generator\")\n    parser.add_argument(\"--type\", choices=[\"title\", \"lower_third\", \"chart\"], default=\"title\")\n    parser.add_argument(\"--style\", choices=[s.value for s in Style], default=\"deep_glow\")\n    parser.add_argument(\"--title\", type=str, default=\"Your Title Here\")\n    parser.add_argument(\"--subtitle\", type=str, default=None)\n    parser.add_argument(\"--output\", type=Path, default=Path(\"output.png\"))\n    parser.add_argument(\"--animated\", action=\"store_true\", help=\"Generate animated video\")\n    parser.add_argument(\"--duration\", type=float, default=3.0, help=\"Animation duration (seconds)\")\n    parser.add_argument(\"--data\", type=str, help=\"JSON data for charts\")\n    parser.add_argument(\"--chart-type\", choices=[\"bar\", \"horizontal_bar\", \"radial\"], default=\"bar\")\n    parser.add_argument(\"--width\", type=int, default=1920)\n    parser.add_argument(\"--height\", type=int, default=1080)\n    parser.add_argument(\"--list-styles\", action=\"store_true\", help=\"List available styles\")\n\n    args = parser.parse_args()\n\n    if args.list_styles:\n        print(\"Available Styles:\")\n        for style in Style:\n            palette = PALETTES[style]\n            print(f\"\\n  {style.value}:\")\n            print(f\"    Background: {palette.background}\")\n            print(f\"    Primary: {palette.primary}\")\n            print(f\"    Secondary: {palette.secondary}\")\n            print(f\"    Accent: {palette.accent}\")\n        return\n\n    style = Style(args.style)\n\n    if args.type == \"title\":\n        if args.animated:\n            output = generate_animated_title(\n                args.title,\n                args.subtitle,\n                style,\n                args.duration,\n                output_path=args.output.with_suffix('.mp4'),\n            )\n            print(f\"Generated animated title: {output}\")\n        else:\n            svg = generate_svg_title_card(\n                args.title,\n                args.subtitle,\n                style,\n                args.width,\n                args.height,\n            )\n            output = svg_to_png(svg, args.output, args.width, args.height)\n            print(f\"Generated title card: {output}\")\n\n    elif args.type == \"lower_third\":\n        svg = generate_svg_lower_third(\n            args.title,\n            args.subtitle or \"Title\",\n            style,\n            args.width,\n            args.height,\n        )\n        output = svg_to_png(svg, args.output, args.width, args.height)\n        print(f\"Generated lower third: {output}\")\n\n    elif args.type == \"chart\":\n        if args.data:\n            data = json.loads(args.data)\n        else:\n            # Sample data\n            data = [\n                {\"label\": \"Jan\", \"value\": 65},\n                {\"label\": \"Feb\", \"value\": 78},\n                {\"label\": \"Mar\", \"value\": 90},\n                {\"label\": \"Apr\", \"value\": 81},\n                {\"label\": \"May\", \"value\": 95},\n            ]\n\n        svg = generate_svg_data_chart(\n            data,\n            args.chart_type,\n            args.title,\n            style,\n            args.width,\n            args.height,\n        )\n        output = svg_to_png(svg, args.output, args.width, args.height)\n        print(f\"Generated chart: {output}\")\n\n\nif __name__ == \"__main__\":\n    main()\n"
        }
      ]
    },
    {
      "name": "workflows",
      "type": "folder",
      "path": "ai-video-production-master/workflows",
      "children": [
        {
          "name": "comfyui_i2v_optimized.json",
          "type": "file",
          "path": "ai-video-production-master/workflows/comfyui_i2v_optimized.json",
          "size": 5894,
          "content": "{\n  \"name\": \"Wan 2.1 I2V Optimized for Apple Silicon\",\n  \"description\": \"Optimized Image-to-Video workflow for M4 Max with GGUF quantization\",\n  \"author\": \"ai-video-production-master skill\",\n  \"version\": \"1.0.0\",\n  \"nodes\": {\n    \"1\": {\n      \"class_type\": \"LoadImage\",\n      \"inputs\": {\n        \"image\": \"input.png\"\n      },\n      \"_meta\": {\n        \"title\": \"Load Keyframe\"\n      }\n    },\n    \"2\": {\n      \"class_type\": \"ImageScale\",\n      \"inputs\": {\n        \"image\": [\"1\", 0],\n        \"width\": 832,\n        \"height\": 480,\n        \"upscale_method\": \"lanczos\",\n        \"crop\": \"center\"\n      },\n      \"_meta\": {\n        \"title\": \"Scale to 480p (optimal for I2V)\"\n      }\n    },\n    \"3\": {\n      \"class_type\": \"UnetLoaderGGUF\",\n      \"inputs\": {\n        \"unet_name\": \"wan2.1-i2v-14b-480p-Q5_K_M.gguf\"\n      },\n      \"_meta\": {\n        \"title\": \"Load Wan 2.1 I2V GGUF (Q5_K_M)\"\n      }\n    },\n    \"4\": {\n      \"class_type\": \"CLIPLoader\",\n      \"inputs\": {\n        \"clip_name\": \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\",\n        \"type\": \"wan\"\n      },\n      \"_meta\": {\n        \"title\": \"Load UMT5 Text Encoder\"\n      }\n    },\n    \"5\": {\n      \"class_type\": \"VAELoader\",\n      \"inputs\": {\n        \"vae_name\": \"wan_2.1_vae.safetensors\"\n      },\n      \"_meta\": {\n        \"title\": \"Load Wan VAE\"\n      }\n    },\n    \"6\": {\n      \"class_type\": \"CLIPTextEncode\",\n      \"inputs\": {\n        \"text\": \"subtle organic motion, gentle breathing, cinematic quality\",\n        \"clip\": [\"4\", 0]\n      },\n      \"_meta\": {\n        \"title\": \"Positive Prompt (Motion Description)\"\n      }\n    },\n    \"7\": {\n      \"class_type\": \"CLIPTextEncode\",\n      \"inputs\": {\n        \"text\": \"blurry, distorted, low quality, watermark, text, static, frozen\",\n        \"clip\": [\"4\", 0]\n      },\n      \"_meta\": {\n        \"title\": \"Negative Prompt\"\n      }\n    },\n    \"8\": {\n      \"class_type\": \"WanImageToVideo\",\n      \"inputs\": {\n        \"positive\": [\"6\", 0],\n        \"negative\": [\"7\", 0],\n        \"vae\": [\"5\", 0],\n        \"width\": 832,\n        \"height\": 480,\n        \"length\": 33,\n        \"batch_size\": 1,\n        \"start_image\": [\"2\", 0]\n      },\n      \"_meta\": {\n        \"title\": \"Wan I2V Conditioning (33 frames = ~2 sec at 16fps)\"\n      }\n    },\n    \"9\": {\n      \"class_type\": \"ModelSamplingSD3\",\n      \"inputs\": {\n        \"model\": [\"3\", 0],\n        \"shift\": 8.0\n      },\n      \"_meta\": {\n        \"title\": \"Apply Flow Matching (shift=8 for I2V)\"\n      }\n    },\n    \"10\": {\n      \"class_type\": \"KSampler\",\n      \"inputs\": {\n        \"model\": [\"9\", 0],\n        \"positive\": [\"8\", 0],\n        \"negative\": [\"8\", 1],\n        \"latent_image\": [\"8\", 2],\n        \"seed\": 42,\n        \"steps\": 6,\n        \"cfg\": 5.0,\n        \"sampler_name\": \"uni_pc\",\n        \"scheduler\": \"normal\",\n        \"denoise\": 1.0\n      },\n      \"_meta\": {\n        \"title\": \"Sample (6 steps optimal for speed/quality)\"\n      }\n    },\n    \"11\": {\n      \"class_type\": \"VAEDecode\",\n      \"inputs\": {\n        \"samples\": [\"10\", 0],\n        \"vae\": [\"5\", 0]\n      },\n      \"_meta\": {\n        \"title\": \"Decode Latents to Frames\"\n      }\n    },\n    \"12\": {\n      \"class_type\": \"VHS_VideoCombine\",\n      \"inputs\": {\n        \"frame_rate\": 16,\n        \"loop_count\": 0,\n        \"filename_prefix\": \"wan_i2v\",\n        \"format\": \"video/h264-mp4\",\n        \"pingpong\": false,\n        \"save_output\": true,\n        \"images\": [\"11\", 0]\n      },\n      \"_meta\": {\n        \"title\": \"Export MP4 (16fps)\"\n      }\n    }\n  },\n  \"parameters\": {\n    \"motion_prompt\": {\n      \"description\": \"Describe the desired motion\",\n      \"default\": \"subtle organic motion, gentle breathing\",\n      \"node\": \"6\",\n      \"input\": \"text\"\n    },\n    \"negative_prompt\": {\n      \"description\": \"What to avoid in the output\",\n      \"default\": \"blurry, distorted, low quality, watermark, text, static, frozen\",\n      \"node\": \"7\",\n      \"input\": \"text\"\n    },\n    \"steps\": {\n      \"description\": \"Sampling steps (more = better quality, slower)\",\n      \"default\": 6,\n      \"range\": [4, 12],\n      \"node\": \"10\",\n      \"input\": \"steps\"\n    },\n    \"cfg\": {\n      \"description\": \"Classifier-free guidance scale\",\n      \"default\": 5.0,\n      \"range\": [3.0, 8.0],\n      \"node\": \"10\",\n      \"input\": \"cfg\"\n    },\n    \"frames\": {\n      \"description\": \"Number of output frames (16fps)\",\n      \"default\": 33,\n      \"range\": [17, 81],\n      \"node\": \"8\",\n      \"input\": \"length\"\n    },\n    \"shift\": {\n      \"description\": \"Flow matching shift (8 for I2V, 5 for T2V)\",\n      \"default\": 8.0,\n      \"range\": [5.0, 12.0],\n      \"node\": \"9\",\n      \"input\": \"shift\"\n    }\n  },\n  \"performance_notes\": {\n    \"apple_silicon\": {\n      \"m4_max_128gb\": {\n        \"steps_6\": \"~90 minutes per clip\",\n        \"steps_4\": \"~60 minutes per clip\",\n        \"memory_usage\": \"~12-15GB unified memory\",\n        \"recommendation\": \"Use for iteration/testing, cloud for batch\"\n      }\n    },\n    \"cloud_gpu\": {\n      \"h100_80gb\": {\n        \"steps_6\": \"~2 minutes per clip\",\n        \"steps_12\": \"~4 minutes per clip\",\n        \"cost\": \"$1.87-2.99/hr on Vast.ai/RunPod\"\n      },\n      \"a100_80gb\": {\n        \"steps_6\": \"~3 minutes per clip\",\n        \"steps_12\": \"~6 minutes per clip\",\n        \"cost\": \"$1.50-1.74/hr\"\n      }\n    }\n  },\n  \"required_models\": {\n    \"unet\": {\n      \"name\": \"wan2.1-i2v-14b-480p-Q5_K_M.gguf\",\n      \"size\": \"12.7 GB\",\n      \"source\": \"https://huggingface.co/city96/Wan2.1-I2V-14B-480P-GGUF\",\n      \"path\": \"models/diffusion_models/\"\n    },\n    \"clip\": {\n      \"name\": \"umt5_xxl_fp8_e4m3fn_scaled.safetensors\",\n      \"size\": \"~5 GB\",\n      \"source\": \"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged\",\n      \"path\": \"models/text_encoders/\"\n    },\n    \"vae\": {\n      \"name\": \"wan_2.1_vae.safetensors\",\n      \"size\": \"~200 MB\",\n      \"source\": \"https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged\",\n      \"path\": \"models/vae/\"\n    }\n  },\n  \"required_nodes\": [\n    \"ComfyUI-GGUF\",\n    \"ComfyUI-VideoHelperSuite\"\n  ]\n}\n"
        }
      ]
    },
    {
      "name": "README.md",
      "type": "file",
      "path": "ai-video-production-master/README.md",
      "size": 16158,
      "content": "# AI Video Production Master Guide\n\n## The Complete System for Script-to-Video on a Home Mac\n\n**Target Hardware:** 128GB M4 Max MacBook Pro\n**Philosophy:** Maximum quality, minimal cloud cost, full creative control\n\n---\n\n## Table of Contents\n\n1. [The Landscape in 2025](#the-landscape-in-2025)\n2. [Architecture Decision: Local vs Cloud Hybrid](#architecture-decision)\n3. [Style & Character Consistency](#style-and-character-consistency)\n4. [LoRA Fine-Tuning Strategy](#lora-fine-tuning-strategy)\n5. [Hiring Artists & Building Datasets](#hiring-artists)\n6. [Synthetic Video Elements](#synthetic-video-elements)\n7. [Cost Comparison: Self-Hosted vs InVideo](#cost-comparison)\n8. [The Complete Pipeline](#the-complete-pipeline)\n9. [Scripts & Workflows](#scripts-and-workflows)\n\n---\n\n## The Landscape in 2025\n\n### Tier 1: Cloud APIs (Highest Quality, Highest Cost)\n| Service | Best For | Cost | Character Consistency |\n|---------|----------|------|----------------------|\n| **Runway Gen-4** | Professional filmmaking | ~$0.05/sec | ⭐⭐⭐⭐⭐ Best in class |\n| **Kling 2.1** | Realistic motion, lip-sync | ~$0.03/sec | ⭐⭐⭐⭐ |\n| **Veo 3.1** | Cinematic polish | Waitlist | ⭐⭐⭐⭐ |\n| **Sora** | Long-form narrative | ~$0.05/sec | ⭐⭐⭐ |\n\n### Tier 2: Self-Hosted (Maximum Control, Setup Required)\n| Model | Best For | VRAM | Apple Silicon |\n|-------|----------|------|---------------|\n| **Wan 2.1/2.2** | Style control, I2V | 12-24GB | ✅ Slow but works |\n| **LTX Video** | Fast iteration | 8-16GB | ✅ Good |\n| **Hunyuan Video** | Quality balance | 24GB+ | ⚠️ Marginal |\n\n### Tier 3: SaaS Platforms (Convenience, Less Control)\n| Service | Monthly Cost | Minutes | Per-Minute Cost |\n|---------|--------------|---------|-----------------|\n| **InVideo Plus** | $20 | 50 | $0.40/min |\n| **InVideo Max** | $48 | 200 | $0.24/min |\n| **InVideo Gen** | $96 | 400 | $0.24/min |\n\n**Key Insight:** For style-specific work, self-hosted + occasional cloud burst is 3-10x cheaper than SaaS platforms while offering superior creative control.\n\n---\n\n## Architecture Decision\n\n### The Hybrid Approach (Recommended)\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    YOUR M4 MAX (128GB)                          │\n├─────────────────────────────────────────────────────────────────┤\n│  LOCAL TASKS (Free, Unlimited)                                  │\n│  ├── Image Generation (Flux via ComfyUI)                       │\n│  ├── LoRA Training (up to rank 32, small datasets)            │\n│  ├── Style Development & Iteration                             │\n│  ├── Audio Generation (TTS, Music)                             │\n│  ├── Video Composition (FFmpeg)                                │\n│  ├── Motion Graphics (Remotion/After Effects)                  │\n│  └── Subtitle/Overlay Rendering                                │\n├─────────────────────────────────────────────────────────────────┤\n│  CLOUD BURST (Pay-per-use)                                     │\n│  ├── Video Generation (Wan I2V on RunPod/Vast.ai)             │\n│  ├── Large LoRA Training (48GB+ VRAM needed)                  │\n│  └── Batch Processing (10+ clips simultaneously)              │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n### Why This Works\n\n1. **Image generation is fast locally** - Flux on M4 Max: 30-60 sec/image\n2. **I2V is slow locally** - Wan 2.1: 15 min/step × 6 steps = 90 min/clip\n3. **Cloud I2V is fast** - Wan 2.1 on H100: ~2 min/clip\n4. **Cloud is cheap** - Vast.ai H100: $1.87/hr = ~$0.06/clip\n\n### Cost Calculation for a 10-Clip Video\n\n| Approach | Time | Cost |\n|----------|------|------|\n| **Full Local (M4 Max)** | 15+ hours | $0 (electricity) |\n| **Hybrid (local img + cloud I2V)** | 2-3 hours | ~$2-4 |\n| **InVideo Max** | 30 min | $48/mo subscription |\n| **Runway Gen-4** | 30 min | ~$15-25 |\n\n**Winner:** Hybrid approach at $2-4 per video vs $48+/mo subscription.\n\n---\n\n## Style and Character Consistency\n\n### The Core Problem\n\nDiffusion models have no memory. Each generation is independent. This causes:\n- Hair color drift\n- Clothing changes\n- Face morphing\n- Style inconsistency\n\n### Solution Matrix\n\n| Technique | Setup Time | Quality | Best For |\n|-----------|-----------|---------|----------|\n| **LoRA Training** | 4-8 hours | ⭐⭐⭐⭐⭐ | Your unique style |\n| **IPAdapter + FaceID** | 20 min | ⭐⭐⭐⭐ | Consistent faces |\n| **Reference Image Workflow** | 5 min | ⭐⭐⭐ | Quick consistency |\n| **Prompt Discipline** | 0 min | ⭐⭐ | Basic consistency |\n\n### Recommended Stack for Maximum Consistency\n\n```python\n# The \"Belt and Suspenders\" Approach\nCONSISTENCY_STACK = {\n    \"style\": \"LoRA (trained on your artistic style)\",\n    \"face\": \"IPAdapter FaceID Plus\",\n    \"composition\": \"ControlNet (pose/depth)\",\n    \"prompt\": \"Structured with locked variables\",\n    \"i2v\": \"Use keyframe as anchor image\",\n}\n```\n\n### IPAdapter + AnimateDiff Pipeline\n\nResearch shows 94% style consistency with this combination vs 68% with AnimateDiff alone.\n\n```\nReference Image → IPAdapter → AnimateDiff → Consistent Animation\n                     ↓\n              Style Transfer\n              (weight: 0.8)\n```\n\n### Prompt Discipline Rules\n\n1. **Lock visual descriptors:** Always say \"brown trench coat\" not \"coat\"\n2. **Fix camera setup:** \"50mm lens, low-angle shot, studio lighting\"\n3. **Use trigger words:** \"txcl_style painting\" for your LoRA\n4. **Repeat key phrases:** Exact same description across all shots\n\n---\n\n## LoRA Fine-Tuning Strategy\n\n### When to Train a LoRA\n\n✅ **Train when:**\n- You need a unique artistic style\n- You want consistent characters\n- You're producing 10+ pieces in the same style\n- You have 20-100 high-quality reference images\n\n❌ **Don't train when:**\n- One-off projects\n- You can achieve results with IPAdapter\n- You don't have quality reference images\n\n### Dataset Requirements\n\n| LoRA Type | Images Needed | Quality | Diversity |\n|-----------|---------------|---------|-----------|\n| **Style** | 50-100 | Very high | Same style, different subjects |\n| **Character** | 20-30 | High | Same character, different poses |\n| **Concept** | 30-50 | High | Same concept, varied contexts |\n\n### Training Parameters (Flux LoRA)\n\n```yaml\n# Conservative start (recommended)\nrank: 32\nalpha: 32\nlearning_rate: 1e-4\nsteps: 1000-2000\nbatch_size: 1\ngradient_accumulation: 4\nresolution: 1024\n\n# Memory-constrained (M4 Max)\nrank: 16\nsteps: 1500\nuse_8bit_adam: true\ngradient_checkpointing: true\n```\n\n### Where to Train\n\n| Platform | Cost | Speed | VRAM |\n|----------|------|-------|------|\n| **Local M4 Max** | Free | Slow (8-12hr) | 128GB unified |\n| **Vast.ai A100** | ~$1.50/hr | Fast (1-2hr) | 80GB |\n| **RunPod H100** | ~$2/hr | Fastest | 80GB |\n| **fal.ai** | ~$5-15/train | Managed | N/A |\n\n---\n\n## Hiring Artists\n\n### Why Commission Original Art?\n\n1. **Copyright clarity** - You own it, no legal ambiguity\n2. **Unique style** - No one else has this LoRA\n3. **Quality control** - Curated dataset, better results\n4. **Ethical foundation** - Artist compensated fairly\n\n### Finding Artists\n\n| Platform | Best For | Budget Range |\n|----------|----------|--------------|\n| **ArtStation** | Professional concept artists | $500-5000+ |\n| **Fiverr** | Quick, budget-friendly | $50-500 |\n| **Upwork** | Long-term collaboration | $200-2000 |\n| **DeviantArt** | Niche styles | $100-1000 |\n| **Direct (Twitter/IG)** | Specific artists | Varies |\n\n### Commission Structure\n\n**What to Request:**\n```\nI'm commissioning [N] illustrations for use as AI training data.\n\nDeliverables:\n- [20-50] high-resolution images (2048x2048+ PNG)\n- Consistent style across all pieces\n- Varied subjects: [list categories]\n- Full commercial rights including AI training\n\nStyle reference: [attach examples]\nTimeline: [X weeks]\nBudget: $[Y]\n\nUsage: These images will train a LoRA model for\n[personal/commercial] video production.\n```\n\n### Contract Essentials\n\n**Must Include:**\n1. ✅ Full commercial usage rights\n2. ✅ AI/ML training rights explicitly stated\n3. ✅ No exclusivity (you can use anywhere)\n4. ✅ Artist credit requirements (if any)\n5. ✅ Revision policy\n6. ✅ Delivery format and resolution\n\n**Sample Clause:**\n```\n\"Client receives perpetual, worldwide, exclusive rights to use\nthe commissioned works for any purpose, including but not limited\nto: training artificial intelligence or machine learning models,\ngenerating derivative works, commercial products, and any future\ntechnologies. Artist retains right to display in portfolio only.\"\n```\n\n### Budget Guidelines\n\n| Project Scale | Images | Budget | Artist Level |\n|---------------|--------|--------|--------------|\n| **MVP** | 20-30 | $200-500 | Emerging |\n| **Production** | 50-100 | $500-2000 | Mid-level |\n| **Premium** | 100+ | $2000-10000 | Professional |\n\n---\n\n## Synthetic Video Elements\n\n### The Modern Motion Graphics Stack\n\n#### 2025 Trends\n\n1. **Deep Glow** - Intense light blooms, layered neons\n2. **Liquid Motion** - Fluid, morphing typography\n3. **3D + 2D Hybrid** - Depth in flat design\n4. **Neo Brutalism** - Raw, glitchy, utilitarian\n5. **Retro Revival** - 80s/90s grain and neon\n\n### Tools for Different Needs\n\n| Tool | Best For | Learning Curve | Output |\n|------|----------|----------------|--------|\n| **After Effects** | Professional broadcast | High | Video files |\n| **Motion** | macOS-native, quick | Medium | Video files |\n| **Remotion** | Code-driven, React devs | Medium | Video/GIF |\n| **Rive** | Interactive, web export | Low | Web/Apps |\n| **Cavalry** | Procedural animation | Medium | Video files |\n| **DaVinci Fusion** | Integrated compositing | High | Video files |\n\n### Remotion for Programmers\n\n```tsx\n// Example: Animated title card with metrics\nimport { useCurrentFrame, interpolate } from 'remotion';\n\nexport const TitleCard: React.FC<{title: string}> = ({title}) => {\n  const frame = useCurrentFrame();\n  const opacity = interpolate(frame, [0, 30], [0, 1]);\n  const scale = interpolate(frame, [0, 30], [0.8, 1]);\n\n  return (\n    <div style={{\n      opacity,\n      transform: `scale(${scale})`,\n      fontFamily: 'SF Pro Display',\n      fontSize: 72,\n      background: 'linear-gradient(135deg, #667eea 0%, #764ba2 100%)',\n      WebkitBackgroundClip: 'text',\n      WebkitTextFillColor: 'transparent',\n    }}>\n      {title}\n    </div>\n  );\n};\n```\n\n### Non-Academic/Business Chart Styles\n\n**Avoid:**\n- Default Excel/PowerPoint charts\n- Clip art and stock icons\n- Generic sans-serif fonts\n- White backgrounds with black text\n\n**Embrace:**\n- Dark backgrounds with neon accents\n- Custom iconography (Phosphor, Heroicons)\n- Variable fonts with animated weight\n- Gradients and glass effects\n- Micro-animations on data points\n\n### Chart Animation Patterns\n\n```\nEntry Animations:\n├── Stagger reveal (data points appear sequentially)\n├── Draw-on (line charts animate along path)\n├── Scale-up (bars grow from axis)\n└── Morph (smooth transition between states)\n\nEmphasis:\n├── Pulse highlight (attention to key data)\n├── Glow intensification (important values)\n└── Color shift (state change)\n```\n\n---\n\n## Cost Comparison\n\n### InVideo AI vs Self-Hosted (Monthly)\n\n**Scenario:** 10 videos/month, 3 minutes each, 10 shots per video\n\n| Component | InVideo Max | Self-Hosted Hybrid |\n|-----------|-------------|-------------------|\n| Subscription | $48/mo | $0 |\n| Cloud GPU (burst) | N/A | ~$20-40/mo |\n| Storage | Included | ~$5/mo (local) |\n| Total | **$48/mo** | **$25-45/mo** |\n| Control | Limited | Full |\n| Style Customization | Template-based | Unlimited |\n| Character Consistency | Basic | Advanced (LoRA/IPAdapter) |\n\n### Break-Even Analysis\n\n```\nInVideo: $48/mo = $576/year\nSelf-hosted setup: ~$200 one-time (software, plugins)\nSelf-hosted running: ~$30/mo = $360/year\n\nYear 1: InVideo $576 vs Self-hosted $560\nYear 2+: InVideo $576 vs Self-hosted $360\n\nSavings after Year 1: $216/year\n```\n\n### When to Use Each\n\n**Use InVideo when:**\n- Time is more valuable than money\n- Corporate/template style is acceptable\n- No custom style requirements\n- Quick turnaround needed\n\n**Use Self-Hosted when:**\n- Unique artistic style required\n- Character consistency critical\n- Budget-conscious\n- Learning/experimentation phase\n- Privacy/data concerns\n\n---\n\n## The Complete Pipeline\n\n### Phase 1: Pre-Production (Local)\n\n```\nScript → Shot List → Visual Prompts → Reference Gathering\n         ↓\n    Style Development (LoRA training if needed)\n         ↓\n    Audio Production (TTS, music)\n```\n\n### Phase 2: Visual Generation (Hybrid)\n\n```\nLOCAL: Flux Image Generation\n  └── Generate all keyframes\n  └── IPAdapter for consistency\n  └── ControlNet for composition\n\nCLOUD: Wan 2.1 I2V (on Vast.ai/RunPod)\n  └── Batch process all shots\n  └── 10 clips × 2 min = 20 min total\n  └── Cost: ~$0.60\n```\n\n### Phase 3: Post-Production (Local)\n\n```\nMotion Graphics Layer (Remotion/AE)\n  └── Title cards\n  └── Lower thirds\n  └── Data visualizations\n  └── Transitions\n\nComposition (FFmpeg/DaVinci)\n  └── Video assembly\n  └── Audio sync\n  └── Color grading\n  └── Final export\n```\n\n### Automation Script\n\nSee `scripts/full_pipeline.py` for the complete automated workflow.\n\n---\n\n## Scripts and Workflows\n\n### Available in this skill:\n\n1. **`scripts/cloud_i2v_batch.py`** - Batch I2V on cloud GPUs\n2. **`scripts/cost_calculator.py`** - Compare costs across platforms\n3. **`scripts/lora_training_cloud.py`** - Train LoRA on Vast.ai\n4. **`scripts/motion_graphics_generator.py`** - Programmatic title cards\n5. **`workflows/comfyui_i2v_optimized.json`** - Optimized ComfyUI workflow\n\n### Quick Start\n\n```bash\n# Calculate costs for your project\npython scripts/cost_calculator.py --shots 10 --duration 5\n\n# Generate title cards\npython scripts/motion_graphics_generator.py --style \"neo-brutalist\"\n\n# Batch I2V on cloud\npython scripts/cloud_i2v_batch.py --images ./keyframes --provider vastai\n```\n\n---\n\n## Sources & Further Reading\n\n### AI Video Generation\n- [Runway Gen-4 Character Consistency](https://venturebeat.com/ai/runways-gen-4-ai-solves-the-character-consistency-challenge-making-ai-filmmaking-actually-useful)\n- [NVIDIA Video Storyboarding Research](https://research.nvidia.com/labs/par/video_storyboarding/)\n- [Wan 2.1 Comparison](https://syntheticlabs.xyz/2025/03/10/wan2-1-ai-video-comparison/)\n\n### LoRA Training\n- [Complete Guide to Video LoRAs](https://runpod.ghost.io/complete-guide-to-training-video-loras/)\n- [Style Transfer Guide](https://lorastudio.org/style-transfer-guide/)\n- [ConsisLoRA Paper](https://arxiv.org/html/2503.10614v1)\n\n### Cloud GPU Pricing\n- [Vast.ai Pricing](https://vast.ai/pricing)\n- [RunPod Pricing](https://www.runpod.io/pricing)\n- [H100 Cloud Comparison](https://intuitionlabs.ai/articles/h100-rental-prices-cloud-comparison)\n\n### Apple Silicon Optimization\n- [ComfyUI MLX Extension](https://apatero.com/blog/comfyui-mlx-extension-70-faster-apple-silicon-guide-2025)\n- [M4 Max Setup Guide](https://apatero.com/blog/comfyui-mac-m4-max-complete-setup-guide-2025)\n\n### Legal & Ethics\n- [Copyright Office AI Training Guidance](https://www.copyright.gov/ai/)\n- [Fair Use Analysis](https://www.wiley.law/alert-Copyright-Office-Issues-Key-Guidance-on-Fair-Use-in-Generative-AI-Training)\n\n### Motion Graphics Trends\n- [2025 Motion Design Trends](https://elements.envato.com/learn/motion-design-trends)\n- [Animation Trends](https://garagefarm.net/blog/16-animation-trends-to-watch-in-2025-key-insights)\n"
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "ai-video-production-master/SKILL.md",
      "size": 4557,
      "content": "---\nname: ai-video-production-master\ndescription: Expert in script-to-video production pipelines for Apple Silicon Macs. Specializes in hybrid local/cloud workflows, LoRA training for character consistency, motion graphics generation, and artist commissioning. Activate on 'AI video production', 'script to video', 'video generation pipeline', 'character consistency', 'LoRA training', 'cloud GPU', 'motion graphics', 'Wan I2V', 'InVideo alternative'. NOT for real-time video editing, video compositing (use DaVinci/Premiere), audio production, or 3D modeling (use Blender/Maya).\nallowed-tools: Read,Write,Edit,Bash(python:*,ffmpeg:*,npm:*),WebFetch,mcp__firecrawl__firecrawl_search\ncategory: AI & Machine Learning\ntags:\n  - video\n  - ai-generation\n  - lora\n  - cloud-gpu\n  - motion-graphics\n  - comfyui\npairs-with:\n  - skill: sound-engineer\n    reason: Audio for AI-generated videos\n  - skill: voice-audio-engineer\n    reason: Voice synthesis for narration\n---\n\n# AI Video Production Master\n\nExpert in script-to-video production pipelines for Apple Silicon Macs. Specializes in:\n- Hybrid local/cloud workflows for cost optimization\n- Style and character consistency (LoRA, IPAdapter, prompt discipline)\n- Motion graphics and synthetic elements (title cards, data viz, lower thirds)\n- Artist commissioning for training datasets\n- Cloud GPU orchestration (Vast.ai, RunPod)\n\n## When to Use This Skill\n\nActivate on:\n- \"AI video production\"\n- \"script to video\"\n- \"video generation pipeline\"\n- \"character consistency\"\n- \"LoRA training for video\"\n- \"cloud GPU for video\"\n- \"motion graphics\"\n- \"hire artist for AI training\"\n- \"InVideo alternative\"\n- \"Wan I2V\"\n\n## Key Capabilities\n\n### 1. Cost Optimization\nCompare and recommend the optimal mix of local (M4 Max) vs cloud (H100/A100) processing:\n```bash\npython scripts/cost_calculator.py --shots 10 --duration 5\n```\n\n### 2. Cloud Batch Processing\nRun I2V generation on cloud GPUs for 50x speedup:\n```bash\npython scripts/cloud_i2v_batch.py --images ./keyframes --provider vastai\n```\n\n### 3. Motion Graphics Generation\nCreate professional title cards, lower thirds, and data visualizations:\n```bash\npython scripts/motion_graphics_generator.py --type title --style deep_glow --title \"Your Title\"\n```\n\n### 4. Style Consistency\nProvide guidance on:\n- LoRA training parameters (rank, alpha, learning rate, steps)\n- IPAdapter + FaceID for character consistency\n- Prompt discipline and trigger words\n- Reference image workflows\n\n### 5. Artist Commissioning\nTemplates and guidance for:\n- Finding artists (ArtStation, Fiverr, Upwork)\n- Structuring commission requests\n- AI training rights contracts\n- Quality control and review processes\n\n## Files in This Skill\n\n```\nai-video-production-master/\n├── README.md                          # Comprehensive guide\n├── SKILL.md                           # This file\n├── scripts/\n│   ├── cost_calculator.py             # Cost comparison tool\n│   ├── cloud_i2v_batch.py             # Cloud batch I2V\n│   └── motion_graphics_generator.py   # Title cards, lower thirds\n├── workflows/\n│   └── comfyui_i2v_optimized.json     # Optimized ComfyUI workflow\n└── docs/\n    ├── ARTIST_COMMISSIONING_GUIDE.md  # Hiring artists\n    └── contracts/\n        └── artist_commission_template.md  # Contract template\n```\n\n## Quick Reference\n\n### Cost Comparison (10-shot video)\n| Approach | Time | Cost |\n|----------|------|------|\n| Full Local (M4 Max) | 15+ hours | $0 |\n| Hybrid (local img + cloud I2V) | 2-3 hours | ~$2-4 |\n| InVideo Max | 30 min | $48/mo |\n| Runway Gen-4 | 30 min | ~$15-25 |\n\n### Cloud GPU Pricing\n| Provider | GPU | $/hr | I2V Time/Clip |\n|----------|-----|------|---------------|\n| Vast.ai | H100 80GB | $1.87 | ~2 min |\n| RunPod | H100 80GB | $1.99 | ~2 min |\n| RunPod | A100 80GB | $1.74 | ~3 min |\n| Lambda | H100 | $2.99 | ~2 min |\n\n### Motion Graphics Styles\n- `neo_brutalist` - Raw, glitchy, utilitarian\n- `deep_glow` - Intense light blooms, layered neons\n- `liquid_motion` - Fluid, morphing typography\n- `retro_revival` - 80s/90s grain and neon\n- `glass_morphism` - Frosted glass, depth layers\n\n## Dependencies\n\nPython packages:\n- httpx (for cloud API calls)\n- argparse, json, subprocess (stdlib)\n\nExternal tools:\n- FFmpeg (video encoding)\n- rsvg-convert or ImageMagick (SVG to PNG)\n- ComfyUI (local generation)\n\n## NOT for\n\n- Real-time video editing\n- Video effects/compositing (use DaVinci/Premiere)\n- Audio production (use dedicated audio tools)\n- 3D modeling/animation (use Blender/Maya)\n"
    }
  ]
}