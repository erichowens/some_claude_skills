{
  "name": "test-automation-expert",
  "type": "folder",
  "path": "test-automation-expert",
  "children": [
    {
      "name": "references",
      "type": "folder",
      "path": "test-automation-expert/references",
      "children": [
        {
          "name": "ci-integration.md",
          "type": "file",
          "path": "test-automation-expert/references/ci-integration.md",
          "size": 13690,
          "content": "# CI/CD Test Integration\n\nComplete configurations for integrating tests into continuous integration pipelines.\n\n## GitHub Actions\n\n### Complete Test Workflow\n\n```yaml\n# .github/workflows/test.yml\nname: Tests\n\non:\n  push:\n    branches: [main, develop]\n  pull_request:\n    branches: [main]\n\nconcurrency:\n  group: ${{ github.workflow }}-${{ github.ref }}\n  cancel-in-progress: true\n\nenv:\n  NODE_VERSION: '20'\n  CI: true\n\njobs:\n  # ============================================\n  # Unit & Integration Tests\n  # ============================================\n  unit-tests:\n    name: Unit Tests\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Run unit tests\n        run: npm test -- --coverage --maxWorkers=2\n\n      - name: Upload coverage\n        uses: codecov/codecov-action@v4\n        with:\n          files: ./coverage/lcov.info\n          flags: unittests\n          fail_ci_if_error: true\n\n      - name: Archive coverage report\n        uses: actions/upload-artifact@v4\n        with:\n          name: coverage-report\n          path: coverage/\n          retention-days: 7\n\n  # ============================================\n  # E2E Tests\n  # ============================================\n  e2e-tests:\n    name: E2E Tests\n    runs-on: ubuntu-latest\n    needs: unit-tests # Only run if unit tests pass\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Install Playwright browsers\n        run: npx playwright install --with-deps chromium firefox\n\n      - name: Build application\n        run: npm run build\n\n      - name: Run E2E tests\n        run: npx playwright test\n        env:\n          BASE_URL: http://localhost:3000\n\n      - name: Upload Playwright report\n        uses: actions/upload-artifact@v4\n        if: failure()\n        with:\n          name: playwright-report\n          path: playwright-report/\n          retention-days: 7\n\n      - name: Upload test results\n        uses: actions/upload-artifact@v4\n        if: always()\n        with:\n          name: test-results\n          path: test-results/\n          retention-days: 7\n\n  # ============================================\n  # Component Tests (Storybook)\n  # ============================================\n  component-tests:\n    name: Component Tests\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0 # For Chromatic\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: ${{ env.NODE_VERSION }}\n          cache: 'npm'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Run Chromatic\n        uses: chromaui/action@latest\n        with:\n          projectToken: ${{ secrets.CHROMATIC_PROJECT_TOKEN }}\n          buildScriptName: build-storybook\n          onlyChanged: true\n\n  # ============================================\n  # Test Summary\n  # ============================================\n  test-summary:\n    name: Test Summary\n    runs-on: ubuntu-latest\n    needs: [unit-tests, e2e-tests]\n    if: always()\n\n    steps:\n      - name: Check test results\n        run: |\n          if [ \"${{ needs.unit-tests.result }}\" == \"failure\" ] || \\\n             [ \"${{ needs.e2e-tests.result }}\" == \"failure\" ]; then\n            echo \"Tests failed!\"\n            exit 1\n          fi\n          echo \"All tests passed!\"\n```\n\n### Matrix Testing (Multiple Versions)\n\n```yaml\njobs:\n  test:\n    runs-on: ${{ matrix.os }}\n    strategy:\n      fail-fast: false\n      matrix:\n        os: [ubuntu-latest, windows-latest, macos-latest]\n        node: [18, 20, 22]\n        exclude:\n          - os: windows-latest\n            node: 18\n\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: ${{ matrix.node }}\n      - run: npm ci\n      - run: npm test\n```\n\n### Parallel E2E with Sharding\n\n```yaml\ne2e-tests:\n  runs-on: ubuntu-latest\n  strategy:\n    fail-fast: false\n    matrix:\n      shard: [1, 2, 3, 4]\n\n  steps:\n    - uses: actions/checkout@v4\n    - uses: actions/setup-node@v4\n    - run: npm ci\n    - run: npx playwright install --with-deps\n\n    - name: Run tests (shard ${{ matrix.shard }}/4)\n      run: npx playwright test --shard=${{ matrix.shard }}/4\n```\n\n## GitLab CI\n\n### Complete Pipeline\n\n```yaml\n# .gitlab-ci.yml\nstages:\n  - install\n  - test\n  - e2e\n  - report\n\nvariables:\n  NODE_VERSION: \"20\"\n  npm_config_cache: \"$CI_PROJECT_DIR/.npm\"\n\ncache:\n  key:\n    files:\n      - package-lock.json\n  paths:\n    - .npm/\n    - node_modules/\n\n# ============================================\n# Install Stage\n# ============================================\ninstall:\n  stage: install\n  image: node:${NODE_VERSION}\n  script:\n    - npm ci\n  artifacts:\n    paths:\n      - node_modules/\n    expire_in: 1 hour\n\n# ============================================\n# Test Stage\n# ============================================\nunit-tests:\n  stage: test\n  image: node:${NODE_VERSION}\n  needs: [install]\n  script:\n    - npm test -- --coverage\n  coverage: '/All files[^|]*\\|[^|]*\\s+([\\d\\.]+)/'\n  artifacts:\n    when: always\n    paths:\n      - coverage/\n    reports:\n      junit: junit.xml\n      coverage_report:\n        coverage_format: cobertura\n        path: coverage/cobertura-coverage.xml\n\nlint:\n  stage: test\n  image: node:${NODE_VERSION}\n  needs: [install]\n  script:\n    - npm run lint\n    - npm run typecheck\n\n# ============================================\n# E2E Stage\n# ============================================\ne2e-tests:\n  stage: e2e\n  image: mcr.microsoft.com/playwright:v1.40.0-jammy\n  needs: [unit-tests]\n  script:\n    - npm ci\n    - npm run build\n    - npx playwright test\n  artifacts:\n    when: always\n    paths:\n      - playwright-report/\n      - test-results/\n    expire_in: 1 week\n\n# ============================================\n# Report Stage\n# ============================================\npages:\n  stage: report\n  needs: [unit-tests, e2e-tests]\n  script:\n    - mkdir public\n    - cp -r coverage/ public/coverage\n    - cp -r playwright-report/ public/e2e\n  artifacts:\n    paths:\n      - public\n  only:\n    - main\n```\n\n## CircleCI\n\n### Complete Config\n\n```yaml\n# .circleci/config.yml\nversion: 2.1\n\norbs:\n  node: circleci/node@5.1.0\n  browser-tools: circleci/browser-tools@1.4.6\n\nexecutors:\n  node-executor:\n    docker:\n      - image: cimg/node:20.10\n    working_directory: ~/project\n\ncommands:\n  install-deps:\n    steps:\n      - restore_cache:\n          keys:\n            - npm-deps-{{ checksum \"package-lock.json\" }}\n      - run: npm ci\n      - save_cache:\n          key: npm-deps-{{ checksum \"package-lock.json\" }}\n          paths:\n            - node_modules\n\njobs:\n  # ============================================\n  # Unit Tests\n  # ============================================\n  unit-tests:\n    executor: node-executor\n    steps:\n      - checkout\n      - install-deps\n      - run:\n          name: Run unit tests\n          command: npm test -- --coverage --ci\n      - store_test_results:\n          path: test-results\n      - store_artifacts:\n          path: coverage\n          destination: coverage\n      - run:\n          name: Upload coverage\n          command: bash <(curl -s https://codecov.io/bash)\n\n  # ============================================\n  # E2E Tests\n  # ============================================\n  e2e-tests:\n    executor: node-executor\n    parallelism: 4\n    steps:\n      - checkout\n      - install-deps\n      - run:\n          name: Install Playwright\n          command: npx playwright install --with-deps\n      - run:\n          name: Build app\n          command: npm run build\n      - run:\n          name: Run E2E tests\n          command: |\n            SHARD=\"$((${CIRCLE_NODE_INDEX}+1))\"\n            npx playwright test --shard=${SHARD}/${CIRCLE_NODE_TOTAL}\n      - store_artifacts:\n          path: playwright-report\n          destination: playwright-report\n      - store_test_results:\n          path: test-results\n\n  # ============================================\n  # Integration Tests (with services)\n  # ============================================\n  integration-tests:\n    docker:\n      - image: cimg/node:20.10\n      - image: postgres:15\n        environment:\n          POSTGRES_DB: test\n          POSTGRES_USER: test\n          POSTGRES_PASSWORD: test\n      - image: redis:7\n    steps:\n      - checkout\n      - install-deps\n      - run:\n          name: Wait for services\n          command: dockerize -wait tcp://localhost:5432 -wait tcp://localhost:6379 -timeout 30s\n      - run:\n          name: Run integration tests\n          command: npm run test:integration\n          environment:\n            DATABASE_URL: postgres://test:test@localhost:5432/test\n            REDIS_URL: redis://localhost:6379\n\nworkflows:\n  test:\n    jobs:\n      - unit-tests\n      - integration-tests:\n          requires:\n            - unit-tests\n      - e2e-tests:\n          requires:\n            - unit-tests\n```\n\n## Jenkins\n\n### Jenkinsfile\n\n```groovy\n// Jenkinsfile\npipeline {\n    agent {\n        docker {\n            image 'node:20'\n        }\n    }\n\n    environment {\n        CI = 'true'\n        npm_config_cache = \"${WORKSPACE}/.npm\"\n    }\n\n    stages {\n        stage('Install') {\n            steps {\n                sh 'npm ci'\n            }\n        }\n\n        stage('Test') {\n            parallel {\n                stage('Unit Tests') {\n                    steps {\n                        sh 'npm test -- --coverage --ci'\n                    }\n                    post {\n                        always {\n                            junit 'junit.xml'\n                            publishHTML([\n                                reportDir: 'coverage/lcov-report',\n                                reportFiles: 'index.html',\n                                reportName: 'Coverage Report'\n                            ])\n                        }\n                    }\n                }\n\n                stage('Lint') {\n                    steps {\n                        sh 'npm run lint'\n                    }\n                }\n            }\n        }\n\n        stage('E2E Tests') {\n            agent {\n                docker {\n                    image 'mcr.microsoft.com/playwright:v1.40.0-jammy'\n                }\n            }\n            steps {\n                sh 'npm ci'\n                sh 'npm run build'\n                sh 'npx playwright test'\n            }\n            post {\n                always {\n                    publishHTML([\n                        reportDir: 'playwright-report',\n                        reportFiles: 'index.html',\n                        reportName: 'Playwright Report'\n                    ])\n                }\n            }\n        }\n    }\n\n    post {\n        failure {\n            slackSend(\n                color: 'danger',\n                message: \"Build Failed: ${env.JOB_NAME} #${env.BUILD_NUMBER}\"\n            )\n        }\n    }\n}\n```\n\n## Test Caching Strategies\n\n### npm/Node.js\n\n```yaml\n# GitHub Actions\n- uses: actions/cache@v4\n  with:\n    path: |\n      ~/.npm\n      node_modules\n    key: npm-${{ hashFiles('package-lock.json') }}\n    restore-keys: npm-\n\n# Playwright browsers\n- uses: actions/cache@v4\n  with:\n    path: ~/.cache/ms-playwright\n    key: playwright-${{ hashFiles('package-lock.json') }}\n```\n\n### pytest\n\n```yaml\n- uses: actions/cache@v4\n  with:\n    path: |\n      ~/.cache/pip\n      .pytest_cache\n    key: pytest-${{ hashFiles('requirements.txt') }}\n```\n\n## Flaky Test Handling\n\n### Retry Configuration\n\n```yaml\n# GitHub Actions\n- name: Run tests with retry\n  uses: nick-fields/retry@v2\n  with:\n    max_attempts: 3\n    timeout_minutes: 10\n    command: npm test\n\n# Playwright built-in\n- name: Run E2E with retries\n  run: npx playwright test --retries=2\n```\n\n### Quarantine Flaky Tests\n\n```javascript\n// Mark flaky tests for tracking\ntest.describe('Flaky Feature', () => {\n  test.fixme('sometimes fails due to race condition', async () => {\n    // Test code\n  });\n});\n\n// Or skip in CI only\ntest('flaky test', async () => {\n  test.skip(process.env.CI, 'Flaky in CI - JIRA-123');\n});\n```\n\n## Notifications\n\n### Slack Integration\n\n```yaml\n# GitHub Actions\n- name: Notify Slack on failure\n  if: failure()\n  uses: slackapi/slack-github-action@v1\n  with:\n    payload: |\n      {\n        \"text\": \"❌ Tests failed on ${{ github.ref }}\",\n        \"blocks\": [\n          {\n            \"type\": \"section\",\n            \"text\": {\n              \"type\": \"mrkdwn\",\n              \"text\": \"*Test Failure*\\n• Repository: ${{ github.repository }}\\n• Branch: ${{ github.ref }}\\n• Commit: ${{ github.sha }}\"\n            }\n          }\n        ]\n      }\n  env:\n    SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}\n```\n\n## Performance Optimization\n\n### Test Splitting\n\n```yaml\n# Dynamic test splitting based on timing\n- name: Run tests with timing\n  run: |\n    npx jest --listTests --json > tests.json\n    npx jest $(cat tests.json | jq -r '.[] | select(. | test(\"unit\"))' | head -n $(($(cat tests.json | jq length) / 4)))\n```\n\n### Selective Testing\n\n```yaml\n# Only run affected tests\n- uses: dorny/paths-filter@v2\n  id: changes\n  with:\n    filters: |\n      src:\n        - 'src/**'\n      tests:\n        - 'tests/**'\n\n- name: Run tests if source changed\n  if: steps.changes.outputs.src == 'true'\n  run: npm test\n```\n"
        },
        {
          "name": "coverage-patterns.md",
          "type": "file",
          "path": "test-automation-expert/references/coverage-patterns.md",
          "size": 11410,
          "content": "# Coverage Optimization Patterns\n\nStrategies for meaningful test coverage that catches bugs without slowing development.\n\n## Understanding Coverage Metrics\n\n### Types of Coverage\n\n| Metric | What It Measures | Target | Priority |\n|--------|-----------------|--------|----------|\n| **Line** | Executed lines | 80% | Medium |\n| **Branch** | Decision paths (if/else) | 75% | High |\n| **Function** | Called functions | 90% | Medium |\n| **Statement** | Executed statements | 80% | Low |\n| **Condition** | Boolean sub-expressions | 70% | High |\n| **Path** | Unique execution paths | 60% | Low |\n\n### Why Branch Coverage Matters Most\n\n```javascript\nfunction processOrder(order) {\n  if (order.isPriority && order.total > 100) {  // 4 branches!\n    applyDiscount(order);\n  }\n  return order;\n}\n\n// Line coverage: 100% with just one test\n// Branch coverage: Only 25% - missing 3 combinations!\n\n// Need tests for:\n// 1. isPriority=true, total&gt;100 (discount applied)\n// 2. isPriority=true, total<=100 (no discount)\n// 3. isPriority=false, total&gt;100 (no discount)\n// 4. isPriority=false, total<=100 (no discount)\n```\n\n## Coverage Configuration\n\n### Vitest\n\n```typescript\n// vitest.config.ts\nimport { defineConfig } from 'vitest/config';\n\nexport default defineConfig({\n  test: {\n    coverage: {\n      provider: 'v8', // or 'istanbul'\n      reporter: ['text', 'json', 'html', 'lcov'],\n      reportsDirectory: './coverage',\n\n      // What to include\n      include: ['src/**/*.{ts,tsx}'],\n\n      // What to exclude\n      exclude: [\n        'node_modules/',\n        'src/**/*.d.ts',\n        'src/**/*.test.{ts,tsx}',\n        'src/**/*.spec.{ts,tsx}',\n        'src/**/*.stories.{ts,tsx}',\n        'src/**/index.ts',      // Barrel files\n        'src/**/types.ts',      // Type definitions\n        'src/**/*.config.*',    // Config files\n        'src/mocks/**',         // Test mocks\n        'src/__fixtures__/**',  // Test fixtures\n      ],\n\n      // Thresholds\n      thresholds: {\n        global: {\n          branches: 75,\n          functions: 80,\n          lines: 80,\n          statements: 80,\n        },\n        // Per-file thresholds\n        'src/utils/**': {\n          branches: 90,\n          functions: 95,\n        },\n        'src/components/**': {\n          branches: 70,\n          lines: 75,\n        },\n      },\n\n      // Fail if coverage drops\n      watermarks: {\n        lines: [70, 80],\n        functions: [70, 80],\n        branches: [70, 75],\n        statements: [70, 80],\n      },\n    },\n  },\n});\n```\n\n### Jest\n\n```javascript\n// jest.config.js\nmodule.exports = {\n  collectCoverage: true,\n  coverageDirectory: 'coverage',\n  coverageReporters: ['text', 'lcov', 'html'],\n\n  collectCoverageFrom: [\n    'src/**/*.{js,jsx,ts,tsx}',\n    '!src/**/*.d.ts',\n    '!src/**/*.stories.*',\n    '!src/**/*.test.*',\n    '!src/**/index.{js,ts}',\n  ],\n\n  coverageThreshold: {\n    global: {\n      branches: 75,\n      functions: 80,\n      lines: 80,\n      statements: 80,\n    },\n    './src/utils/': {\n      branches: 90,\n      functions: 95,\n    },\n  },\n};\n```\n\n### pytest\n\n```toml\n# pyproject.toml\n[tool.coverage.run]\nbranch = true\nsource = [\"src\"]\nomit = [\n    \"tests/*\",\n    \"**/__init__.py\",\n    \"**/conftest.py\",\n    \"**/*_test.py\",\n]\nparallel = true\n\n[tool.coverage.report]\nfail_under = 80\nshow_missing = true\nskip_covered = false\nexclude_lines = [\n    \"pragma: no cover\",\n    \"def __repr__\",\n    \"raise NotImplementedError\",\n    \"if TYPE_CHECKING:\",\n    \"if __name__ == .__main__.:\",\n    \"@abstractmethod\",\n]\n\n[tool.coverage.html]\ndirectory = \"coverage_html\"\n```\n\n## Finding Coverage Gaps\n\n### Command Line Analysis\n\n```bash\n# Vitest - show uncovered lines\nnpx vitest run --coverage\n\n# Jest - detailed report\nnpx jest --coverage --coverageReporters=text-summary\n\n# Find files with low coverage\nnpx vitest run --coverage --reporter=json | \\\n  jq '.coverageMap | to_entries |\n      map(select(.value.s | values | map(select(. == 0)) | length > 0)) |\n      .[].key'\n\n# pytest - show missing lines\npytest --cov=src --cov-report=term-missing\n```\n\n### HTML Report Navigation\n\n1. Generate HTML report: `npx vitest run --coverage`\n2. Open `coverage/index.html`\n3. Sort by \"Branches\" or \"Lines\" (ascending)\n4. Click into files with low coverage\n5. Red highlights = uncovered code\n\n### Identifying Critical Gaps\n\n**Priority order for fixing gaps:**\n\n1. **Error handling paths** - Often untested but critical\n   ```javascript\n   try {\n     await api.call();\n   } catch (error) {\n     // This branch often untested!\n     logger.error(error);\n     throw new AppError('API failed');\n   }\n   ```\n\n2. **Edge cases in conditionals**\n   ```javascript\n   if (value === null || value === undefined) {\n     // Null/undefined often missed\n   }\n   ```\n\n3. **Async error paths**\n   ```javascript\n   promise.catch(error => {\n     // Rejection handlers often untested\n   });\n   ```\n\n4. **Default switch cases**\n   ```javascript\n   switch (status) {\n     case 'active': return 'green';\n     case 'pending': return 'yellow';\n     default: return 'gray'; // Often untested\n   }\n   ```\n\n## Coverage Anti-Patterns\n\n### Anti-Pattern: Coverage Without Assertions\n\n```javascript\n// ❌ Bad: Executes code but tests nothing\nit('covers the function', () => {\n  processOrder({ id: 1 });\n  // No expect()!\n});\n\n// ✅ Good: Meaningful assertions\nit('processes valid order', () => {\n  const result = processOrder({ id: 1, items: [{ price: 10 }] });\n  expect(result.total).toBe(10);\n  expect(result.status).toBe('processed');\n});\n```\n\n### Anti-Pattern: Testing Private Internals\n\n```javascript\n// ❌ Bad: Testing implementation detail\nit('sets internal flag', () => {\n  const service = new UserService();\n  service._processUser(user);\n  expect(service._isProcessed).toBe(true);\n});\n\n// ✅ Good: Test observable behavior\nit('marks user as active after processing', () => {\n  const service = new UserService();\n  const result = service.processUser(user);\n  expect(result.status).toBe('active');\n});\n```\n\n### Anti-Pattern: Excessive Mocking\n\n```javascript\n// ❌ Bad: Everything mocked, testing nothing real\njest.mock('./database');\njest.mock('./logger');\njest.mock('./validator');\njest.mock('./formatter');\n\nit('processes data', () => {\n  // All real code is mocked out!\n});\n\n// ✅ Good: Mock only boundaries\njest.mock('./database'); // External system\n\nit('validates and saves data', () => {\n  // Real validation, real formatting\n  // Only DB is mocked\n});\n```\n\n## Effective Coverage Strategies\n\n### Strategy 1: Test Behaviors, Not Lines\n\n```javascript\n// Instead of testing each line...\ndescribe('OrderCalculator', () => {\n  // Test the behaviors users care about\n  it('calculates subtotal from items', () => {});\n  it('applies percentage discount', () => {});\n  it('applies fixed discount', () => {});\n  it('calculates tax after discounts', () => {});\n  it('rounds total to 2 decimal places', () => {});\n});\n```\n\n### Strategy 2: Boundary Value Testing\n\n```javascript\n// Test at boundaries, not random values\ndescribe('validateAge', () => {\n  it('rejects age below minimum (17)', () => {\n    expect(validateAge(17)).toBe(false);\n  });\n\n  it('accepts age at minimum (18)', () => {\n    expect(validateAge(18)).toBe(true);\n  });\n\n  it('accepts age at maximum (120)', () => {\n    expect(validateAge(120)).toBe(true);\n  });\n\n  it('rejects age above maximum (121)', () => {\n    expect(validateAge(121)).toBe(false);\n  });\n});\n```\n\n### Strategy 3: Error Path Testing\n\n```javascript\ndescribe('fetchUser', () => {\n  it('returns user on success', async () => {\n    mockApi.get.mockResolvedValue({ data: { id: 1 } });\n    const user = await fetchUser(1);\n    expect(user.id).toBe(1);\n  });\n\n  // Explicitly test each error case\n  it('throws NotFoundError for 404', async () => {\n    mockApi.get.mockRejectedValue({ status: 404 });\n    await expect(fetchUser(999)).rejects.toThrow(NotFoundError);\n  });\n\n  it('throws NetworkError for timeout', async () => {\n    mockApi.get.mockRejectedValue({ code: 'ETIMEDOUT' });\n    await expect(fetchUser(1)).rejects.toThrow(NetworkError);\n  });\n\n  it('retries on 503 before failing', async () => {\n    mockApi.get\n      .mockRejectedValueOnce({ status: 503 })\n      .mockRejectedValueOnce({ status: 503 })\n      .mockResolvedValue({ data: { id: 1 } });\n\n    const user = await fetchUser(1);\n    expect(mockApi.get).toHaveBeenCalledTimes(3);\n  });\n});\n```\n\n### Strategy 4: Parameterized Tests\n\n```javascript\n// Test multiple cases efficiently\ndescribe.each([\n  ['valid@email.com', true],\n  ['user+tag@domain.co', true],\n  ['name@subdomain.domain.org', true],\n  ['invalid', false],\n  ['@missing.com', false],\n  ['spaces @domain.com', false],\n  ['', false],\n  [null, false],\n])('validateEmail(%s)', (email, expected) => {\n  it(`returns ${expected}`, () => {\n    expect(validateEmail(email)).toBe(expected);\n  });\n});\n```\n\n### Strategy 5: Coverage-Driven Refactoring\n\nWhen you find untestable code, refactor it:\n\n```javascript\n// ❌ Hard to test: side effects mixed with logic\nfunction processOrder(order) {\n  const discount = order.customer.tier === 'gold' ? 0.1 : 0;\n  const total = order.items.reduce((sum, i) => sum + i.price, 0);\n  const finalTotal = total * (1 - discount);\n\n  database.save({ ...order, total: finalTotal }); // Side effect!\n  emailService.send(order.customer.email, finalTotal); // Side effect!\n\n  return finalTotal;\n}\n\n// ✅ Testable: Pure calculation, separate side effects\nfunction calculateOrderTotal(order) {\n  const discount = getCustomerDiscount(order.customer);\n  const subtotal = calculateSubtotal(order.items);\n  return applyDiscount(subtotal, discount);\n}\n\nfunction processOrder(order) {\n  const total = calculateOrderTotal(order);\n  await saveOrder({ ...order, total });\n  await notifyCustomer(order.customer, total);\n  return total;\n}\n\n// Now you can unit test calculateOrderTotal with 100% coverage\n// And integration test processOrder separately\n```\n\n## Coverage in CI/CD\n\n### GitHub Actions\n\n```yaml\n- name: Run tests with coverage\n  run: npm test -- --coverage --coverageReporters=json-summary\n\n- name: Check coverage thresholds\n  run: |\n    COVERAGE=$(cat coverage/coverage-summary.json | jq '.total.lines.pct')\n    if (( $(echo \"$COVERAGE < 80\" | bc -l) )); then\n      echo \"Coverage $COVERAGE% is below 80%\"\n      exit 1\n    fi\n\n- name: Upload coverage to Codecov\n  uses: codecov/codecov-action@v4\n  with:\n    files: ./coverage/lcov.info\n    fail_ci_if_error: true\n```\n\n### Coverage Diff on PRs\n\n```yaml\n- name: Coverage diff\n  uses: ArtiomTr/jest-coverage-report-action@v2\n  with:\n    threshold: 80\n    annotations: coverage\n```\n\n## Coverage Reporting Tools\n\n| Tool | Features | Best For |\n|------|----------|----------|\n| Codecov | Diff, history, PR comments | Open source, teams |\n| Coveralls | Simple, badges | Small projects |\n| SonarQube | Quality gates, security | Enterprise |\n| Code Climate | Maintainability metrics | Full analysis |\n\n## Realistic Targets\n\n### By Project Type\n\n| Type | Line | Branch | Rationale |\n|------|------|--------|-----------|\n| Library/SDK | 90% | 85% | High reuse, stable |\n| API Service | 80% | 75% | Core paths critical |\n| Web App | 75% | 70% | UI harder to test |\n| CLI Tool | 85% | 80% | Fewer UI concerns |\n| Legacy Migration | 60% | 50% | Start somewhere |\n\n### Incremental Improvement\n\nWeek 1: Establish baseline, add CI check\nWeek 2: Target +5% on lowest files\nWeek 4: Hit 60% if starting low\nWeek 8: Hit 70% overall\nWeek 12: Hit 80% stable target\n"
        },
        {
          "name": "framework-comparison.md",
          "type": "file",
          "path": "test-automation-expert/references/framework-comparison.md",
          "size": 9669,
          "content": "# Testing Framework Comparison\n\nDetailed comparison of modern testing frameworks to help select the right tools.\n\n## Unit Testing Frameworks\n\n### JavaScript/TypeScript\n\n| Feature | Jest | Vitest | Mocha + Chai |\n|---------|------|--------|--------------|\n| **Speed** | Fast | Fastest | Medium |\n| **ESM Support** | Partial | Native | Good |\n| **TypeScript** | Via transform | Native | Via ts-node |\n| **Watch Mode** | Yes | Yes (HMR) | Yes |\n| **Snapshots** | Built-in | Built-in | Plugin |\n| **Coverage** | Built-in | Built-in (v8/c8) | Plugin |\n| **Mocking** | Built-in | Built-in (vi) | Plugin (sinon) |\n| **Parallel** | Workers | Threads | Limited |\n| **Config** | Low | Minimal | High |\n| **Community** | Largest | Growing fast | Established |\n\n### When to Choose Each\n\n**Choose Jest when:**\n- Existing React project with CRA or Next.js\n- Team already knows Jest\n- Need extensive plugin ecosystem\n- Snapshot testing is primary strategy\n\n**Choose Vitest when:**\n- New project, especially with Vite\n- Performance is priority\n- Native ESM/TypeScript needed\n- Want Jest-compatible API with modern DX\n\n**Choose Mocha when:**\n- Need maximum flexibility\n- Custom reporting requirements\n- Legacy project compatibility\n- Specific assertion library preference\n\n### Configuration Examples\n\n**Jest (jest.config.js):**\n```javascript\nmodule.exports = {\n  preset: 'ts-jest',\n  testEnvironment: 'jsdom',\n  setupFilesAfterEnv: ['<rootDir>/jest.setup.js'],\n  moduleNameMapper: {\n    '^@/(.*)$': '<rootDir>/src/$1',\n    '\\\\.(css|less|scss)$': 'identity-obj-proxy'\n  },\n  collectCoverageFrom: [\n    'src/**/*.{ts,tsx}',\n    '!src/**/*.d.ts'\n  ],\n  coverageThreshold: {\n    global: {\n      branches: 75,\n      functions: 80,\n      lines: 80\n    }\n  }\n};\n```\n\n**Vitest (vitest.config.ts):**\n```typescript\nimport { defineConfig } from 'vitest/config';\nimport react from '@vitejs/plugin-react';\n\nexport default defineConfig({\n  plugins: [react()],\n  test: {\n    environment: 'jsdom',\n    setupFiles: ['./vitest.setup.ts'],\n    globals: true,\n    coverage: {\n      provider: 'v8',\n      reporter: ['text', 'json', 'html'],\n      exclude: ['node_modules/', 'test/']\n    }\n  },\n  resolve: {\n    alias: {\n      '@': '/src'\n    }\n  }\n});\n```\n\n## E2E Testing Frameworks\n\n### Comparison Matrix\n\n| Feature | Playwright | Cypress | Selenium |\n|---------|------------|---------|----------|\n| **Browsers** | All major | Chrome, FF, Edge | All major |\n| **Speed** | Fast | Fast | Slow |\n| **Parallel** | Native | Paid feature | External |\n| **Mobile** | Emulation | Limited | Appium |\n| **Network Mock** | Built-in | Built-in | Manual |\n| **Auto-wait** | Excellent | Good | Manual |\n| **Debugging** | Inspector, trace | Time travel | Limited |\n| **Language** | JS/TS/Python/C#/.NET | JS/TS only | Many |\n| **CI/CD** | Excellent | Good | Complex |\n| **Learning Curve** | Low | Low | High |\n| **Open Source** | Yes | Yes (core) | Yes |\n\n### When to Choose Each\n\n**Choose Playwright when:**\n- Cross-browser testing is critical\n- Need WebKit/Safari testing\n- API testing alongside E2E\n- Multiple language support needed\n- Complex scenarios (multiple tabs, auth)\n\n**Choose Cypress when:**\n- Developer experience is priority\n- React/Vue/Angular component testing\n- Team new to E2E testing\n- Single browser is acceptable\n- Real-time debugging needed\n\n**Choose Selenium when:**\n- Legacy infrastructure exists\n- Need language flexibility\n- Complex enterprise requirements\n- Grid-based parallel execution\n\n### Configuration Examples\n\n**Playwright (playwright.config.ts):**\n```typescript\nimport { defineConfig, devices } from '@playwright/test';\n\nexport default defineConfig({\n  testDir: './e2e',\n  fullyParallel: true,\n  forbidOnly: !!process.env.CI,\n  retries: process.env.CI ? 2 : 0,\n  workers: process.env.CI ? 2 : undefined,\n  reporter: [\n    ['html'],\n    ['junit', { outputFile: 'results.xml' }]\n  ],\n  use: {\n    baseURL: 'http://localhost:3000',\n    trace: 'on-first-retry',\n    screenshot: 'only-on-failure'\n  },\n  projects: [\n    {\n      name: 'chromium',\n      use: { ...devices['Desktop Chrome'] }\n    },\n    {\n      name: 'firefox',\n      use: { ...devices['Desktop Firefox'] }\n    },\n    {\n      name: 'webkit',\n      use: { ...devices['Desktop Safari'] }\n    },\n    {\n      name: 'mobile-chrome',\n      use: { ...devices['Pixel 5'] }\n    }\n  ],\n  webServer: {\n    command: 'npm run dev',\n    url: 'http://localhost:3000',\n    reuseExistingServer: !process.env.CI\n  }\n});\n```\n\n**Cypress (cypress.config.ts):**\n```typescript\nimport { defineConfig } from 'cypress';\n\nexport default defineConfig({\n  e2e: {\n    baseUrl: 'http://localhost:3000',\n    viewportWidth: 1280,\n    viewportHeight: 720,\n    video: false,\n    screenshotOnRunFailure: true,\n    retries: {\n      runMode: 2,\n      openMode: 0\n    },\n    setupNodeEvents(on, config) {\n      // Task plugins\n      on('task', {\n        seedDatabase(data) {\n          return db.seed(data);\n        }\n      });\n    }\n  },\n  component: {\n    devServer: {\n      framework: 'react',\n      bundler: 'vite'\n    }\n  }\n});\n```\n\n## Python Testing Frameworks\n\n### Comparison\n\n| Feature | pytest | unittest | nose2 |\n|---------|--------|----------|-------|\n| **Syntax** | Simple | Verbose | Simple |\n| **Fixtures** | Powerful | setUp/tearDown | Basic |\n| **Plugins** | 800+ | Limited | Some |\n| **Parametrize** | Built-in | SubTest | Plugin |\n| **Assertions** | Plain assert | self.assertEqual | Plain |\n| **Discovery** | Automatic | Manual | Automatic |\n| **Markers** | Flexible | Limited | Limited |\n| **Parallel** | pytest-xdist | No | Limited |\n\n### pytest Configuration\n\n**pyproject.toml:**\n```toml\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\", \"*_test.py\"]\npython_functions = [\"test_*\"]\naddopts = [\n    \"-v\",\n    \"--strict-markers\",\n    \"--cov=src\",\n    \"--cov-report=term-missing\",\n    \"--cov-report=html\",\n    \"--cov-fail-under=80\"\n]\nmarkers = [\n    \"slow: marks tests as slow\",\n    \"integration: marks tests as integration tests\",\n    \"e2e: marks tests as end-to-end tests\"\n]\nfilterwarnings = [\n    \"error\",\n    \"ignore::DeprecationWarning\"\n]\n\n[tool.coverage.run]\nbranch = true\nsource = [\"src\"]\nomit = [\"tests/*\", \"**/__init__.py\"]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"if TYPE_CHECKING:\",\n    \"raise NotImplementedError\"\n]\n```\n\n**conftest.py:**\n```python\nimport pytest\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker\n\n@pytest.fixture(scope=\"session\")\ndef engine():\n    return create_engine(\"sqlite:///:memory:\")\n\n@pytest.fixture(scope=\"function\")\ndef db_session(engine):\n    Session = sessionmaker(bind=engine)\n    session = Session()\n    yield session\n    session.rollback()\n    session.close()\n\n@pytest.fixture\ndef client(app):\n    return app.test_client()\n\n@pytest.fixture\ndef auth_headers(client):\n    response = client.post('/login', json={\n        'username': 'test',\n        'password': 'test123'\n    })\n    token = response.json['token']\n    return {'Authorization': f'Bearer {token}'}\n```\n\n## Component Testing\n\n### React Testing Library vs Enzyme\n\n| Feature | React Testing Library | Enzyme |\n|---------|----------------------|--------|\n| **Philosophy** | User behavior | Implementation |\n| **Queries** | Accessibility-first | Component internals |\n| **Shallow** | Not supported | Supported |\n| **Maintained** | Active | Limited |\n| **React 18** | Full support | Partial |\n| **Learning** | Moderate | Easy |\n\n**React Testing Library (Recommended):**\n```javascript\nimport { render, screen, waitFor } from '@testing-library/react';\nimport userEvent from '@testing-library/user-event';\n\ntest('submits form with user data', async () => {\n  const user = userEvent.setup();\n  const onSubmit = jest.fn();\n\n  render(<LoginForm onSubmit={onSubmit} />);\n\n  await user.type(screen.getByLabelText(/email/i), 'test@example.com');\n  await user.type(screen.getByLabelText(/password/i), 'password123');\n  await user.click(screen.getByRole('button', { name: /sign in/i }));\n\n  await waitFor(() => {\n    expect(onSubmit).toHaveBeenCalledWith({\n      email: 'test@example.com',\n      password: 'password123'\n    });\n  });\n});\n```\n\n## API Testing\n\n### Framework Options\n\n| Tool | Language | Best For |\n|------|----------|----------|\n| Supertest | JS/TS | Express/Node |\n| Playwright Request | JS/TS | With E2E |\n| pytest + requests | Python | Flask/Django |\n| REST Assured | Java | Spring |\n| Postman/Newman | Any | CI/CD |\n\n**Supertest Example:**\n```javascript\nimport request from 'supertest';\nimport app from '../src/app';\n\ndescribe('Users API', () => {\n  it('GET /users returns list', async () => {\n    const response = await request(app)\n      .get('/api/users')\n      .set('Authorization', `Bearer ${token}`)\n      .expect('Content-Type', /json/)\n      .expect(200);\n\n    expect(response.body).toHaveLength(10);\n    expect(response.body[0]).toHaveProperty('id');\n  });\n});\n```\n\n## Decision Matrix\n\n### Quick Selection Guide\n\n```\nNeed to test...\n│\n├── Unit/Business Logic\n│   ├── JavaScript/TypeScript\n│   │   ├── Using Vite → Vitest\n│   │   ├── Existing Jest → Keep Jest\n│   │   └── New project → Vitest\n│   └── Python → pytest\n│\n├── React Components\n│   ├── User behavior → React Testing Library\n│   └── With stories → Storybook + Chromatic\n│\n├── API Endpoints\n│   ├── Node.js → Supertest\n│   ├── With E2E → Playwright API\n│   └── Python → pytest + requests\n│\n└── E2E/Browser\n    ├── Cross-browser critical → Playwright\n    ├── Developer experience → Cypress\n    └── Legacy/Enterprise → Selenium\n```\n"
        },
        {
          "name": "test-strategy.md",
          "type": "file",
          "path": "test-automation-expert/references/test-strategy.md",
          "size": 6634,
          "content": "# Test Strategy Framework\n\nA comprehensive guide to building effective test strategies for modern applications.\n\n## Test Strategy Document Template\n\n### 1. Scope Definition\n\n**In Scope:**\n- [ ] Unit tests for business logic\n- [ ] Integration tests for API contracts\n- [ ] E2E tests for critical user journeys\n- [ ] Component tests for UI elements\n- [ ] Accessibility testing\n- [ ] Visual regression testing\n\n**Out of Scope:**\n- [ ] Performance/load testing (separate strategy)\n- [ ] Security testing (separate strategy)\n- [ ] Manual exploratory testing\n\n### 2. Test Pyramid Implementation\n\n```\nTarget Distribution:\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\nE2E:         ████████░░░░░░░░░░░░░░░░░░░░░░  10%\nIntegration: ████████████████░░░░░░░░░░░░░░  20%\nUnit:        ████████████████████████████████ 70%\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n```\n\n### 3. Critical Path Identification\n\n**Tier 1 - Must Never Break (100% coverage):**\n- User authentication flow\n- Payment/checkout process\n- Core data mutations\n- Security-critical operations\n\n**Tier 2 - Important (90% coverage):**\n- User profile management\n- Search and filtering\n- Navigation flows\n- Form submissions\n\n**Tier 3 - Nice to Have (70% coverage):**\n- Edge cases in UI\n- Administrative features\n- Logging and analytics\n\n### 4. Test Data Strategy\n\n**Approaches:**\n1. **Factory Pattern** - Generate test data programmatically\n2. **Fixtures** - Static test data files\n3. **Seeding** - Database seeding scripts\n4. **Mocking** - API response mocks\n\n**Data Isolation:**\n```javascript\n// Each test gets fresh data\nbeforeEach(async () => {\n  await db.truncate(['users', 'orders']);\n  await seedTestData();\n});\n```\n\n### 5. Environment Strategy\n\n| Environment | Purpose | Data | Speed |\n|-------------|---------|------|-------|\n| Local | Development | Mocked/Seeded | Fast |\n| CI | Automation | Seeded | Medium |\n| Staging | Pre-prod validation | Sanitized prod | Slow |\n| Production | Smoke tests only | Real | N/A |\n\n### 6. Ownership Model\n\n| Test Type | Owner | Review |\n|-----------|-------|--------|\n| Unit tests | Feature developer | Code review |\n| Integration tests | Feature team | Tech lead |\n| E2E tests | QA/Platform team | QA lead |\n| Performance tests | Platform team | SRE |\n\n## Risk-Based Testing\n\n### Risk Assessment Matrix\n\n```\nImpact\n  ↑\n  │  ┌─────────┬─────────┐\nH │  │ MEDIUM  │  HIGH   │  ← Comprehensive testing\n  │  │ Priority│ Priority│\n  │  ├─────────┼─────────┤\nM │  │   LOW   │ MEDIUM  │  ← Standard testing\n  │  │ Priority│ Priority│\n  │  ├─────────┼─────────┤\nL │  │  MINIMAL│   LOW   │  ← Basic coverage\n  │  │ Priority│ Priority│\n  └──┴─────────┴─────────┴──→\n        Low      High    Probability\n```\n\n### Coverage by Risk Level\n\n| Risk | Min Coverage | Test Types |\n|------|-------------|------------|\n| High | 95% | Unit + Integration + E2E |\n| Medium | 80% | Unit + Integration |\n| Low | 60% | Unit |\n| Minimal | 40% | Unit (happy path) |\n\n## Testing Quadrants\n\n```\n        Business-Facing\n              ↑\n   Q2 │          │ Q3\nFunctional    │    Exploratory\n  Tests       │    Testing\n              │\n──────────────┼──────────────→ Manual\n              │\n   Q1 │          │ Q4\n   Unit       │   Performance\n   Tests      │   Security\n              │\n        Technology-Facing\n              ↓\n         Automated\n```\n\n**Q1 (Technology/Automated):** Unit tests, component tests\n**Q2 (Business/Automated):** Functional tests, API tests\n**Q3 (Business/Manual):** Exploratory testing, usability\n**Q4 (Technology/Tools):** Performance, security, load\n\n## Test Maintenance Strategy\n\n### Keeping Tests Healthy\n\n1. **Regular Review Cycles**\n   - Weekly: Flaky test triage\n   - Monthly: Coverage gaps analysis\n   - Quarterly: Strategy review\n\n2. **Test Debt Tracking**\n   ```\n   # In test file headers\n   // @tech-debt: Needs refactor when API v2 ships\n   // @flaky: Intermittent timeout - tracking in JIRA-123\n   // @skip-reason: Blocked by feature flag removal\n   ```\n\n3. **Deletion Criteria**\n   - Test for removed feature\n   - Duplicate coverage\n   - Permanently flaky with no fix path\n   - Testing implementation details\n\n### Metrics to Track\n\n| Metric | Target | Alert |\n|--------|--------|-------|\n| Test pass rate | &gt;99% | &lt;98% |\n| Flaky test rate | &lt;1% | &gt;2% |\n| Coverage trend | Stable/Up | -5% |\n| Test run time | &lt;10min | &gt;15min |\n| Time to fix failed test | &lt;4hrs | &gt;24hrs |\n\n## Test Documentation Standards\n\n### Test Naming Convention\n\n```javascript\n// Pattern: should_[expected]_when_[condition]\nit('should_return_user_when_id_exists', () => {});\nit('should_throw_error_when_id_invalid', () => {});\n\n// Or: describe behavior\ndescribe('UserService', () => {\n  describe('getUser', () => {\n    it('returns user for valid ID', () => {});\n    it('throws NotFoundError for unknown ID', () => {});\n  });\n});\n```\n\n### Test File Organization\n\n```\nsrc/\n├── components/\n│   ├── Button/\n│   │   ├── Button.tsx\n│   │   ├── Button.test.tsx      # Unit tests\n│   │   └── Button.stories.tsx   # Visual tests\n├── services/\n│   ├── user.ts\n│   └── user.test.ts\ntests/\n├── integration/\n│   ├── api/\n│   │   └── users.test.ts\n│   └── setup.ts\n├── e2e/\n│   ├── checkout.spec.ts\n│   └── auth.spec.ts\n└── fixtures/\n    └── users.json\n```\n\n## Continuous Improvement\n\n### Retrospective Questions\n\n1. What tests caught real bugs this sprint?\n2. What bugs escaped to production - why no test?\n3. Which tests are frequently skipped/ignored?\n4. What's the test run time trend?\n5. Are developers writing tests first (TDD) or after?\n\n### Improvement Actions\n\n| Issue | Action |\n|-------|--------|\n| Low coverage | Pair on test writing |\n| Slow tests | Parallelize, mock more |\n| Flaky tests | Dedicated fix sprints |\n| Missing E2E | Map critical paths |\n| Test debt | Budget 10% for maintenance |\n"
        }
      ]
    },
    {
      "name": "CHANGELOG.md",
      "type": "file",
      "path": "test-automation-expert/CHANGELOG.md",
      "size": 1332,
      "content": "# Changelog\n\nAll notable changes to the test-automation-expert skill will be documented in this file.\n\n## [1.0.0] - 2024-12-08\n\n### Added\n- Initial release of test-automation-expert skill\n- Comprehensive SKILL.md with test pyramid philosophy\n- Framework selection guidance (Jest, Vitest, Playwright, Cypress, pytest)\n- Unit testing patterns with mocking strategies\n- Integration testing patterns for APIs and components\n- E2E testing best practices with Playwright\n- Flaky test detection and prevention guide\n- Coverage optimization strategies\n- CI/CD integration examples (GitHub Actions)\n- Anti-patterns documentation\n- Reference files:\n  - `references/test-strategy.md` - Test strategy framework\n  - `references/framework-comparison.md` - Framework comparison matrix\n  - `references/coverage-patterns.md` - Coverage techniques\n  - `references/ci-integration.md` - CI/CD configurations\n\n### Technical Decisions\n- Chose Vitest as recommended modern default over Jest\n- Playwright recommended over Cypress for cross-browser needs\n- Test pyramid distribution: 70% unit, 20% integration, 10% E2E\n- Coverage thresholds: 80% lines, 75% branches, 90% functions\n\n### References\n- Based on testing best practices from Testing Library, Playwright, and Vitest documentation\n- Anti-patterns derived from common issues in production codebases\n"
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "test-automation-expert/SKILL.md",
      "size": 13987,
      "content": "---\nname: test-automation-expert\ndescription: Comprehensive test automation specialist covering unit, integration, and E2E testing strategies. Expert in Jest, Vitest, Playwright, Cypress, pytest, and modern testing frameworks. Guides test pyramid design, coverage optimization, flaky test detection, and CI/CD integration. Activate on 'test strategy', 'unit tests', 'integration tests', 'E2E testing', 'test coverage', 'flaky tests', 'mocking', 'test fixtures', 'TDD', 'BDD', 'test automation'. NOT for manual QA processes, load/performance testing (use performance-engineer), or security testing (use security-auditor).\nallowed-tools: Read,Write,Edit,Bash(npm test:*,npx jest:*,npx vitest:*,npx playwright:*,pytest:*),Grep,Glob\ncategory: Code Quality & Testing\ntags:\n  - testing\n  - jest\n  - playwright\n  - tdd\n  - coverage\npairs-with:\n  - skill: refactoring-surgeon\n    reason: Tests before refactoring\n  - skill: devops-automator\n    reason: CI/CD test integration\n---\n\n# Test Automation Expert\n\nComprehensive testing guidance from unit to E2E. Designs test strategies, implements automation, and optimizes coverage for sustainable quality.\n\n## When to Use\n\n**Use for:**\n- Designing test strategy for new projects\n- Setting up testing frameworks (Jest, Vitest, Playwright, Cypress, pytest)\n- Writing effective unit, integration, and E2E tests\n- Optimizing test coverage and eliminating gaps\n- Debugging flaky tests\n- CI/CD test pipeline configuration\n- Test-Driven Development (TDD) guidance\n- Mocking strategies and test fixtures\n\n**Do NOT use for:**\n- Manual QA test case writing - this is automation-focused\n- Load/performance testing - use performance-engineer skill\n- Security testing - use security-auditor skill\n- API contract testing only - use backend-architect for API design\n\n## Test Pyramid Philosophy\n\n```\n         /\\\n        /  \\      E2E Tests (10%)\n       /----\\     - Critical user journeys\n      /      \\    - Cross-browser validation\n     /--------\\\n    /          \\  Integration Tests (20%)\n   /            \\ - API contracts\n  /--------------\\- Component interactions\n /                \\\n/------------------\\ Unit Tests (70%)\n                    - Fast, isolated, deterministic\n                    - Business logic validation\n```\n\n### Distribution Guidelines\n\n| Test Type | Percentage | Execution Time | Purpose |\n|-----------|------------|----------------|---------|\n| Unit | 70% | &lt; 100ms each | Logic validation |\n| Integration | 20% | &lt; 1s each | Component contracts |\n| E2E | 10% | &lt; 30s each | Critical paths |\n\n## Framework Selection\n\n### JavaScript/TypeScript\n\n| Framework | Best For | Speed | Config Complexity |\n|-----------|----------|-------|-------------------|\n| **Vitest** | Vite projects, modern ESM | Fastest | Low |\n| **Jest** | React, established projects | Fast | Medium |\n| **Playwright** | E2E, cross-browser | N/A | Low |\n| **Cypress** | E2E, component testing | N/A | Medium |\n\n### Python\n\n| Framework | Best For | Speed | Features |\n|-----------|----------|-------|----------|\n| **pytest** | Everything | Fast | Fixtures, plugins |\n| **unittest** | Standard library | Medium | Built-in |\n| **hypothesis** | Property-based | Varies | Generative |\n\n### Decision Tree: Framework Selection\n\n```\nNew project?\n├── Yes → Using Vite?\n│   ├── Yes → Vitest\n│   └── No → Jest or Vitest (both work)\n└── No → What exists?\n    ├── Jest → Keep Jest (migration cost rarely worth it)\n    ├── Mocha → Consider migration to Vitest\n    └── Nothing → Vitest (modern default)\n\nNeed E2E?\n├── Cross-browser critical → Playwright\n├── Developer experience priority → Cypress\n└── Both → Playwright (more flexible)\n```\n\n## Unit Testing Patterns\n\n### Good Unit Test Anatomy\n\n```javascript\ndescribe('UserService', () => {\n  describe('validateEmail', () => {\n    // Arrange-Act-Assert pattern\n    it('should accept valid email formats', () => {\n      // Arrange\n      const validEmails = ['user@example.com', 'name+tag@domain.co'];\n\n      // Act & Assert\n      validEmails.forEach(email => {\n        expect(validateEmail(email)).toBe(true);\n      });\n    });\n\n    it('should reject invalid email formats', () => {\n      // Arrange\n      const invalidEmails = ['invalid', '@missing.com', 'no@tld'];\n\n      // Act & Assert\n      invalidEmails.forEach(email => {\n        expect(validateEmail(email)).toBe(false);\n      });\n    });\n\n    // Edge cases explicitly tested\n    it('should handle empty string', () => {\n      expect(validateEmail('')).toBe(false);\n    });\n\n    it('should handle null/undefined', () => {\n      expect(validateEmail(null)).toBe(false);\n      expect(validateEmail(undefined)).toBe(false);\n    });\n  });\n});\n```\n\n### Mocking Strategies\n\n```javascript\n// ✅ Good: Mock at boundaries\njest.mock('../services/api', () => ({\n  fetchUser: jest.fn()\n}));\n\n// ✅ Good: Explicit mock setup per test\nbeforeEach(() => {\n  fetchUser.mockReset();\n});\n\nit('handles user not found', async () => {\n  fetchUser.mockRejectedValue(new NotFoundError());\n  await expect(getUser(123)).rejects.toThrow('User not found');\n});\n\n// ❌ Bad: Mocking implementation details\njest.mock('../utils/internal-helper'); // Don't mock internals\n```\n\n### Test Isolation Checklist\n\n- [ ] Each test can run independently\n- [ ] No shared mutable state between tests\n- [ ] Database/API state reset between tests\n- [ ] No test order dependencies\n- [ ] Parallel execution safe\n\n## Integration Testing Patterns\n\n### API Integration Test\n\n```javascript\ndescribe('POST /api/users', () => {\n  let app;\n  let db;\n\n  beforeAll(async () => {\n    db = await createTestDatabase();\n    app = createApp({ db });\n  });\n\n  afterAll(async () => {\n    await db.close();\n  });\n\n  beforeEach(async () => {\n    await db.clear();\n  });\n\n  it('creates user with valid data', async () => {\n    const response = await request(app)\n      .post('/api/users')\n      .send({ name: 'Test', email: 'test@example.com' })\n      .expect(201);\n\n    expect(response.body).toMatchObject({\n      id: expect.any(String),\n      name: 'Test',\n      email: 'test@example.com'\n    });\n\n    // Verify side effects\n    const dbUser = await db.users.findById(response.body.id);\n    expect(dbUser).toBeDefined();\n  });\n\n  it('rejects duplicate email', async () => {\n    await db.users.create({ name: 'Existing', email: 'test@example.com' });\n\n    await request(app)\n      .post('/api/users')\n      .send({ name: 'New', email: 'test@example.com' })\n      .expect(409);\n  });\n});\n```\n\n### Component Integration (React)\n\n```javascript\nimport { render, screen, waitFor } from '@testing-library/react';\nimport userEvent from '@testing-library/user-event';\nimport { UserProfile } from './UserProfile';\nimport { UserProvider } from '../context/UserContext';\n\ndescribe('UserProfile integration', () => {\n  it('loads and displays user data', async () => {\n    render(\n      <UserProvider>\n        <UserProfile userId=\"123\" />\n      </UserProvider>\n    );\n\n    // Verify loading state\n    expect(screen.getByRole('progressbar')).toBeInTheDocument();\n\n    // Wait for data\n    await waitFor(() => {\n      expect(screen.getByText('John Doe')).toBeInTheDocument();\n    });\n\n    // Verify loaded state\n    expect(screen.queryByRole('progressbar')).not.toBeInTheDocument();\n  });\n});\n```\n\n## E2E Testing Patterns\n\n### Playwright Best Practices\n\n```javascript\nimport { test, expect } from '@playwright/test';\n\ntest.describe('Checkout Flow', () => {\n  test.beforeEach(async ({ page }) => {\n    // Seed test data via API\n    await page.request.post('/api/test/seed', {\n      data: { scenario: 'checkout-ready' }\n    });\n  });\n\n  test('complete purchase with credit card', async ({ page }) => {\n    await page.goto('/cart');\n\n    // Use accessible selectors\n    await page.getByRole('button', { name: 'Proceed to checkout' }).click();\n\n    // Fill payment form\n    await page.getByLabel('Card number').fill('4242424242424242');\n    await page.getByLabel('Expiry').fill('12/25');\n    await page.getByLabel('CVC').fill('123');\n\n    // Complete purchase\n    await page.getByRole('button', { name: 'Pay now' }).click();\n\n    // Verify success\n    await expect(page.getByRole('heading', { name: 'Order confirmed' })).toBeVisible();\n    await expect(page.getByText(/Order #\\d+/)).toBeVisible();\n  });\n\n  test('shows error for declined card', async ({ page }) => {\n    await page.goto('/checkout');\n\n    // Use test card that triggers decline\n    await page.getByLabel('Card number').fill('4000000000000002');\n    await page.getByLabel('Expiry').fill('12/25');\n    await page.getByLabel('CVC').fill('123');\n\n    await page.getByRole('button', { name: 'Pay now' }).click();\n\n    await expect(page.getByRole('alert')).toContainText('Card declined');\n  });\n});\n```\n\n### Flaky Test Detection & Prevention\n\n**Common Causes:**\n1. Race conditions in async operations\n2. Time-dependent tests\n3. Shared state between tests\n4. Network variability\n5. Animation/transition timing\n\n**Fixes:**\n\n```javascript\n// ❌ Bad: Fixed timeout\nawait page.waitForTimeout(2000);\n\n// ✅ Good: Wait for specific condition\nawait expect(page.getByText('Loaded')).toBeVisible();\n\n// ❌ Bad: Checking exact time\nexpect(new Date()).toEqual(specificDate);\n\n// ✅ Good: Mock time\njest.useFakeTimers();\njest.setSystemTime(new Date('2024-01-15'));\n\n// ❌ Bad: Depending on animation completion\nawait page.click('.button');\nexpect(await page.isVisible('.modal')).toBe(true);\n\n// ✅ Good: Wait for animation\nawait page.click('.button');\nawait expect(page.locator('.modal')).toBeVisible();\n```\n\n## Coverage Optimization\n\n### What to Measure\n\n| Metric | Target | Priority |\n|--------|--------|----------|\n| Line coverage | 80%+ | Medium |\n| Branch coverage | 75%+ | High |\n| Function coverage | 90%+ | Medium |\n| Critical path coverage | 100% | Critical |\n\n### Coverage Configuration\n\n```javascript\n// vitest.config.js\nexport default defineConfig({\n  test: {\n    coverage: {\n      provider: 'v8',\n      reporter: ['text', 'json', 'html'],\n      exclude: [\n        'node_modules/',\n        'test/',\n        '**/*.d.ts',\n        '**/*.config.*',\n        '**/index.ts', // barrel files\n      ],\n      thresholds: {\n        branches: 75,\n        functions: 80,\n        lines: 80,\n        statements: 80\n      }\n    }\n  }\n});\n```\n\n### Finding Coverage Gaps\n\n```bash\n# Generate detailed coverage report\nnpx vitest run --coverage\n\n# Find untested files\nnpx vitest run --coverage --reporter=json | jq '.coverageMap | to_entries | map(select(.value.s | values | any(. == 0))) | .[].key'\n```\n\n## CI/CD Integration\n\n### GitHub Actions\n\n```yaml\nname: Tests\non: [push, pull_request]\n\njobs:\n  unit-tests:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          cache: 'npm'\n      - run: npm ci\n      - run: npm test -- --coverage\n      - uses: codecov/codecov-action@v4\n\n  e2e-tests:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n      - run: npm ci\n      - run: npx playwright install --with-deps\n      - run: npm run test:e2e\n      - uses: actions/upload-artifact@v4\n        if: failure()\n        with:\n          name: playwright-report\n          path: playwright-report/\n```\n\n### Test Parallelization\n\n```javascript\n// vitest.config.js - parallel by default\nexport default defineConfig({\n  test: {\n    pool: 'threads',\n    poolOptions: {\n      threads: {\n        singleThread: false\n      }\n    }\n  }\n});\n\n// playwright.config.js\nexport default defineConfig({\n  workers: process.env.CI ? 2 : undefined,\n  fullyParallel: true\n});\n```\n\n## Anti-Patterns\n\n### Anti-Pattern: Testing Implementation Details\n\n**What it looks like:**\n```javascript\n// ❌ Testing internal state\nexpect(component.state.isLoading).toBe(true);\n\n// ❌ Testing private methods\nexpect(service._calculateHash()).toBe('abc123');\n```\n\n**Why wrong:** Couples tests to implementation, breaks on refactors\n\n**Instead:**\n```javascript\n// ✅ Test observable behavior\nexpect(screen.getByRole('progressbar')).toBeInTheDocument();\n\n// ✅ Test public interface\nexpect(service.getHash()).toBe('abc123');\n```\n\n### Anti-Pattern: Over-Mocking\n\n**What it looks like:**\n```javascript\n// ❌ Mocking everything\njest.mock('../utils/format');\njest.mock('../utils/validate');\njest.mock('../utils/transform');\n```\n\n**Why wrong:** Tests pass even when real code is broken\n\n**Instead:** Mock only at system boundaries (APIs, databases, external services)\n\n### Anti-Pattern: Flaky Acceptance\n\n**What it looks like:** \"That test is just flaky, skip it\"\n\n**Why wrong:** Flaky tests indicate real problems (race conditions, timing issues)\n\n**Instead:** Fix the flakiness or quarantine while fixing\n\n### Anti-Pattern: Coverage Theater\n\n**What it looks like:**\n```javascript\n// ❌ Testing for coverage, not behavior\nit('covers the function', () => {\n  myFunction();\n  // No assertions!\n});\n```\n\n**Why wrong:** 100% coverage with 0% confidence\n\n**Instead:** Every test should assert meaningful behavior\n\n## Quick Commands\n\n```bash\n# Run all tests\nnpm test\n\n# Run with coverage\nnpm test -- --coverage\n\n# Run specific file\nnpm test -- src/utils/format.test.ts\n\n# Run in watch mode\nnpm test -- --watch\n\n# Run E2E tests\nnpx playwright test\n\n# Run E2E with UI\nnpx playwright test --ui\n\n# Debug E2E test\nnpx playwright test --debug\n\n# Update snapshots\nnpm test -- -u\n```\n\n## Reference Files\n\n- `references/test-strategy.md` - Comprehensive test strategy framework\n- `references/framework-comparison.md` - Detailed framework comparison\n- `references/coverage-patterns.md` - Coverage optimization techniques\n- `references/ci-integration.md` - CI/CD pipeline configurations\n\n---\n\n**Covers**: Test strategy | Unit testing | Integration testing | E2E testing | Coverage | CI/CD | Flaky test debugging\n\n**Use with**: security-auditor (security tests) | performance-engineer (load tests) | code-reviewer (test quality)\n"
    }
  ]
}