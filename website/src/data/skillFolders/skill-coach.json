{
  "name": "skill-coach",
  "type": "folder",
  "path": "skill-coach",
  "children": [
    {
      "name": "examples",
      "type": "folder",
      "path": "skill-coach/examples",
      "children": [
        {
          "name": "good",
          "type": "folder",
          "path": "skill-coach/examples/good",
          "children": [
            {
              "name": "clip-aware-embeddings",
              "type": "folder",
              "path": "skill-coach/examples/good/clip-aware-embeddings",
              "children": [
                {
                  "name": "scripts",
                  "type": "folder",
                  "path": "skill-coach/examples/good/clip-aware-embeddings/scripts",
                  "children": [
                    {
                      "name": "validate_clip_usage.py",
                      "type": "file",
                      "path": "skill-coach/examples/good/clip-aware-embeddings/scripts/validate_clip_usage.py",
                      "size": 7094,
                      "content": "#!/usr/bin/env python3\n\"\"\"\nCLIP Usage Validator - Checks if CLIP is appropriate for a given query\n\nThis demonstrates domain-specific validation that encodes expert knowledge.\n\"\"\"\n\nimport sys\nimport re\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n\nclass TaskType(Enum):\n    SEMANTIC_SEARCH = \"semantic_search\"\n    COUNTING = \"counting\"\n    FINE_GRAINED = \"fine_grained\"\n    SPATIAL = \"spatial\"\n    COMPOSITIONAL = \"compositional\"\n    ZERO_SHOT = \"zero_shot\"\n\n\n@dataclass\nclass ValidationResult:\n    is_appropriate: bool\n    task_type: TaskType\n    confidence: float\n    reason: str\n    alternative: Optional[str] = None\n\n\nclass CLIPValidator:\n    \"\"\"Validates whether CLIP is appropriate for a given task.\"\"\"\n    \n    # Keywords that indicate specific task types\n    COUNTING_KEYWORDS = [\n        'how many', 'count', 'number of', 'total', 'quantity',\n        'several', 'few', 'multiple'\n    ]\n    \n    SPATIAL_KEYWORDS = [\n        'left', 'right', 'above', 'below', 'next to', 'beside',\n        'between', 'in front', 'behind', 'under', 'over', 'near'\n    ]\n    \n    FINE_GRAINED_DOMAINS = [\n        'celebrity', 'celebrities', 'actor', 'actress',\n        'car model', 'vehicle model', 'car make',\n        'flower species', 'bird species', 'dog breed',\n        'person', 'face', 'people'\n    ]\n    \n    COMPOSITIONAL_PATTERNS = [\n        r'(\\w+)\\s+(\\w+)\\s+and\\s+(\\w+)\\s+(\\w+)',  # \"red car and blue truck\"\n        r'both\\s+',\n        r'neither\\s+',\n        r'either\\s+',\n    ]\n    \n    GOOD_USE_CASES = [\n        'find images', 'search for', 'similar to', 'looks like',\n        'classify', 'categorize', 'what is this', 'identify',\n        'semantic', 'concept', 'theme'\n    ]\n    \n    def validate(self, query: str) -> ValidationResult:\n        \"\"\"\n        Validate if CLIP is appropriate for the query.\n        \n        Args:\n            query: Natural language query\n            \n        Returns:\n            ValidationResult with recommendation\n        \"\"\"\n        query_lower = query.lower()\n        \n        # Check for counting tasks\n        if any(kw in query_lower for kw in self.COUNTING_KEYWORDS):\n            return ValidationResult(\n                is_appropriate=False,\n                task_type=TaskType.COUNTING,\n                confidence=0.95,\n                reason=\"Query requires counting objects. CLIP cannot preserve spatial information needed for counting.\",\n                alternative=\"Use object detection models: DETR, Faster R-CNN, YOLO\"\n            )\n        \n        # Check for spatial reasoning\n        if any(kw in query_lower for kw in self.SPATIAL_KEYWORDS):\n            return ValidationResult(\n                is_appropriate=False,\n                task_type=TaskType.SPATIAL,\n                confidence=0.90,\n                reason=\"Query requires spatial understanding. CLIP's embeddings lose spatial topology.\",\n                alternative=\"Use spatial reasoning models: GQA, SWIG, Visual Genome models\"\n            )\n        \n        # Check for fine-grained classification\n        if any(domain in query_lower for domain in self.FINE_GRAINED_DOMAINS):\n            return ValidationResult(\n                is_appropriate=False,\n                task_type=TaskType.FINE_GRAINED,\n                confidence=0.85,\n                reason=\"Query requires fine-grained classification. CLIP trained on coarse categories.\",\n                alternative=\"Use specialized models: Fine-tuned ResNet/EfficientNet for the specific domain\"\n            )\n        \n        # Check for compositional reasoning\n        if any(re.search(pattern, query_lower) for pattern in self.COMPOSITIONAL_PATTERNS):\n            return ValidationResult(\n                is_appropriate=False,\n                task_type=TaskType.COMPOSITIONAL,\n                confidence=0.80,\n                reason=\"Query requires attribute binding. CLIP cannot bind attributes to specific objects.\",\n                alternative=\"Use compositional models: DCSMs (Dense Cosine Similarity Maps), PC-CLIP\"\n            )\n        \n        # Check if it's a good CLIP use case\n        if any(use_case in query_lower for use_case in self.GOOD_USE_CASES):\n            return ValidationResult(\n                is_appropriate=True,\n                task_type=TaskType.SEMANTIC_SEARCH,\n                confidence=0.90,\n                reason=\"Query is appropriate for CLIP: semantic search or broad categorization.\",\n                alternative=None\n            )\n        \n        # Default: probably okay but lower confidence\n        return ValidationResult(\n            is_appropriate=True,\n            task_type=TaskType.ZERO_SHOT,\n            confidence=0.60,\n            reason=\"Query appears suitable for CLIP, but verify results carefully.\",\n            alternative=\"If results are poor, consider task-specific models\"\n        )\n\n\ndef print_result(query: str, result: ValidationResult):\n    \"\"\"Pretty-print validation results.\"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(f\"CLIP USAGE VALIDATION\")\n    print(\"=\"*70)\n    print(f\"\\nQuery: {query}\")\n    print(f\"Task Type: {result.task_type.value}\")\n    print(f\"Confidence: {result.confidence:.0%}\")\n    print()\n    \n    if result.is_appropriate:\n        print(\"‚úÖ CLIP IS APPROPRIATE\")\n        print(f\"\\nReason: {result.reason}\")\n        if result.alternative:\n            print(f\"\\nüí° Note: {result.alternative}\")\n    else:\n        print(\"‚ùå CLIP IS NOT APPROPRIATE\")\n        print(f\"\\nReason: {result.reason}\")\n        print(f\"\\nüí° Use Instead: {result.alternative}\")\n    \n    print(\"\\n\" + \"=\"*70 + \"\\n\")\n\n\ndef run_examples():\n    \"\"\"Run validation on example queries.\"\"\"\n    examples = [\n        \"Find images of beaches at sunset\",\n        \"How many cars are in this image?\",\n        \"Identify which celebrity this is\",\n        \"Is the cat to the left or right of the dog?\",\n        \"Find images with a red car and a blue truck\",\n        \"Classify this image as indoor or outdoor\",\n    ]\n    \n    validator = CLIPValidator()\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"EXAMPLE VALIDATIONS\")\n    print(\"=\"*70)\n    \n    for query in examples:\n        result = validator.validate(query)\n        print(f\"\\n{query}\")\n        print(f\"  ‚Üí {'‚úÖ CLIP' if result.is_appropriate else '‚ùå Alternative'}: {result.task_type.value}\")\n        if not result.is_appropriate:\n            print(f\"  ‚Üí {result.alternative}\")\n    \n    print(\"\\n\" + \"=\"*70 + \"\\n\")\n\n\ndef main():\n    if len(sys.argv) < 2:\n        print(\"Usage:\")\n        print(\"  python validate_clip_usage.py 'your query here'\")\n        print(\"  python validate_clip_usage.py --examples\")\n        print(\"\\nExample:\")\n        print(\"  python validate_clip_usage.py 'Find images of mountains'\")\n        sys.exit(1)\n    \n    if sys.argv[1] == '--examples':\n        run_examples()\n        return\n    \n    query = ' '.join(sys.argv[1:])\n    validator = CLIPValidator()\n    result = validator.validate(query)\n    print_result(query, result)\n    \n    # Exit code: 0 if appropriate, 1 if not\n    sys.exit(0 if result.is_appropriate else 1)\n\n\nif __name__ == '__main__':\n    main()\n"
                    }
                  ]
                },
                {
                  "name": "SKILL_.md",
                  "type": "file",
                  "path": "skill-coach/examples/good/clip-aware-embeddings/SKILL_.md",
                  "size": 8947,
                  "content": "---\nname: clip-aware-embeddings\ndescription: Semantic image-text matching with CLIP and alternatives. Use for image search, zero-shot classification, similarity matching. NOT for counting objects, fine-grained classification (celebrities, car models), spatial reasoning, or compositional queries. Mention CLIP, embeddings, image similarity, or semantic search.\nallowed-tools: Read,Write,Bash(pip:install)\n---\n\n# CLIP-Aware Image Embeddings\n\nSmart image-text matching that knows when CLIP works and when to use alternatives.\n\n## Quick Decision Tree\n\n```\nYour task:\n‚îú‚îÄ Semantic search (\"find beach images\") ‚Üí CLIP ‚úì\n‚îú‚îÄ Zero-shot classification (broad categories) ‚Üí CLIP ‚úì\n‚îú‚îÄ Counting objects ‚Üí DETR, Faster R-CNN ‚úó\n‚îú‚îÄ Fine-grained ID (celebrities, car models) ‚Üí Specialized model ‚úó\n‚îú‚îÄ Spatial relations (\"cat left of dog\") ‚Üí GQA, SWIG ‚úó\n‚îî‚îÄ Compositional (\"red car AND blue truck\") ‚Üí DCSMs, PC-CLIP ‚úó\n```\n\n## When to Use This Skill\n\n‚úÖ **Use for**:\n- Semantic image search\n- Broad category classification\n- Image similarity matching\n- Zero-shot tasks on new categories\n\n‚ùå **Do NOT use for**:\n- Counting objects in images\n- Fine-grained classification\n- Spatial understanding\n- Attribute binding\n- Negation handling\n\n## Installation\n\n```bash\npip install transformers pillow torch sentence-transformers --break-system-packages\n```\n\n**Validation**: Run `python scripts/validate_setup.py`\n\n## Basic Usage\n\n### Image Search\n\n```python\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n\n# Embed images\nimages = [Image.open(f\"img{i}.jpg\") for i in range(10)]\ninputs = processor(images=images, return_tensors=\"pt\")\nimage_features = model.get_image_features(**inputs)\n\n# Search with text\ntext_inputs = processor(text=[\"a beach at sunset\"], return_tensors=\"pt\")\ntext_features = model.get_text_features(**text_inputs)\n\n# Compute similarity\nsimilarity = (image_features @ text_features.T).softmax(dim=0)\n```\n\n## Common Anti-Patterns\n\n### Anti-Pattern 1: \"CLIP for Everything\"\n\n**‚ùå Wrong**:\n```python\n# Using CLIP to count cars in an image\nprompt = \"How many cars are in this image?\"\n# CLIP cannot count - it will give nonsense results\n```\n\n**Why wrong**: CLIP's architecture collapses spatial information into a single vector. It literally cannot count.\n\n**‚úì Right**:\n```python\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n\nprocessor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\nmodel = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n\n# Detect objects\nresults = model(**processor(images=image, return_tensors=\"pt\"))\n# Filter for cars and count\ncar_detections = [d for d in results if d['label'] == 'car']\ncount = len(car_detections)\n```\n\n**How to detect**: If query contains \"how many\", \"count\", or numeric questions ‚Üí Use object detection\n\n---\n\n### Anti-Pattern 2: Fine-Grained Classification\n\n**‚ùå Wrong**:\n```python\n# Trying to identify specific celebrities with CLIP\nprompts = [\"Tom Hanks\", \"Brad Pitt\", \"Morgan Freeman\"]\n# CLIP will perform poorly - not trained for fine-grained face ID\n```\n\n**Why wrong**: CLIP trained on coarse categories. Fine-grained faces, car models, flower species require specialized models.\n\n**‚úì Right**:\n```python\n# Use a fine-tuned face recognition model\nfrom transformers import AutoFeatureExtractor, AutoModelForImageClassification\n\nmodel = AutoModelForImageClassification.from_pretrained(\n    \"microsoft/resnet-50\"  # Then fine-tune on celebrity dataset\n)\n# Or use dedicated face recognition: ArcFace, CosFace\n```\n\n**How to detect**: If query asks to distinguish between similar items in same category ‚Üí Use specialized model\n\n---\n\n### Anti-Pattern 3: Spatial Understanding\n\n**‚ùå Wrong**:\n```python\n# CLIP cannot understand spatial relationships\nprompts = [\n    \"cat to the left of dog\",\n    \"cat to the right of dog\"\n]\n# Will give nearly identical scores\n```\n\n**Why wrong**: CLIP embeddings lose spatial topology. \"Left\" and \"right\" are treated as bag-of-words.\n\n**‚úì Right**:\n```python\n# Use a spatial reasoning model\n# Examples: GQA models, Visual Genome models, SWIG\nfrom swig_model import SpatialRelationModel\n\nmodel = SpatialRelationModel()\nresult = model.predict_relation(image, \"cat\", \"dog\")\n# Returns: \"left\", \"right\", \"above\", \"below\", etc.\n```\n\n**How to detect**: If query contains directional words (left, right, above, under, next to) ‚Üí Use spatial model\n\n---\n\n### Anti-Pattern 4: Attribute Binding\n\n**‚ùå Wrong**:\n```python\nprompts = [\n    \"red car and blue truck\",\n    \"blue car and red truck\"\n]\n# CLIP often gives similar scores for both\n```\n\n**Why wrong**: CLIP cannot bind attributes to objects. It sees \"red, blue, car, truck\" as a bag of concepts.\n\n**‚úì Right - Use PC-CLIP or DCSMs**:\n```python\n# PC-CLIP: Fine-tuned for pairwise comparisons\nfrom pc_clip import PCCLIPModel\n\nmodel = PCCLIPModel.from_pretrained(\"pc-clip-vit-l\")\n# Or use DCSMs (Dense Cosine Similarity Maps)\n```\n\n**How to detect**: If query has multiple objects with different attributes ‚Üí Use compositional model\n\n---\n\n## Evolution Timeline\n\n### 2021: CLIP Released\n- Revolutionary: zero-shot, 400M image-text pairs\n- Widely adopted for everything\n- Limitations not yet understood\n\n### 2022-2023: Limitations Discovered\n- Cannot count objects\n- Poor at fine-grained classification\n- Fails spatial reasoning\n- Can't bind attributes\n\n### 2024: Alternatives Emerge\n- **DCSMs**: Preserve patch/token topology\n- **PC-CLIP**: Trained on pairwise comparisons\n- **SpLiCE**: Sparse interpretable embeddings\n\n### 2025: Current Best Practices\n- Use CLIP for what it's good at\n- Task-specific models for limitations\n- Compositional models for complex queries\n\n**LLM Mistake**: LLMs trained on 2021-2023 data will suggest CLIP for everything because limitations weren't widely known. This skill corrects that.\n\n---\n\n## Validation Script\n\nBefore using CLIP, check if it's appropriate:\n\n```bash\npython scripts/validate_clip_usage.py \\\n    --query \"your query here\" \\\n    --check-all\n```\n\nReturns:\n- ‚úÖ CLIP is appropriate\n- ‚ùå Use alternative (with suggestion)\n\n## Task-Specific Guidance\n\n### Image Search (CLIP ‚úì)\n```python\n# Good use of CLIP\nqueries = [\"beach\", \"mountain\", \"city skyline\"]\n# Works well for broad semantic concepts\n```\n\n### Zero-Shot Classification (CLIP ‚úì)\n```python\n# Good: Broad categories\ncategories = [\"indoor\", \"outdoor\", \"nature\", \"urban\"]\n# CLIP excels at this\n```\n\n### Object Counting (CLIP ‚úó)\n```python\n# Use object detection instead\nfrom transformers import DetrImageProcessor, DetrForObjectDetection\n# See /references/object_detection.md\n```\n\n### Fine-Grained Classification (CLIP ‚úó)\n```python\n# Use specialized models\n# See /references/fine_grained_models.md\n```\n\n### Spatial Reasoning (CLIP ‚úó)\n```python\n# Use spatial relation models\n# See /references/spatial_models.md\n```\n\n---\n\n## Troubleshooting\n\n### Issue: CLIP gives unexpected results\n\n**Check**:\n1. Is this a counting task? ‚Üí Use object detection\n2. Fine-grained classification? ‚Üí Use specialized model\n3. Spatial query? ‚Üí Use spatial model\n4. Multiple objects with attributes? ‚Üí Use compositional model\n\n**Validation**:\n```bash\npython scripts/diagnose_clip_issue.py --image path/to/image --query \"your query\"\n```\n\n### Issue: Low similarity scores\n\n**Possible causes**:\n1. Query too specific (CLIP works better with broad concepts)\n2. Fine-grained task (not CLIP's strength)\n3. Need to adjust threshold\n\n**Solution**: Try broader query or use alternative model\n\n---\n\n## Model Selection Guide\n\n| Model | Best For | Avoid For |\n|-------|----------|-----------|\n| CLIP ViT-L/14 | Semantic search, broad categories | Counting, fine-grained, spatial |\n| DETR | Object detection, counting | Semantic similarity |\n| DINOv2 | Fine-grained features | Text-image matching |\n| PC-CLIP | Attribute binding, comparisons | General embedding |\n| DCSMs | Compositional reasoning | Simple similarity |\n\n## Performance Notes\n\n**CLIP models**:\n- ViT-B/32: Fast, lower quality\n- ViT-L/14: Balanced (recommended)\n- ViT-g-14: Highest quality, slower\n\n**Inference time** (single image, CPU):\n- ViT-B/32: ~100ms\n- ViT-L/14: ~300ms\n- ViT-g-14: ~1000ms\n\n## Further Reading\n\n- `/references/clip_limitations.md` - Detailed analysis of CLIP's failures\n- `/references/alternatives.md` - When to use what model\n- `/references/compositional_reasoning.md` - DCSMs and PC-CLIP deep dive\n- `/scripts/validate_clip_usage.py` - Pre-flight validation tool\n- `/scripts/diagnose_clip_issue.py` - Debug unexpected results\n\n## Changelog\n\n### v1.2.0 (2025-03-15)\n- Added DCSMs and PC-CLIP alternatives\n- Updated for 2025 best practices\n- Improved validation scripts\n\n### v1.1.0 (2024-06-10)\n- Added anti-pattern detection\n- Expanded troubleshooting\n\n### v1.0.0 (2024-01-15)\n- Initial release\n"
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "name": "references",
      "type": "folder",
      "path": "skill-coach/references",
      "children": [
        {
          "name": "antipatterns.md",
          "type": "file",
          "path": "skill-coach/references/antipatterns.md",
          "size": 11685,
          "content": "# Skill Anti-Patterns: The Shibboleths\n\nThis document catalogs **domain-specific knowledge that separates novices from experts** - the things LLMs get wrong because their training data includes outdated patterns, oversimplified tutorials, or cargo-culted code.\n\n## Table of Contents\n\n1. [ML/AI Model Selection](#mlai-model-selection)\n2. [Framework Evolution](#framework-evolution)\n3. [Tool Architecture](#tool-architecture)\n4. [Skill Design](#skill-design)\n\n---\n\n## ML/AI Model Selection\n\n### Anti-Pattern: CLIP for Everything\n\n**Novice thinking**: \"CLIP is pre-trained on 400M image-text pairs and does zero-shot classification. Use it for all image-text tasks!\"\n\n**Reality**: CLIP has **fundamental geometric limitations**. Research from 2023-2025 proves it cannot simultaneously handle:\n\n1. Basic descriptions\n2. Attribute binding (\"red car AND blue truck\" vs \"blue car AND red truck\")\n3. Spatial relationships (\"cat left of dog\" vs \"dog left of cat\")\n4. Negation (\"not a cat\")\n\n**What CLIP fails at**:\n- ‚ùå Counting objects in images\n- ‚ùå Fine-grained classification (celebrity ID, car models, flower species)\n- ‚ùå Compositional reasoning\n- ‚ùå Spatial understanding\n- ‚ùå Handwritten text (MNIST-style)\n\n**When to use alternatives**:\n\n| Task | Use Instead | Why |\n|------|-------------|-----|\n| Counting objects | DETR, Faster R-CNN | Object detection models built for counting |\n| Fine-grained classification | EfficientNet + task head | Transfer learning on specific domain |\n| Compositional reasoning | DCSMs, PC-CLIP | Preserve patch/token topology |\n| Spatial relationships | GQA models, SWIG | Built for spatial understanding |\n| Attribute binding | PC-CLIP (pairwise) | Trained on comparative data |\n\n**Timeline**:\n- 2021: Original CLIP released\n- 2022-2023: Limitations discovered in research\n- 2024: DCSMs (Dense Cosine Similarity Maps) paper\n- 2024: PC-CLIP (Pairwise Comparison CLIP)\n- 2025: SpLiCE (Sparse Linear Concept Embeddings)\n\n**LLM mistake**: LLMs trained on 2021-2023 data will suggest CLIP for everything because limitations weren't widely known yet.\n\n---\n\n### Anti-Pattern: Single Embedding Model\n\n**Novice thinking**: \"Pick one embedding model and use it everywhere\"\n\n**Expert knowledge**: Different tasks need different models:\n\n**Text embeddings**:\n- Semantic search: `text-embedding-3-large`, `voyage-2`\n- Code search: `voyage-code-2`, `text-embedding-ada-002`\n- Multi-lingual: `multilingual-e5-large`\n- Long documents: `jina-embeddings-v2` (8k tokens)\n\n**Image embeddings**:\n- General: CLIP ViT-L/14\n- Fine-grained: DINOv2\n- Medical: BiomedCLIP\n- Faces: ArcFace, CosFace\n\n**Multi-modal**:\n- Image-text: CLIP, BLIP-2\n- Video: X-CLIP, VideoCLIP\n- 3D: ULIP, PointCLIP\n\n**Why this matters**: Embedding quality directly impacts retrieval accuracy. Using the wrong model can drop accuracy by 20-40%.\n\n---\n\n### Anti-Pattern: Ignoring Model Versioning\n\n**Problem**: \"We're using `text-embedding-ada-002`\" (doesn't specify when)\n\n**Why wrong**: Models evolve:\n- `text-embedding-ada-002` (Dec 2022) vs `text-embedding-3-small` (Jan 2024)\n- CLIP ViT-B/32 vs ViT-L/14 vs ViT-g-14\n- Different training data, different capabilities\n\n**Best practice**: Pin versions, document when you adopted them:\n```python\n# embeddings.py\nMODEL = \"text-embedding-3-large\"  # Adopted: 2024-03-15\nMODEL_DIMENSIONS = 3072\nTRAINING_CUTOFF = \"2023-09\"  # Approximate\n```\n\n---\n\n## Framework Evolution\n\n### Anti-Pattern: Pages Router in App Router Projects\n\n**Context**: Next.js 13 (Oct 2022) introduced App Router, fundamentally changing architecture.\n\n**Outdated pattern** (Pages Router):\n```javascript\n// pages/api/users.js\nexport default function handler(req, res) {\n  res.json({ users: [] })\n}\n\n// pages/users.js\nexport async function getServerSideProps() {\n  return { props: { users: [] } }\n}\n```\n\n**Current pattern** (App Router):\n```javascript\n// app/api/users/route.js\nexport async function GET() {\n  return Response.json({ users: [] })\n}\n\n// app/users/page.js\nasync function UsersPage() {\n  const users = await fetchUsers()  // Server Component\n  return <UserList users={users} />\n}\n```\n\n**Why it matters**: Pages Router patterns don't work in App Router and vice versa.\n\n**LLM mistake**: Training data from 2020-2023 overwhelmingly shows Pages Router. LLMs will default to old patterns unless specifically prompted.\n\n**Timeline**:\n- 2016-2022: Pages Router only\n- Oct 2022: App Router introduced (beta)\n- May 2023: App Router stable\n- 2024+: App Router is default\n\n---\n\n### Anti-Pattern: Redux for Everything\n\n**Novice thinking**: \"Global state needs Redux\"\n\n**Timeline**:\n- 2015-2020: Redux dominated\n- 2019: Context API improved in React 16.3\n- 2020: Zustand, Jotai emerged\n- 2023: React Server Components changed the game\n\n**Current wisdom**:\n- **Local UI state**: `useState`, `useReducer`\n- **Derived state**: `useMemo`, selectors\n- **Global state (simple)**: Context API\n- **Global state (complex)**: Zustand, Jotai\n- **Server state**: React Query, SWR\n- **URL state**: Next.js searchParams\n- **Redux**: Only if you need time-travel debugging or complex middleware\n\n**Why Redux fell out of favor**:\n- Boilerplate heavy\n- Server Components make much state \"server-native\"\n- Simpler alternatives emerged\n\n**LLM mistake**: LLMs will suggest Redux by default because 80% of training data predates alternatives.\n\n---\n\n### Anti-Pattern: Class Components\n\n**Timeline**:\n- 2013-2018: Class components only\n- Feb 2019: Hooks introduced (React 16.8)\n- 2020+: Functional components are standard\n\n**Outdated**:\n```javascript\nclass UserProfile extends React.Component {\n  state = { user: null }\n  \n  componentDidMount() {\n    fetchUser().then(user => this.setState({ user }))\n  }\n  \n  render() {\n    return <div>{this.state.user?.name}</div>\n  }\n}\n```\n\n**Current**:\n```javascript\nfunction UserProfile() {\n  const [user, setUser] = useState(null)\n  \n  useEffect(() => {\n    fetchUser().then(setUser)\n  }, [])\n  \n  return <div>{user?.name}</div>\n}\n```\n\n**When class components are still valid**:\n- Error boundaries (no hook equivalent yet)\n- Legacy codebases\n\n**LLM mistake**: Will generate class components for complex state management\n\n---\n\n## Tool Architecture\n\n### Anti-Pattern: MCP for Everything\n\n**Novice thinking**: \"MCP is the new standard, make everything an MCP!\"\n\n**Expert reality**: MCPs have overhead. Use them strategically.\n\n**Use MCP when**:\n- ‚úÖ External API with authentication\n- ‚úÖ Stateful connections (WebSocket, database)\n- ‚úÖ Real-time data streams\n- ‚úÖ Security boundaries (credentials, OAuth)\n\n**Use Scripts when**:\n- ‚úÖ Local file operations\n- ‚úÖ Batch transformations\n- ‚úÖ Stateless computations\n- ‚úÖ CLI wrappers\n\n**Example - Wrong**:\n```python\n# mcp_server_for_json_parsing.py - OVERKILL!\n@mcp.tool()\ndef parse_json(file_path: str):\n    with open(file_path) as f:\n        return json.load(f)\n```\n\n**Example - Right**:\n```python\n# scripts/parse_json.py - Simple script!\nimport json\nimport sys\n\nwith open(sys.argv[1]) as f:\n    data = json.load(f)\n    print(json.dumps(data, indent=2))\n```\n\n**Philosophy**: \"MCP's job isn't to abstract reality for the agent; its job is to manage the auth, networking, and security boundaries and then get out of the way.\"\n\n---\n\n### Anti-Pattern: Premature Abstraction\n\n**Problem**: Building a complex MCP before understanding the use case\n\n**Better approach**: Start with scripts, graduate to MCP when you need:\n1. Auth/security boundaries\n2. Multiple tools in same domain\n3. State management\n4. Error handling standardization\n\n**Evolution path**:\n```\nScript ‚Üí Multiple Scripts ‚Üí Helper Library ‚Üí MCP Server\n```\n\nOnly promote to MCP when complexity justifies it.\n\n---\n\n## Skill Design\n\n### Anti-Pattern: Skill as Documentation Dump\n\n**Bad**:\n```markdown\n---\nname: react-guide\ndescription: Everything about React\n---\n\n# React Guide\n\nReact is a JavaScript library for building user interfaces...\n[50 pages of tutorial content]\n```\n\n**Why wrong**: Not progressive disclosure, not actionable, not targeted.\n\n**Good**:\n```markdown\n---\nname: react-server-components\ndescription: Use React Server Components correctly. Use when working with Next.js App Router, async components, or server-side data fetching.\n---\n\n# React Server Components\n\n## Quick Decision Tree\n\nIs your component:\n- Fetching data? ‚Üí Server Component\n- Using hooks/events? ‚Üí Client Component\n- Both? ‚Üí Server Component wrapper + Client Component child\n\n## Common Anti-Pattern: Everything is 'use client'\n\n‚ùå **Wrong**:\n```jsx\n'use client'\nasync function Page() {  // This doesn't work!\n  const data = await fetch(...)\n  return <div>{data}</div>\n}\n```\n\n‚úÖ **Right**:\n```jsx\n// Server Component (default)\nasync function Page() {\n  const data = await fetchData()\n  return <ClientComponent data={data} />\n}\n\n// client-component.jsx\n'use client'\nfunction ClientComponent({ data }) {\n  const [count, setCount] = useState(0)\n  return <div onClick={() => setCount(count + 1)}>{data}</div>\n}\n```\n\n## When This Pattern Changed\n\n- Pre-Next.js 13: All components are client-side\n- Next.js 13+: Server Components by default\n- LLM confusion: Will add 'use client' everywhere because older patterns\n\nSee /references/server-components-deep-dive.md for more.\n```\n\n---\n\n### Anti-Pattern: Missing \"When NOT to Use\"\n\n**Problem**: Skills activate on false positives\n\n**Example - Without negatives**:\n```yaml\ndescription: Processes images using computer vision techniques\n```\nActivates for: image resizing, image generation, image editing, OCR, face detection, etc.\n\n**Example - With negatives**:\n```yaml\ndescription: Semantic image search using CLIP embeddings. Use for finding similar images, zero-shot classification. NOT for image generation, editing, or OCR. NOT for counting objects or fine-grained classification.\n```\n\n**Pattern**: Always include \"NOT for X, Y, Z\" to prevent false activation.\n\n---\n\n### Anti-Pattern: No Validation Script\n\n**Problem**: Skill gives instructions but no way to check correctness\n\n**Better**: Include validation\n\n```python\n# scripts/validate.py\ndef validate_setup():\n    \"\"\"Check if environment is configured correctly.\"\"\"\n    checks = {\n        \"Node version\": check_node_version(),\n        \"Dependencies\": check_dependencies(),\n        \"API keys\": check_api_keys(),\n    }\n    \n    for name, passed in checks.items():\n        print(f\"{'‚úÖ' if passed else '‚ùå'} {name}\")\n    \n    return all(checks.values())\n```\n\n---\n\n### Anti-Pattern: Overly Permissive Tools\n\n**Bad**:\n```yaml\nallowed-tools: Bash\n```\n\n**Why**: Can execute ANY bash command\n\n**Better**:\n```yaml\nallowed-tools: Bash(git:*,npm:run,npm:install),Read,Write\n```\n\n**Principle**: Least privilege - only grant what's needed\n\n---\n\n## Temporal Knowledge Patterns\n\nWhen documenting anti-patterns, always include:\n\n1. **Timeline**: When was this practice common?\n2. **Why deprecated**: What replaced it and why?\n3. **LLM confusion**: Why will LLMs suggest the old pattern?\n4. **Migration path**: How to update from old to new?\n\n**Template**:\n```markdown\n### Anti-Pattern: [Pattern Name]\n\n**Used**: [Date range]\n**Replaced by**: [New approach]\n**Why deprecated**: [Reason]\n\n**Old way**:\n[code example]\n\n**New way**:\n[code example]\n\n**LLM mistake**: [Why LLM suggests old pattern]\n**How to detect**: [Validation rule]\n```\n\n---\n\n## Contributing\n\nWhen you discover a new anti-pattern:\n\n1. Document what looks right but is wrong\n2. Explain the fundamental reason it's wrong\n3. Show the correct approach\n4. Include temporal context (when did this change?)\n5. Note why LLMs make this mistake\n6. Add detection/validation if possible\n\n**Remember**: The goal is to encode the knowledge that separates \"it compiles\" from \"it's correct\" - the shibboleths that reveal expertise.\n"
        },
        {
          "name": "mcp_vs_scripts.md",
          "type": "file",
          "path": "skill-coach/references/mcp_vs_scripts.md",
          "size": 22397,
          "content": "# Skills vs Agents vs MCPs vs Scripts: An Architectural Decision Guide\n\n## TL;DR\n\n**Use Skills** for: Domain expertise, anti-patterns, decision trees (no runtime state)\n**Use Agents** for: Multi-step workflows needing tool orchestration and autonomy\n**Use MCPs** for: External APIs, auth boundaries, stateful connections\n**Use Scripts** for: Local, stateless operations with no auth\n\n## The Philosophy\n\n> \"MCP's job isn't to abstract reality for the agent; it's to manage the auth, networking, and security boundaries and then get out of the way.\"\n>\n> ‚Äî Shrivu Shankar, \"How I Use Every Claude Code Feature\"\n\nEach tool serves a distinct purpose:\n\n- **Skills** encode domain expertise and decision trees without runtime state\n- **Agents** orchestrate multi-step workflows with tool autonomy\n- **MCPs** manage auth boundaries and external service connections\n- **Scripts** handle local, stateless operations\n\nNone is inherently \"better\" - they solve different problems at different layers.\n\n## Decision Matrix\n\n```\n                           ‚îÇ Expertise ‚îÇ Multi-step ‚îÇ Runtime ‚îÇ Local ‚îÇ Remote ‚îÇ Auth ‚îÇ Decision\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nCLIP anti-patterns         ‚îÇ ‚úì         ‚îÇ            ‚îÇ         ‚îÇ       ‚îÇ        ‚îÇ      ‚îÇ Skill\nCode review workflow       ‚îÇ           ‚îÇ ‚úì          ‚îÇ ‚úì       ‚îÇ       ‚îÇ        ‚îÇ      ‚îÇ Agent\nJSON parsing               ‚îÇ           ‚îÇ            ‚îÇ         ‚îÇ ‚úì     ‚îÇ        ‚îÇ      ‚îÇ Script\nAWS S3 operations          ‚îÇ           ‚îÇ            ‚îÇ         ‚îÇ       ‚îÇ ‚úì      ‚îÇ ‚úì    ‚îÇ MCP\nDatabase queries           ‚îÇ           ‚îÇ            ‚îÇ ‚úì       ‚îÇ       ‚îÇ ‚úì      ‚îÇ ‚úì    ‚îÇ MCP\nPR creation workflow       ‚îÇ           ‚îÇ ‚úì          ‚îÇ ‚úì       ‚îÇ       ‚îÇ        ‚îÇ      ‚îÇ Agent\nGit operations             ‚îÇ           ‚îÇ            ‚îÇ         ‚îÇ ‚úì     ‚îÇ        ‚îÇ      ‚îÇ Script\nJira API                   ‚îÇ           ‚îÇ            ‚îÇ         ‚îÇ       ‚îÇ ‚úì      ‚îÇ ‚úì    ‚îÇ MCP\nFramework evolution guide  ‚îÇ ‚úì         ‚îÇ            ‚îÇ         ‚îÇ       ‚îÇ        ‚îÇ      ‚îÇ Skill\nImage resizing             ‚îÇ           ‚îÇ            ‚îÇ         ‚îÇ ‚úì     ‚îÇ        ‚îÇ      ‚îÇ Script\nTesting pipeline           ‚îÇ           ‚îÇ ‚úì          ‚îÇ ‚úì       ‚îÇ       ‚îÇ        ‚îÇ      ‚îÇ Agent\nWebSocket client           ‚îÇ           ‚îÇ            ‚îÇ ‚úì       ‚îÇ       ‚îÇ ‚úì      ‚îÇ ‚úì    ‚îÇ MCP\nPDF generation             ‚îÇ           ‚îÇ            ‚îÇ         ‚îÇ ‚úì     ‚îÇ        ‚îÇ      ‚îÇ Script\nGitHub API                 ‚îÇ           ‚îÇ            ‚îÇ         ‚îÇ       ‚îÇ ‚úì      ‚îÇ ‚úì    ‚îÇ MCP\nFile organization          ‚îÇ           ‚îÇ            ‚îÇ         ‚îÇ ‚úì     ‚îÇ        ‚îÇ      ‚îÇ Script\n```\n\n**Legend:**\n- **Expertise**: Domain knowledge, anti-patterns, decision trees\n- **Multi-step**: Orchestrates multiple operations autonomously\n- **Runtime**: Maintains state across operations\n- **Local**: File system operations\n- **Remote**: External service calls\n- **Auth**: Requires authentication/authorization\n\n## Use Skills When...\n\n### ‚úÖ Domain Expertise Needed\n\n```yaml\n# .claude/skills/clip-aware-embeddings/SKILL.md\n---\nname: clip-aware-embeddings\ndescription: CLIP semantic search expertise. Use for image-text matching, zero-shot classification. NOT for counting, fine-grained classification, spatial reasoning.\n---\n\n## When NOT to Use CLIP\n\n### Anti-Pattern: Using CLIP to Count Objects\n**Why wrong**: CLIP's architecture cannot preserve spatial information\n**What to do**: Use DETR or Faster R-CNN for object detection\n**How to detect**: If query contains \"how many\" or \"count\"\n```\n\n**Why Skill**:\n- No runtime state needed\n- Encodes expert knowledge (shibboleths)\n- Prevents common mistakes via anti-patterns\n- Available across all conversations\n\n### ‚úÖ Framework Evolution Knowledge\n\n```yaml\n# .claude/skills/react-performance-expert/SKILL.md\n---\nname: react-performance-expert\ndescription: React performance optimization expertise. Pre-2024 patterns vs modern best practices. NOT for Vue/Angular.\n---\n\n## Evolution Timeline\n- Pre-2019: Class components + shouldComponentUpdate\n- 2019-2023: Hooks + React.memo\n- 2024+: React Compiler (automatic memoization)\n\n## Watch For\nLLMs may suggest manual useMemo/useCallback when React Compiler handles it automatically.\n```\n\n**Why Skill**:\n- Captures temporal knowledge\n- Warns about deprecated patterns\n- No execution needed, just guidance\n\n### ‚úÖ Architectural Decision Trees\n\n```yaml\n# .claude/skills/state-management-advisor/SKILL.md\n---\nname: state-management-advisor\ndescription: State management decision guidance. Use when choosing Redux vs Zustand vs Context. NOT for implementation.\n---\n\n## Decision Tree\n- Simple boolean/string state shared by 2-3 components ‚Üí Context\n- Complex state with actions (todo list, shopping cart) ‚Üí Zustand\n- Time-travel debugging required ‚Üí Redux Toolkit\n- NEVER: Redux for simple state\n```\n\n**Why Skill**:\n- Decision logic, not code templates\n- Prevents overengineering\n- No tools needed, just expertise\n\n## Use Agents When...\n\n### ‚úÖ Multi-Step Workflows with Tool Orchestration\n\n```python\n# Agents orchestrate multiple tools autonomously\n# Example: Code Review Agent\n\nfrom anthropic import Agent\n\nreview_agent = Agent(\n    name=\"code-reviewer\",\n    instructions=\"\"\"\n    1. Read modified files\n    2. Run linter and tests\n    3. Check for security issues\n    4. Generate review comments\n    5. Create summary report\n    \"\"\",\n    tools=[\"Read\", \"Bash\", \"Grep\", \"Write\"]\n)\n\n# Agent autonomously:\n# - Decides which files to read\n# - Runs appropriate tests\n# - Generates contextual feedback\n```\n\n**Why Agent**:\n- Multiple steps requiring decisions\n- Needs tool access (Read, Bash, etc.)\n- Maintains context across operations\n- Autonomy in execution order\n\n### ‚úÖ Task Decomposition and Parallel Execution\n\n```python\n# Example: Testing Pipeline Agent\n\ntesting_agent = Agent(\n    name=\"test-runner\",\n    instructions=\"\"\"\n    1. Identify all test files\n    2. Run unit tests in parallel\n    3. Run integration tests\n    4. Generate coverage report\n    5. Fail fast on critical errors\n    \"\"\",\n    tools=[\"Bash\", \"Read\", \"Write\"]\n)\n\n# Agent manages:\n# - Parallel test execution\n# - State aggregation (pass/fail counts)\n# - Conditional logic (fail fast)\n```\n\n**Why Agent**:\n- Orchestrates multiple bash commands\n- Maintains state (test results)\n- Makes runtime decisions (fail fast)\n\n### ‚úÖ Complex Debugging Workflows\n\n```python\n# Example: Bug Investigation Agent\n\ndebug_agent = Agent(\n    name=\"debugger\",\n    instructions=\"\"\"\n    1. Search codebase for error patterns\n    2. Read relevant files\n    3. Identify root cause\n    4. Propose fixes\n    5. Test proposed solutions\n    \"\"\",\n    tools=[\"Grep\", \"Read\", \"Edit\", \"Bash\"]\n)\n\n# Agent autonomously:\n# - Searches strategically\n# - Follows leads based on findings\n# - Iterates on hypotheses\n```\n\n**Why Agent**:\n- Non-linear investigation path\n- Requires multiple tool types\n- Runtime decision-making\n- Iterative refinement\n\n### ‚ùå When NOT to Use Agents\n\n**Don't use Agent for**:\n- Single operations (just use tool directly)\n- Pure expertise (use Skill instead)\n- External API calls (use MCP)\n- Simple scripts (use Script)\n\n**Anti-Pattern: Agent for Static Knowledge**\n```python\n# BAD: Agent that just returns information\nAgent(\n    name=\"python-docs\",\n    instructions=\"Answer Python questions\",\n    tools=[]\n)\n# BETTER: Use a Skill with /references/ to documentation\n```\n\n## Use Scripts When...\n\n### ‚úÖ Local File Operations\n\n```python\n# scripts/organize_photos.py\nimport os\nimport shutil\nfrom datetime import datetime\n\ndef organize_by_date(source_dir):\n    for file in os.listdir(source_dir):\n        if file.lower().endswith(('.jpg', '.png')):\n            creation_time = os.path.getctime(os.path.join(source_dir, file))\n            date = datetime.fromtimestamp(creation_time).strftime('%Y-%m')\n            os.makedirs(f\"{source_dir}/{date}\", exist_ok=True)\n            shutil.move(f\"{source_dir}/{file}\", f\"{source_dir}/{date}/{file}\")\n```\n\n**Why script**: No auth, no external APIs, pure file operations.\n\n### ‚úÖ Stateless Transformations\n\n```python\n# scripts/convert_markdown.py\nimport markdown\nimport sys\n\nwith open(sys.argv[1]) as f:\n    html = markdown.markdown(f.read())\n    print(html)\n```\n\n**Why script**: Input ‚Üí Output, no state, no network.\n\n### ‚úÖ CLI Wrappers\n\n```python\n# scripts/git_summary.py\nimport subprocess\nimport json\n\ndef get_commit_summary(since=\"1 week ago\"):\n    result = subprocess.run(\n        ['git', 'log', f'--since={since}', '--oneline'],\n        capture_output=True,\n        text=True\n    )\n    return result.stdout.split('\\n')\n\nprint(json.dumps(get_commit_summary()))\n```\n\n**Why script**: Wrapping existing CLI tools, no auth needed.\n\n### ‚úÖ Batch Processing\n\n```bash\n# scripts/batch_resize.sh\n#!/bin/bash\nfor img in *.jpg; do\n    convert \"$img\" -resize 800x600 \"resized_$img\"\ndone\n```\n\n**Why script**: Simple, local, no coordination needed.\n\n## Use MCPs When...\n\n### ‚úÖ External APIs with Auth\n\n```python\n# Good MCP example\nfrom mcp.server import Server\nfrom anthropic import Anthropic\n\napp = Server(\"claude-api\")\nclient = Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n\n@app.tool()\nasync def ask_claude(prompt: str) -> str:\n    \"\"\"Query Claude API with authentication.\"\"\"\n    message = client.messages.create(\n        model=\"claude-sonnet-4-20250514\",\n        max_tokens=1024,\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    return message.content[0].text\n```\n\n**Why MCP**: \n- Authentication (API key)\n- External service\n- Standardized error handling\n- Rate limiting concerns\n\n### ‚úÖ Stateful Connections\n\n```python\n# Database MCP\nfrom mcp.server import Server\nimport psycopg2\n\napp = Server(\"postgres-mcp\")\nconn = None  # Persistent connection\n\n@app.tool()\nasync def connect_db(connection_string: str):\n    \"\"\"Establish database connection.\"\"\"\n    global conn\n    conn = psycopg2.connect(connection_string)\n    return \"Connected\"\n\n@app.tool()\nasync def query_db(sql: str):\n    \"\"\"Execute query on open connection.\"\"\"\n    if not conn:\n        raise Exception(\"Not connected\")\n    cursor = conn.cursor()\n    cursor.execute(sql)\n    return cursor.fetchall()\n```\n\n**Why MCP**: \n- Maintains state (connection)\n- Multiple related operations\n- Connection pooling\n- Transaction management\n\n### ‚úÖ Real-Time Data\n\n```python\n# Stock price MCP\nfrom mcp.server import Server\nimport websocket\nimport json\n\napp = Server(\"stock-prices\")\nws = None\n\n@app.tool()\nasync def subscribe_stock(ticker: str):\n    \"\"\"Subscribe to real-time stock updates.\"\"\"\n    global ws\n    ws = websocket.WebSocketApp(\n        f\"wss://api.example.com/stocks/{ticker}\",\n        on_message=handle_message\n    )\n    ws.run_forever()\n```\n\n**Why MCP**: WebSocket connection, real-time updates, persistent connection.\n\n### ‚úÖ Multiple Related Tools\n\n```python\n# GitHub MCP\nfrom mcp.server import Server\nimport requests\n\napp = Server(\"github-api\")\nBASE = \"https://api.github.com\"\nTOKEN = os.getenv(\"GITHUB_TOKEN\")\nHEADERS = {\"Authorization\": f\"token {TOKEN}\"}\n\n@app.tool()\nasync def list_repos(org: str):\n    \"\"\"List organization repositories.\"\"\"\n    r = requests.get(f\"{BASE}/orgs/{org}/repos\", headers=HEADERS)\n    return r.json()\n\n@app.tool()\nasync def create_issue(repo: str, title: str, body: str):\n    \"\"\"Create GitHub issue.\"\"\"\n    r = requests.post(\n        f\"{BASE}/repos/{repo}/issues\",\n        headers=HEADERS,\n        json={\"title\": title, \"body\": body}\n    )\n    return r.json()\n\n@app.tool()\nasync def get_pr(repo: str, pr_number: int):\n    \"\"\"Get pull request details.\"\"\"\n    r = requests.get(f\"{BASE}/repos/{repo}/pulls/{pr_number}\", headers=HEADERS)\n    return r.json()\n```\n\n**Why MCP**: \n- All tools share auth\n- Related domain (GitHub)\n- Standardized error handling\n- Single configuration\n\n## Anti-Patterns\n\n### ‚ùå Skill for Runtime Execution\n\n**Bad**:\n```yaml\n# .claude/skills/file-organizer/SKILL.md\n---\nname: file-organizer\ndescription: Organizes files by date\nallowed-tools: Bash,Read,Write\n---\n\nRun this script to organize files:\npython scripts/organize.py /path/to/files\n```\n\n**Why it's wrong**: Skills provide expertise, not execution. Use Script or Agent for actual work.\n\n**What to do instead**:\n- **Skill**: Provide decision tree (\"When to organize by date vs by type\")\n- **Script**: Do the actual organizing\n- **Agent**: Orchestrate multiple organization strategies\n\n### ‚ùå Agent for Static Knowledge\n\n**Bad**:\n```python\n# Agent that just returns information\nAgent(\n    name=\"python-syntax-helper\",\n    instructions=\"Answer Python syntax questions\",\n    tools=[]\n)\n```\n\n**Why it's wrong**: No tools needed, no multi-step workflow, just knowledge lookup.\n\n**What to do instead**: Use a Skill with /references/ to documentation.\n\n### ‚ùå Agent for Single Operations\n\n**Bad**:\n```python\n# Agent that just runs one command\nAgent(\n    name=\"test-runner\",\n    instructions=\"Run pytest on the codebase\",\n    tools=[\"Bash\"]\n)\n```\n\n**Why it's wrong**: Single bash command doesn't justify agent overhead.\n\n**What to do instead**: Just run `pytest` directly via Bash tool.\n\n### ‚ùå MCP for Local Operations\n\n**Bad**:\n```python\n# mcp_server_json.py - OVERKILL\nfrom mcp.server import Server\nimport json\n\napp = Server(\"json-parser\")\n\n@app.tool()\nasync def parse_json(file_path: str):\n    with open(file_path) as f:\n        return json.load(f)\n\n@app.tool()\nasync def write_json(file_path: str, data: dict):\n    with open(file_path, 'w') as f:\n        json.dump(data, f, indent=2)\n```\n\n**Better**:\n```python\n# scripts/json_utils.py\nimport json\nimport sys\n\n# Parse\nwith open(sys.argv[1]) as f:\n    print(json.dumps(json.load(f), indent=2))\n```\n\n**Why**: No auth, no network, no state. Script is simpler.\n\n### ‚ùå Script for Authenticated APIs\n\n**Bad**:\n```python\n# scripts/query_jira.py\nimport requests\n\n# API key hardcoded or in environment - not great!\nresponse = requests.get(\n    \"https://company.atlassian.net/rest/api/3/issue/PROJ-123\",\n    auth=(\"user@example.com\", os.getenv(\"JIRA_TOKEN\"))\n)\nprint(response.json())\n```\n\n**Why problematic**:\n- Credentials in script or environment\n- No error handling\n- No rate limiting\n- Can't compose with other Jira operations\n- Each agent invocation re-authenticates\n\n**Better**: MCP with proper auth flow, connection reuse, error handling.\n\n### ‚ùå Overengineered MCP\n\n**Bad**:\n```python\n# mcp_server_calculator.py - TOO SIMPLE FOR MCP\nfrom mcp.server import Server\n\napp = Server(\"calculator\")\n\n@app.tool()\nasync def add(a: float, b: float) -> float:\n    return a + b\n\n@app.tool()\nasync def subtract(a: float, b: float) -> float:\n    return a - b\n```\n\n**Better**: Claude can do this natively. No tool needed.\n\n## Evolution Path\n\nGood architecture evolves through layers of abstraction:\n\n### Stage 0: Direct Tool Use\n```bash\n# Just use Claude's tools directly\nRead file ‚Üí Edit file ‚Üí Bash command\n```\n\n### Stage 1: Single Script\n```bash\n# fetch_data.py\nimport requests\ndata = requests.get(\"https://api.example.com/data\").json()\nprint(data)\n```\n\n**Use when**: One-off operation, local, no auth\n\n### Stage 2: Multiple Scripts\n```bash\nscripts/\n‚îú‚îÄ‚îÄ fetch_data.py\n‚îú‚îÄ‚îÄ process_data.py\n‚îî‚îÄ‚îÄ upload_results.py\n```\n\n**Use when**: Related operations, still local, no orchestration needed\n\n### Stage 3: Skill for Expertise\n```yaml\n# .claude/skills/data-pipeline-expert/SKILL.md\n---\nname: data-pipeline-expert\ndescription: Data pipeline anti-patterns and decision trees\n---\n\n## When to Batch Process\n- Data size > 100MB ‚Üí Batch with pagination\n- Real-time updates needed ‚Üí Use streaming instead\n\n## Anti-Patterns\n- Processing entire dataset in memory ‚Üí OOM errors\n```\n\n**Use when**: You have domain expertise to encode, prevent common mistakes\n\n### Stage 4: Agent for Orchestration\n```python\n# When scripts need coordination\nAgent(\n    name=\"pipeline-runner\",\n    instructions=\"\"\"\n    1. Validate data format\n    2. Run fetch_data.py\n    3. If fetch succeeds, run process_data.py\n    4. If validation passes, run upload_results.py\n    5. Generate summary report\n    \"\"\",\n    tools=[\"Bash\", \"Read\", \"Write\"]\n)\n```\n\n**Use when**: Multi-step workflow with runtime decisions, state management\n\n### Stage 5: Helper Library\n```python\n# lib/data_client.py\nclass DataClient:\n    def fetch(self): ...\n    def process(self): ...\n    def upload(self): ...\n\n# scripts/run_pipeline.py\nfrom lib.data_client import DataClient\nclient = DataClient()\nclient.fetch()\nclient.process()\nclient.upload()\n```\n\n**Use when**: Shared logic across multiple scripts, testability needed\n\n### Stage 6: MCP Server\n```python\n# Only when you need:\n# - Auth management\n# - Multiple agents using it\n# - External API access\n# - Connection pooling\n\nfrom mcp.server import Server\nfrom lib.data_client import DataClient\n\napp = Server(\"data-pipeline\")\nclient = DataClient()\n\n@app.tool()\nasync def fetch_data():\n    return client.fetch()\n# ... etc\n```\n\n**Use when**: External APIs, auth boundaries, stateful connections\n\n**The Rule**: Start simple. Each stage adds complexity - only evolve when the complexity pays for itself.\n\n## Performance Considerations\n\n### Skills\n- ‚úÖ Zero runtime overhead (loaded as context)\n- ‚úÖ Prevents mistakes before execution\n- ‚úÖ Cached across conversations\n- ‚ùå Takes up context window\n- ‚ùå No execution capability\n\n### Agents\n- ‚úÖ Autonomy reduces back-and-forth\n- ‚úÖ Parallel tool execution\n- ‚úÖ Context maintained across steps\n- ‚ùå Higher token usage\n- ‚ùå More complex debugging\n\n### Scripts\n- ‚úÖ Zero latency overhead\n- ‚úÖ Simple debugging\n- ‚úÖ No network calls for local ops\n- ‚ùå No connection reuse\n- ‚ùå No shared state across calls\n\n### MCPs\n- ‚úÖ Connection pooling\n- ‚úÖ Shared state\n- ‚úÖ Standardized errors\n- ‚ùå Network overhead\n- ‚ùå More complex debugging\n\n## Security Considerations\n\n### Scripts\n- ‚úÖ No credential management complexity\n- ‚úÖ Run in user context\n- ‚ùå Credentials in environment or hardcoded\n- ‚ùå Each invocation re-authenticates\n\n### MCPs\n- ‚úÖ Centralized credential management\n- ‚úÖ OAuth flows\n- ‚úÖ Connection reuse (fewer auth requests)\n- ‚ùå More attack surface\n- ‚ùå Requires secure credential storage\n\n## Testing\n\n### Scripts\n```bash\n# Easy to test\npython scripts/process_data.py test_input.json\n```\n\n### MCPs\n```python\n# MCP Inspector or custom client needed\nfrom mcp.client import Client\n\nasync def test():\n    async with Client(\"http://localhost:8000\") as client:\n        result = await client.call_tool(\"process_data\", {\"input\": \"test\"})\n        assert result == expected\n```\n\n## Documentation Recommendations\n\nIn your skill's SKILL.md:\n\n```markdown\n## Tools Required\n\nThis skill uses:\n- **Scripts** for local processing: `/scripts/validate.py`\n- **MCP** for GitHub API access: Requires `github-mcp` installed\n\n### Setup MCP\n\n```bash\n/plugin marketplace add github-mcp\n```\n\nOr use CLI directly if GitHub MCP unavailable:\n```bash\ngh issue list\n```\n```\n\n## Decision Flowchart\n\n```\nDo you need to encode expertise/anti-patterns?\n‚îú‚îÄ Yes ‚Üí Skill (with decision trees, no execution)\n‚îî‚îÄ No ‚Üí Is it multi-step with runtime decisions?\n    ‚îú‚îÄ Yes ‚Üí Agent (orchestrates tools autonomously)\n    ‚îî‚îÄ No ‚Üí Is it a local operation?\n        ‚îú‚îÄ Yes ‚Üí Script (stateless, no auth)\n        ‚îî‚îÄ No ‚Üí Does it require auth/external API?\n            ‚îú‚îÄ Yes ‚Üí MCP Server (manages auth boundaries)\n            ‚îî‚îÄ No ‚Üí Script (with curl/CLI)\n```\n\n**Key Questions:**\n1. **Expertise?** ‚Üí Skill (anti-patterns, decision trees)\n2. **Multi-step + decisions?** ‚Üí Agent (tool orchestration)\n3. **External API + auth?** ‚Üí MCP (connection management)\n4. **Everything else?** ‚Üí Script (simple execution)\n\n## Real-World Examples\n\n### Good: CLIP Limitations as Skill\n- Domain expertise (what NOT to use CLIP for)\n- Anti-patterns with alternatives\n- No runtime execution needed\n- Prevents common mistakes before coding\n\n### Good: Code Review as Agent\n- Multi-step workflow (read ‚Üí lint ‚Üí test ‚Üí summarize)\n- Runtime decisions (which files to read)\n- Tool orchestration (Read, Bash, Grep, Write)\n- Autonomous execution\n\n### Good: Git as Script\n- CLI wrapper\n- Local operations\n- No auth needed\n- Simple subprocess calls\n\n### Good: Playwright as MCP\n- Complex browser automation\n- Stateful (browser context)\n- Multiple related operations\n- Security boundaries (sandbox)\n\n### Good: Framework Evolution as Skill\n- Temporal knowledge (pre-2024 vs 2024+)\n- Warns about deprecated patterns\n- Decision trees for migration\n- No execution needed\n\n### Good: Testing Pipeline as Agent\n- Orchestrates test suite\n- Parallel execution management\n- State aggregation (pass/fail counts)\n- Fail-fast logic\n\n### Good: Image Processing as Script\n- Local file operations\n- Stateless transformations\n- No network needed\n- Simple input/output\n\n### Good: AWS SDK as MCP\n- Many related services\n- Auth required\n- Connection pooling\n- Error handling standardization\n\n## Summary\n\n**Skills win on**:\n- Domain expertise encoding\n- Anti-pattern prevention\n- Zero runtime overhead\n- Context persistence\n- Decision tree guidance\n\n**Agents win on**:\n- Multi-step orchestration\n- Tool autonomy\n- Runtime decision-making\n- Workflow automation\n- Parallel execution\n\n**Scripts win on**:\n- Simplicity\n- Local operations\n- No dependencies\n- Easy testing\n- Zero overhead\n\n**MCPs win on**:\n- Auth management\n- Connection reuse\n- Multiple related operations\n- Stateful interactions\n- Standardization\n\n**The Hierarchy**:\n1. **Start with**: Direct tool use (Read, Edit, Bash)\n2. **Extract to**: Script (when operation repeats)\n3. **Add**: Skill (when expertise/anti-patterns emerge)\n4. **Coordinate with**: Agent (when multi-step workflows appear)\n5. **Promote to**: MCP (when auth/external APIs needed)\n\n**The Rule**: Each layer adds complexity. Only add layers when the value justifies the cost.\n\n---\n\n## Further Reading\n\n- `/references/antipatterns.md` - \"MCP for Everything\" anti-pattern\n- `/examples/good/mcp-vs-script-comparison/` - Side-by-side examples\n- Model Context Protocol docs: https://modelcontextprotocol.io/\n"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "folder",
      "path": "skill-coach/scripts",
      "children": [
        {
          "name": "validate_skill.py",
          "type": "file",
          "path": "skill-coach/scripts/validate_skill.py",
          "size": 11942,
          "content": "#!/usr/bin/env python3\n\"\"\"\nSkill Validator - Pre-flight checks for Agent Skills\n\nValidates skill structure, content quality, and best practices.\n\"\"\"\n\nimport os\nimport sys\nimport re\nimport yaml\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple\nfrom dataclasses import dataclass\nfrom enum import Enum\n\n\nclass Severity(Enum):\n    ERROR = \"ERROR\"\n    WARNING = \"WARNING\"\n    INFO = \"INFO\"\n\n\n@dataclass\nclass ValidationIssue:\n    severity: Severity\n    message: str\n    line: int = None\n    suggestion: str = None\n\n\nclass SkillValidator:\n    def __init__(self, skill_path: str):\n        self.skill_path = Path(skill_path)\n        self.issues: List[ValidationIssue] = []\n        self.skill_md = self.skill_path / \"SKILL.md\"\n        \n    def validate(self) -> List[ValidationIssue]:\n        \"\"\"Run all validation checks.\"\"\"\n        self.check_structure()\n        self.check_skill_md()\n        self.check_description_quality()\n        self.check_progressive_disclosure()\n        self.check_antipatterns_section()\n        self.check_allowed_tools()\n        return self.issues\n    \n    def check_structure(self):\n        \"\"\"Verify required files and folders exist.\"\"\"\n        if not self.skill_path.exists():\n            self.issues.append(ValidationIssue(\n                Severity.ERROR,\n                f\"Skill directory not found: {self.skill_path}\"\n            ))\n            return\n        \n        if not self.skill_md.exists():\n            self.issues.append(ValidationIssue(\n                Severity.ERROR,\n                \"SKILL.md file missing\"\n            ))\n            return\n        \n        # Check for recommended structure\n        scripts_dir = self.skill_path / \"scripts\"\n        if not scripts_dir.exists():\n            self.issues.append(ValidationIssue(\n                Severity.INFO,\n                \"No /scripts directory found. Consider adding validation or example scripts.\"\n            ))\n        \n        references_dir = self.skill_path / \"references\"\n        if not references_dir.exists():\n            self.issues.append(ValidationIssue(\n                Severity.INFO,\n                \"No /references directory. Consider splitting detailed docs into references.\"\n            ))\n    \n    def check_skill_md(self):\n        \"\"\"Validate SKILL.md content and structure.\"\"\"\n        with open(self.skill_md, 'r', encoding='utf-8') as f:\n            content = f.read()\n        \n        # Check for YAML frontmatter\n        if not content.startswith('---'):\n            self.issues.append(ValidationIssue(\n                Severity.ERROR,\n                \"Missing YAML frontmatter. Must start with '---'\"\n            ))\n            return\n        \n        # Extract frontmatter\n        try:\n            parts = content.split('---', 2)\n            frontmatter = yaml.safe_load(parts[1])\n        except Exception as e:\n            self.issues.append(ValidationIssue(\n                Severity.ERROR,\n                f\"Invalid YAML frontmatter: {e}\"\n            ))\n            return\n        \n        # Check required fields\n        if 'name' not in frontmatter:\n            self.issues.append(ValidationIssue(\n                Severity.ERROR,\n                \"Missing required field: name\"\n            ))\n        else:\n            # Validate name format\n            name = frontmatter['name']\n            if not re.match(r'^[a-z0-9-]+$', name):\n                self.issues.append(ValidationIssue(\n                    Severity.ERROR,\n                    f\"Invalid name format: '{name}'. Must use lowercase letters, numbers, and hyphens only.\"\n                ))\n            if len(name) > 64:\n                self.issues.append(ValidationIssue(\n                    Severity.ERROR,\n                    f\"Name too long: {len(name)} chars (max 64)\"\n                ))\n        \n        if 'description' not in frontmatter:\n            self.issues.append(ValidationIssue(\n                Severity.ERROR,\n                \"Missing required field: description\"\n            ))\n        \n        # Check line count\n        lines = content.split('\\n')\n        if len(lines) > 500:\n            self.issues.append(ValidationIssue(\n                Severity.WARNING,\n                f\"SKILL.md is {len(lines)} lines (recommended: <500). Consider splitting into /references.\"\n            ))\n    \n    def check_description_quality(self):\n        \"\"\"Analyze description field quality.\"\"\"\n        with open(self.skill_md, 'r') as f:\n            content = f.read()\n        \n        try:\n            parts = content.split('---', 2)\n            frontmatter = yaml.safe_load(parts[1])\n            description = frontmatter.get('description', '')\n        except:\n            return  # Already reported in check_skill_md\n        \n        if not description:\n            return\n        \n        # Check length\n        if len(description) > 1024:\n            self.issues.append(ValidationIssue(\n                Severity.ERROR,\n                f\"Description too long: {len(description)} chars (max 1024)\"\n            ))\n        \n        if len(description) < 20:\n            self.issues.append(ValidationIssue(\n                Severity.WARNING,\n                \"Description too short. Should explain what, when, and triggers.\"\n            ))\n        \n        # Check for key components\n        has_what = any(word in description.lower() for word in ['create', 'analyze', 'generate', 'process', 'handle', 'manage'])\n        has_when = any(word in description.lower() for word in ['when', 'use for', 'use when', 'for'])\n        \n        if not has_what:\n            self.issues.append(ValidationIssue(\n                Severity.WARNING,\n                \"Description should explain WHAT the skill does\"\n            ))\n        \n        if not has_when:\n            self.issues.append(ValidationIssue(\n                Severity.WARNING,\n                \"Description should explain WHEN to use it\",\n                suggestion=\"Add: 'Use when...' or 'Use for...'\"\n            ))\n        \n        # Check for negative triggers (what NOT to use it for)\n        has_not = 'not' in description.lower() or \"don't\" in description.lower()\n        if not has_not:\n            self.issues.append(ValidationIssue(\n                Severity.INFO,\n                \"Consider adding what NOT to use this skill for to prevent false activation\"\n            ))\n        \n        # Check point of view\n        if any(word in description.lower() for word in ['i ', 'you ', 'your ', 'my ']):\n            self.issues.append(ValidationIssue(\n                Severity.WARNING,\n                \"Description should use third person, not first/second person\",\n                suggestion=\"Change 'you can use this to...' to 'Use for...'\"\n            ))\n    \n    def check_progressive_disclosure(self):\n        \"\"\"Check for proper progressive disclosure structure.\"\"\"\n        with open(self.skill_md, 'r') as f:\n            content = f.read()\n        \n        # Check for references to external files\n        has_references = bool(re.search(r'/references/', content) or \n                             re.search(r'/scripts/', content))\n        \n        lines = content.split('\\n')\n        if len(lines) > 300 and not has_references:\n            self.issues.append(ValidationIssue(\n                Severity.WARNING,\n                \"Long SKILL.md without references. Consider splitting detailed content.\",\n                suggestion=\"Move deep dives to /references/ and link from main file\"\n            ))\n        \n        # Check for \"See X for details\" patterns\n        if len(lines) > 200:\n            has_see_references = bool(re.search(r'See .* for', content))\n            if not has_see_references:\n                self.issues.append(ValidationIssue(\n                    Severity.INFO,\n                    \"Consider adding 'See /references/X for...' to defer detailed content\"\n                ))\n    \n    def check_antipatterns_section(self):\n        \"\"\"Check if skill includes anti-pattern guidance.\"\"\"\n        with open(self.skill_md, 'r') as f:\n            content = f.read().lower()\n        \n        has_antipatterns = any(term in content for term in [\n            'anti-pattern', 'antipattern', 'common mistake', \n            'avoid', 'don\\'t', 'deprecated', 'wrong'\n        ])\n        \n        if not has_antipatterns:\n            self.issues.append(ValidationIssue(\n                Severity.INFO,\n                \"No anti-pattern guidance found. Consider adding common mistakes section.\",\n                suggestion=\"Add '## Common Anti-Patterns' section\"\n            ))\n    \n    def check_allowed_tools(self):\n        \"\"\"Validate allowed-tools field.\"\"\"\n        with open(self.skill_md, 'r') as f:\n            content = f.read()\n        \n        try:\n            parts = content.split('---', 2)\n            frontmatter = yaml.safe_load(parts[1])\n            allowed_tools = frontmatter.get('allowed-tools', '')\n        except:\n            return\n        \n        if not allowed_tools:\n            self.issues.append(ValidationIssue(\n                Severity.INFO,\n                \"No allowed-tools specified. Claude will ask for permission.\"\n            ))\n            return\n        \n        # Check for overly permissive tools\n        tools = [t.strip() for t in str(allowed_tools).split(',')]\n        \n        if 'Bash' in tools and not any('Bash(' in t for t in tools):\n            self.issues.append(ValidationIssue(\n                Severity.WARNING,\n                \"Unrestricted Bash access. Consider scoping: Bash(git:*,npm:*)\",\n                suggestion=\"Restrict bash to specific commands\"\n            ))\n        \n        # Check for unnecessary tools\n        body_content = content.split('---', 2)[2].lower()\n        for tool in tools:\n            tool_lower = tool.split('(')[0].lower()\n            if tool_lower not in body_content:\n                self.issues.append(ValidationIssue(\n                    Severity.INFO,\n                    f\"Tool '{tool}' in allowed-tools but not mentioned in content\"\n                ))\n\n\ndef print_report(issues: List[ValidationIssue]):\n    \"\"\"Print validation report.\"\"\"\n    if not issues:\n        print(\"‚úÖ Validation passed! No issues found.\")\n        return\n    \n    errors = [i for i in issues if i.severity == Severity.ERROR]\n    warnings = [i for i in issues if i.severity == Severity.WARNING]\n    info = [i for i in issues if i.severity == Severity.INFO]\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"VALIDATION REPORT\")\n    print(f\"{'='*60}\\n\")\n    \n    if errors:\n        print(f\"‚ùå ERRORS ({len(errors)}):\")\n        for issue in errors:\n            print(f\"  ‚Ä¢ {issue.message}\")\n            if issue.suggestion:\n                print(f\"    üí° {issue.suggestion}\")\n        print()\n    \n    if warnings:\n        print(f\"‚ö†Ô∏è  WARNINGS ({len(warnings)}):\")\n        for issue in warnings:\n            print(f\"  ‚Ä¢ {issue.message}\")\n            if issue.suggestion:\n                print(f\"    üí° {issue.suggestion}\")\n        print()\n    \n    if info:\n        print(f\"‚ÑπÔ∏è  SUGGESTIONS ({len(info)}):\")\n        for issue in info:\n            print(f\"  ‚Ä¢ {issue.message}\")\n            if issue.suggestion:\n                print(f\"    üí° {issue.suggestion}\")\n        print()\n    \n    print(f\"{'='*60}\")\n    print(f\"Summary: {len(errors)} errors, {len(warnings)} warnings, {len(info)} suggestions\")\n    print(f\"{'='*60}\\n\")\n\n\ndef main():\n    if len(sys.argv) < 2:\n        print(\"Usage: python validate_skill.py <skill_path>\")\n        print(\"\\nExample:\")\n        print(\"  python validate_skill.py ~/my-skill/\")\n        sys.exit(1)\n    \n    skill_path = sys.argv[1]\n    \n    print(f\"Validating skill at: {skill_path}\")\n    print()\n    \n    validator = SkillValidator(skill_path)\n    issues = validator.validate()\n    \n    print_report(issues)\n    \n    # Exit code based on errors\n    errors = [i for i in issues if i.severity == Severity.ERROR]\n    sys.exit(1 if errors else 0)\n\n\nif __name__ == '__main__':\n    main()\n"
        }
      ]
    },
    {
      "name": "website",
      "type": "folder",
      "path": "skill-coach/website",
      "children": [
        {
          "name": "src",
          "type": "folder",
          "path": "skill-coach/website/src",
          "children": [
            {
              "name": "data",
              "type": "folder",
              "path": "skill-coach/website/src/data",
              "children": [
                {
                  "name": "artifacts",
                  "type": "folder",
                  "path": "skill-coach/website/src/data/artifacts",
                  "children": [
                    {
                      "name": "comparisons",
                      "type": "folder",
                      "path": "skill-coach/website/src/data/artifacts/comparisons",
                      "children": []
                    },
                    {
                      "name": "multi-skill",
                      "type": "folder",
                      "path": "skill-coach/website/src/data/artifacts/multi-skill",
                      "children": []
                    },
                    {
                      "name": "single-skill",
                      "type": "folder",
                      "path": "skill-coach/website/src/data/artifacts/single-skill",
                      "children": [
                        {
                          "name": "skill-coach",
                          "type": "folder",
                          "path": "skill-coach/website/src/data/artifacts/single-skill/skill-coach",
                          "children": [
                            {
                              "name": "001-self-improvement",
                              "type": "folder",
                              "path": "skill-coach/website/src/data/artifacts/single-skill/skill-coach/001-self-improvement",
                              "children": [
                                {
                                  "name": "after",
                                  "type": "folder",
                                  "path": "skill-coach/website/src/data/artifacts/single-skill/skill-coach/001-self-improvement/after",
                                  "children": []
                                },
                                {
                                  "name": "assets",
                                  "type": "folder",
                                  "path": "skill-coach/website/src/data/artifacts/single-skill/skill-coach/001-self-improvement/assets",
                                  "children": []
                                },
                                {
                                  "name": "before",
                                  "type": "folder",
                                  "path": "skill-coach/website/src/data/artifacts/single-skill/skill-coach/001-self-improvement/before",
                                  "children": []
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "name": "schemas",
                  "type": "folder",
                  "path": "skill-coach/website/src/data/schemas",
                  "children": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "name": "CHANGELOG.md",
      "type": "file",
      "path": "skill-coach/CHANGELOG.md",
      "size": 1349,
      "content": "# Changelog\n\nAll notable changes to the skill-coach skill will be documented in this file.\n\n## [1.2.0] - 2025-11-26\n\n### Added\n- **MCP & Tool Research (MANDATORY)** section - comprehensive guide for researching MCPs\n- Research process with 4 steps: Web Search, Check Registries, Evaluate Quality, Add to Skill\n- Domain-Specific MCP Examples table\n- Anti-pattern: Assuming No MCPs Exist\n- Anti-pattern: Adding MCPs Without Testing\n- MCP research added to Quick Start workflow (step 2)\n- MCP research added to Review Checklist (CRITICAL section)\n\n### Changed\n- Updated Review Checklist: `allowed-tools` guidance now emphasizes including relevant MCPs\n- Quick Start now has 6 steps instead of 5 (added MCP research step)\n\n## [1.1.0] - 2025-11-26\n\n### Added\n- Versioning Skills section with complete guidance\n- CHANGELOG.md format template\n- Version numbering explanation (MAJOR/MINOR/PATCH)\n- \"Why version skills?\" rationale\n- Recommended structure now includes CHANGELOG.md\n- CHANGELOG.md tracking added to Review Checklist (HIGH PRIORITY)\n\n## [1.0.0] - 2025-01-01\n\n### Added\n- Initial skill creation\n- Progressive disclosure architecture\n- Description field design patterns\n- Anti-pattern detection framework\n- Temporal knowledge capture\n- Domain-specific shibboleths\n- Skill review checklist\n- Testing guidelines\n- Decision trees for skill creation\n"
    },
    {
      "name": "OVERVIEW.md",
      "type": "file",
      "path": "skill-coach/OVERVIEW.md",
      "size": 7447,
      "content": "# Skill-Coach: Overview\n\n## What This Is\n\nA **meta-skill** that guides creation of expert-level Agent Skills - the kind that encode real domain knowledge and shibboleths, not just surface-level instructions.\n\n**Status**: Iteratively self-improved 5 times using its own guidance (Nov 2025), demonstrating the improvement loop it teaches.\n\n## Key Innovation: Encoding the Shibboleths\n\nMost skills say: \"Here's how to use X\"\n\nThis teaches: \"Here's how to use X, and here's where everyone gets it wrong, and why, and what to use instead\"\n\n## Structure\n\n```\nskill-coach/\n‚îú‚îÄ‚îÄ README.md                           # Start here\n‚îú‚îÄ‚îÄ SKILL.md                            # The coach skill itself\n‚îú‚îÄ‚îÄ scripts/\n‚îÇ   ‚îî‚îÄ‚îÄ validate_skill.py              # ‚úÖ Validates skill structure & quality\n‚îú‚îÄ‚îÄ references/\n‚îÇ   ‚îú‚îÄ‚îÄ antipatterns.md                # üéØ Domain-specific shibboleths\n‚îÇ   ‚îî‚îÄ‚îÄ mcp_vs_scripts.md              # When to use MCP vs Scripts\n‚îî‚îÄ‚îÄ examples/\n    ‚îî‚îÄ‚îÄ good/\n        ‚îî‚îÄ‚îÄ clip-aware-embeddings/     # üåü Exemplary skill\n            ‚îú‚îÄ‚îÄ SKILL.md\n            ‚îî‚îÄ‚îÄ scripts/\n                ‚îî‚îÄ‚îÄ validate_clip_usage.py  # Domain-specific validator\n```\n\n## The CLIP Example: Why This Matters\n\nLook at `examples/good/clip-aware-embeddings/SKILL.md` - it doesn't just say \"use CLIP for image-text matching.\"\n\nIt says:\n\n**Novice knowledge** (what LLMs trained on 2021-2023 data know):\n> \"CLIP is pre-trained on 400M image-text pairs! Use it for all image tasks!\"\n\n**Expert knowledge** (the shibboleth):\n> \"CLIP has fundamental geometric limitations. It CANNOT:\n> - Count objects (use DETR instead)\n> - Do fine-grained classification (use specialized models)\n> - Understand spatial relationships (use GQA models)\n> - Bind attributes ('red car AND blue truck' ‚Üí use DCSMs)\"\n\nThis is the knowledge gap that separates \"it compiles\" from \"it's correct.\"\n\n## What Makes This Different\n\n### 1. Anti-Patterns Catalog\n\n`references/antipatterns.md` documents:\n- CLIP's actual limitations (with research citations)\n- Framework evolution (Next.js Pages ‚Üí App Router)\n- Architecture decisions (MCP vs Scripts philosophy)\n- Temporal context (when things changed and why)\n\n### 2. Validation Tooling\n\n**General validation** (`scripts/validate_skill.py`):\n- Checks structure (YAML, required fields)\n- Validates description quality\n- Ensures progressive disclosure\n- Checks line count (<500)\n- Verifies allowed-tools scope\n\n**Domain-specific validation** (`examples/.../validate_clip_usage.py`):\n- Detects counting queries ‚Üí suggests object detection\n- Identifies spatial queries ‚Üí suggests spatial models\n- Catches fine-grained tasks ‚Üí suggests specialized models\n\nThis is **executable domain knowledge**.\n\n### 3. Progressive Disclosure Done Right\n\nThe CLIP skill is 380 lines but FEELS concise because:\n- Quick decision tree upfront\n- Anti-patterns clearly marked\n- References to deep dives (not inline)\n- Validation scripts (run, don't read)\n\n### 4. Temporal Knowledge\n\nEvery anti-pattern includes:\n- **Timeline**: \"2021: CLIP released, 2023: limitations discovered\"\n- **Why LLMs get it wrong**: \"Training data predates the research\"\n- **Migration path**: \"If you're doing X, use Y instead\"\n\n## Test It Out\n\n### Validate Your Skills\n\n```bash\ncd skill-coach\npython scripts/validate_skill.py /path/to/your-skill/\n```\n\n### See Domain Validation\n\n```bash\ncd examples/good/clip-aware-embeddings\npython scripts/validate_clip_usage.py \"How many cars are in this image?\"\n# ‚Üí ‚ùå Use object detection: DETR, Faster R-CNN, YOLO\n\npython scripts/validate_clip_usage.py \"Find images of beaches\"\n# ‚Üí ‚úÖ CLIP is appropriate\n```\n\n## The MCP vs Scripts Philosophy\n\nFrom `references/mcp_vs_scripts.md`:\n\n> \"MCP's job isn't to abstract reality for the agent; it's to manage the auth, networking, and security boundaries and then get out of the way.\"\n\n**Use Scripts for**:\n- Local file operations\n- Stateless transformations\n- CLI wrappers\n- Batch processing\n\n**Use MCPs for**:\n- External APIs with auth\n- Stateful connections\n- Real-time data\n- Multiple related operations\n\nThe guide includes decision matrix, evolution path, and anti-examples.\n\n## Key Shibboleths Encoded\n\n### ML/AI\n- CLIP's geometric impossibilities\n- Embedding model selection by task\n- Model versioning and temporal changes\n\n### Frameworks\n- Next.js: Pages Router ‚Üí App Router (Oct 2022)\n- React: Class Components ‚Üí Hooks (Feb 2019)\n- State: Redux ‚Üí Zustand/Context (2020+)\n\n### Architecture\n- When complexity justifies MCP over scripts\n- Security via least-privilege tool access\n- Performance vs simplicity tradeoffs\n\n## What You Can Do With This\n\n1. **Use it as-is**: Ask Claude to apply skill-coach when creating skills\n2. **Study the example**: See all principles in action\n3. **Add your shibboleths**: Contribute domain knowledge you've learned\n4. **Validate existing skills**: Run the validator on skills you have\n\n## Recent Improvements (5 Iterations)\n\nThe skill-coach has been iteratively improved using its own guidance:\n\n**Iteration 1**: Foundation\n- Added 5 skill-specific anti-patterns (Reference Illusion, Description Soup, Template Theater, Everything Skill, Orphaned Sections)\n- Added Evolution Timeline (2024-2025 skill framework best practices)\n- Created comprehensive Skill Review Checklist\n- Removed all references to non-existent files\n\n**Iteration 2**: Actionability\n- Added 3 Common Workflows (create, debug activation, reduce false positives)\n- Made iteration strategy actionable with specific prompts\n- Explained why THIS skill uses each tool\n- Condensed validation patterns to concepts\n\n**Iteration 3**: Expert Knowledge\n- Added Skill Creation Shibboleths (novice vs expert skill creator)\n- Enhanced \"What Makes a Great Skill\" (5‚Üí7 items)\n- Condensed domain examples\n- Added meta-note about self-improvement\n\n**Iteration 4**: Usability\n- Added \"Quick Wins\" - 5 immediate improvements\n- Simplified skill structure (honest about what's needed)\n- Description progression (Bad‚ÜíBetter‚ÜíGood)\n- Realistic file structure (SKILL.md only is mandatory)\n\n**Iteration 5**: Decision Support\n- Added Decision Trees (when to create new skill, Skill vs Subagent vs MCP)\n- Prioritized checklist (CRITICAL/HIGH PRIORITY/NICE TO HAVE)\n- Final polish and consistency\n\n**Result**: 482 ‚Üí 470 lines, more concise yet more comprehensive.\n\n## The Meta Point\n\nThis skill **practices what it preaches**:\n\n- ‚úÖ Progressive disclosure (SKILL.md ‚Üí references/)\n- ‚úÖ Anti-patterns specific to skill creation\n- ‚úÖ Validation tooling (validate_skill.py)\n- ‚úÖ Working examples (CLIP skill)\n- ‚úÖ Temporal knowledge (2024-2025 evolution)\n- ‚úÖ Clear decision trees (when to create, Skill vs MCP)\n- ‚úÖ Iteratively improved using its own guidance\n\nIt's not just teaching - it's demonstrating.\n\n## Start Here\n\n1. Read `README.md` for getting started\n2. Check `SKILL.md` for Quick Wins (immediate improvements)\n3. Study `examples/good/clip-aware-embeddings/SKILL.md`\n4. Review `references/antipatterns.md` for domain shibboleths\n5. Use skill-coach when creating/improving your own skills\n\n## The Philosophy\n\n> \"Great skills don't just say 'here's how' - they say 'here's how, and here's where everyone gets it wrong, and why, and what to use instead.'\"\n\nThis is about encoding expertise and shibboleths, not just instructions.\n\n---\n\nCreated: 2025-11-23\nLast Improved: 2025-11-24 (5 iterations)\nVersion: 2.0.0\n"
    },
    {
      "name": "README.md",
      "type": "file",
      "path": "skill-coach/README.md",
      "size": 10570,
      "content": "# Skill Coach: Master Agent Skills Development\n\nA comprehensive guide and toolkit for creating expert-level Agent Skills that encode real domain knowledge, not just surface-level instructions.\n\n**Latest**: Iteratively self-improved 5 times (Nov 2024), demonstrating the improvement loop it teaches.\n\n## What This Skill Does\n\nSkill Coach helps you build skills that:\n- **Activate precisely** - Specific keywords + NOT clause prevents false activation\n- **Encode shibboleths** - Domain knowledge separating experts from novices\n- **Surface anti-patterns** - \"If you see X, that's wrong because Y, use Z\"\n- **Capture temporal knowledge** - \"Pre-2024: X. 2024+: Y. Watch for LLMs suggesting X\"\n- **Know their limits** - \"Use this for A, B, C. NOT for D, E, F\"\n- **Provide decision trees** - Not templates, but \"If X then A, if Y then B, never C\"\n- **Include validation** - Pre-flight checks catching errors early\n\n## Quick Start\n\n### 1. Install and Use\n\nCopy this folder to your skills directory:\n\n```bash\n# For Claude Code\ncp -r skill-coach ~/.claude/skills/\n\n# For Claude.ai\n# Upload via the Skills interface\n```\n\n### 2. Validate Your Skills\n\n```bash\ncd skill-coach\npython scripts/validate_skill.py /path/to/your-skill/\n```\n\n### 3. Study Examples\n\nLook at `/examples/good/clip-aware-embeddings/` to see all principles in action.\n\n## What's Inside\n\n```\nskill-coach/\n‚îú‚îÄ‚îÄ SKILL.md                    # Main skill instructions\n‚îú‚îÄ‚îÄ scripts/\n‚îÇ   ‚îî‚îÄ‚îÄ validate_skill.py       # Skill validation tool\n‚îú‚îÄ‚îÄ references/\n‚îÇ   ‚îú‚îÄ‚îÄ antipatterns.md         # Domain shibboleths catalog\n‚îÇ   ‚îî‚îÄ‚îÄ mcp_vs_scripts.md       # Architecture decisions\n‚îî‚îÄ‚îÄ examples/\n    ‚îú‚îÄ‚îÄ good/\n    ‚îÇ   ‚îî‚îÄ‚îÄ clip-aware-embeddings/  # Exemplary skill\n    ‚îî‚îÄ‚îÄ bad/\n        ‚îî‚îÄ‚îÄ (anti-examples)\n```\n\n## Key Concepts\n\n### 1. Progressive Disclosure\n\nSkills load in three phases:\n- **Phase 1 (~100 tokens)**: Metadata - \"Should I activate?\"\n- **Phase 2 (<5k tokens)**: Instructions - \"How do I do this?\"\n- **Phase 3 (as needed)**: Details - \"Show me more\"\n\n### 2. The Shibboleths\n\nDeep knowledge that reveals expertise:\n\n**Example - CLIP Embeddings**:\n- **Novice**: \"CLIP is great for image-text tasks!\"\n- **Expert**: \"CLIP fails at counting, fine-grained classification, spatial reasoning, and attribute binding. Use DETR for counting, specialized models for fine-grained, DCSMs for compositional.\"\n\n### 3. Anti-Pattern Detection\n\nGreat skills actively warn about mistakes:\n\n```markdown\n### Anti-Pattern: Using CLIP to Count Objects\n\n**Why wrong**: CLIP's architecture cannot preserve spatial information\n**What to do**: Use DETR or Faster R-CNN\n**How to detect**: If query contains \"how many\" or \"count\"\n```\n\n### 4. Temporal Knowledge\n\nCapture what changed and when:\n\n```markdown\n## Evolution Timeline\n- Pre-2024: Redux for all state management\n- 2024+: Zustand/Jotai for global state, Context for simple cases\n- Watch for: LLMs suggesting Redux by default\n```\n\n## Creating Your First Skill\n\n### Step 1: Define Scope\n\n```markdown\n---\nname: your-skill-name\ndescription: [What it does] [When to use] [Specific triggers]. NOT for [What it's NOT for].\n---\n```\n\n### Step 2: Add Instructions\n\n```markdown\n# Your Skill\n\n## When to Use\n‚úÖ Use for: ...\n‚ùå Do NOT use for: ...\n\n## Quick Start\n[Minimal working example]\n\n## Common Anti-Patterns\n[What looks right but is wrong]\n```\n\n### Step 3: Include Validation\n\n```python\n# scripts/validate.py\ndef validate_setup():\n    # Check environment, dependencies, config\n    pass\n```\n\n### Step 4: Test\n\n```bash\npython scripts/validate_skill.py your-skill/\n```\n\n## Quick Wins (Improve Existing Skills Fast)\n\nApply these immediately to existing skills:\n\n1. **Add NOT clause** to description ‚Üí Prevents false activation\n2. **Add 1-2 anti-patterns** ‚Üí Prevents common mistakes\n3. **Check line count** (`wc -l`) ‚Üí Should be <500\n4. **Remove dead files** ‚Üí Delete unreferenced scripts/references\n5. **Test activation** ‚Üí Ask questions that should/shouldn't trigger it\n\n## Validation Checklist (Prioritized)\n\n**CRITICAL** (must-have):\n- [ ] Description has keywords AND NOT clause\n- [ ] SKILL.md under 500 lines\n- [ ] All referenced files exist\n- [ ] Test activation: Does it activate when it should?\n- [ ] Test non-activation: Doesn't activate when it shouldn't?\n\n**HIGH PRIORITY** (should-have):\n- [ ] Has \"When to Use\" and \"When NOT to Use\" sections\n- [ ] Includes 1-3 anti-patterns with \"Why it's wrong\"\n- [ ] Encodes domain shibboleths (expert vs novice knowledge)\n- [ ] `allowed-tools` is minimal\n\n**NICE TO HAVE** (polish):\n- [ ] Temporal knowledge (what changed when)\n- [ ] Working code examples (not just templates)\n- [ ] References for deep dives\n- [ ] Bash restrictions if applicable\n\n## Real Examples\n\n### Good: CLIP-Aware Embeddings\n\nSee `/examples/good/clip-aware-embeddings/` for a skill that:\n- Knows when CLIP works and when it doesn't\n- Provides alternatives for each limitation\n- Includes validation scripts\n- Documents evolution (2021 ‚Üí 2025)\n- Has clear anti-patterns\n\n### Study This Example\n\nIt demonstrates:\n1. ‚úÖ Progressive disclosure\n2. ‚úÖ Anti-pattern detection\n3. ‚úÖ Temporal knowledge\n4. ‚úÖ Task-specific guidance\n5. ‚úÖ Validation tooling\n6. ‚úÖ Clear alternatives\n\n## Domain-Specific Shibboleths\n\nThese are the knowledge gaps where skills add most value:\n\n### ML/AI Models\n- CLIP limitations (counting, fine-grained, spatial)\n- When to use specialized models\n- Embedding model selection by task\n\n### Framework Evolution\n- Next.js: Pages Router ‚Üí App Router (2022)\n- React: Class Components ‚Üí Hooks (2019)\n- State Management: Redux ‚Üí Zustand (2020+)\n\n### Architecture\n- When to use MCP vs Scripts\n- Evolution from scripts ‚Üí library ‚Üí MCP\n- Security and performance tradeoffs\n\n**See `/references/antipatterns.md` for comprehensive catalog**\n\n## Best Practices\n\n### Description Field\n\n**Good**:\n```yaml\ndescription: Semantic image search with CLIP. Use for finding similar images, zero-shot classification. NOT for counting objects, fine-grained classification, or spatial reasoning. Mention CLIP, embeddings, image similarity.\n```\n\n**Bad**:\n```yaml\ndescription: Helps with images\n```\n\n### Progressive Structure\n\n**Good**:\n```markdown\n# Skill Name\n\n## Quick Decision Tree\n[Fast decision making]\n\n## Common Anti-Patterns\n[What to avoid]\n\n## Validation\n[How to check]\n\nSee /references/deep_dive.md for detailed theory\n```\n\n**Bad**:\n```markdown\n# Skill Name\n\n[50 pages of comprehensive tutorial]\n```\n\n### Validation\n\n**Good**:\n```python\n# scripts/validate.py\ndef check_environment():\n    \"\"\"Specific, actionable errors\"\"\"\n    if not has_model():\n        raise Error(\"Model X not found. Install: pip install x\")\n\ndef check_task_appropriate(query):\n    \"\"\"Task-specific validation\"\"\"\n    if \"count\" in query.lower():\n        raise Error(\"Use object detection for counting, not CLIP\")\n```\n\n**Bad**:\n```python\n# No validation script\n# Or generic \"check passed/failed\" with no guidance\n```\n\n## Tools & Scripts\n\n### Validate Skill Structure\n\n```bash\npython scripts/validate_skill.py your-skill/\n```\n\nChecks:\n- Required files and structure\n- Description quality\n- Line count (<500)\n- Progressive disclosure\n- Anti-patterns section\n- allowed-tools scope\n\n### Create New Skill\n\nAsk Claude:\n```\nUsing the skill-coach skill, help me create a new skill for [your domain].\nFocus on anti-patterns where novices get it wrong.\n```\n\n## Common Mistakes\n\n### ‚ùå Skill as Documentation Dump\n\nDon't create a 500-line tutorial. Create actionable instructions with references.\n\n### ‚ùå Missing \"NOT for\"\n\nWithout negative triggers, skills activate on false positives.\n\n### ‚ùå No Temporal Context\n\nLLMs suggest outdated patterns. Document what changed and when.\n\n### ‚ùå Overly Permissive Tools\n\n```yaml\nallowed-tools: Bash  # Can execute ANYTHING\n```\n\nBetter:\n```yaml\nallowed-tools: Bash(git:*,npm:run),Read,Write\n```\n\n### ‚ùå No Validation\n\nSkills should include scripts to check if environment is correct.\n\n## Integration with Other Tools\n\n### Works with MCP\n\nSkills can reference MCPs:\n```markdown\n## Requirements\n- GitHub MCP (for API access)\n- Scripts for local validation\n\nInstall: `/plugin marketplace add github-mcp`\n```\n\n### Works with Subagents\n\nSubagents can use skills for domain expertise:\n```\nSkill provides knowledge ‚Üí Subagent executes with tools\n```\n\n### Works with Projects\n\nSkills available across all conversations in a project.\n\n## Contributing Patterns\n\nWhen you discover a new anti-pattern:\n\n1. **Document what looks right but is wrong**\n2. **Explain WHY it's wrong** (fundamental reason)\n3. **Show the correct approach**\n4. **Add temporal context** (when did this change?)\n5. **Note why LLMs make this mistake**\n6. **Include detection/validation if possible**\n\n## Resources\n\n### In This Skill\n\n- `/references/antipatterns.md` - Comprehensive anti-pattern catalog\n- `/references/mcp_vs_scripts.md` - When to use what\n- `/examples/good/` - Exemplary skills to study\n- `/scripts/validate_skill.py` - Validation tool\n\n### External\n\n- [Anthropic Skills Docs](https://docs.claude.com/en/docs/agents-and-tools/agent-skills)\n- [Skills Explained](https://claude.com/blog/skills-explained)\n- [Equipping Agents](https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills)\n- [MCP Documentation](https://modelcontextprotocol.io/)\n\n## Version History\n\n### v2.0.0 (2025-11-24)\n**5 Iterations of Self-Improvement:**\n- Iteration 1: Added 5 skill-specific anti-patterns, Evolution Timeline, removed non-existent file references\n- Iteration 2: Added Common Workflows, Tool Permissions explanation, actionable iteration strategy\n- Iteration 3: Added Skill Creation Shibboleths, enhanced \"What Makes a Great Skill\" (5‚Üí7 items)\n- Iteration 4: Added Quick Wins, simplified structure, Description progression (Bad‚ÜíBetter‚ÜíGood)\n- Iteration 5: Added Decision Trees (when to create, Skill vs MCP), prioritized checklist\n- Result: 482 ‚Üí 470 lines, more concise yet comprehensive\n\n### v1.0.0 (2025-11-23)\n- Initial release\n- Comprehensive anti-patterns catalog\n- CLIP-aware embeddings example\n- Validation tooling\n- MCP vs Scripts guide\n\n---\n\n## Get Started\n\n1. Read SKILL.md **Quick Wins** for immediate improvements\n2. Study `/examples/good/clip-aware-embeddings/`\n3. Run validation on your existing skills\n4. Use this skill when creating new skills\n5. Share your domain-specific shibboleths\n\n**Remember**: Great skills don't just say \"here's how\" - they say \"here's how, and here's where everyone gets it wrong, and why, and what to use instead.\"\n"
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "skill-coach/SKILL.md",
      "size": 15655,
      "content": "---\nname: skill-coach\ndescription: \"Guides creation of high-quality Agent Skills with domain expertise, anti-pattern detection, and progressive disclosure best practices. Use when creating skills, reviewing existing skills, or when users mention improving skill quality, encoding expertise, or avoiding common AI tooling mistakes. Activate on keywords: create skill, review skill, skill quality, skill best practices, skill anti-patterns. NOT for general coding advice or non-skill Claude Code features.\"\nallowed-tools: Read,Write,Bash,Glob,Grep,Edit\n---\n\n# Skill Coach: Creating Expert-Level Agent Skills\n\nThis skill helps you create Agent Skills that encode real domain expertise, not just surface-level instructions. It focuses on the **shibboleths** - the deep knowledge that separates novices from experts.\n\n## When to Use This Skill\n\n‚úÖ **Use for:**\n- Creating new Agent Skills from scratch\n- Reviewing/auditing existing skills\n- Improving skill activation rates\n- Adding domain expertise to skills\n- Debugging why skills don't activate\n\n‚ùå **NOT for:**\n- General Claude Code features (slash commands, MCPs)\n- Non-skill coding advice\n- Debugging runtime errors in skills (use specific domain skills)\n- Project setup unrelated to skills\n\n## What Makes a Great Skill\n\nGreat skills are **progressive disclosure machines** that:\n1. **Activate precisely** - Specific keywords trigger, NOT clause prevents false activation\n2. **Encode shibboleths** - Expert knowledge that separates novice from expert approaches\n3. **Surface anti-patterns** - \"If you see X, that's wrong because Y, use Z instead\"\n4. **Capture temporal knowledge** - \"Pre-2024: X. 2024+: Y. Watch for: LLMs suggesting X\"\n5. **Know their limits** - \"Use this for A, B, C. NOT for D, E, F. For D use skill-name-2\"\n6. **Provide decision trees** - Not templates, but \"If X then A, if Y then B, never C\"\n7. **Stay under 500 lines** - Core in SKILL.md, deep dives in /references\n\n## Quick Wins\n\n**Immediate improvements for existing skills**:\n1. **Add NOT clause** to description ‚Üí Prevents false activation\n2. **Add 1-2 anti-patterns** ‚Üí Prevents common mistakes\n3. **Check line count** (`wc -l`) ‚Üí Should be fewer than 500 lines\n4. **Remove dead files** ‚Üí Delete unreferenced scripts/references\n5. **Test activation** ‚Üí Ask questions that should/shouldn't trigger it\n\n## Quick Start\n\n**Creating a New Skill**:\n1. Define scope: What expertise? What keywords? What NOT to handle?\n2. Write description with keywords and NOT clause\n3. Add anti-patterns you've observed\n4. Test activation thoroughly\n5. Use Review Checklist below\n\n## Core Principles\n\n### 1. Progressive Disclosure Architecture\n\nSkills load in three phases:\n- **Phase 1 (~100 tokens)**: Metadata (name, description) - \"Should I activate?\"\n- **Phase 2 (under 5k tokens)**: Main instructions in SKILL.md - \"How do I do this?\"\n- **Phase 3 (as needed)**: Scripts, references, assets - \"Show me the details\"\n\n**Critical**: Keep SKILL.md under 500 lines. Split details into `/references`.\n\n### 2. Description Field Design\n\nThe description is your activation trigger. Formula: **[What] [Use for] [Keywords] NOT for [Exclusions]**\n\n**Progression from Bad ‚Üí Good**:\n\n‚ùå **Bad**: `description: Helps with images`\n- Too vague, no keywords, no exclusions\n\n‚ö†Ô∏è **Better**: `description: Image processing with CLIP`\n- Has keyword (CLIP) but no use cases or exclusions\n\n‚úÖ **Good**: `description: CLIP semantic search. Use for image-text matching, zero-shot classification. Activate on \"CLIP\", \"embeddings\", \"image search\". NOT for counting, fine-grained classification, spatial reasoning.`\n- Clear capability, use cases, keywords, and exclusions\n\n### 3. Anti-Pattern Detection\n\nGreat skills actively warn about common mistakes. Structure:\n\n```markdown\n## Common Anti-Patterns\n\n### Pattern: [Name]\n**What it looks like**: [Code example or description]\n**Why it's wrong**: [Fundamental reason]\n**What to do instead**: [Better approach]\n**How to detect**: [Validation rule]\n```\n\n### 4. Temporal Knowledge\n\nTechnology evolves. Capture what changed and when:\n\n```markdown\n## Evolution Timeline\n\n### Pre-2024: Old Approach\n[What people used to do]\n\n### 2024-Present: Current Best Practice\n[What changed and why]\n\n### Watch For\n[Deprecated patterns LLMs might still suggest]\n```\n\n## Skill Structure\n\n**Mandatory**:\n```\nyour-skill/\n‚îî‚îÄ‚îÄ SKILL.md           # Core instructions (max 500 lines)\n```\n\n**Optional** (only if needed):\n```\n‚îú‚îÄ‚îÄ scripts/           # Working code (not templates)\n‚îú‚îÄ‚îÄ references/        # Deep dives (referenced from SKILL.md)\n‚îú‚îÄ‚îÄ assets/            # Config files, templates\n‚îî‚îÄ‚îÄ examples/          # Concrete good/bad examples\n```\n\n**Anti-pattern**: Creating structure \"just in case\" - only add files that SKILL.md references\n\n## SKILL.md Template Structure\n\n```markdown\n---\nname: your-skill-name\ndescription: [What] [When] [Triggers]. NOT for [Exclusions].\nallowed-tools: Read,Write  # Minimal only\n---\n\n# Skill Name\n[One sentence purpose]\n\n## When to Use\n‚úÖ Use for: [A, B, C]\n‚ùå NOT for: [D, E, F]\n\n## Core Instructions\n[Step-by-step, decision trees, not templates]\n\n## Common Anti-Patterns\n### [Pattern]\n**Symptom**: [Recognition]\n**Problem**: [Why wrong]\n**Solution**: [Better approach]\n```\n\n## Anti-Patterns in Skill Creation\n\n### Anti-Pattern: The Reference Illusion\n\n**What it looks like**: Skill references scripts/files that don't exist\n```yaml\n# Quick Start\nRun `python scripts/validate.py` to check your skill\n```\nBut `/scripts/validate.py` doesn't exist in the skill directory.\n\n**Why it's wrong**: Claude will try to use non-existent files, causing errors and confusion.\n\n**What to do instead**: Only reference files that actually exist. If you want to suggest scripts, either:\n1. Include them in the skill\n2. Show inline code examples\n3. Clearly mark as \"Example - not included\"\n\n**How to detect**: `find skill-dir/ -type f` and verify all referenced paths exist\n\n### Anti-Pattern: Description Soup\n\n**What it looks like**:\n```yaml\ndescription: Helps with many things including X, Y, Z, and also A, B, C, plus general assistance\n```\n\n**Why it's wrong**: Vague descriptions cause:\n- False activations (activates when shouldn't)\n- Missed activations (doesn't activate when should)\n- Token waste (loads unnecessary context)\n\n**What to do instead**: Specific trigger keywords + clear exclusions\n```yaml\ndescription: [Core capability]. Use for [A, B, C]. Activate on keywords: \"X\", \"Y\", \"Z\". NOT for [D, E, F].\n```\n\n### Anti-Pattern: Template Theater\n\n**What it looks like**: Skill is 90% templates and examples, 10% actual instructions\n\n**Why it's wrong**: Claude doesn't need templates - it needs expert knowledge and decision trees. Templates are for humans.\n\n**What to do instead**:\n- Focus on WHEN to use patterns, not just WHAT the patterns are\n- Encode decision logic: \"If X, use A; if Y, use B; never use C\"\n- Include anti-patterns and edge cases\n\n### Anti-Pattern: The Everything Skill\n\n**What it looks like**: One skill trying to handle an entire domain\n```yaml\nname: web-dev-expert\ndescription: Handles all web development tasks\n```\n\n**Why it's wrong**:\n- Too broad to activate correctly\n- Mixes concerns (React ‚â† API design ‚â† CSS)\n- Violates progressive disclosure\n\n**What to do instead**: Create focused, composable skills:\n- `react-performance-expert`\n- `api-design-expert`\n- `css-layout-expert`\n\n### Anti-Pattern: Orphaned Sections\n\n**What it looks like**: Skill has `/references/deep_dive.md` but never tells Claude when to read it\n\n**Why it's wrong**: Files exist but are never used = wasted space\n\n**What to do instead**: Explicit triggers in main SKILL.md:\n```markdown\nFor database-specific anti-patterns, see `/references/database_antipatterns.md`\n```\n\n## Evolution Timeline: Skill Framework Best Practices\n\n### 2024 Early: First Skills\n- Basic SKILL.md files\n- Heavy use of bash scripts\n- Minimal structure\n\n### 2024 Mid: Progressive Disclosure\n- Introduction of phased loading\n- `allowed-tools` constraints\n- Reference directory pattern\n\n### 2024 Late: Anti-Pattern Focus\n- Shift from \"what to do\" to \"what NOT to do\"\n- Temporal knowledge capture\n- Shibboleth encoding\n\n### 2025: Current Best Practices\n- Sub-500 line SKILL.md\n- Validation-first approach\n- Clear activation boundaries\n- Working code examples (not just templates)\n\n## Domain-Specific Shibboleths\n\nShibboleths = deep knowledge that separates novices from experts.\n\n### Skill Creation Shibboleths\n\n**Novice skill creator**:\n- \"I'll make a comprehensive skill that handles everything related to X\"\n- Focuses on templates and examples\n- Description: \"Helps with many things\"\n- Thinks more tools = better\n\n**Expert skill creator**:\n- \"I'll create a focused skill that encodes THIS specific expertise about X\"\n- Focuses on decision trees and anti-patterns\n- Description: \"Does A, B, C. Activate on keywords X, Y. NOT for D, E, F.\"\n- Minimal tools, knows when NOT to use the skill\n- Encodes temporal knowledge: \"Pre-2024 pattern X was common, now use Y\"\n\n### Domain Example Shibboleths\n\n**CLIP Embeddings**:\n- Novice: \"CLIP is great for image-text matching\"\n- Expert: \"CLIP fails at: counting, fine-grained classification, attribute binding, spatial relationships, negation. Use DCSMs for compositional, PC-CLIP for geometric, specialized models for counting.\"\n\n**MCPs vs Scripts**:\n- Novice: \"MCPs are better because they're more powerful\"\n- Expert: \"MCP for auth/external APIs. Script for local/stateless. Building an MCP when a script would suffice = anti-pattern.\"\n\n## Validation Best Practices\n\n**Plan-Validate-Execute Pattern**:\n1. Generate plan ‚Üí 2. Validate BEFORE execution ‚Üí 3. Execute ‚Üí 4. Verify\n\n**Pre-Flight Checks** (include in skills that modify state):\n- Structure validation (files exist, naming conventions)\n- Description quality (keywords, exclusions, length)\n- Anti-pattern detection\n- Progressive disclosure compliance\n- Line count (max 500 for SKILL.md)\n\n## Example: Good vs Bad Skills\n\n**Good Skill** - Specific, expert knowledge, clear boundaries:\n```yaml\nname: clip-aware-embeddings\ndescription: CLIP semantic search. Use for image-text matching, zero-shot classification. Activate on \"CLIP\", \"embeddings\", \"image search\". NOT for counting, fine-grained classification, spatial reasoning.\n\n‚úÖ Includes: When NOT to use, alternatives (DETR/PC-CLIP), temporal evolution, anti-patterns\n```\n\n**Bad Skill** - Vague, template-heavy, no expertise:\n```yaml\nname: image-processing\ndescription: Processes images\n\n‚ùå Problems: No activation triggers, no exclusions, no expert knowledge, just generic templates\n```\n\n## Integration with Other Tools\n\n### Works Well With\n\n- **MCP Servers**: For API access, skill provides the workflow\n- **Subagents**: Skill gives expertise, subagent gets tool permissions\n- **Projects**: Skill available across all conversations\n\n### Conflicts With\n\n- **Overly specific prompts**: Skill already encodes the pattern\n- **Too many tools**: Use `allowed-tools` to constrain scope\n\n## Common Workflows\n\n**Workflow 1: Create Skill from Expertise**\n1. You have domain expertise ‚Üí Activate skill-coach\n2. Ask: \"Help me create a skill for [domain]\"\n3. Define scope, keywords, exclusions\n4. Encode shibboleths (expert knowledge)\n5. Add anti-patterns you've observed\n6. Test activation\n\n**Workflow 2: Debug Activation Issues**\n1. Skill not activating ‚Üí Activate skill-coach\n2. Ask: \"Review my skill's description and activation triggers\"\n3. Add missing keywords\n4. Clarify NOT clause\n5. Test with specific phrases\n\n**Workflow 3: Reduce False Activations**\n1. Skill activates too often ‚Üí Activate skill-coach\n2. Ask: \"Help me narrow this skill's scope\"\n3. Add NOT clause with exclusions\n4. Consider splitting into multiple focused skills\n5. Test edge cases\n\n## Iterating on Skills\n\n**Improvement Loop** (use Claude to improve skills):\n```bash\n# 1. Use the skill on real tasks\n# 2. Ask: \"What anti-patterns did you encounter?\"\n# 3. Ask: \"What decision points were unclear?\"\n# 4. Update SKILL.md with learnings\n# 5. Test: Does it activate correctly now?\n```\n\n**Red Flags**:\n- Skill doesn't activate when it should ‚Üí Fix description keywords\n- Activates too often ‚Üí Add NOT clause\n- Claude ignores sections ‚Üí Move to main SKILL.md or delete\n- Claude can't find referenced files ‚Üí Remove or create them\n\n## Tool Permissions\n\n**This skill uses**: `Read,Write,Bash,Glob,Grep,Edit`\n- **Read,Glob,Grep**: Find and read existing skills\n- **Edit**: Update skills in place\n- **Write**: Create new skill files\n- **Bash**: Validate file structure (`find`, `wc -l`)\n\n**Guidelines**:\n- Read-only skill: `Read,Grep,Glob`\n- File modifier: `Read,Write,Edit`\n- Build integration: `Read,Write,Bash(npm:*,git:*)`\n- ‚ö†Ô∏è **Never**: Unrestricted `Bash` for untrusted skills\n\n**Security Audit**:\n- [ ] Read all scripts before enabling skill\n- [ ] Check for network calls / data exfiltration\n- [ ] Verify allowed-tools are minimal\n- [ ] Test in isolated project first\n\n## Skill Review Checklist\n\n**CRITICAL** (must-have):\n- [ ] Description has keywords AND NOT clause\n- [ ] SKILL.md under 500 lines\n- [ ] All referenced files exist (`find skill-dir/ -type f`)\n- [ ] Test activation: Does it activate when it should?\n- [ ] Test non-activation: Does it NOT activate when it shouldn't?\n\n**HIGH PRIORITY** (should-have):\n- [ ] Has \"When to Use\" and \"When NOT to Use\" sections\n- [ ] Includes 1-3 anti-patterns with \"Why it's wrong\"\n- [ ] Encodes domain shibboleths (expert vs novice knowledge)\n- [ ] `allowed-tools` is minimal\n\n**NICE TO HAVE** (polish):\n- [ ] Temporal knowledge (what changed when)\n- [ ] Working code examples (not just templates)\n- [ ] References for deep dives\n- [ ] Bash restrictions if applicable\n\n## Testing Your Skill\n\n### Activation Test\n\nAsk Claude questions that SHOULD trigger the skill:\n```bash\n# Example for a React skill:\n\"Help me optimize this React component's re-renders\"\n# Check: Did the skill activate?\n```\n\nAsk questions that SHOULD NOT trigger the skill:\n```bash\n# Example for a React skill:\n\"Help me write a Python script\"\n# Check: Did it correctly NOT activate?\n```\n\n### Integration Test\n\n- Test with related skills (do they conflict or complement?)\n- Test with MCPs (does skill guide MCP usage?)\n- Test in different project contexts\n\n## Decision Trees\n\n**When to create a NEW skill?**\n- ‚úÖ You have domain expertise not in existing skills\n- ‚úÖ Pattern repeats across 3+ projects\n- ‚úÖ Anti-patterns you want to prevent\n- ‚ùå One-time task ‚Üí Just do it directly\n- ‚ùå Existing skill could be extended ‚Üí Improve that one\n\n**Skill vs Subagent vs MCP?**\n- **Skill**: Domain expertise, decision trees, anti-patterns (no runtime state)\n- **Subagent**: Multi-step workflows needing tool orchestration\n- **MCP**: External APIs, auth, stateful connections\n\n## Common Questions\n\n**Q: SKILL.md vs /references?**\nSKILL.md: Core instructions (max 500 lines). /references: Deep dives (loaded as needed).\n\n**Q: How do I handle deprecated patterns?**\n```markdown\n## ‚ö†Ô∏è Deprecated: [Pattern]\n**Until**: [Date] | **Why**: [Reason] | **Now use**: [Current]\n**Watch**: LLMs may suggest this due to training data\n```\n\n## Success Metrics\n\n- **Activation**: 90%+ when appropriate, under 5% false positives\n- **Token efficiency**: Under 5k tokens typical invocation\n- **Error prevention**: Measurable reduction in common mistakes\n\n---\n\n**This skill guides**: Skill creation | Skill auditing | Anti-pattern detection | Progressive disclosure | Domain expertise encoding\n\n**Meta-note**: This skill practices what it preaches - it has been iteratively improved using its own guidance, demonstrating the iteration loop it recommends.\n"
    }
  ]
}