{
  "name": "automatic-stateful-prompt-improver",
  "type": "folder",
  "path": "automatic-stateful-prompt-improver",
  "children": [
    {
      "name": "references",
      "type": "folder",
      "path": "automatic-stateful-prompt-improver/references",
      "children": [
        {
          "name": "ape-opro-implementation.md",
          "type": "file",
          "path": "automatic-stateful-prompt-improver/references/ape-opro-implementation.md",
          "size": 20062,
          "content": "# APE and OPRO Implementation Guide\n\nThis document details the implementation of Automatic Prompt Engineer (APE) and Optimization by Prompting (OPRO) algorithms for prompt improvement.\n\n## APE: Automatic Prompt Engineer\n\n**Source**: Zhou et al., 2022 - \"Large Language Models Are Human-Level Prompt Engineers\"\n\n### Core Idea\n\nUse LLMs to generate instruction candidates, then select the best based on evaluation.\n\n### Algorithm\n\n```\nAPE Algorithm:\n1. Given: Task description T, examples E = {(x_i, y_i)}\n2. Generate instruction candidates:\n   - Prompt LLM: \"Given these examples, what instruction would produce this output?\"\n   - Generate N candidates (N=20 typical)\n3. Evaluate each candidate:\n   - For each instruction I_j:\n     - Score = accuracy on held-out examples\n     - Or: Score = log_prob(y | I_j, x)\n4. Select best:\n   - I* = argmax_j Score(I_j)\n5. Optionally refine:\n   - Generate variations of I*\n   - Re-evaluate and select best variation\n```\n\n### Implementation\n\n```python\nclass APEOptimizer:\n    \"\"\"\n    Automatic Prompt Engineer implementation.\n    \"\"\"\n\n    def __init__(self, llm_client, num_candidates: int = 20):\n        self.llm = llm_client\n        self.num_candidates = num_candidates\n\n    async def generate_candidates(\n        self,\n        task_description: str,\n        examples: list[tuple[str, str]],\n        num_candidates: int = None\n    ) -> list[str]:\n        \"\"\"\n        Generate instruction candidates from examples.\n        \"\"\"\n        n = num_candidates or self.num_candidates\n\n        # Format examples\n        examples_text = \"\\n\".join([\n            f\"Input: {x}\\nOutput: {y}\"\n            for x, y in examples[:5]  # Use first 5 examples\n        ])\n\n        generation_prompt = f\"\"\"\n        I have a task where given certain inputs, I want specific outputs.\n\n        Here are some examples:\n        {examples_text}\n\n        Your task: Generate {n} different instructions that would cause an AI to produce these outputs from these inputs.\n\n        Requirements:\n        - Each instruction should be clear and complete\n        - Instructions should be diverse (different phrasings, approaches)\n        - Instructions should be concise but complete\n\n        Generate exactly {n} instructions, one per line, numbered 1-{n}:\n        \"\"\"\n\n        response = await self.llm.generate(generation_prompt)\n        candidates = self._parse_numbered_list(response)\n\n        return candidates[:n]\n\n    async def evaluate_candidate(\n        self,\n        instruction: str,\n        eval_examples: list[tuple[str, str]]\n    ) -> float:\n        \"\"\"\n        Evaluate an instruction on held-out examples.\n        \"\"\"\n        correct = 0\n        total = len(eval_examples)\n\n        for x, expected_y in eval_examples:\n            prompt = f\"{instruction}\\n\\nInput: {x}\\n\\nOutput:\"\n            response = await self.llm.generate(prompt)\n\n            # Simple exact match (could use semantic similarity)\n            if self._normalize(response) == self._normalize(expected_y):\n                correct += 1\n\n        return correct / total if total > 0 else 0\n\n    async def optimize(\n        self,\n        task_description: str,\n        examples: list[tuple[str, str]],\n        refinement_iterations: int = 3\n    ) -> tuple[str, float]:\n        \"\"\"\n        Full APE optimization pipeline.\n\n        Returns:\n            (best_instruction, best_score)\n        \"\"\"\n        # Split examples\n        train_examples = examples[:len(examples)//2]\n        eval_examples = examples[len(examples)//2:]\n\n        # Generate initial candidates\n        candidates = await self.generate_candidates(\n            task_description,\n            train_examples\n        )\n\n        # Evaluate each\n        scores = []\n        for candidate in candidates:\n            score = await self.evaluate_candidate(candidate, eval_examples)\n            scores.append((candidate, score))\n\n        # Sort by score\n        scores.sort(key=lambda x: x[1], reverse=True)\n        best_instruction, best_score = scores[0]\n\n        # Refinement iterations\n        for i in range(refinement_iterations):\n            # Generate variations of best\n            variations = await self._generate_variations(best_instruction)\n\n            # Evaluate variations\n            for variation in variations:\n                score = await self.evaluate_candidate(variation, eval_examples)\n                if score > best_score:\n                    best_instruction = variation\n                    best_score = score\n\n        return best_instruction, best_score\n\n    async def _generate_variations(\n        self,\n        instruction: str,\n        num_variations: int = 5\n    ) -> list[str]:\n        \"\"\"\n        Generate variations of an instruction.\n        \"\"\"\n        variation_prompt = f\"\"\"\n        Generate {num_variations} variations of the following instruction.\n        Keep the semantic meaning but vary the phrasing, structure, or emphasis.\n\n        Original instruction:\n        {instruction}\n\n        Variations (numbered 1-{num_variations}):\n        \"\"\"\n\n        response = await self.llm.generate(variation_prompt)\n        return self._parse_numbered_list(response)\n\n    def _parse_numbered_list(self, text: str) -> list[str]:\n        \"\"\"Parse numbered list from LLM output.\"\"\"\n        lines = text.strip().split('\\n')\n        items = []\n        for line in lines:\n            # Remove numbering (1., 1), etc.)\n            cleaned = re.sub(r'^[\\d]+[\\.\\)\\:]?\\s*', '', line.strip())\n            if cleaned:\n                items.append(cleaned)\n        return items\n\n    def _normalize(self, text: str) -> str:\n        \"\"\"Normalize text for comparison.\"\"\"\n        return text.strip().lower()\n```\n\n### APE for Prompt Improvement\n\n```python\nasync def improve_prompt_ape_style(\n    original_prompt: str,\n    examples: list[tuple[str, str]],\n    llm_client\n) -> str:\n    \"\"\"\n    Improve a prompt using APE methodology.\n    \"\"\"\n    optimizer = APEOptimizer(llm_client)\n\n    # Use original prompt as task description\n    task_description = f\"Original prompt: {original_prompt}\"\n\n    # Optimize\n    best_instruction, score = await optimizer.optimize(\n        task_description,\n        examples,\n        refinement_iterations=3\n    )\n\n    return best_instruction\n```\n\n## OPRO: Optimization by Prompting\n\n**Source**: Yang et al., 2023 - \"Large Language Models as Optimizers\"\n\n### Core Idea\n\nTreat optimization itself as a prompting task. The LLM sees previous solutions and their scores, then generates new (hopefully better) solutions.\n\n### Algorithm\n\n```\nOPRO Algorithm:\n1. Initialize: meta_prompt = empty history\n2. For i in range(max_iterations):\n   a. Add instruction to meta_prompt:\n      \"[previous solutions + scores]\n       Generate a new solution that scores higher.\"\n   b. LLM generates new candidate solution\n   c. Evaluate candidate on validation set\n   d. Add (candidate, score) to history\n   e. If converged: break\n3. Return best solution from history\n```\n\n### Implementation\n\n```python\nclass OPROOptimizer:\n    \"\"\"\n    Optimization by Prompting implementation.\n    \"\"\"\n\n    def __init__(\n        self,\n        llm_client,\n        max_history: int = 20,\n        temperature: float = 0.7\n    ):\n        self.llm = llm_client\n        self.max_history = max_history\n        self.temperature = temperature\n        self.history = []\n\n    def _build_meta_prompt(self, task_description: str) -> str:\n        \"\"\"\n        Build the meta-prompt showing previous solutions and scores.\n        \"\"\"\n        # Sort history by score (ascending) - show progression\n        sorted_history = sorted(self.history, key=lambda x: x[1])\n\n        # Take most recent/best entries\n        recent = sorted_history[-self.max_history:]\n\n        history_text = \"\\n\".join([\n            f\"Instruction: {inst}\\nScore: {score:.3f}\"\n            for inst, score in recent\n        ])\n\n        meta_prompt = f\"\"\"\n        Your task is to generate an instruction that will achieve a high score on this task:\n\n        {task_description}\n\n        Here are some previous instructions and their scores (higher is better):\n\n        {history_text}\n\n        Based on the patterns in what works well:\n        1. Analyze why high-scoring instructions perform better\n        2. Identify elements that correlate with success\n        3. Generate a NEW instruction that should score even higher\n\n        Requirements:\n        - The instruction should be different from previous ones\n        - Build on what worked, avoid what didn't\n        - Be specific and clear\n\n        New instruction (just the instruction, nothing else):\n        \"\"\"\n\n        return meta_prompt\n\n    async def generate_candidate(self, task_description: str) -> str:\n        \"\"\"\n        Generate a new candidate instruction based on history.\n        \"\"\"\n        meta_prompt = self._build_meta_prompt(task_description)\n\n        response = await self.llm.generate(\n            meta_prompt,\n            temperature=self.temperature\n        )\n\n        return response.strip()\n\n    async def evaluate(\n        self,\n        instruction: str,\n        eval_fn: callable\n    ) -> float:\n        \"\"\"\n        Evaluate an instruction using provided evaluation function.\n        \"\"\"\n        return await eval_fn(instruction)\n\n    async def optimize(\n        self,\n        task_description: str,\n        eval_fn: callable,\n        initial_candidates: list[str] = None,\n        max_iterations: int = 20,\n        convergence_threshold: float = 0.01,\n        convergence_window: int = 5\n    ) -> tuple[str, float, list]:\n        \"\"\"\n        Run OPRO optimization.\n\n        Args:\n            task_description: Description of the task\n            eval_fn: Async function that scores an instruction\n            initial_candidates: Optional starting candidates\n            max_iterations: Maximum optimization iterations\n            convergence_threshold: Stop if improvement < this\n            convergence_window: Check convergence over this many iterations\n\n        Returns:\n            (best_instruction, best_score, optimization_history)\n        \"\"\"\n        # Initialize with any provided candidates\n        if initial_candidates:\n            for candidate in initial_candidates:\n                score = await self.evaluate(candidate, eval_fn)\n                self.history.append((candidate, score))\n\n        # Track scores for convergence detection\n        scores_over_time = [h[1] for h in self.history]\n\n        for i in range(max_iterations):\n            # Generate new candidate\n            candidate = await self.generate_candidate(task_description)\n\n            # Evaluate\n            score = await self.evaluate(candidate, eval_fn)\n\n            # Add to history\n            self.history.append((candidate, score))\n            scores_over_time.append(score)\n\n            # Check convergence\n            if len(scores_over_time) >= convergence_window:\n                recent_best = max(scores_over_time[-convergence_window:])\n                previous_best = max(scores_over_time[:-convergence_window])\n                if recent_best - previous_best < convergence_threshold:\n                    break\n\n        # Find best\n        best_instruction, best_score = max(self.history, key=lambda x: x[1])\n\n        return best_instruction, best_score, self.history\n```\n\n### OPRO for Prompt Improvement\n\n```python\nasync def improve_prompt_opro_style(\n    original_prompt: str,\n    eval_examples: list[tuple[str, str]],\n    llm_client,\n    max_iterations: int = 15\n) -> tuple[str, float]:\n    \"\"\"\n    Improve a prompt using OPRO methodology.\n    \"\"\"\n    optimizer = OPROOptimizer(llm_client)\n\n    # Create evaluation function\n    async def eval_fn(instruction: str) -> float:\n        correct = 0\n        for x, expected_y in eval_examples:\n            prompt = f\"{instruction}\\n\\nInput: {x}\"\n            response = await llm_client.generate(prompt)\n            if expected_y.lower() in response.lower():\n                correct += 1\n        return correct / len(eval_examples)\n\n    # Task description\n    task_description = f\"\"\"\n    Original prompt: {original_prompt}\n\n    The goal is to find an instruction that produces correct outputs for various inputs.\n    The instruction should be clear, specific, and effective.\n    \"\"\"\n\n    # Start with original as baseline\n    initial_candidates = [original_prompt]\n\n    # Optimize\n    best, score, history = await optimizer.optimize(\n        task_description=task_description,\n        eval_fn=eval_fn,\n        initial_candidates=initial_candidates,\n        max_iterations=max_iterations\n    )\n\n    return best, score\n```\n\n## Combining APE and OPRO\n\n### Hybrid Approach\n\n```python\nclass HybridPromptOptimizer:\n    \"\"\"\n    Combines APE (generation) with OPRO (optimization) for best results.\n\n    1. APE generates diverse initial candidates\n    2. OPRO iteratively improves using history\n    \"\"\"\n\n    def __init__(self, llm_client):\n        self.llm = llm_client\n        self.ape = APEOptimizer(llm_client, num_candidates=10)\n        self.opro = OPROOptimizer(llm_client, max_history=20)\n\n    async def optimize(\n        self,\n        original_prompt: str,\n        examples: list[tuple[str, str]],\n        max_iterations: int = 20\n    ) -> tuple[str, float, dict]:\n        \"\"\"\n        Hybrid optimization combining APE and OPRO.\n        \"\"\"\n        # Split examples\n        train_examples = examples[:len(examples)//2]\n        eval_examples = examples[len(examples)//2:]\n\n        # Phase 1: APE generates initial candidates\n        candidates = await self.ape.generate_candidates(\n            f\"Task: {original_prompt}\",\n            train_examples,\n            num_candidates=10\n        )\n\n        # Add original prompt as candidate\n        candidates = [original_prompt] + candidates\n\n        # Evaluate initial candidates\n        initial_scores = []\n        for candidate in candidates:\n            score = await self._evaluate(candidate, eval_examples)\n            initial_scores.append((candidate, score))\n\n        # Sort and take top 5 as OPRO seed\n        initial_scores.sort(key=lambda x: x[1], reverse=True)\n        top_candidates = [c for c, s in initial_scores[:5]]\n\n        # Phase 2: OPRO iterative optimization\n        task_description = f\"\"\"\n        Task: Generate instructions for this prompt optimization task.\n        Original prompt: {original_prompt}\n\n        The instruction should make the AI produce correct outputs for various inputs.\n        \"\"\"\n\n        # Initialize OPRO with APE candidates\n        for candidate, score in initial_scores[:5]:\n            self.opro.history.append((candidate, score))\n\n        # OPRO iterations\n        async def eval_fn(instruction):\n            return await self._evaluate(instruction, eval_examples)\n\n        best, score, history = await self.opro.optimize(\n            task_description=task_description,\n            eval_fn=eval_fn,\n            max_iterations=max_iterations - 5  # Already did 5 in APE\n        )\n\n        return best, score, {\n            'ape_candidates': len(candidates),\n            'opro_iterations': len(history) - 5,\n            'final_history': history\n        }\n\n    async def _evaluate(\n        self,\n        instruction: str,\n        examples: list[tuple[str, str]]\n    ) -> float:\n        \"\"\"Simple evaluation function.\"\"\"\n        correct = 0\n        for x, expected_y in examples:\n            prompt = f\"{instruction}\\n\\nInput: {x}\"\n            response = await self.llm.generate(prompt)\n            if self._matches(response, expected_y):\n                correct += 1\n        return correct / len(examples)\n\n    def _matches(self, response: str, expected: str) -> bool:\n        \"\"\"Check if response matches expected.\"\"\"\n        return expected.lower().strip() in response.lower()\n```\n\n## Without LLM Calls: Pattern-Based Improvement\n\nWhen you can't make LLM calls for optimization, use pattern-based rules:\n\n```python\nclass PatternBasedOptimizer:\n    \"\"\"\n    Improve prompts using known-good patterns without LLM calls.\n    \"\"\"\n\n    IMPROVEMENT_PATTERNS = [\n        {\n            \"name\": \"add_structure\",\n            \"check\": lambda p: not any(x in p.lower() for x in [\"1.\", \"2.\", \"step\", \"first\"]),\n            \"apply\": lambda p: f\"{p}\\n\\nProvide your response in this format:\\n1. [First point]\\n2. [Second point]\\n3. [Summary]\",\n            \"expected_improvement\": 0.15\n        },\n        {\n            \"name\": \"add_cot\",\n            \"check\": lambda p: \"step by step\" not in p.lower() and \"think\" not in p.lower(),\n            \"apply\": lambda p: f\"{p}\\n\\nThink through this step by step before providing your final answer.\",\n            \"expected_improvement\": 0.20\n        },\n        {\n            \"name\": \"add_constraints\",\n            \"check\": lambda p: len(p) < 100,\n            \"apply\": lambda p: f\"{p}\\n\\nRequirements:\\n- Be specific and precise\\n- Support claims with evidence\\n- Keep response focused\",\n            \"expected_improvement\": 0.10\n        },\n        {\n            \"name\": \"add_role\",\n            \"check\": lambda p: not p.lower().startswith((\"you are\", \"as a\", \"act as\")),\n            \"apply\": lambda p: f\"You are an expert in this domain. {p}\",\n            \"expected_improvement\": 0.05\n        },\n        {\n            \"name\": \"add_output_format\",\n            \"check\": lambda p: \"format\" not in p.lower() and \"json\" not in p.lower(),\n            \"apply\": lambda p: f\"{p}\\n\\nFormat your response as a clear, structured answer.\",\n            \"expected_improvement\": 0.10\n        }\n    ]\n\n    def improve(self, prompt: str) -> tuple[str, list[str]]:\n        \"\"\"\n        Apply applicable improvement patterns.\n\n        Returns:\n            (improved_prompt, list of applied patterns)\n        \"\"\"\n        improved = prompt\n        applied = []\n\n        for pattern in self.IMPROVEMENT_PATTERNS:\n            if pattern[\"check\"](improved):\n                improved = pattern[\"apply\"](improved)\n                applied.append(pattern[\"name\"])\n\n        return improved, applied\n\n    def estimate_improvement(self, applied_patterns: list[str]) -> float:\n        \"\"\"\n        Estimate expected improvement based on applied patterns.\n        \"\"\"\n        total = 0\n        for pattern in self.IMPROVEMENT_PATTERNS:\n            if pattern[\"name\"] in applied_patterns:\n                total += pattern[\"expected_improvement\"]\n        return min(total, 0.5)  # Cap at 50% improvement\n```\n\n## Convergence and Stopping Criteria\n\n### When to Stop Optimization\n\n```python\ndef check_optimization_convergence(\n    scores: list[float],\n    iteration: int,\n    config: dict = None\n) -> tuple[bool, str]:\n    \"\"\"\n    Check if optimization should stop.\n\n    Config options:\n    - max_iterations: Hard limit (default: 20)\n    - min_iterations: Minimum before stopping (default: 5)\n    - plateau_window: Iterations to check for plateau (default: 5)\n    - plateau_threshold: Minimum improvement (default: 0.01)\n    - target_score: Stop if reached (default: 0.95)\n    \"\"\"\n    config = config or {}\n    max_iter = config.get('max_iterations', 20)\n    min_iter = config.get('min_iterations', 5)\n    window = config.get('plateau_window', 5)\n    threshold = config.get('plateau_threshold', 0.01)\n    target = config.get('target_score', 0.95)\n\n    # Not enough iterations\n    if iteration < min_iter:\n        return False, \"below_minimum_iterations\"\n\n    # Max iterations reached\n    if iteration >= max_iter:\n        return True, \"max_iterations_reached\"\n\n    # Target reached\n    if scores[-1] >= target:\n        return True, f\"target_reached ({scores[-1]:.3f} >= {target})\"\n\n    # Plateau detection\n    if len(scores) >= window:\n        recent_improvement = max(scores[-window:]) - scores[-window]\n        if recent_improvement < threshold:\n            return True, f\"plateau_detected (improvement: {recent_improvement:.4f})\"\n\n    # Decreasing performance (possible overfitting)\n    if len(scores) >= 3 and all(scores[-i-1] < scores[-i-2] for i in range(2)):\n        return True, \"performance_decreasing\"\n\n    return False, \"continue\"\n```\n\n---\n\nThese implementations provide the foundation for systematic prompt optimization using research-backed algorithms.\n"
        },
        {
          "name": "dspy-patterns.md",
          "type": "file",
          "path": "automatic-stateful-prompt-improver/references/dspy-patterns.md",
          "size": 11842,
          "content": "# DSPy Optimization Patterns for Prompt Learning\n\nThis document covers DSPy-style programmatic prompt optimization patterns.\n\n## Core Philosophy\n\nDSPy treats prompts as **trainable parameters** rather than brittle strings. Instead of manually crafting prompts, you define:\n1. What the module should do (signature)\n2. How to measure success (metric)\n3. Let the optimizer find the best prompt\n\n## Key Concepts\n\n### Signatures\n\nSignatures declare input/output behavior:\n\n```python\n# Simple signature\nclass QA(dspy.Signature):\n    \"\"\"Answer questions with brief responses.\"\"\"\n    question = dspy.InputField()\n    answer = dspy.OutputField()\n\n# Complex signature with constraints\nclass CodeReview(dspy.Signature):\n    \"\"\"Review code for bugs, style issues, and improvements.\"\"\"\n    code = dspy.InputField(desc=\"The code to review\")\n    language = dspy.InputField(desc=\"Programming language\")\n    severity_filter = dspy.InputField(desc=\"min, major, critical\", default=\"min\")\n\n    bugs = dspy.OutputField(desc=\"List of potential bugs found\")\n    style_issues = dspy.OutputField(desc=\"Style violations\")\n    improvements = dspy.OutputField(desc=\"Suggested improvements\")\n```\n\n### Modules\n\nModules define HOW to accomplish the signature:\n\n```python\n# Basic prediction\npredictor = dspy.Predict(QA)\n\n# Chain-of-thought reasoning\ncot = dspy.ChainOfThought(QA)\n\n# ReAct with tool use\nreact = dspy.ReAct(QA, tools=[search_tool, calculator])\n\n# Multi-step pipeline\nclass RAGPipeline(dspy.Module):\n    def __init__(self, num_passages=3):\n        self.retrieve = dspy.Retrieve(k=num_passages)\n        self.generate = dspy.ChainOfThought(\"context, question -> answer\")\n\n    def forward(self, question):\n        context = self.retrieve(question).passages\n        return self.generate(context=context, question=question)\n```\n\n## Optimization Algorithms\n\n### BootstrapRS (Random Search)\n\n**Best for**: Quick optimization, small datasets\n\n```python\n# Generates few-shot examples from successful executions\noptimizer = dspy.BootstrapRS(\n    metric=my_metric,\n    max_bootstrapped_demos=4,  # Examples to include\n    max_labeled_demos=4,\n    num_candidate_programs=16\n)\n\noptimized = optimizer.compile(\n    student=my_module,\n    trainset=train_examples\n)\n```\n\n**How it works**:\n1. Run the module on training examples\n2. Identify successful executions\n3. Use those as few-shot demonstrations\n4. Select best combination randomly\n\n### MIPROv2 (Bayesian Optimization)\n\n**Best for**: Production optimization, larger datasets\n\n```python\noptimizer = dspy.MIPROv2(\n    metric=my_metric,\n    auto=\"medium\",  # light, medium, heavy\n    num_threads=4\n)\n\noptimized = optimizer.compile(\n    student=my_module,\n    trainset=train_examples,\n    valset=val_examples,  # Important for evaluation\n    max_bootstrapped_demos=4,\n    max_labeled_demos=4\n)\n```\n\n**How it works**:\n1. **Bootstrap**: Collect successful execution traces\n2. **Propose**: LLM generates instruction candidates\n3. **Search**: Bayesian optimization over instruction space\n4. **Refine**: Iteratively improve best candidates\n\n**Cost/Performance (from research)**:\n| Mode | Examples | Time | Cost | Improvement |\n|------|----------|------|------|-------------|\n| Light | 500 | 20min | ~$2 | 10-20% |\n| Medium | 1000 | 1-2hr | ~$10 | 20-35% |\n| Heavy | 2000+ | 4-8hr | ~$50 | 35-50% |\n\n### COPRO (Coordinate Optimization)\n\n**Best for**: Instruction-only optimization\n\n```python\noptimizer = dspy.COPRO(\n    metric=my_metric,\n    depth=3,  # Optimization iterations\n    breadth=10,  # Candidates per iteration\n    init_temperature=1.0\n)\n\n# Only optimizes instructions, not examples\noptimized = optimizer.compile(\n    student=my_module,\n    trainset=train_examples\n)\n```\n\n## Practical Patterns for Prompt Improvement\n\n### Pattern 1: Bootstrap → Optimize → Ensemble\n\n```python\ndef full_optimization_pipeline(module, trainset, valset):\n    \"\"\"\n    Three-stage optimization for best results.\n    \"\"\"\n    # Stage 1: Bootstrap few-shot examples\n    bootstrap = dspy.BootstrapRS(\n        metric=accuracy_metric,\n        max_bootstrapped_demos=4\n    )\n    bootstrapped = bootstrap.compile(module, trainset=trainset)\n\n    # Stage 2: Optimize instructions\n    mipro = dspy.MIPROv2(metric=accuracy_metric, auto=\"medium\")\n    optimized = mipro.compile(\n        bootstrapped,\n        trainset=trainset,\n        valset=valset\n    )\n\n    # Stage 3: Create ensemble (optional)\n    # Run optimization 3 times, ensemble the results\n    programs = []\n    for _ in range(3):\n        opt = dspy.MIPROv2(metric=accuracy_metric, auto=\"light\")\n        programs.append(opt.compile(module, trainset=trainset, valset=valset))\n\n    ensemble = dspy.Ensemble(programs, strategy=\"majority\")\n\n    return optimized, ensemble\n```\n\n### Pattern 2: Metric-Driven Optimization\n\n```python\ndef create_composite_metric():\n    \"\"\"\n    Multi-objective metric combining accuracy, cost, and quality.\n    \"\"\"\n    def metric(example, prediction, trace=None):\n        # Accuracy (0-1)\n        accuracy = 1.0 if prediction.answer == example.expected else 0.0\n\n        # Token efficiency (penalize verbose responses)\n        tokens_used = len(prediction.answer.split())\n        efficiency = 1.0 / (1.0 + tokens_used / 100)\n\n        # Quality heuristics\n        has_reasoning = \"because\" in prediction.answer.lower()\n        quality = 1.0 if has_reasoning else 0.5\n\n        # Weighted combination\n        return 0.6 * accuracy + 0.2 * efficiency + 0.2 * quality\n\n    return metric\n```\n\n### Pattern 3: Assertion-Based Validation\n\n```python\nclass ValidatedQA(dspy.Module):\n    \"\"\"\n    Module with built-in validation via assertions.\n    \"\"\"\n    def __init__(self):\n        self.generate = dspy.ChainOfThought(\"question -> answer\")\n\n    def forward(self, question):\n        response = self.generate(question=question)\n\n        # Validate response\n        dspy.Assert(\n            len(response.answer) > 10,\n            \"Answer too short - provide more detail\"\n        )\n\n        dspy.Assert(\n            response.answer[-1] in \".!?\",\n            \"Answer should end with punctuation\"\n        )\n\n        dspy.Suggest(\n            \"step\" in response.rationale.lower() or \"because\" in response.rationale.lower(),\n            \"Consider explaining your reasoning step by step\"\n        )\n\n        return response\n```\n\n### Pattern 4: Multi-Stage Pipelines\n\n```python\nclass OptimizedPipeline(dspy.Module):\n    \"\"\"\n    Multi-stage pipeline where each stage can be optimized independently.\n    \"\"\"\n    def __init__(self):\n        # Stage 1: Extract key information\n        self.extract = dspy.ChainOfThought(\n            \"document -> key_points, entities, summary\"\n        )\n\n        # Stage 2: Reason about the information\n        self.reason = dspy.ChainOfThought(\n            \"key_points, entities, question -> reasoning\"\n        )\n\n        # Stage 3: Generate final answer\n        self.answer = dspy.Predict(\n            \"summary, reasoning, question -> answer\"\n        )\n\n    def forward(self, document, question):\n        # Stage 1\n        extracted = self.extract(document=document)\n\n        # Stage 2\n        reasoned = self.reason(\n            key_points=extracted.key_points,\n            entities=extracted.entities,\n            question=question\n        )\n\n        # Stage 3\n        result = self.answer(\n            summary=extracted.summary,\n            reasoning=reasoned.reasoning,\n            question=question\n        )\n\n        return result\n```\n\n## Adapting DSPy Patterns for Claude Code\n\nSince we can't use DSPy's Python framework directly in Claude Code, we adapt the patterns:\n\n### Mental Bootstrapping\n\n```markdown\n## Bootstrap Pattern (Manual)\n\nWhen improving a prompt, I will:\n\n1. **Generate variations**: Create 5-10 instruction candidates\n2. **Mental evaluation**: For each, consider:\n   - Clarity: Is it unambiguous?\n   - Completeness: Does it cover all cases?\n   - Constraint density: Right amount of guidance?\n3. **Select best**: Choose highest-scoring candidate\n4. **Extract patterns**: What made it better?\n\nExample:\nOriginal: \"Summarize the text\"\n\nCandidates:\n- \"Extract the main points from this text\" (clarity: 7)\n- \"Provide a 3-sentence summary\" (constraint: 8)\n- \"Identify thesis, evidence, and conclusion\" (structure: 9)\n\nBest: Combine structure (9) + constraint (8)\nResult: \"Identify the thesis, key evidence, and conclusion, then summarize in 2-3 sentences\"\n```\n\n### Instruction Optimization Loop\n\n```markdown\n## COPRO Pattern (Manual)\n\nFor instruction optimization:\n\n1. **Baseline**: Evaluate current prompt\n2. **Generate candidates**: Create variations\n3. **Evaluate each**: Score on clarity/specificity/effectiveness\n4. **Select best**: Choose highest scorer\n5. **Iterate**: Use best as new baseline\n6. **Stop when**: Improvement < threshold or max iterations\n\nTrack:\n- Iteration 1: Score 0.65\n- Iteration 2: Score 0.72 (+0.07)\n- Iteration 3: Score 0.75 (+0.03)\n- Iteration 4: Score 0.76 (+0.01) ← Converging\n- STOP: Improvement < 0.02\n```\n\n### Few-Shot Selection\n\n```markdown\n## Example Selection Pattern\n\nWhen adding few-shot examples:\n\n1. **Diversity**: Cover different scenarios\n2. **Relevance**: Similar to expected inputs\n3. **Difficulty**: Progress from easy to hard\n4. **Quality**: Only include high-quality examples\n\nAnti-patterns:\n- All examples from same category ← Low coverage\n- Examples too different from test case ← Low relevance\n- All easy or all hard ← Imbalanced\n```\n\n## Convergence Criteria\n\n### When to Stop Optimizing\n\n```python\ndef should_stop_optimization(\n    scores: list[float],\n    iterations: int,\n    max_iterations: int = 20,\n    window: int = 3,\n    threshold: float = 0.01\n) -> tuple[bool, str]:\n    \"\"\"\n    Determine if optimization should stop.\n\n    Returns:\n        (should_stop, reason)\n    \"\"\"\n    # Max iterations reached\n    if iterations >= max_iterations:\n        return True, \"max_iterations_reached\"\n\n    # Not enough data\n    if len(scores) < window:\n        return False, \"insufficient_data\"\n\n    # Performance plateau\n    recent_improvement = max(scores[-window:]) - scores[-window]\n    if recent_improvement < threshold:\n        return True, f\"plateau_detected (improvement: {recent_improvement:.4f})\"\n\n    # High enough score\n    if scores[-1] > 0.95:\n        return True, f\"target_reached (score: {scores[-1]:.4f})\"\n\n    return False, \"continue\"\n```\n\n### Iteration Count Heuristics\n\nBased on research and empirical testing:\n\n| Task Type | Recommended Iterations | Reason |\n|-----------|----------------------|--------|\n| Simple classification | 3-5 | Small search space |\n| Text generation | 5-10 | Format variations |\n| Complex reasoning | 10-15 | Multiple strategies |\n| Multi-step pipeline | 15-20 | Component interactions |\n| Agent optimization | 20-30 | High complexity |\n\n## Implementation Notes\n\n### Memory Efficiency\n\n```python\n# Keep only top-k candidates to manage memory\nclass CandidatePool:\n    def __init__(self, max_size: int = 10):\n        self.candidates = []\n        self.max_size = max_size\n\n    def add(self, candidate: str, score: float):\n        self.candidates.append((candidate, score))\n        self.candidates.sort(key=lambda x: x[1], reverse=True)\n        self.candidates = self.candidates[:self.max_size]\n\n    def get_best(self) -> str:\n        return self.candidates[0][0] if self.candidates else None\n```\n\n### Reproducibility\n\n```python\n# Track optimization history for reproducibility\noptimization_history = {\n    \"initial_prompt\": \"...\",\n    \"iterations\": [\n        {\"candidate\": \"...\", \"score\": 0.65, \"selected\": False},\n        {\"candidate\": \"...\", \"score\": 0.72, \"selected\": True},\n    ],\n    \"final_prompt\": \"...\",\n    \"final_score\": 0.76,\n    \"convergence_reason\": \"plateau_detected\"\n}\n```\n\n---\n\nThese patterns form the foundation for systematic, measurable prompt optimization that can be applied even without access to DSPy's infrastructure.\n"
        },
        {
          "name": "embedding-architecture.md",
          "type": "file",
          "path": "automatic-stateful-prompt-improver/references/embedding-architecture.md",
          "size": 16096,
          "content": "# Embedding Architecture for Prompt Learning\n\nThis document details the embedding and retrieval architecture for stateful prompt learning.\n\n## Design Principles\n\n1. **Contextual Embeddings**: Add domain context before embedding (49% error reduction)\n2. **Hybrid Search**: Vector + BM25 for best recall\n3. **Recency Weighting**: Balance historical stability with recent relevance\n4. **Drift Detection**: Detect and adapt to distribution shifts\n\n## Embedding Strategy\n\n### Contextual Embedding Pipeline\n\nFollowing Anthropic's contextual retrieval methodology:\n\n```python\nclass ContextualPromptEmbedder:\n    \"\"\"\n    Create embeddings that include domain context for better retrieval.\n\n    Standard embedding: embed(prompt_text)\n    Contextual embedding: embed(context + prompt_text)\n\n    This reduces retrieval errors by 49% (with BM25) to 67% (with reranking).\n    \"\"\"\n\n    def __init__(self, embedding_model: str = \"text-embedding-3-large\"):\n        self.model = embedding_model\n        self.context_cache = {}  # Cache context generation\n\n    async def create_embedding(\n        self,\n        prompt: str,\n        domain: str,\n        task_type: str\n    ) -> dict:\n        \"\"\"\n        Create contextual embedding for a prompt.\n\n        Args:\n            prompt: The prompt text to embed\n            domain: Domain classification (e.g., 'code_review', 'summarization')\n            task_type: Task type (e.g., 'generation', 'classification')\n\n        Returns:\n            {\n                'embedding': [...],\n                'contextualized_text': str,\n                'metadata': dict\n            }\n        \"\"\"\n        # Generate contextualizing information\n        context = await self._generate_context(prompt, domain, task_type)\n\n        # Combine context with prompt\n        contextualized = f\"{context}\\n\\n{prompt}\"\n\n        # Generate embedding\n        embedding = await self._embed(contextualized)\n\n        return {\n            'embedding': embedding,\n            'contextualized_text': contextualized,\n            'metadata': {\n                'domain': domain,\n                'task_type': task_type,\n                'original_length': len(prompt),\n                'context_length': len(context)\n            }\n        }\n\n    async def _generate_context(\n        self,\n        prompt: str,\n        domain: str,\n        task_type: str\n    ) -> str:\n        \"\"\"\n        Generate 50-100 token context for a prompt.\n\n        Uses LLM to create contextualizing information that improves retrieval.\n        Results are cached to reduce API calls.\n        \"\"\"\n        cache_key = f\"{domain}:{task_type}:{hash(prompt[:100])}\"\n\n        if cache_key in self.context_cache:\n            return self.context_cache[cache_key]\n\n        context_prompt = f\"\"\"\n        Generate a brief context (50-100 tokens) for this prompt to improve retrieval.\n\n        Domain: {domain}\n        Task Type: {task_type}\n        Prompt: {prompt[:500]}\n\n        Include:\n        - Key domain terminology\n        - Task intent\n        - Important entities/concepts\n\n        Context only, no explanation:\n        \"\"\"\n\n        context = await self._call_llm(context_prompt)\n        self.context_cache[cache_key] = context\n        return context\n```\n\n### Embedding Model Selection\n\n| Model | Dimensions | Strengths | Cost |\n|-------|-----------|-----------|------|\n| `text-embedding-3-large` | 3072 | Best quality, multi-lingual | $$$ |\n| `text-embedding-3-small` | 1536 | Good balance | $$ |\n| `text-embedding-ada-002` | 1536 | Legacy, good compatibility | $ |\n| Cohere `embed-v3` | 1024 | Good for retrieval | $$ |\n\n**Recommendation**: Start with `text-embedding-3-small`, upgrade to `large` for production.\n\n## Retrieval Architecture\n\n### Hybrid Search Pipeline\n\n```python\nclass HybridPromptRetrieval:\n    \"\"\"\n    Combine vector similarity with BM25 for optimal retrieval.\n\n    Research shows hybrid search outperforms either alone:\n    - Vector only: Good semantic matching\n    - BM25 only: Good keyword matching\n    - Hybrid: Best of both (5-10% improvement)\n    \"\"\"\n\n    def __init__(\n        self,\n        vector_db: VectorDatabase,\n        bm25_index: BM25Index,\n        reranker: Optional[Reranker] = None\n    ):\n        self.vector_db = vector_db\n        self.bm25 = bm25_index\n        self.reranker = reranker\n\n    async def search(\n        self,\n        query: str,\n        query_embedding: list,\n        top_k: int = 10,\n        filters: dict = None\n    ) -> list:\n        \"\"\"\n        Hybrid retrieval with optional reranking.\n\n        Pipeline:\n        1. Vector search (retrieve 3x candidates)\n        2. BM25 search (retrieve 3x candidates)\n        3. Reciprocal Rank Fusion (combine results)\n        4. Optional reranking (refine top candidates)\n        \"\"\"\n        # Over-retrieve for fusion\n        fetch_k = top_k * 3\n\n        # Parallel retrieval\n        vector_results, bm25_results = await asyncio.gather(\n            self._vector_search(query_embedding, fetch_k, filters),\n            self._bm25_search(query, fetch_k, filters)\n        )\n\n        # Reciprocal Rank Fusion\n        fused = self._rrf_fusion(vector_results, bm25_results)\n\n        # Optional reranking\n        if self.reranker and len(fused) > 0:\n            reranked = await self.reranker.rerank(\n                query=query,\n                documents=fused[:top_k * 2],\n                top_k=top_k\n            )\n            return reranked\n\n        return fused[:top_k]\n\n    def _rrf_fusion(\n        self,\n        vector_results: list,\n        bm25_results: list,\n        k: int = 60\n    ) -> list:\n        \"\"\"\n        Reciprocal Rank Fusion: 1/(k + rank) scoring.\n\n        Combines rankings from multiple retrieval methods.\n        k=60 is standard from RRF paper.\n        \"\"\"\n        scores = {}\n        doc_map = {}\n\n        for rank, doc in enumerate(vector_results):\n            doc_id = doc['id']\n            scores[doc_id] = scores.get(doc_id, 0) + 1 / (k + rank + 1)\n            doc_map[doc_id] = doc\n\n        for rank, doc in enumerate(bm25_results):\n            doc_id = doc['id']\n            scores[doc_id] = scores.get(doc_id, 0) + 1 / (k + rank + 1)\n            doc_map[doc_id] = doc\n\n        # Sort by fused score\n        sorted_ids = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)\n\n        return [\n            {**doc_map[doc_id], 'rrf_score': scores[doc_id]}\n            for doc_id in sorted_ids\n            if doc_id in doc_map\n        ]\n```\n\n### Reranking (Optional but Recommended)\n\n```python\nclass CohereReranker:\n    \"\"\"\n    Use Cohere's reranker for final result refinement.\n\n    Adds ~100ms latency but improves relevance significantly.\n    \"\"\"\n\n    def __init__(self, api_key: str):\n        self.client = cohere.Client(api_key)\n\n    async def rerank(\n        self,\n        query: str,\n        documents: list,\n        top_k: int = 5\n    ) -> list:\n        \"\"\"\n        Rerank documents using Cohere's reranker model.\n        \"\"\"\n        response = self.client.rerank(\n            model=\"rerank-english-v2.0\",\n            query=query,\n            documents=[d['prompt_text'] for d in documents],\n            top_n=top_k\n        )\n\n        return [\n            {\n                **documents[r.index],\n                'rerank_score': r.relevance_score\n            }\n            for r in response.results\n        ]\n```\n\n## Performance Tracking\n\n### Metric Structure\n\n```python\n@dataclass\nclass PromptMetrics:\n    \"\"\"\n    Performance metrics for a prompt.\n\n    Updated using exponential moving average for recency weighting.\n    \"\"\"\n    success_rate: float       # 0-1, task completion rate\n    avg_latency_ms: float     # Response time\n    token_efficiency: float   # quality_score / tokens_used\n    coherence_score: float    # Logical consistency\n    observation_count: int    # How many times evaluated\n    last_updated: datetime    # For recency calculations\n\n    def update(self, outcome: dict, alpha: float = 0.3):\n        \"\"\"\n        Exponential moving average update.\n\n        alpha = 0.3 means:\n        - 30% weight to new observation\n        - 70% weight to historical average\n        \"\"\"\n        self.success_rate = alpha * float(outcome['success']) + (1 - alpha) * self.success_rate\n        self.avg_latency_ms = alpha * outcome.get('latency_ms', self.avg_latency_ms) + (1 - alpha) * self.avg_latency_ms\n        self.observation_count += 1\n        self.last_updated = datetime.utcnow()\n```\n\n### Temporal Weighting\n\n```python\nclass TemporalWeighting:\n    \"\"\"\n    Apply time-decay to balance recency vs. historical performance.\n\n    half_life: Time for weight to decay to 50%\n    - 30 days: Standard for most use cases\n    - 7 days: Fast-changing domains\n    - 90 days: Stable domains\n    \"\"\"\n\n    def __init__(self, half_life_days: int = 30):\n        self.half_life = timedelta(days=half_life_days)\n\n    def calculate_weight(self, observation_time: datetime) -> float:\n        \"\"\"\n        Calculate time-decay weight.\n\n        weight = 0.5^(time_diff / half_life)\n        \"\"\"\n        time_diff = datetime.utcnow() - observation_time\n        half_lives_elapsed = time_diff / self.half_life\n        return math.pow(0.5, half_lives_elapsed)\n\n    def weighted_score(\n        self,\n        observations: list[tuple[float, datetime]]\n    ) -> float:\n        \"\"\"\n        Calculate weighted average with time decay.\n        \"\"\"\n        total_weight = 0\n        weighted_sum = 0\n\n        for value, timestamp in observations:\n            weight = self.calculate_weight(timestamp)\n            weighted_sum += value * weight\n            total_weight += weight\n\n        return weighted_sum / total_weight if total_weight > 0 else 0\n```\n\n## Drift Detection\n\n### Distribution Shift Detection\n\n```python\nclass DriftDetector:\n    \"\"\"\n    Detect when prompt distribution shifts (new domains, patterns).\n\n    Triggers adaptive learning rate adjustment.\n    \"\"\"\n\n    def __init__(\n        self,\n        window_size: int = 100,\n        drift_threshold: float = 0.15\n    ):\n        self.window_size = window_size\n        self.threshold = drift_threshold\n        self.embedding_history = deque(maxlen=window_size)\n\n    def check_drift(self, new_embedding: np.ndarray) -> bool:\n        \"\"\"\n        Check if new embedding indicates distribution shift.\n\n        Uses Maximum Mean Discrepancy (simplified).\n        \"\"\"\n        if len(self.embedding_history) < 50:\n            self.embedding_history.append(new_embedding)\n            return False\n\n        # Calculate centroid of historical embeddings\n        historical = np.array(list(self.embedding_history))\n        centroid = np.mean(historical, axis=0)\n\n        # Measure distance from centroid\n        distance = np.linalg.norm(new_embedding - centroid)\n\n        # Normalize by average distance in history\n        avg_distance = np.mean([\n            np.linalg.norm(e - centroid)\n            for e in historical\n        ])\n\n        normalized_distance = distance / avg_distance if avg_distance > 0 else 0\n\n        self.embedding_history.append(new_embedding)\n\n        return normalized_distance > (1 + self.threshold)\n\n    def get_drift_metrics(self) -> dict:\n        \"\"\"\n        Get drift detection metrics for monitoring.\n        \"\"\"\n        if len(self.embedding_history) < 10:\n            return {'status': 'insufficient_data'}\n\n        historical = np.array(list(self.embedding_history))\n        centroid = np.mean(historical, axis=0)\n\n        distances = [np.linalg.norm(e - centroid) for e in historical]\n\n        return {\n            'avg_distance': np.mean(distances),\n            'std_distance': np.std(distances),\n            'max_distance': np.max(distances),\n            'sample_size': len(self.embedding_history)\n        }\n```\n\n## Vector Database Selection\n\n### Comparison Matrix\n\n| Feature | Qdrant | Pinecone | Chroma | pgvector |\n|---------|--------|----------|--------|----------|\n| **Self-hosted** | Yes | No | Yes | Yes |\n| **Managed** | Yes | Yes | Yes | Depends |\n| **Latency (p50)** | ~30ms | ~25ms | ~50ms | ~31ms |\n| **Scale** | Billions | Billions | Millions | Millions |\n| **Filtering** | Rich | Rich | Basic | SQL |\n| **BM25 Support** | Yes | No | No | No (use pg_trgm) |\n| **Cost** | $ | $$$ | Free | $ |\n\n### Qdrant Setup (Recommended)\n\n```python\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.models import Distance, VectorParams, PayloadSchemaType\n\nasync def setup_qdrant():\n    client = QdrantClient(url=\"http://localhost:6333\")\n\n    # Create collection\n    await client.create_collection(\n        collection_name=\"prompt_embeddings\",\n        vectors_config=VectorParams(\n            size=3072,  # text-embedding-3-large\n            distance=Distance.COSINE\n        )\n    )\n\n    # Create indexes for filtering\n    await client.create_payload_index(\n        collection_name=\"prompt_embeddings\",\n        field_name=\"metrics.success_rate\",\n        field_schema=PayloadSchemaType.FLOAT\n    )\n\n    await client.create_payload_index(\n        collection_name=\"prompt_embeddings\",\n        field_name=\"domain\",\n        field_schema=PayloadSchemaType.KEYWORD\n    )\n\n    await client.create_payload_index(\n        collection_name=\"prompt_embeddings\",\n        field_name=\"created_at\",\n        field_schema=PayloadSchemaType.DATETIME\n    )\n\n    return client\n```\n\n### Chroma Setup (Quick Start)\n\n```python\nimport chromadb\nfrom chromadb.config import Settings\n\ndef setup_chroma():\n    # Persistent storage\n    client = chromadb.PersistentClient(\n        path=\"./chroma_data\",\n        settings=Settings(anonymized_telemetry=False)\n    )\n\n    collection = client.get_or_create_collection(\n        name=\"prompt_embeddings\",\n        metadata={\"hnsw:space\": \"cosine\"}\n    )\n\n    return collection\n```\n\n## Caching Strategy\n\n### Embedding Cache\n\n```python\nclass EmbeddingCache:\n    \"\"\"\n    Cache embeddings to reduce API calls.\n\n    Cache hit rate typically 60-80% for repeated prompts.\n    \"\"\"\n\n    def __init__(self, redis_client: Redis, ttl_hours: int = 24):\n        self.redis = redis_client\n        self.ttl = ttl_hours * 3600\n\n    async def get_or_create(\n        self,\n        text: str,\n        embed_fn: Callable\n    ) -> list:\n        \"\"\"\n        Get embedding from cache or create new.\n        \"\"\"\n        cache_key = f\"emb:{hashlib.sha256(text.encode()).hexdigest()}\"\n\n        # Try cache\n        cached = await self.redis.get(cache_key)\n        if cached:\n            return json.loads(cached)\n\n        # Generate new\n        embedding = await embed_fn(text)\n\n        # Cache with TTL\n        await self.redis.setex(\n            cache_key,\n            self.ttl,\n            json.dumps(embedding)\n        )\n\n        return embedding\n```\n\n### Query Result Cache\n\n```python\nclass QueryCache:\n    \"\"\"\n    Cache frequent queries for faster retrieval.\n\n    Short TTL (5-15 min) since results may change.\n    \"\"\"\n\n    def __init__(self, redis_client: Redis, ttl_minutes: int = 10):\n        self.redis = redis_client\n        self.ttl = ttl_minutes * 60\n\n    async def get_or_query(\n        self,\n        query: str,\n        filters: dict,\n        query_fn: Callable\n    ) -> list:\n        \"\"\"\n        Get results from cache or execute query.\n        \"\"\"\n        cache_key = f\"query:{hashlib.sha256(f'{query}:{json.dumps(filters)}'.encode()).hexdigest()}\"\n\n        cached = await self.redis.get(cache_key)\n        if cached:\n            return json.loads(cached)\n\n        results = await query_fn(query, filters)\n\n        await self.redis.setex(\n            cache_key,\n            self.ttl,\n            json.dumps(results)\n        )\n\n        return results\n```\n\n## Performance Targets\n\n| Operation | Target Latency | Notes |\n|-----------|---------------|-------|\n| Embedding generation | &lt;100ms (p99) | Use caching |\n| Vector search | &lt;50ms (p99) | HNSW index |\n| Hybrid search | &lt;100ms (p99) | Parallel retrieval |\n| With reranking | &lt;200ms (p99) | Only for top-k |\n| Metric update | &lt;10ms (p99) | Async |\n| Full pipeline | &lt;300ms (p99) | Cold |\n| Full pipeline | &lt;100ms (p99) | Cached |\n\n---\n\nThis architecture provides a scalable foundation for stateful prompt learning that can grow from prototype to production.\n"
        },
        {
          "name": "iteration-strategy.md",
          "type": "file",
          "path": "automatic-stateful-prompt-improver/references/iteration-strategy.md",
          "size": 2746,
          "content": "# Iteration Strategy\n\nDecision framework for optimization depth.\n\n## Decision Matrix\n\n| Factor | Low (3-5 iter) | Medium (5-10 iter) | High (10-20 iter) |\n|--------|----------------|-------------------|-------------------|\n| **Complexity** | Simple classification | Multi-step reasoning | Agent/pipeline |\n| **Ambiguity** | Clear requirements | Some interpretation | Underspecified |\n| **Domain** | Well-researched | Moderate coverage | Novel domain |\n| **Search Space** | Single instruction | Few-shot selection | Full program |\n| **Stakes** | Low impact | Moderate impact | Critical path |\n\n## Complexity Assessment\n\n```\nComplexity Score (0-1):\n- Task decomposition depth (0.3): How many sub-tasks?\n- Reasoning steps required (0.3): Chain length\n- Domain specificity (0.2): Specialized knowledge?\n- Output structure (0.2): Format complexity\n\nScore &lt; 0.3 → 3-5 iterations\nScore 0.3-0.6 → 5-10 iterations\nScore &gt; 0.6 → 10-20 iterations\n```\n\n## Ambiguity Assessment\n\n```\nAmbiguity Score (0-1):\n- Requirement clarity (0.4): Are success criteria explicit?\n- Interpretation variance (0.3): Multiple valid readings?\n- Example availability (0.3): Concrete examples provided?\n\nHigh ambiguity → +5 iterations (for exploration)\n```\n\n## Decision Tree: When to Iterate More\n\n```\nSTART\n│\n├─ Is task critical? ──YES──→ +5 iterations\n│\n├─ Is domain novel? ──YES──→ +5 iterations\n│\n├─ Are requirements ambiguous? ──YES──→ +5 iterations\n│\n├─ Do I have similar prompts? ──YES──→ -3 iterations (start better)\n│\n└─ Base: 5 iterations\n\nTOTAL = Base + adjustments (min 3, max 20)\n```\n\n## Convergence Criteria\n\nStop iterating when:\n1. **Performance plateau**: No improvement &gt; 1% over last 3 iterations\n2. **Diminishing returns**: Cost per improvement unit exceeds threshold\n3. **Statistical significance**: Confidence interval &lt; 2%\n4. **Budget exhausted**: Max iterations or token limit reached\n\n```python\ndef check_convergence(scores, window=3, threshold=0.01):\n    \"\"\"Stop if improvement &lt; threshold over window iterations\"\"\"\n    if len(scores) &lt; window:\n        return False\n    recent_improvement = max(scores[-window:]) - scores[-window]\n    return recent_improvement &lt; threshold\n```\n\n## Stop Conditions\n\n```\nSTOP if ANY:\n- Improvement &lt; 1% for 3 consecutive iterations\n- User signals satisfaction\n- Token budget exhausted\n- 20 iterations reached\n- Validation score &gt; 0.95\n```\n\n## Performance Expectations\n\n| Scenario | Expected Improvement | Iterations |\n|----------|---------------------|------------|\n| Simple task | 10-20% | 3-5 |\n| Complex reasoning | 20-40% | 10-15 |\n| Agent/pipeline | 30-50% | 15-20 |\n| With history | +10-15% additional | Varies |\n"
        },
        {
          "name": "learning-architecture.md",
          "type": "file",
          "path": "automatic-stateful-prompt-improver/references/learning-architecture.md",
          "size": 2579,
          "content": "# Learning Architecture\n\nStateful learning through embedding-indexed history.\n\n## Retrieval Strategy (Warm Start)\n\n```\n1. Embed the current prompt with contextual metadata:\n   - Domain classification\n   - Task type\n   - Complexity score\n\n2. Hybrid search (vector similarity + BM25 keyword):\n   - Retrieve top-20 candidates\n   - Rerank to top-5\n\n3. Filter by performance threshold:\n   - Only prompts with success_rate > 0.7\n   - Weight by recency (exponential decay, half-life: 30 days)\n\n4. Generate improvements based on:\n   - What made similar prompts succeed\n   - Patterns in high-performing variants\n```\n\n## Performance Metrics\n\n| Metric | Description | Weight |\n|--------|-------------|--------|\n| `success_rate` | Task completion accuracy | 0.40 |\n| `token_efficiency` | Output quality / tokens used | 0.20 |\n| `coherence` | Logical consistency score | 0.15 |\n| `user_satisfaction` | Explicit feedback | 0.15 |\n| `latency_ms` | Response time | 0.10 |\n\n## Continuous Learning Loop\n\n```\n1. OBSERVE: Track prompt → outcome pairs\n   - Store prompt embedding\n   - Record performance metrics\n   - Capture context (domain, task type, complexity)\n\n2. INDEX: Build retrievable knowledge base\n   - Vector database for semantic similarity\n   - BM25 for keyword matching\n   - Metadata for filtering\n\n3. UPDATE: Exponential moving average\n   - α = 0.3 (30% new, 70% historical)\n   - Recency decay: half-life 30 days\n   - Distribution drift detection\n\n4. ADAPT: Adjust strategies based on patterns\n   - Which techniques work for which domains?\n   - What iteration counts converge fastest?\n   - Where do certain patterns fail?\n```\n\n## Drift Detection\n\nWhen the distribution of prompts shifts (new domain, new patterns):\n\n```\nIf embedding_drift > 0.15:\n  - Increase learning rate (α → 0.5)\n  - Flag for human review\n  - Log potential new domain\n```\n\n## MCP Server Setup\n\nTo enable persistent learning:\n\n```json\n{\n  \"mcpServers\": {\n    \"prompt-learning\": {\n      \"command\": \"node\",\n      \"args\": [\"path/to/prompt-learning-server/index.js\"],\n      \"env\": {\n        \"VECTOR_DB_URL\": \"your-vector-db-url\",\n        \"REDIS_URL\": \"your-redis-url\"\n      }\n    }\n  }\n}\n```\n\n**Required Tools** (exposed by MCP):\n- `retrieve_prompts`: Get similar high-performing prompts\n- `record_feedback`: Update prompt metrics\n- `suggest_improvements`: RAG-based optimization\n- `get_analytics`: Performance insights\n\n## Cold Start Mode\n\nWithout MCP, operate in cold-start mode only:\n- Apply research-backed strategies\n- No retrieval from history\n- No persistent learning\n- Still effective, just not stateful\n"
        },
        {
          "name": "mcp-server-spec.md",
          "type": "file",
          "path": "automatic-stateful-prompt-improver/references/mcp-server-spec.md",
          "size": 17740,
          "content": "# MCP Server Specification: Prompt Learning Server\n\nThis document specifies the MCP server implementation for stateful prompt learning.\n\n## Overview\n\nThe `prompt-learning` MCP server provides persistent storage and retrieval of prompt performance data, enabling the skill to learn and improve over time.\n\n## Architecture\n\n```\n┌─────────────────────────────────────────────────────┐\n│                 Claude Code                          │\n│                                                      │\n│  ┌──────────────────────────────────────────────┐   │\n│  │     automatic-stateful-prompt-improver        │   │\n│  └────────────────────┬─────────────────────────┘   │\n│                       │ MCP Protocol                 │\n└───────────────────────┼─────────────────────────────┘\n                        │\n┌───────────────────────┼─────────────────────────────┐\n│                       ▼                              │\n│            prompt-learning MCP Server                │\n│                                                      │\n│  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  │\n│  │   Tools     │  │  Resources  │  │   State     │  │\n│  │             │  │             │  │             │  │\n│  │ retrieve    │  │ performance │  │   Redis     │  │\n│  │ record      │  │ analytics   │  │   Cache     │  │\n│  │ suggest     │  │ history     │  │             │  │\n│  └──────┬──────┘  └──────┬──────┘  └──────┬──────┘  │\n│         │                │                │          │\n│         └────────────────┼────────────────┘          │\n│                          │                           │\n└──────────────────────────┼───────────────────────────┘\n                           │\n         ┌─────────────────┼─────────────────┐\n         │                 ▼                 │\n         │          Vector Database          │\n         │     (Qdrant / Pinecone / Chroma)  │\n         │                                   │\n         │  ┌─────────────────────────────┐  │\n         │  │     Prompt Embeddings       │  │\n         │  │   + Performance Metrics     │  │\n         │  │   + Metadata                │  │\n         │  └─────────────────────────────┘  │\n         │                                   │\n         └───────────────────────────────────┘\n```\n\n## Tools\n\n### 1. `retrieve_prompts`\n\nRetrieve similar high-performing prompts from the embedding database.\n\n**Input Schema**:\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"query\": {\n      \"type\": \"string\",\n      \"description\": \"The prompt to find similar examples for\"\n    },\n    \"domain\": {\n      \"type\": \"string\",\n      \"description\": \"Domain classification (e.g., 'code_review', 'summarization')\"\n    },\n    \"top_k\": {\n      \"type\": \"integer\",\n      \"default\": 5,\n      \"description\": \"Number of results to return\"\n    },\n    \"min_performance\": {\n      \"type\": \"number\",\n      \"default\": 0.7,\n      \"description\": \"Minimum success_rate threshold\"\n    }\n  },\n  \"required\": [\"query\"]\n}\n```\n\n**Output**:\n```json\n{\n  \"results\": [\n    {\n      \"prompt_id\": \"uuid\",\n      \"prompt_text\": \"string\",\n      \"similarity_score\": 0.92,\n      \"metrics\": {\n        \"success_rate\": 0.85,\n        \"avg_latency_ms\": 450,\n        \"token_efficiency\": 0.78\n      },\n      \"domain\": \"code_review\",\n      \"created_at\": \"2024-01-15T10:30:00Z\"\n    }\n  ]\n}\n```\n\n### 2. `record_feedback`\n\nRecord the outcome of a prompt execution for learning.\n\n**Input Schema**:\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"prompt_id\": {\n      \"type\": \"string\",\n      \"description\": \"ID of the prompt (or 'new' to create)\"\n    },\n    \"prompt_text\": {\n      \"type\": \"string\",\n      \"description\": \"The prompt text (required if prompt_id is 'new')\"\n    },\n    \"domain\": {\n      \"type\": \"string\",\n      \"description\": \"Domain classification\"\n    },\n    \"outcome\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"success\": {\"type\": \"boolean\"},\n        \"latency_ms\": {\"type\": \"number\"},\n        \"output_tokens\": {\"type\": \"integer\"},\n        \"quality_score\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1}\n      },\n      \"required\": [\"success\"]\n    },\n    \"user_feedback\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"satisfaction\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1},\n        \"comments\": {\"type\": \"string\"}\n      }\n    }\n  },\n  \"required\": [\"prompt_id\", \"outcome\"]\n}\n```\n\n**Output**:\n```json\n{\n  \"status\": \"recorded\",\n  \"prompt_id\": \"uuid\",\n  \"updated_metrics\": {\n    \"success_rate\": 0.82,\n    \"observation_count\": 15\n  }\n}\n```\n\n### 3. `suggest_improvements`\n\nGenerate improvement suggestions based on similar high-performers.\n\n**Input Schema**:\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"prompt\": {\n      \"type\": \"string\",\n      \"description\": \"The prompt to improve\"\n    },\n    \"current_performance\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"success_rate\": {\"type\": \"number\"},\n        \"avg_latency_ms\": {\"type\": \"number\"},\n        \"token_efficiency\": {\"type\": \"number\"}\n      }\n    },\n    \"improvement_focus\": {\n      \"type\": \"string\",\n      \"enum\": [\"clarity\", \"specificity\", \"efficiency\", \"all\"],\n      \"default\": \"all\"\n    }\n  },\n  \"required\": [\"prompt\"]\n}\n```\n\n**Output**:\n```json\n{\n  \"suggestions\": [\n    {\n      \"type\": \"add_structure\",\n      \"description\": \"Add numbered output format\",\n      \"example\": \"Respond with:\\n1. Summary\\n2. Key points\\n3. Recommendations\",\n      \"expected_improvement\": 0.15\n    }\n  ],\n  \"based_on\": {\n    \"similar_prompts_analyzed\": 5,\n    \"avg_performance_of_similar\": 0.87\n  }\n}\n```\n\n### 4. `get_analytics`\n\nRetrieve performance analytics and trends.\n\n**Input Schema**:\n```json\n{\n  \"type\": \"object\",\n  \"properties\": {\n    \"domain\": {\n      \"type\": \"string\",\n      \"description\": \"Filter by domain (optional)\"\n    },\n    \"time_range\": {\n      \"type\": \"string\",\n      \"enum\": [\"7d\", \"30d\", \"90d\", \"all\"],\n      \"default\": \"30d\"\n    },\n    \"metrics\": {\n      \"type\": \"array\",\n      \"items\": {\"type\": \"string\"},\n      \"default\": [\"success_rate\", \"token_efficiency\"]\n    }\n  }\n}\n```\n\n**Output**:\n```json\n{\n  \"summary\": {\n    \"total_prompts\": 150,\n    \"avg_success_rate\": 0.78,\n    \"improvement_trend\": 0.05\n  },\n  \"by_domain\": {\n    \"code_review\": {\"count\": 45, \"avg_success\": 0.82},\n    \"summarization\": {\"count\": 30, \"avg_success\": 0.75}\n  },\n  \"top_patterns\": [\n    {\"pattern\": \"structured_output\", \"success_rate\": 0.89},\n    {\"pattern\": \"chain_of_thought\", \"success_rate\": 0.85}\n  ]\n}\n```\n\n## Implementation\n\n### TypeScript Server\n\n```typescript\nimport { Server } from \"@modelcontextprotocol/sdk/server/index.js\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\nimport { QdrantClient } from \"@qdrant/js-client-rest\";\nimport Redis from \"ioredis\";\n\n// Configuration\nconst EMBEDDING_MODEL = \"text-embedding-3-large\";\nconst EMBEDDING_DIM = 3072;\nconst COLLECTION_NAME = \"prompt_embeddings\";\n\ninterface PromptMetrics {\n  success_rate: number;\n  avg_latency_ms: number;\n  token_efficiency: number;\n  observation_count: number;\n}\n\nclass PromptLearningServer {\n  private server: Server;\n  private vectorDb: QdrantClient;\n  private cache: Redis;\n  private embeddingClient: any; // OpenAI or similar\n\n  constructor() {\n    this.server = new Server(\n      { name: \"prompt-learning\", version: \"1.0.0\" },\n      { capabilities: { tools: {}, resources: {} } }\n    );\n\n    this.vectorDb = new QdrantClient({\n      url: process.env.VECTOR_DB_URL || \"http://localhost:6333\"\n    });\n\n    this.cache = new Redis(process.env.REDIS_URL || \"redis://localhost:6379\");\n\n    this.registerTools();\n  }\n\n  private registerTools() {\n    this.server.setRequestHandler(\"tools/list\", async () => ({\n      tools: [\n        {\n          name: \"retrieve_prompts\",\n          description: \"Find similar high-performing prompts\",\n          inputSchema: { /* schema above */ }\n        },\n        {\n          name: \"record_feedback\",\n          description: \"Record prompt execution outcome\",\n          inputSchema: { /* schema above */ }\n        },\n        {\n          name: \"suggest_improvements\",\n          description: \"Generate improvement suggestions\",\n          inputSchema: { /* schema above */ }\n        },\n        {\n          name: \"get_analytics\",\n          description: \"Get performance analytics\",\n          inputSchema: { /* schema above */ }\n        }\n      ]\n    }));\n\n    this.server.setRequestHandler(\"tools/call\", async (request) => {\n      const { name, arguments: args } = request.params;\n\n      switch (name) {\n        case \"retrieve_prompts\":\n          return await this.retrievePrompts(args);\n        case \"record_feedback\":\n          return await this.recordFeedback(args);\n        case \"suggest_improvements\":\n          return await this.suggestImprovements(args);\n        case \"get_analytics\":\n          return await this.getAnalytics(args);\n        default:\n          throw new Error(`Unknown tool: ${name}`);\n      }\n    });\n  }\n\n  private async retrievePrompts(args: any) {\n    // 1. Generate embedding for query\n    const queryEmbedding = await this.getEmbedding(args.query);\n\n    // 2. Search vector DB\n    const results = await this.vectorDb.search(COLLECTION_NAME, {\n      vector: queryEmbedding,\n      limit: args.top_k * 2, // Over-retrieve for filtering\n      filter: {\n        must: [\n          { key: \"metrics.success_rate\", range: { gte: args.min_performance } }\n        ],\n        ...(args.domain && {\n          must: [{ key: \"domain\", match: { value: args.domain } }]\n        })\n      }\n    });\n\n    // 3. Format and return\n    return {\n      content: [{\n        type: \"text\",\n        text: JSON.stringify({\n          results: results.slice(0, args.top_k).map(r => ({\n            prompt_id: r.id,\n            prompt_text: r.payload?.prompt_text,\n            similarity_score: r.score,\n            metrics: r.payload?.metrics,\n            domain: r.payload?.domain\n          }))\n        })\n      }]\n    };\n  }\n\n  private async recordFeedback(args: any) {\n    const alpha = 0.3; // EMA weight for new observations\n\n    // Get or create prompt record\n    let metrics: PromptMetrics;\n    if (args.prompt_id === \"new\") {\n      // Create new prompt\n      const embedding = await this.getEmbedding(args.prompt_text);\n      const promptId = crypto.randomUUID();\n\n      metrics = {\n        success_rate: args.outcome.success ? 1.0 : 0.0,\n        avg_latency_ms: args.outcome.latency_ms || 0,\n        token_efficiency: args.outcome.quality_score || 0,\n        observation_count: 1\n      };\n\n      await this.vectorDb.upsert(COLLECTION_NAME, {\n        points: [{\n          id: promptId,\n          vector: embedding,\n          payload: {\n            prompt_text: args.prompt_text,\n            domain: args.domain,\n            metrics,\n            created_at: new Date().toISOString()\n          }\n        }]\n      });\n\n      args.prompt_id = promptId;\n    } else {\n      // Update existing\n      const existing = await this.vectorDb.retrieve(COLLECTION_NAME, {\n        ids: [args.prompt_id],\n        with_payload: true\n      });\n\n      if (existing.length === 0) {\n        throw new Error(\"Prompt not found\");\n      }\n\n      const oldMetrics = existing[0].payload?.metrics as PromptMetrics;\n\n      // Exponential moving average update\n      metrics = {\n        success_rate: alpha * (args.outcome.success ? 1 : 0) + (1 - alpha) * oldMetrics.success_rate,\n        avg_latency_ms: alpha * (args.outcome.latency_ms || 0) + (1 - alpha) * oldMetrics.avg_latency_ms,\n        token_efficiency: alpha * (args.outcome.quality_score || oldMetrics.token_efficiency) + (1 - alpha) * oldMetrics.token_efficiency,\n        observation_count: oldMetrics.observation_count + 1\n      };\n\n      await this.vectorDb.setPayload(COLLECTION_NAME, {\n        points: [args.prompt_id],\n        payload: { metrics }\n      });\n    }\n\n    return {\n      content: [{\n        type: \"text\",\n        text: JSON.stringify({\n          status: \"recorded\",\n          prompt_id: args.prompt_id,\n          updated_metrics: metrics\n        })\n      }]\n    };\n  }\n\n  private async suggestImprovements(args: any) {\n    // Retrieve similar high-performers\n    const similar = await this.retrievePrompts({\n      query: args.prompt,\n      top_k: 5,\n      min_performance: 0.8\n    });\n\n    // Analyze patterns in high performers\n    // (In production, this would use an LLM to generate suggestions)\n\n    return {\n      content: [{\n        type: \"text\",\n        text: JSON.stringify({\n          suggestions: [\n            // Pattern-based suggestions\n          ],\n          based_on: {\n            similar_prompts_analyzed: 5,\n            avg_performance_of_similar: 0.87\n          }\n        })\n      }]\n    };\n  }\n\n  private async getAnalytics(args: any) {\n    // Aggregate metrics from vector DB\n    // (Implementation depends on specific vector DB capabilities)\n\n    return {\n      content: [{\n        type: \"text\",\n        text: JSON.stringify({\n          summary: {\n            total_prompts: 0,\n            avg_success_rate: 0,\n            improvement_trend: 0\n          }\n        })\n      }]\n    };\n  }\n\n  private async getEmbedding(text: string): Promise<number[]> {\n    // Call embedding API (OpenAI, Cohere, etc.)\n    // Implementation depends on your embedding provider\n    return [];\n  }\n\n  async start() {\n    const transport = new StdioServerTransport();\n    await this.server.connect(transport);\n  }\n}\n\nconst server = new PromptLearningServer();\nserver.start();\n```\n\n## Setup Instructions\n\n### 1. Prerequisites\n\n```bash\n# Install dependencies\nnpm install @modelcontextprotocol/sdk @qdrant/js-client-rest ioredis openai\n\n# Start Qdrant (Docker)\ndocker run -p 6333:6333 qdrant/qdrant\n\n# Start Redis (Docker)\ndocker run -p 6379:6379 redis\n```\n\n### 2. Initialize Vector Collection\n\n```typescript\nconst client = new QdrantClient({ url: \"http://localhost:6333\" });\n\nawait client.createCollection(\"prompt_embeddings\", {\n  vectors: {\n    size: 3072, // text-embedding-3-large\n    distance: \"Cosine\"\n  }\n});\n\n// Create payload indexes for filtering\nawait client.createPayloadIndex(\"prompt_embeddings\", {\n  field_name: \"metrics.success_rate\",\n  field_schema: \"float\"\n});\n\nawait client.createPayloadIndex(\"prompt_embeddings\", {\n  field_name: \"domain\",\n  field_schema: \"keyword\"\n});\n```\n\n### 3. Configure Claude Code\n\nAdd to your `claude_desktop_config.json` or project config:\n\n```json\n{\n  \"mcpServers\": {\n    \"prompt-learning\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/prompt-learning-server/dist/index.js\"],\n      \"env\": {\n        \"VECTOR_DB_URL\": \"http://localhost:6333\",\n        \"REDIS_URL\": \"redis://localhost:6379\",\n        \"OPENAI_API_KEY\": \"sk-...\"\n      }\n    }\n  }\n}\n```\n\n## Data Model\n\n### Prompt Embedding Record\n\n```typescript\ninterface PromptRecord {\n  id: string;                    // UUID\n  vector: number[];              // Embedding (3072 dim)\n  payload: {\n    prompt_text: string;         // Original prompt\n    contextualized: string;      // With domain context\n    domain: string;              // Classification\n    task_type: string;           // e.g., \"classification\", \"generation\"\n    metrics: {\n      success_rate: number;      // 0-1, EMA\n      avg_latency_ms: number;    // Response time\n      token_efficiency: number;  // quality/tokens\n      observation_count: number; // How many times evaluated\n    };\n    created_at: string;          // ISO timestamp\n    updated_at: string;          // Last metric update\n    tags: string[];              // User-defined tags\n  };\n}\n```\n\n### Session State (Redis)\n\n```typescript\ninterface SessionState {\n  session_id: string;\n  user_id: string;\n  current_prompt_id: string | null;\n  iteration_count: number;\n  best_score: number;\n  history: Array<{\n    prompt_id: string;\n    score: number;\n    timestamp: string;\n  }>;\n}\n```\n\n## Security Considerations\n\n1. **Data Isolation**: Use Redis key prefixes for multi-tenant isolation\n2. **API Keys**: Store securely, rotate regularly\n3. **Rate Limiting**: Implement per-user rate limits\n4. **Encryption**: TLS for all connections, encryption at rest for sensitive prompts\n5. **GDPR**: Implement user data deletion workflows\n\n## Monitoring\n\nTrack these metrics:\n- Embedding latency (p50, p95, p99)\n- Retrieval latency (p50, p95, p99)\n- Success rate improvement over time\n- Cache hit rate\n- Error rates by tool\n\n---\n\nThis MCP server enables the prompt improver skill to learn and adapt over time, making each optimization better informed by past successes.\n"
        },
        {
          "name": "optimization-techniques.md",
          "type": "file",
          "path": "automatic-stateful-prompt-improver/references/optimization-techniques.md",
          "size": 3021,
          "content": "# Optimization Techniques\n\nResearch-backed strategies for prompt improvement.\n\n## APE-Style Instruction Generation\n\n**Source**: Zhou et al., 2022 (outperformed human prompts on 19/24 NLP tasks)\n\n```\nGiven your task and examples:\nTask: [description]\nExamples: Input → Output pairs\n\nGenerate 5-10 instruction candidates, then select best based on:\n- Clarity score (0-1): How unambiguous is the instruction?\n- Completeness (0-1): Does it cover all requirements?\n- Constraint density: Appropriate constraints without over-specification\n```\n\n## OPRO Meta-Prompting\n\n**Source**: Yang et al., 2023 (up to 50% improvement on Big-Bench Hard)\n\nTreat prompt optimization AS a prompting task:\n\n```\nPrevious prompt attempts and their scores:\n- \"Step by step, solve...\" | Score: 0.65\n- \"Carefully analyze each...\" | Score: 0.72\n\nGenerate a new instruction that will achieve a higher score.\n```\n\n## Chain-of-Thought Enhancement\n\n**Source**: Wei et al., 2022 (best for complex reasoning)\n\nAdd reasoning scaffolds:\n- \"Let's think step by step\" (zero-shot CoT)\n- Structured reasoning templates\n- Self-consistency through multiple paths\n\n## Task Decomposition\n\nBreak complex prompts into modular components:\n1. **Context setting** - Domain and background\n2. **Instruction specification** - What to do\n3. **Output format** - How to respond\n4. **Constraints** - What to avoid\n\n## Instruction Rewriting\n\n**Pattern**: Generate variants, evaluate, select best\n\n```markdown\nOriginal: \"Summarize this text\"\n\nGenerated Variants:\n1. \"Extract the key points from this text in bullet form\"\n2. \"Provide a concise 2-3 sentence summary capturing the main argument\"\n3. \"Identify the thesis and supporting evidence, then summarize\"\n\nSelection Criteria:\n- Specificity: Variant 2 wins (format specified)\n- Clarity: Variant 1 wins (clear structure)\n- Completeness: Variant 3 wins (methodology included)\n\nBest: Combine insights → \"Extract the thesis and key supporting points,\nthen provide a concise 2-3 sentence summary in bullet form\"\n```\n\n## Few-Shot Optimization\n\n**Pattern**: Select examples that maximize performance\n\n```\nSelection Methods:\n1. Semantic similarity: Examples similar to test case\n2. Diversity: Cover different scenarios\n3. Difficulty progression: Easy → Hard examples\n4. Contrastive: Include near-misses for boundary learning\n```\n\n## Constraint Engineering\n\n**Pattern**: Add/remove constraints to improve output\n\n```\nUnder-constrained:\n\"Write code for a sorting function\"\n\nAfter Constraint Engineering:\n\"Write a Python function that sorts a list of integers in ascending order.\nRequirements:\n- Use O(n log n) time complexity\n- Handle empty lists gracefully\n- Include type hints\n- Do not use built-in sort()\"\n```\n\n## Output Format Specification\n\n**Pattern**: Explicit format reduces ambiguity\n\n```\nBefore: \"Analyze this data\"\n\nAfter: \"Analyze this data and respond with:\n1. **Summary** (2-3 sentences): Key findings\n2. **Metrics** (bullet list): Quantitative observations\n3. **Recommendations** (numbered): Actionable next steps\"\n```\n"
        }
      ]
    },
    {
      "name": "CHANGELOG.md",
      "type": "file",
      "path": "automatic-stateful-prompt-improver/CHANGELOG.md",
      "size": 835,
      "content": "# Changelog\n\n## [2.0.0] - 2024-12-XX\n\n### Changed\n- **SKILL.md restructured** for progressive disclosure (481 → ~128 lines)\n- **Frontmatter fixed**: `tools:` YAML list → `allowed-tools:` comma-separated\n\n### Added\n- `references/optimization-techniques.md` - APE, OPRO, Chain-of-Thought, DSPy patterns\n- `references/learning-architecture.md` - Warm start, embedding indexing, performance tracking\n- `references/iteration-strategy.md` - Decision matrices, convergence criteria\n- Clear convergence thresholds (≥0.85 success rate, ≤10% improvement)\n- Detailed iteration budget management\n\n### Migration Guide\n- **Breaking**: If you use this skill via configuration, update frontmatter format\n- Old: `tools:\\n  - tool1\\n  - tool2`\n- New: `allowed-tools: tool1,tool2,tool3`\n- Reference files provide implementation details on demand\n"
    },
    {
      "name": "SETUP.md",
      "type": "file",
      "path": "automatic-stateful-prompt-improver/SETUP.md",
      "size": 10154,
      "content": "# Setup Guide: Automatic Stateful Prompt Improver\n\nThis guide walks you through setting up the full stateful learning infrastructure for the prompt improver skill.\n\n## One-Command Install\n\n**The fastest way to get started:**\n\n```bash\ncurl -fsSL https://someclaudeskills.com/install/prompt-learning.sh | bash\n```\n\nOr clone and install manually:\n\n```bash\ngit clone https://github.com/erichowens/prompt-learning-mcp.git ~/mcp-servers/prompt-learning\ncd ~/mcp-servers/prompt-learning\nnpm install && npm run build\nnpm run setup\n```\n\n**Repository:** [github.com/erichowens/prompt-learning-mcp](https://github.com/erichowens/prompt-learning-mcp)\n\n## Quick Start (Cold Start Mode)\n\n**No setup required!** The skill works immediately in cold-start mode:\n- Uses research-backed optimization strategies (APE, OPRO, DSPy patterns)\n- No persistent learning, but still highly effective\n- Great for testing before investing in infrastructure\n\nJust invoke the skill:\n```\n\"optimize this prompt: [your prompt]\"\n```\n\n## Full Setup (Stateful Learning Mode)\n\nFor persistent learning across conversations, you need:\n1. Vector database (prompt embeddings)\n2. Redis (session cache, metrics)\n3. MCP server (protocol bridge)\n\n### Prerequisites\n\n```bash\n# Docker (recommended for quick setup)\ndocker --version  # Ensure Docker is installed\n\n# Node.js 18+ (for MCP server)\nnode --version\n\n# OpenAI API key (for embeddings and evaluation)\nexport OPENAI_API_KEY=sk-your-key-here\n```\n\n### Step 1: Start Vector Database (Qdrant)\n\nQdrant is recommended for its hybrid search capabilities (vector + BM25).\n\n```bash\n# Quick start with Docker\ndocker run -d \\\n  --name qdrant \\\n  -p 6333:6333 \\\n  -p 6334:6334 \\\n  -v qdrant_storage:/qdrant/storage \\\n  qdrant/qdrant\n\n# Verify it's running\ncurl http://localhost:6333/collections\n```\n\n**Alternative: Chroma (simpler, smaller scale)**\n```bash\ndocker run -d \\\n  --name chroma \\\n  -p 8000:8000 \\\n  -v chroma_data:/chroma/chroma \\\n  chromadb/chroma\n```\n\n### Step 2: Start Redis (Session Cache)\n\n```bash\ndocker run -d \\\n  --name redis \\\n  -p 6379:6379 \\\n  -v redis_data:/data \\\n  redis:alpine redis-server --appendonly yes\n\n# Verify\nredis-cli ping  # Should return PONG\n```\n\n### Step 3: Initialize Vector Collection\n\nCreate the prompt embeddings collection:\n\n```bash\n# Using curl\ncurl -X PUT http://localhost:6333/collections/prompt_embeddings \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"vectors\": {\n      \"size\": 3072,\n      \"distance\": \"Cosine\"\n    }\n  }'\n\n# Create indexes for filtering\ncurl -X PUT http://localhost:6333/collections/prompt_embeddings/index \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"field_name\": \"metrics.success_rate\",\n    \"field_schema\": \"float\"\n  }'\n\ncurl -X PUT http://localhost:6333/collections/prompt_embeddings/index \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"field_name\": \"domain\",\n    \"field_schema\": \"keyword\"\n  }'\n```\n\n### Step 4: Setup MCP Server\n\nCreate the MCP server project:\n\n```bash\nmkdir -p ~/mcp-servers/prompt-learning\ncd ~/mcp-servers/prompt-learning\nnpm init -y\nnpm install @modelcontextprotocol/sdk @qdrant/js-client-rest ioredis openai\n```\n\nCreate `index.js`:\n```javascript\n// See /references/mcp-server-spec.md for full implementation\n// This is a minimal working version\n\nimport { Server } from \"@modelcontextprotocol/sdk/server/index.js\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\nimport { QdrantClient } from \"@qdrant/js-client-rest\";\nimport Redis from \"ioredis\";\nimport OpenAI from \"openai\";\n\nconst vectorDb = new QdrantClient({ url: process.env.VECTOR_DB_URL || \"http://localhost:6333\" });\nconst redis = new Redis(process.env.REDIS_URL || \"redis://localhost:6379\");\nconst openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });\n\nconst server = new Server(\n  { name: \"prompt-learning\", version: \"1.0.0\" },\n  { capabilities: { tools: {} } }\n);\n\n// Tool implementations...\n// (See full spec in references/mcp-server-spec.md)\n\nconst transport = new StdioServerTransport();\nawait server.connect(transport);\n```\n\n### Step 5: Configure Claude Code\n\nAdd to your Claude Code configuration (`~/.claude.json` or project config):\n\n```json\n{\n  \"mcpServers\": {\n    \"prompt-learning\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mcp-servers/prompt-learning/index.js\"],\n      \"env\": {\n        \"VECTOR_DB_URL\": \"http://localhost:6333\",\n        \"REDIS_URL\": \"redis://localhost:6379\",\n        \"OPENAI_API_KEY\": \"sk-your-key-here\"\n      }\n    }\n  }\n}\n```\n\n### Step 6: Verify Setup\n\nTest the MCP server is working:\n```bash\n# In Claude Code, try:\n\"retrieve similar prompts for: summarize the document\"\n```\n\nIf you see results from the vector database, you're all set!\n\n## Architecture Overview\n\n```\n┌────────────────────────────────────────────┐\n│           Claude Code                       │\n│                                            │\n│  ┌────────────────────────────────────┐   │\n│  │  automatic-stateful-prompt-improver │   │\n│  │                                     │   │\n│  │  Cold Start Mode:                   │   │\n│  │  - APE/OPRO patterns               │   │\n│  │  - DSPy-style optimization         │   │\n│  │  - Pattern-based improvement       │   │\n│  │                                     │   │\n│  │  Warm Start Mode (with MCP):       │   │\n│  │  - Retrieve similar prompts        │   │\n│  │  - Learn from outcomes             │   │\n│  │  - Adaptive iteration count        │   │\n│  └─────────────┬──────────────────────┘   │\n│                │                           │\n│                │ MCP Protocol              │\n└────────────────┼───────────────────────────┘\n                 │\n┌────────────────┼───────────────────────────┐\n│                ▼                            │\n│     prompt-learning MCP Server              │\n│                                            │\n│  Tools:                                    │\n│  - retrieve_prompts                        │\n│  - record_feedback                         │\n│  - suggest_improvements                    │\n│  - get_analytics                           │\n│                                            │\n└─────────┬──────────────┬───────────────────┘\n          │              │\n          ▼              ▼\n    ┌──────────┐   ┌──────────┐\n    │  Qdrant  │   │  Redis   │\n    │ (Vector) │   │ (Cache)  │\n    └──────────┘   └──────────┘\n```\n\n## Embedding Model Options\n\nThe system uses embeddings to find similar prompts. Choose based on your needs:\n\n| Model | Dimensions | Quality | Cost | Latency |\n|-------|-----------|---------|------|---------|\n| `text-embedding-3-large` | 3072 | Best | $$$ | ~200ms |\n| `text-embedding-3-small` | 1536 | Good | $$ | ~100ms |\n| `text-embedding-ada-002` | 1536 | Good | $ | ~150ms |\n| Cohere `embed-v3` | 1024 | Good | $$ | ~100ms |\n\n**Recommendation**: Start with `text-embedding-3-small`, upgrade if needed.\n\nTo change models, update the collection:\n```bash\n# For 1536-dim model\ncurl -X PUT http://localhost:6333/collections/prompt_embeddings \\\n  -d '{\"vectors\": {\"size\": 1536, \"distance\": \"Cosine\"}}'\n```\n\n## Cost Considerations\n\n### Monthly Estimates (assuming moderate usage)\n\n| Component | Cost | Notes |\n|-----------|------|-------|\n| Qdrant (self-hosted) | ~$0-20 | Docker on existing machine |\n| Qdrant (cloud) | $25-100 | Managed, higher availability |\n| Redis (self-hosted) | ~$0 | Docker on existing machine |\n| Redis (cloud) | $10-50 | Managed |\n| OpenAI Embeddings | ~$5-20 | ~1M tokens/month |\n| OpenAI API (optimization) | ~$10-50 | Depends on usage |\n\n**Total self-hosted**: ~$15-70/month\n**Total cloud-managed**: ~$50-200/month\n\n### Cost Optimization Tips\n\n1. **Cache embeddings**: Same prompts don't need re-embedding\n2. **Batch operations**: Group embedding calls\n3. **Use smaller models**: `text-embedding-3-small` is often sufficient\n4. **Rate limit**: Prevent runaway API calls\n\n## Troubleshooting\n\n### MCP Server Not Connecting\n\n```bash\n# Check if server is running\nps aux | grep prompt-learning\n\n# Test manually\ncd ~/mcp-servers/prompt-learning\nnode index.js\n\n# Check Claude Code logs\ntail -f ~/.claude/logs/mcp.log\n```\n\n### Vector Database Issues\n\n```bash\n# Check Qdrant health\ncurl http://localhost:6333/health\n\n# Check collection exists\ncurl http://localhost:6333/collections/prompt_embeddings\n\n# Recreate if corrupted\ncurl -X DELETE http://localhost:6333/collections/prompt_embeddings\n# Then re-run Step 3\n```\n\n### Redis Connection Issues\n\n```bash\n# Check Redis is running\ndocker ps | grep redis\n\n# Test connection\nredis-cli ping\n\n# Check memory usage\nredis-cli info memory\n```\n\n## Scaling Considerations\n\n### When to Scale\n\n- **&gt;100k prompts**: Consider Qdrant cloud or cluster\n- **&gt;10 QPS**: Add Redis caching layer\n- **Multi-user**: Add tenant isolation\n\n### Production Checklist\n\n- [ ] TLS enabled for all connections\n- [ ] API keys in secrets manager\n- [ ] Backup strategy for vector DB\n- [ ] Monitoring and alerting\n- [ ] Rate limiting\n- [ ] GDPR data deletion workflows\n\n## Next Steps\n\n1. **Start in cold-start mode**: Test the skill without infrastructure\n2. **Add basic persistence**: Set up Qdrant + Redis locally\n3. **Collect data**: Use the skill, let it learn\n4. **Analyze patterns**: Use `get_analytics` to see what works\n5. **Scale as needed**: Move to managed services if usage grows\n\n---\n\nQuestions? Issues? The skill's `/references` directory has detailed implementation specs.\n"
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "automatic-stateful-prompt-improver/SKILL.md",
      "size": 4741,
      "content": "---\nname: automatic-stateful-prompt-improver\ndescription: Automatically intercepts and optimizes prompts using the prompt-learning MCP server. Learns from performance over time via embedding-indexed history. Uses APE, OPRO, DSPy patterns. Activate on \"optimize prompt\", \"improve this prompt\", \"prompt engineering\", or ANY complex task request. Requires prompt-learning MCP server. NOT for simple questions (just answer them), NOT for direct commands (just execute them), NOT for conversational responses (no optimization needed).\nallowed-tools: mcp__prompt-learning__optimize_prompt,mcp__prompt-learning__retrieve_prompts,mcp__prompt-learning__record_feedback,mcp__prompt-learning__suggest_improvements,mcp__prompt-learning__get_analytics,mcp__SequentialThinking__sequentialthinking\n---\n\n# Automatic Stateful Prompt Improver\n\n## MANDATORY AUTOMATIC BEHAVIOR\n\n**When this skill is active, I MUST follow these rules:**\n\n### Auto-Optimization Triggers\n\nI AUTOMATICALLY call `mcp__prompt-learning__optimize_prompt` BEFORE responding when:\n\n1. **Complex task** (multi-step, requires reasoning)\n2. **Technical output** (code, analysis, structured data)\n3. **Reusable content** (system prompts, templates, instructions)\n4. **Explicit request** (\"improve\", \"better\", \"optimize\")\n5. **Ambiguous requirements** (underspecified, multiple interpretations)\n6. **Precision-critical** (code, legal, medical, financial)\n\n### Auto-Optimization Process\n\n```\n1. INTERCEPT the user's request\n2. CALL: mcp__prompt-learning__optimize_prompt\n   - prompt: [user's original request]\n   - domain: [inferred domain]\n   - max_iterations: [3-20 based on complexity]\n3. RECEIVE: optimized prompt + improvement details\n4. INFORM user briefly: \"I've refined your request for [reason]\"\n5. PROCEED with the OPTIMIZED version\n```\n\n### Do NOT Optimize\n\n- Simple questions (\"what is X?\")\n- Direct commands (\"run npm install\")\n- Conversational responses (\"hello\", \"thanks\")\n- File operations without reasoning\n- Already-optimized prompts\n\n## Learning Loop (Post-Response)\n\nAfter completing ANY significant task:\n\n```\n1. ASSESS: Did the response achieve the goal?\n2. CALL: mcp__prompt-learning__record_feedback\n   - prompt_id: [from optimization response]\n   - success: [true/false]\n   - quality_score: [0.0-1.0]\n3. This enables future retrievals to learn from outcomes\n```\n\n## Quick Reference\n\n### Iteration Decision\n\n| Factor | Low (3-5) | Medium (5-10) | High (10-20) |\n|--------|-----------|---------------|--------------|\n| Complexity | Simple | Multi-step | Agent/pipeline |\n| Ambiguity | Clear | Some | Underspecified |\n| Domain | Known | Moderate | Novel |\n| Stakes | Low | Moderate | Critical |\n\n### Convergence (When to Stop)\n\n- Improvement &lt; 1% for 3 iterations\n- User satisfied\n- Token budget exhausted\n- 20 iterations reached\n- Validation score &gt; 0.95\n\n### Performance Expectations\n\n| Scenario | Improvement | Iterations |\n|----------|-------------|------------|\n| Simple task | 10-20% | 3-5 |\n| Complex reasoning | 20-40% | 10-15 |\n| Agent/pipeline | 30-50% | 15-20 |\n| With history | +10-15% bonus | Varies |\n\n## Anti-Patterns\n\n### Over-Optimization\n\n| What it looks like | Why it's wrong |\n|--------------------|----------------|\n| Prompt becomes overly complex with many constraints | Causes brittleness, model confusion, token waste |\n| **Instead**: Apply Occam's Razor - simplest sufficient prompt wins |\n\n### Template Obsession\n\n| What it looks like | Why it's wrong |\n|--------------------|----------------|\n| Focusing on templates rather than task understanding | Templates don't generalize; understanding does |\n| **Instead**: Focus on WHAT the task requires, not HOW to format it |\n\n### Iteration Without Measurement\n\n| What it looks like | Why it's wrong |\n|--------------------|----------------|\n| Multiple rewrites without tracking improvements | Can't know if changes help without metrics |\n| **Instead**: Always define success criteria before optimizing |\n\n### Ignoring Model Capabilities\n\n| What it looks like | Why it's wrong |\n|--------------------|----------------|\n| Assumes model can't do things it can | Over-scaffolding wastes tokens |\n| **Instead**: Test capabilities before heavy prompting |\n\n## Reference Files\n\nLoad for detailed implementations:\n\n| File | Contents |\n|------|----------|\n| `references/optimization-techniques.md` | APE, OPRO, CoT, instruction rewriting, constraint engineering |\n| `references/learning-architecture.md` | Warm start, embedding retrieval, MCP setup, drift detection |\n| `references/iteration-strategy.md` | Decision matrices, complexity scoring, convergence algorithms |\n\n---\n\n**Goal**: Simplest prompt that achieves the outcome reliably. Optimize for clarity, specificity, and measurable improvement.\n"
    }
  ]
}