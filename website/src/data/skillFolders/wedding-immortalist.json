{
  "name": "wedding-immortalist",
  "type": "folder",
  "path": "wedding-immortalist",
  "children": [
    {
      "name": "references",
      "type": "folder",
      "path": "wedding-immortalist/references",
      "children": [
        {
          "name": "face-clustering-aesthetics.md",
          "type": "file",
          "path": "wedding-immortalist/references/face-clustering-aesthetics.md",
          "size": 15911,
          "content": "# Face Clustering & Aesthetic Photo Selection\n\n## Overview\n\nEvery wedding guest deserves great photos of themselves. This system automatically:\n1. Detects all faces across thousands of photos\n2. Clusters them by identity\n3. Scores each photo for aesthetic quality\n4. Selects the best N photos per person\n\n## Face Detection Pipeline\n\n### Detection Models Comparison\n\n| Model | Speed | Accuracy | Best For |\n|-------|-------|----------|----------|\n| **RetinaFace** | Medium | Highest | Production quality |\n| **MTCNN** | Slow | High | Fallback for hard cases |\n| **YOLOv8-face** | Fast | Good | Quick preview |\n| **MediaPipe** | Very Fast | Medium | Real-time applications |\n\n### RetinaFace Implementation\n\n```python\nfrom retinaface import RetinaFace\nimport cv2\nimport numpy as np\n\ndef detect_faces(image_path: str, threshold: float = 0.9):\n    \"\"\"\n    Detect all faces in an image with landmarks.\n\n    Returns list of face dictionaries with:\n    - bbox: [x1, y1, x2, y2]\n    - landmarks: 5-point facial landmarks\n    - confidence: detection confidence\n    \"\"\"\n    faces = RetinaFace.detect_faces(image_path, threshold=threshold)\n\n    if not isinstance(faces, dict):\n        return []\n\n    results = []\n    for face_id, face_data in faces.items():\n        results.append({\n            'bbox': face_data['facial_area'],  # [x1, y1, x2, y2]\n            'landmarks': face_data['landmarks'],  # 5 points\n            'confidence': face_data['score']\n        })\n\n    return results\n\ndef extract_aligned_face(\n    image: np.ndarray,\n    landmarks: dict,\n    output_size: tuple = (112, 112)\n) -> np.ndarray:\n    \"\"\"\n    Align face using 5-point landmarks for consistent embeddings.\n\n    Standard alignment targets (for 112x112):\n    - Left eye center: (38.29, 51.69)\n    - Right eye center: (73.53, 51.69)\n    - Nose tip: (56.02, 71.73)\n    - Left mouth: (41.54, 92.36)\n    - Right mouth: (70.72, 92.36)\n    \"\"\"\n    # Standard reference points\n    ref_pts = np.array([\n        [38.29, 51.69],   # left eye\n        [73.53, 51.69],   # right eye\n        [56.02, 71.73],   # nose\n        [41.54, 92.36],   # left mouth\n        [70.72, 92.36]    # right mouth\n    ], dtype=np.float32)\n\n    # Source points from detection\n    src_pts = np.array([\n        landmarks['left_eye'],\n        landmarks['right_eye'],\n        landmarks['nose'],\n        landmarks['mouth_left'],\n        landmarks['mouth_right']\n    ], dtype=np.float32)\n\n    # Compute similarity transform\n    transform = cv2.estimateAffinePartial2D(src_pts, ref_pts)[0]\n\n    # Apply transformation\n    aligned = cv2.warpAffine(\n        image, transform, output_size,\n        borderMode=cv2.BORDER_REPLICATE\n    )\n\n    return aligned\n```\n\n## Face Embedding & Clustering\n\n### Embedding Models\n\n| Model | Dimensions | Accuracy (LFW) | Speed |\n|-------|------------|----------------|-------|\n| **ArcFace** | 512 | 99.83% | Fast |\n| **AdaFace** | 512 | 99.82% | Fast |\n| **CosFace** | 512 | 99.73% | Fast |\n| **FaceNet** | 128/512 | 99.65% | Medium |\n\n### ArcFace Embedding\n\n```python\nimport torch\nfrom insightface.app import FaceAnalysis\n\nclass FaceEmbedder:\n    def __init__(self, model_name: str = 'buffalo_l'):\n        \"\"\"\n        Initialize face embedding model.\n\n        buffalo_l: ArcFace with ResNet100 backbone\n        buffalo_s: Lighter version for faster processing\n        \"\"\"\n        self.app = FaceAnalysis(\n            name=model_name,\n            providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n        )\n        self.app.prepare(ctx_id=0, det_size=(640, 640))\n\n    def get_embedding(self, image: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Get 512-dimensional face embedding.\n        \"\"\"\n        faces = self.app.get(image)\n        if len(faces) == 0:\n            return None\n\n        # Return embedding of largest face\n        largest_face = max(faces, key=lambda x: (x.bbox[2]-x.bbox[0]) * (x.bbox[3]-x.bbox[1]))\n        return largest_face.embedding\n\n    def get_all_embeddings(self, image: np.ndarray) -> list:\n        \"\"\"\n        Get embeddings for all faces in image.\n        \"\"\"\n        faces = self.app.get(image)\n        return [\n            {\n                'embedding': face.embedding,\n                'bbox': face.bbox.tolist(),\n                'landmarks': face.landmark_2d_106.tolist() if hasattr(face, 'landmark_2d_106') else None,\n                'age': face.age if hasattr(face, 'age') else None,\n                'gender': face.gender if hasattr(face, 'gender') else None\n            }\n            for face in faces\n        ]\n```\n\n### HDBSCAN Clustering\n\n```python\nimport hdbscan\nfrom sklearn.preprocessing import normalize\nimport numpy as np\n\ndef cluster_faces(\n    embeddings: np.ndarray,\n    min_cluster_size: int = 3,\n    min_samples: int = 2,\n    cluster_selection_epsilon: float = 0.3\n):\n    \"\"\"\n    Cluster face embeddings using HDBSCAN.\n\n    Why HDBSCAN over K-means?\n    - Doesn't require knowing number of guests in advance\n    - Handles noise (non-face detections, strangers)\n    - Works with varying cluster densities\n\n    Parameters tuned for wedding photos:\n    - min_cluster_size=3: At least 3 photos to be considered a \"person\"\n    - min_samples=2: Robust to outliers\n    - cluster_selection_epsilon=0.3: Allow some variation in embeddings\n    \"\"\"\n    # Normalize embeddings to unit sphere (cosine similarity)\n    embeddings_norm = normalize(embeddings)\n\n    # Cluster\n    clusterer = hdbscan.HDBSCAN(\n        min_cluster_size=min_cluster_size,\n        min_samples=min_samples,\n        metric='euclidean',  # On normalized vectors = cosine\n        cluster_selection_epsilon=cluster_selection_epsilon,\n        cluster_selection_method='eom',  # Excess of mass\n        prediction_data=True  # For adding new faces later\n    )\n\n    labels = clusterer.fit_predict(embeddings_norm)\n\n    # Get cluster centers for each identity\n    unique_labels = set(labels) - {-1}  # -1 is noise\n    centers = {}\n    for label in unique_labels:\n        mask = labels == label\n        centers[label] = embeddings_norm[mask].mean(axis=0)\n\n    return labels, centers, clusterer\n\ndef assign_new_face(\n    embedding: np.ndarray,\n    clusterer: hdbscan.HDBSCAN,\n    threshold: float = 0.6\n):\n    \"\"\"\n    Assign a new face to existing clusters.\n    Returns cluster label or -1 if no match.\n    \"\"\"\n    embedding_norm = normalize(embedding.reshape(1, -1))\n\n    # Use approximate_predict for new points\n    label, strength = hdbscan.approximate_predict(clusterer, embedding_norm)\n\n    if strength[0] > threshold:\n        return label[0]\n    return -1\n```\n\n## Aesthetic Quality Scoring\n\n### Multi-Factor Scoring Model\n\n```python\nimport cv2\nimport numpy as np\nfrom dataclasses import dataclass\n\n@dataclass\nclass AestheticScore:\n    technical: float  # Sharpness, exposure, noise\n    composition: float  # Rule of thirds, framing\n    expression: float  # Smile, eyes open, genuine emotion\n    context: float  # Group inclusion, moment importance\n    overall: float  # Weighted combination\n\ndef calculate_aesthetic_score(\n    image: np.ndarray,\n    face_bbox: list,\n    face_landmarks: dict,\n    is_candid: bool = True\n) -> AestheticScore:\n    \"\"\"\n    Calculate comprehensive aesthetic score for a face in a photo.\n    \"\"\"\n\n    # 1. Technical Quality (25%)\n    technical = calculate_technical_score(image, face_bbox)\n\n    # 2. Composition (20%)\n    composition = calculate_composition_score(image, face_bbox)\n\n    # 3. Expression (35%)\n    expression = calculate_expression_score(image, face_landmarks)\n\n    # 4. Context (20%)\n    context = calculate_context_score(image, face_bbox, is_candid)\n\n    # Weighted combination\n    overall = (\n        0.25 * technical +\n        0.20 * composition +\n        0.35 * expression +\n        0.20 * context\n    )\n\n    return AestheticScore(\n        technical=technical,\n        composition=composition,\n        expression=expression,\n        context=context,\n        overall=overall\n    )\n\ndef calculate_technical_score(image: np.ndarray, bbox: list) -> float:\n    \"\"\"\n    Score technical quality: sharpness, exposure, noise.\n    \"\"\"\n    x1, y1, x2, y2 = [int(v) for v in bbox]\n    face_region = image[y1:y2, x1:x2]\n\n    # Sharpness via Laplacian variance\n    gray = cv2.cvtColor(face_region, cv2.COLOR_BGR2GRAY)\n    sharpness = cv2.Laplacian(gray, cv2.CV_64F).var()\n    sharpness_score = min(1.0, sharpness / 500)  # Normalize\n\n    # Exposure via histogram analysis\n    hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n    hist = hist.flatten() / hist.sum()\n\n    # Penalize if too much in shadows (0-50) or highlights (200-255)\n    shadow_ratio = hist[:50].sum()\n    highlight_ratio = hist[200:].sum()\n    exposure_score = 1.0 - (shadow_ratio + highlight_ratio) * 0.5\n\n    # Noise estimation via median filter difference\n    denoised = cv2.medianBlur(gray, 3)\n    noise = np.abs(gray.astype(float) - denoised.astype(float)).mean()\n    noise_score = max(0, 1.0 - noise / 20)\n\n    return (sharpness_score + exposure_score + noise_score) / 3\n\ndef calculate_expression_score(image: np.ndarray, landmarks: dict) -> float:\n    \"\"\"\n    Score facial expression quality.\n\n    Factors:\n    - Eye openness (blink detection)\n    - Smile detection (Duchenne marker)\n    - Gaze direction\n    - Overall expression quality\n    \"\"\"\n    scores = []\n\n    # Eye openness\n    # Calculate eye aspect ratio (EAR)\n    left_eye = landmarks.get('left_eye')\n    right_eye = landmarks.get('right_eye')\n\n    if left_eye and right_eye:\n        # Simple EAR approximation\n        # Real implementation would use 6 points per eye\n        eye_openness = 0.8  # Placeholder\n        blink_penalty = 0.0 if eye_openness > 0.2 else 0.5\n        scores.append(1.0 - blink_penalty)\n\n    # Smile detection\n    mouth_left = landmarks.get('mouth_left')\n    mouth_right = landmarks.get('mouth_right')\n\n    if mouth_left and mouth_right:\n        # Mouth width relative to face width\n        mouth_width = np.linalg.norm(\n            np.array(mouth_right) - np.array(mouth_left)\n        )\n        # Wider smile = higher score (to a point)\n        smile_score = min(1.0, mouth_width / 50)\n        scores.append(smile_score)\n\n    # Gaze direction (looking at camera vs. away)\n    # For candids, looking away can be good\n    # For portraits, looking at camera is preferred\n    gaze_score = 0.7  # Placeholder\n    scores.append(gaze_score)\n\n    return np.mean(scores) if scores else 0.5\n\ndef calculate_composition_score(image: np.ndarray, bbox: list) -> float:\n    \"\"\"\n    Score composition quality.\n    \"\"\"\n    h, w = image.shape[:2]\n    x1, y1, x2, y2 = bbox\n    face_center_x = (x1 + x2) / 2 / w\n    face_center_y = (y1 + y2) / 2 / h\n\n    # Rule of thirds scoring\n    thirds_x = [1/3, 1/2, 2/3]\n    thirds_y = [1/3, 2/3]\n\n    min_dist_x = min(abs(face_center_x - t) for t in thirds_x)\n    min_dist_y = min(abs(face_center_y - t) for t in thirds_y)\n\n    thirds_score = 1.0 - (min_dist_x + min_dist_y)\n\n    # Face size (not too small, not too cropped)\n    face_area = (x2 - x1) * (y2 - y1)\n    image_area = w * h\n    face_ratio = face_area / image_area\n\n    # Optimal face ratio: 5-25% of image\n    if 0.05 <= face_ratio <= 0.25:\n        size_score = 1.0\n    elif face_ratio < 0.05:\n        size_score = face_ratio / 0.05\n    else:\n        size_score = max(0, 1.0 - (face_ratio - 0.25) * 2)\n\n    return (thirds_score + size_score) / 2\n```\n\n### Diversity-Aware Selection\n\n```python\ndef select_best_photos_diverse(\n    cluster_photos: list,\n    n: int = 5,\n    diversity_threshold: float = 0.7\n) -> list:\n    \"\"\"\n    Select top N photos for a person with diversity constraint.\n\n    Avoids selecting N nearly-identical shots from the same moment.\n    Instead, picks best photo from each distinct moment/pose.\n    \"\"\"\n\n    # Score all photos\n    scored = []\n    for photo in cluster_photos:\n        score = calculate_aesthetic_score(\n            photo['image'],\n            photo['bbox'],\n            photo['landmarks']\n        )\n        scored.append({\n            **photo,\n            'aesthetic_score': score\n        })\n\n    # Sort by overall score\n    scored.sort(key=lambda x: x['aesthetic_score'].overall, reverse=True)\n\n    # Select with diversity constraint\n    selected = []\n    for candidate in scored:\n        if len(selected) >= n:\n            break\n\n        # Check diversity against already selected\n        is_diverse = True\n        for existing in selected:\n            similarity = compute_photo_similarity(\n                candidate['embedding'],\n                existing['embedding']\n            )\n            if similarity > diversity_threshold:\n                is_diverse = False\n                break\n\n        if is_diverse:\n            selected.append(candidate)\n\n    # If we couldn't get N diverse photos, fill with best remaining\n    if len(selected) < n:\n        for candidate in scored:\n            if candidate not in selected:\n                selected.append(candidate)\n            if len(selected) >= n:\n                break\n\n    return selected\n\ndef compute_photo_similarity(emb1: np.ndarray, emb2: np.ndarray) -> float:\n    \"\"\"\n    Compute similarity between two photo embeddings.\n    Uses face embedding + pose + timestamp proximity.\n    \"\"\"\n    # Cosine similarity of face embeddings\n    face_sim = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n\n    return face_sim\n```\n\n## Identity Linking Workflow\n\n### Priority-Based Naming\n\n```python\nIDENTITY_PRIORITY = [\n    ('couple', ['bride', 'groom', 'spouse_1', 'spouse_2']),\n    ('wedding_party', ['best_man', 'maid_of_honor', 'bridesmaid', 'groomsman']),\n    ('parents', ['mother_bride', 'father_bride', 'mother_groom', 'father_groom']),\n    ('grandparents', ['grandmother', 'grandfather']),\n    ('siblings', ['sister', 'brother']),\n    ('extended_family', ['aunt', 'uncle', 'cousin']),\n    ('friends', []),\n    ('vendors', ['photographer', 'dj', 'coordinator']),\n]\n\ndef link_identities(\n    clusters: dict,\n    seed_identities: dict,  # User-provided: {cluster_id: \"Aunt Martha\"}\n    guest_list: list = None  # Optional: [\"Aunt Martha\", \"Uncle Bob\", ...]\n) -> dict:\n    \"\"\"\n    Link cluster IDs to human-readable names.\n\n    Workflow:\n    1. User tags couple in 2-3 photos → seeds those clusters\n    2. User optionally tags wedding party\n    3. System propagates through all photos\n    4. Remaining clusters get generic names or guest list matching\n    \"\"\"\n\n    identity_map = {}\n\n    # Start with user-provided seeds\n    for cluster_id, name in seed_identities.items():\n        identity_map[cluster_id] = {\n            'name': name,\n            'confidence': 1.0,\n            'source': 'user_tagged'\n        }\n\n    # Remaining clusters\n    unnamed_clusters = set(clusters.keys()) - set(identity_map.keys())\n\n    for i, cluster_id in enumerate(unnamed_clusters):\n        cluster_data = clusters[cluster_id]\n\n        # Try to match with guest list using any available signals\n        if guest_list:\n            # Could use location proximity to tagged people, etc.\n            pass\n\n        # Fallback to generic naming\n        identity_map[cluster_id] = {\n            'name': f\"Guest {i + 1}\",\n            'confidence': 0.5,\n            'source': 'auto_assigned'\n        }\n\n    return identity_map\n```\n\n## Output Format\n\n```json\n{\n  \"wedding_id\": \"smith-jones-2024\",\n  \"processed_date\": \"2024-12-15T10:30:00Z\",\n  \"total_photos\": 3847,\n  \"total_faces_detected\": 12453,\n  \"unique_identities\": 127,\n\n  \"identities\": [\n    {\n      \"cluster_id\": 0,\n      \"name\": \"Alex Smith\",\n      \"role\": \"spouse_1\",\n      \"photo_count\": 487,\n      \"best_photos\": [\n        {\n          \"photo_id\": \"IMG_2847.jpg\",\n          \"score\": 0.94,\n          \"scores\": {\n            \"technical\": 0.91,\n            \"composition\": 0.88,\n            \"expression\": 0.98,\n            \"context\": 0.95\n          },\n          \"moment\": \"first_dance\",\n          \"timestamp\": \"2024-11-15T20:45:00Z\"\n        }\n      ],\n      \"thumbnail\": \"clusters/0/thumbnail.jpg\"\n    }\n  ]\n}\n```\n"
        },
        {
          "name": "gaussian-splatting-pipeline.md",
          "type": "file",
          "path": "wedding-immortalist/references/gaussian-splatting-pipeline.md",
          "size": 13846,
          "content": "# 3D Gaussian Splatting Pipeline for Weddings\n\n## Complete Technical Pipeline\n\n### Overview\n\n3D Gaussian Splatting (3DGS) creates photorealistic, real-time renderable 3D scenes from photos/video. For weddings, we're reconstructing entire venues as explorable memory spaces.\n\n## Phase 1: Data Ingestion\n\n### Video Frame Extraction\n\n```python\nimport cv2\nimport os\nfrom pathlib import Path\n\ndef extract_frames(video_path: str, output_dir: str, fps: float = 2.0):\n    \"\"\"\n    Extract frames from wedding video at optimal rate for 3DGS.\n\n    Why 2-3 fps?\n    - Wedding videos are typically 30fps\n    - Adjacent frames are nearly identical (redundant)\n    - 2-3fps maintains 80%+ overlap while reducing processing 10x\n    - More frames ≠ better quality after sufficient overlap\n    \"\"\"\n    cap = cv2.VideoCapture(video_path)\n    video_fps = cap.get(cv2.CAP_PROP_FPS)\n    frame_interval = int(video_fps / fps)\n\n    Path(output_dir).mkdir(parents=True, exist_ok=True)\n\n    frame_count = 0\n    saved_count = 0\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        if frame_count % frame_interval == 0:\n            # Check for blur before saving\n            laplacian_var = cv2.Laplacian(frame, cv2.CV_64F).var()\n            if laplacian_var > 100:  # Reject blurry frames\n                cv2.imwrite(f\"{output_dir}/frame_{saved_count:06d}.jpg\", frame)\n                saved_count += 1\n\n        frame_count += 1\n\n    cap.release()\n    return saved_count\n\n# Quality thresholds\nBLUR_THRESHOLD = 100  # Laplacian variance\nMIN_IMAGES_PER_SPACE = 50\nOPTIMAL_IMAGES_PER_SPACE = 150\nMAX_IMAGES_PER_SPACE = 300  # Diminishing returns after this\n```\n\n### Photo Organization\n\n```python\nfrom datetime import datetime\nfrom PIL import Image\nfrom PIL.ExifTags import TAGS\nimport shutil\n\ndef organize_wedding_photos(source_dir: str, output_dir: str):\n    \"\"\"\n    Organize photos by time and location for multi-space reconstruction.\n    \"\"\"\n    photos = []\n\n    for img_path in Path(source_dir).glob(\"**/*.{jpg,jpeg,JPG,JPEG,png,PNG}\"):\n        try:\n            img = Image.open(img_path)\n            exif = img._getexif()\n\n            timestamp = None\n            gps = None\n\n            if exif:\n                for tag_id, value in exif.items():\n                    tag = TAGS.get(tag_id, tag_id)\n                    if tag == \"DateTimeOriginal\":\n                        timestamp = datetime.strptime(value, \"%Y:%m:%d %H:%M:%S\")\n                    elif tag == \"GPSInfo\":\n                        gps = value\n\n            photos.append({\n                'path': img_path,\n                'timestamp': timestamp,\n                'gps': gps,\n                'resolution': img.size\n            })\n        except Exception as e:\n            print(f\"Skipping {img_path}: {e}\")\n\n    # Sort by timestamp\n    photos.sort(key=lambda x: x['timestamp'] or datetime.min)\n\n    # Cluster into spaces based on time gaps\n    spaces = cluster_into_spaces(photos)\n\n    return spaces\n\ndef cluster_into_spaces(photos, gap_threshold_minutes=15):\n    \"\"\"\n    Cluster photos into distinct spaces/moments based on time gaps.\n\n    Typical wedding timeline:\n    - Getting ready (1-2 hours)\n    - Ceremony (30-60 min)\n    - Cocktail hour (1 hour)\n    - Reception entrance + dinner (1-2 hours)\n    - Dancing + party (2-3 hours)\n    \"\"\"\n    spaces = []\n    current_space = []\n\n    for i, photo in enumerate(photos):\n        if i == 0:\n            current_space.append(photo)\n            continue\n\n        prev_time = photos[i-1]['timestamp']\n        curr_time = photo['timestamp']\n\n        if prev_time and curr_time:\n            gap = (curr_time - prev_time).total_seconds() / 60\n            if gap > gap_threshold_minutes:\n                spaces.append(current_space)\n                current_space = []\n\n        current_space.append(photo)\n\n    if current_space:\n        spaces.append(current_space)\n\n    return spaces\n```\n\n## Phase 2: COLMAP Structure from Motion\n\n### Feature Extraction\n\n```bash\n#!/bin/bash\n# colmap_sfm.sh - Structure from Motion pipeline\n\nWORKSPACE=$1\nIMAGE_PATH=$2\n\n# 1. Feature extraction with SIFT\ncolmap feature_extractor \\\n    --database_path $WORKSPACE/database.db \\\n    --image_path $IMAGE_PATH \\\n    --ImageReader.single_camera 0 \\\n    --ImageReader.camera_model OPENCV \\\n    --SiftExtraction.max_image_size 3200 \\\n    --SiftExtraction.max_num_features 8192 \\\n    --SiftExtraction.first_octave -1 \\\n    --SiftExtraction.num_threads -1\n\n# 2. Feature matching\n# For wedding photos with lots of similar views, exhaustive matching works best\ncolmap exhaustive_matcher \\\n    --database_path $WORKSPACE/database.db \\\n    --SiftMatching.guided_matching 1 \\\n    --SiftMatching.max_ratio 0.8 \\\n    --SiftMatching.max_distance 0.7\n\n# 3. Sparse reconstruction (SfM)\nmkdir -p $WORKSPACE/sparse\ncolmap mapper \\\n    --database_path $WORKSPACE/database.db \\\n    --image_path $IMAGE_PATH \\\n    --output_path $WORKSPACE/sparse \\\n    --Mapper.ba_refine_focal_length 1 \\\n    --Mapper.ba_refine_principal_point 1 \\\n    --Mapper.ba_refine_extra_params 1\n\n# 4. Undistort images for dense reconstruction\ncolmap image_undistorter \\\n    --image_path $IMAGE_PATH \\\n    --input_path $WORKSPACE/sparse/0 \\\n    --output_path $WORKSPACE/dense \\\n    --output_type COLMAP\n\necho \"SfM complete. Check $WORKSPACE/sparse/0 for camera poses.\"\n```\n\n### Handling Multiple Spaces\n\n```python\ndef merge_reconstructions(spaces: list, output_path: str):\n    \"\"\"\n    For weddings spanning multiple distinct spaces (ceremony, reception),\n    we have two options:\n\n    1. SEPARATE SCENES: Train individual 3DGS models per space\n       - Pros: Better quality per scene, simpler training\n       - Cons: Need scene transitions in viewer\n\n    2. MERGED SCENE: Use shared features to align all spaces\n       - Pros: Seamless navigation\n       - Cons: Harder to reconstruct, may need manual alignment\n\n    For weddings, SEPARATE SCENES is usually better.\n    \"\"\"\n\n    # Create navigation graph between spaces\n    navigation = {\n        'spaces': [],\n        'transitions': []\n    }\n\n    for i, space in enumerate(spaces):\n        navigation['spaces'].append({\n            'id': f'space_{i}',\n            'name': space['name'],  # e.g., \"ceremony\", \"reception\"\n            'model_path': f'{output_path}/space_{i}',\n            'entry_point': space.get('entry_camera'),  # Best starting view\n            'thumbnail': space.get('thumbnail')\n        })\n\n    # Define logical transitions\n    navigation['transitions'] = [\n        {'from': 'ceremony', 'to': 'cocktail', 'type': 'fade'},\n        {'from': 'cocktail', 'to': 'reception', 'type': 'walk'},\n        # etc.\n    ]\n\n    return navigation\n```\n\n## Phase 3: 3DGS Training\n\n### Training Configuration\n\n```python\n# wedding_3dgs_config.py\n\nWEDDING_3DGS_CONFIG = {\n    # Training iterations\n    'iterations': 50_000,  # High quality for permanent archive\n\n    # Densification settings\n    'densify_from_iter': 500,\n    'densify_until_iter': 15_000,\n    'densification_interval': 100,\n    'opacity_reset_interval': 3000,\n\n    # Gaussian parameters\n    'sh_degree': 3,  # Full spherical harmonics for complex lighting\n    'percent_dense': 0.01,\n    'densify_grad_threshold': 0.0002,\n\n    # Pruning\n    'min_opacity': 0.005,\n    'max_screen_size': 20,  # Max pixel size before splitting\n\n    # Learning rates\n    'position_lr_init': 0.00016,\n    'position_lr_final': 0.0000016,\n    'position_lr_delay_mult': 0.01,\n    'position_lr_max_steps': 30_000,\n    'feature_lr': 0.0025,\n    'opacity_lr': 0.05,\n    'scaling_lr': 0.005,\n    'rotation_lr': 0.001,\n\n    # Loss weights\n    'lambda_dssim': 0.2,  # Structural similarity weight\n\n    # Performance\n    'white_background': False,  # Wedding venues rarely have white bg\n    'data_device': 'cuda',\n    'convert_SHs_python': False,\n    'compute_cov3D_python': False,\n}\n\n# Quality presets\nQUALITY_PRESETS = {\n    'preview': {\n        'iterations': 7_000,\n        'densify_until_iter': 5_000,\n        'description': 'Quick preview in ~5 minutes'\n    },\n    'standard': {\n        'iterations': 30_000,\n        'densify_until_iter': 15_000,\n        'description': 'Good quality in ~30 minutes'\n    },\n    'high': {\n        'iterations': 50_000,\n        'densify_until_iter': 20_000,\n        'description': 'High quality in ~1 hour'\n    },\n    'archival': {\n        'iterations': 100_000,\n        'densify_until_iter': 30_000,\n        'description': 'Maximum quality in ~3 hours'\n    }\n}\n```\n\n### Training Script\n\n```python\nimport torch\nfrom gaussian_splatting import GaussianModel, train\nfrom scene import Scene\nimport os\n\ndef train_wedding_scene(\n    source_path: str,\n    output_path: str,\n    quality: str = 'high'\n):\n    \"\"\"\n    Train 3DGS model for a wedding space.\n\n    Args:\n        source_path: COLMAP output directory\n        output_path: Where to save trained model\n        quality: 'preview', 'standard', 'high', or 'archival'\n    \"\"\"\n\n    config = {**WEDDING_3DGS_CONFIG, **QUALITY_PRESETS[quality]}\n\n    # Initialize Gaussian model\n    gaussians = GaussianModel(config['sh_degree'])\n\n    # Load scene from COLMAP\n    scene = Scene(source_path, gaussians)\n\n    # Training loop with wedding-specific optimizations\n    for iteration in range(config['iterations']):\n        # Render\n        render_pkg = render(\n            scene.getTrainCameras()[iteration % len(scene.getTrainCameras())],\n            gaussians,\n            background=torch.zeros(3).cuda()\n        )\n\n        # Loss\n        image = render_pkg['render']\n        gt_image = scene.getTrainCameras()[iteration % len(scene.getTrainCameras())].original_image\n\n        l1_loss = torch.abs(image - gt_image).mean()\n        ssim_loss = 1.0 - ssim(image, gt_image)\n        loss = (1 - config['lambda_dssim']) * l1_loss + config['lambda_dssim'] * ssim_loss\n\n        loss.backward()\n\n        # Densification\n        if iteration < config['densify_until_iter']:\n            if iteration > config['densify_from_iter'] and iteration % config['densification_interval'] == 0:\n                gaussians.densify_and_prune(\n                    config['densify_grad_threshold'],\n                    config['min_opacity'],\n                    scene.cameras_extent,\n                    config['max_screen_size']\n                )\n\n        # Optimizer step\n        gaussians.optimizer.step()\n        gaussians.optimizer.zero_grad()\n\n        # Logging\n        if iteration % 1000 == 0:\n            print(f\"Iteration {iteration}: Loss = {loss.item():.6f}\")\n\n        # Save checkpoint\n        if iteration % 10000 == 0:\n            torch.save(gaussians.capture(), f\"{output_path}/checkpoint_{iteration}.pth\")\n\n    # Final save\n    gaussians.save_ply(f\"{output_path}/point_cloud.ply\")\n\n    return output_path\n```\n\n## Phase 4: Web Viewer Integration\n\n### Viewer Architecture\n\n```typescript\n// WeddingViewer.tsx\nimport { useEffect, useRef, useState } from 'react';\nimport * as THREE from 'three';\nimport { SplatLoader } from '@mkkellogg/gaussian-splats-3d';\n\ninterface WeddingViewerProps {\n  spaces: WeddingSpace[];\n  moments: TheatreMoment[];\n  onMomentClick: (moment: TheatreMoment) => void;\n}\n\nexport function WeddingViewer({ spaces, moments, onMomentClick }: WeddingViewerProps) {\n  const containerRef = useRef<HTMLDivElement>(null);\n  const viewerRef = useRef<GaussianSplatViewer | null>(null);\n  const [currentSpace, setCurrentSpace] = useState(0);\n  const [loading, setLoading] = useState(true);\n\n  useEffect(() => {\n    if (!containerRef.current) return;\n\n    // Initialize Three.js scene\n    const scene = new THREE.Scene();\n    const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);\n    const renderer = new THREE.WebGLRenderer({ antialias: true });\n\n    // Load Gaussian Splat\n    const loader = new SplatLoader();\n    loader.load(spaces[currentSpace].modelPath, (splat) => {\n      scene.add(splat);\n      setLoading(false);\n    });\n\n    // Add moment markers\n    moments.forEach(moment => {\n      const marker = createMomentMarker(moment);\n      marker.onClick = () => onMomentClick(moment);\n      scene.add(marker);\n    });\n\n    // Controls\n    const controls = new OrbitControls(camera, renderer.domElement);\n    controls.enableDamping = true;\n\n    // Animation loop\n    function animate() {\n      requestAnimationFrame(animate);\n      controls.update();\n      renderer.render(scene, camera);\n    }\n    animate();\n\n    return () => {\n      renderer.dispose();\n    };\n  }, [currentSpace, spaces, moments]);\n\n  return (\n    <div ref={containerRef} className=\"wedding-viewer\">\n      {loading && <LoadingOverlay theme={spaces[currentSpace].theme} />}\n      <SpaceNavigator\n        spaces={spaces}\n        current={currentSpace}\n        onChange={setCurrentSpace}\n      />\n    </div>\n  );\n}\n```\n\n## Hardware Requirements\n\n| Component | Minimum | Recommended | Notes |\n|-----------|---------|-------------|-------|\n| GPU | RTX 3060 12GB | RTX 4080 16GB | VRAM is the bottleneck |\n| RAM | 32GB | 64GB | For large photo sets |\n| Storage | 100GB SSD | 500GB NVMe | Fast I/O matters |\n| CPU | 8 cores | 16+ cores | For COLMAP parallelization |\n\n## Troubleshooting\n\n### Common Issues\n\n1. **Sparse point cloud**: Not enough image overlap\n   - Solution: Add more photos from in-between angles\n\n2. **Floaters**: Random gaussians in empty space\n   - Solution: Increase opacity pruning, reduce learning rate\n\n3. **Blurry reconstruction**: Motion blur in source images\n   - Solution: Filter frames with Laplacian variance &lt; 100\n\n4. **Memory errors**: Too many gaussians\n   - Solution: Reduce densification, increase pruning\n\n### Quality Checklist\n\n- [ ] Minimum 50 images per distinct space\n- [ ] 60-80% overlap between adjacent views\n- [ ] No motion blur (Laplacian variance > 100)\n- [ ] Consistent lighting (avoid mixed indoor/outdoor)\n- [ ] All guests' faces visible in at least 3 angles\n"
        },
        {
          "name": "theme-extraction.md",
          "type": "file",
          "path": "wedding-immortalist/references/theme-extraction.md",
          "size": 14905,
          "content": "# Wedding Theme Extraction & Design System\n\n## Overview\n\nEvery wedding has a unique aesthetic identity. This system extracts that identity from photos and generates a cohesive design system for the digital experience.\n\n## Theme Detection Pipeline\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                  THEME EXTRACTION PIPELINE                       │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                  │\n│  1. COLOR EXTRACTION     2. STYLE CLASSIFICATION                 │\n│  ├─ Dominant colors      ├─ Era detection (70s, modern, etc.)   │\n│  ├─ Palette clustering   ├─ Formality level                     │\n│  └─ Accent identification└─ Cultural markers                    │\n│                                                                  │\n│  3. TYPOGRAPHY MATCH     4. UI GENERATION                        │\n│  ├─ Era-appropriate      ├─ Component theming                   │\n│  ├─ Mood-aligned         ├─ Gradient definitions                │\n│  └─ Readability check    └─ Animation style                     │\n│                                                                  │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n## Color Extraction\n\n### Dominant Color Analysis\n\n```python\nimport cv2\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom collections import Counter\nfrom colorthief import ColorThief\n\ndef extract_wedding_palette(images: list, n_colors: int = 6):\n    \"\"\"\n    Extract dominant color palette from wedding photos.\n\n    Strategy:\n    1. Sample from key photo categories (venue, florals, attire, decor)\n    2. Weight certain areas higher (florals > background)\n    3. Cluster into cohesive palette\n    \"\"\"\n    all_colors = []\n\n    for img_path in images:\n        # Extract using ColorThief (fast, quality-focused)\n        thief = ColorThief(img_path)\n        palette = thief.get_palette(color_count=6, quality=1)\n        all_colors.extend(palette)\n\n    # Cluster all extracted colors\n    colors_array = np.array(all_colors)\n    kmeans = KMeans(n_clusters=n_colors, random_state=42)\n    kmeans.fit(colors_array)\n\n    # Sort by frequency\n    labels = kmeans.labels_\n    label_counts = Counter(labels)\n    sorted_labels = sorted(label_counts.keys(), key=lambda x: label_counts[x], reverse=True)\n\n    palette = []\n    for label in sorted_labels:\n        rgb = kmeans.cluster_centers_[label].astype(int)\n        palette.append({\n            'rgb': tuple(rgb),\n            'hex': '#{:02x}{:02x}{:02x}'.format(*rgb),\n            'frequency': label_counts[label] / len(labels)\n        })\n\n    return palette\n\ndef categorize_palette_roles(palette: list) -> dict:\n    \"\"\"\n    Assign semantic roles to extracted colors.\n\n    Roles:\n    - primary: Main brand/theme color\n    - secondary: Complementary accent\n    - background: Light/neutral base\n    - text: Dark/readable color\n    - accent: Pop of color for CTAs\n    - highlight: Subtle emphasis\n    \"\"\"\n    from colormath.color_objects import sRGBColor, LabColor\n    from colormath.color_conversions import convert_color\n\n    roles = {}\n    remaining = list(palette)\n\n    # Find lightest for background\n    remaining.sort(key=lambda c: sum(c['rgb']))\n    roles['background'] = remaining.pop()\n\n    # Find darkest for text\n    remaining.sort(key=lambda c: sum(c['rgb']), reverse=True)\n    roles['text'] = remaining.pop()\n\n    # Most frequent remaining is primary\n    remaining.sort(key=lambda c: c['frequency'], reverse=True)\n    roles['primary'] = remaining.pop(0)\n\n    # Second most frequent is secondary\n    if remaining:\n        roles['secondary'] = remaining.pop(0)\n\n    # Most saturated remaining is accent\n    if remaining:\n        def saturation(c):\n            r, g, b = [x/255 for x in c['rgb']]\n            max_c, min_c = max(r, g, b), min(r, g, b)\n            return (max_c - min_c) / (max_c + 0.001)\n\n        remaining.sort(key=saturation, reverse=True)\n        roles['accent'] = remaining.pop(0)\n\n    # Rest are highlights\n    if remaining:\n        roles['highlight'] = remaining[0]\n\n    return roles\n```\n\n### Era & Style Detection\n\n```python\nfrom enum import Enum\nfrom typing import Tuple\nimport colorsys\n\nclass WeddingEra(Enum):\n    SEVENTIES_DISCO = \"70s_disco\"\n    EIGHTIES_GLAM = \"80s_glam\"\n    NINETIES_MINIMALIST = \"90s_minimalist\"\n    RUSTIC_BARN = \"rustic_barn\"\n    MODERN_MINIMAL = \"modern_minimal\"\n    BEACH_COASTAL = \"beach_coastal\"\n    GARDEN_ROMANTIC = \"garden_romantic\"\n    GLAMOROUS_GATSBY = \"gatsby_glamour\"\n    BOHEMIAN = \"bohemian\"\n    CULTURAL_TRADITIONAL = \"cultural_traditional\"\n    QUEER_CELEBRATION = \"queer_celebration\"\n\ndef detect_wedding_era(\n    palette: list,\n    detected_objects: list,  # From object detection\n    venue_type: str = None\n) -> Tuple[WeddingEra, float]:\n    \"\"\"\n    Detect the wedding's aesthetic era/style.\n\n    Signals:\n    - Color palette temperature and saturation\n    - Detected objects (disco ball, barn wood, beach)\n    - Venue type if provided\n    - Attire style (detected or user-specified)\n    \"\"\"\n\n    scores = {era: 0.0 for era in WeddingEra}\n\n    # Analyze palette characteristics\n    avg_saturation = np.mean([\n        colorsys.rgb_to_hsv(*[c/255 for c in color['rgb']])[1]\n        for color in palette\n    ])\n\n    warm_ratio = sum(\n        1 for c in palette\n        if c['rgb'][0] > c['rgb'][2]  # R > B = warm\n    ) / len(palette)\n\n    # 70s Disco indicators\n    if any(obj in detected_objects for obj in ['disco_ball', 'mirror_ball', 'sequins']):\n        scores[WeddingEra.SEVENTIES_DISCO] += 0.5\n\n    # Color patterns for 70s\n    earth_tones = ['#D2691E', '#8B4513', '#DAA520', '#CD853F']\n    if palette_matches_tones(palette, earth_tones, threshold=0.3):\n        scores[WeddingEra.SEVENTIES_DISCO] += 0.3\n    if warm_ratio > 0.7:\n        scores[WeddingEra.SEVENTIES_DISCO] += 0.2\n\n    # Rustic indicators\n    if any(obj in detected_objects for obj in ['barn', 'wood', 'burlap', 'mason_jar']):\n        scores[WeddingEra.RUSTIC_BARN] += 0.5\n\n    # Modern minimal indicators\n    if avg_saturation < 0.3:  # Desaturated palette\n        scores[WeddingEra.MODERN_MINIMAL] += 0.3\n\n    # Beach indicators\n    if any(obj in detected_objects for obj in ['beach', 'ocean', 'sand', 'palm']):\n        scores[WeddingEra.BEACH_COASTAL] += 0.5\n\n    # Rainbow/pride indicators for queer celebrations\n    rainbow_coverage = check_rainbow_coverage(palette)\n    if rainbow_coverage > 0.5:\n        scores[WeddingEra.QUEER_CELEBRATION] += 0.4\n\n    # Find highest scoring era\n    best_era = max(scores, key=scores.get)\n    confidence = scores[best_era]\n\n    # Normalize confidence\n    total = sum(scores.values())\n    if total > 0:\n        confidence = scores[best_era] / total\n\n    return best_era, confidence\n\ndef palette_matches_tones(palette: list, reference_tones: list, threshold: float) -> bool:\n    \"\"\"Check if palette matches reference color tones.\"\"\"\n    from colormath.color_objects import sRGBColor, LabColor\n    from colormath.color_conversions import convert_color\n    from colormath.color_diff import delta_e_cie2000\n\n    matches = 0\n    for color in palette:\n        rgb = sRGBColor(*[c/255 for c in color['rgb']])\n        lab = convert_color(rgb, LabColor)\n\n        for ref_hex in reference_tones:\n            ref_rgb = sRGBColor.new_from_rgb_hex(ref_hex)\n            ref_lab = convert_color(ref_rgb, LabColor)\n\n            delta = delta_e_cie2000(lab, ref_lab)\n            if delta < 20:  # Close enough\n                matches += 1\n                break\n\n    return matches / len(palette) >= threshold\n```\n\n## Theme Templates\n\n### 70s Disco Theme\n\n```typescript\n// themes/70s-disco.ts\nexport const discoTheme = {\n  name: \"70s Disco\",\n\n  colors: {\n    primary: '#D2691E',      // Burnt orange\n    secondary: '#DAA520',    // Goldenrod\n    accent: '#8B008B',       // Dark magenta\n    background: '#1a1a2e',   // Deep purple-black\n    surface: '#2d2d44',      // Lighter purple\n    text: '#FFFFFF',\n    textMuted: '#B0A090',\n  },\n\n  gradients: {\n    sunset: 'linear-gradient(180deg, #FF6B35 0%, #D2691E 50%, #8B008B 100%)',\n    disco: 'linear-gradient(45deg, #FFD700, #FF6B35, #8B008B, #4169E1)',\n    gold: 'linear-gradient(180deg, #FFD700 0%, #DAA520 100%)',\n  },\n\n  typography: {\n    display: \"'Playfair Display', serif\",  // Elegant, era-appropriate\n    heading: \"'Bebas Neue', sans-serif\",   // Bold, groovy\n    body: \"'Lato', sans-serif\",            // Clean readability\n    accent: \"'Pacifico', cursive\",         // Fun script moments\n  },\n\n  effects: {\n    glowColor: 'rgba(255, 215, 0, 0.6)',\n    shadowColor: 'rgba(139, 0, 139, 0.3)',\n    borderRadius: '0px',  // Sharp 70s edges\n    borderStyle: '3px solid',\n  },\n\n  patterns: {\n    starburst: true,\n    mirrorBall: true,\n    geometricShapes: ['hexagon', 'star', 'diamond'],\n  },\n\n  animations: {\n    type: 'groovy',\n    easing: 'cubic-bezier(0.68, -0.55, 0.265, 1.55)',  // Bouncy\n    duration: '0.6s',\n  },\n\n  components: {\n    button: {\n      background: 'var(--gradient-gold)',\n      color: '#1a1a2e',\n      fontFamily: 'var(--font-heading)',\n      textTransform: 'uppercase',\n      letterSpacing: '2px',\n      border: 'none',\n      boxShadow: '0 4px 15px var(--glow-color)',\n    },\n    card: {\n      background: 'var(--surface)',\n      border: '2px solid var(--primary)',\n      boxShadow: '0 0 20px var(--shadow-color)',\n    },\n    header: {\n      background: 'var(--gradient-sunset)',\n      color: 'white',\n      fontFamily: 'var(--font-display)',\n    },\n  },\n};\n```\n\n### Modern Minimal Theme\n\n```typescript\n// themes/modern-minimal.ts\nexport const modernMinimalTheme = {\n  name: \"Modern Minimal\",\n\n  colors: {\n    primary: '#2C3E50',      // Deep slate\n    secondary: '#E8E8E8',    // Warm gray\n    accent: '#C9A959',       // Muted gold\n    background: '#FFFFFF',\n    surface: '#F8F8F8',\n    text: '#1A1A1A',\n    textMuted: '#6B7280',\n  },\n\n  gradients: {\n    subtle: 'linear-gradient(180deg, #FFFFFF 0%, #F8F8F8 100%)',\n    accent: 'linear-gradient(90deg, #C9A959 0%, #D4AF37 100%)',\n  },\n\n  typography: {\n    display: \"'Cormorant Garamond', serif\",\n    heading: \"'Montserrat', sans-serif\",\n    body: \"'Open Sans', sans-serif\",\n    accent: \"'Cormorant Garamond', serif\",\n  },\n\n  effects: {\n    shadowColor: 'rgba(0, 0, 0, 0.05)',\n    borderRadius: '2px',\n    borderStyle: '1px solid #E8E8E8',\n  },\n\n  animations: {\n    type: 'subtle',\n    easing: 'ease-out',\n    duration: '0.3s',\n  },\n};\n```\n\n### Queer Celebration Theme\n\n```typescript\n// themes/queer-celebration.ts\nexport const queerCelebrationTheme = {\n  name: \"Queer Celebration\",\n\n  colors: {\n    // Rainbow spectrum\n    red: '#E50000',\n    orange: '#FF8D00',\n    yellow: '#FFEE00',\n    green: '#028121',\n    blue: '#004CFF',\n    purple: '#770088',\n\n    // UI colors\n    primary: '#770088',\n    secondary: '#004CFF',\n    accent: '#FFEE00',\n    background: '#FFFFFF',\n    surface: '#F5F0FF',\n    text: '#1A1A1A',\n  },\n\n  gradients: {\n    pride: 'linear-gradient(90deg, #E50000, #FF8D00, #FFEE00, #028121, #004CFF, #770088)',\n    prideVertical: 'linear-gradient(180deg, #E50000, #FF8D00, #FFEE00, #028121, #004CFF, #770088)',\n    trans: 'linear-gradient(180deg, #55CDFC, #F7A8B8, #FFFFFF, #F7A8B8, #55CDFC)',\n    bi: 'linear-gradient(180deg, #D60270, #9B4F96, #0038A8)',\n    nonbinary: 'linear-gradient(180deg, #FCF434, #FFFFFF, #9C59D1, #2C2C2C)',\n  },\n\n  typography: {\n    display: \"'Playfair Display', serif\",\n    heading: \"'Poppins', sans-serif\",\n    body: \"'Inter', sans-serif\",\n    accent: \"'Dancing Script', cursive\",\n  },\n\n  effects: {\n    glowColor: 'rgba(119, 0, 136, 0.4)',\n    borderRadius: '8px',\n    borderStyle: '2px solid',\n  },\n\n  patterns: {\n    rainbow: true,\n    hearts: true,\n    confetti: true,\n  },\n\n  animations: {\n    type: 'joyful',\n    easing: 'cubic-bezier(0.34, 1.56, 0.64, 1)',\n    duration: '0.5s',\n  },\n\n  specialElements: {\n    pronounBadges: true,\n    chosenFamilyHighlight: true,\n    prideFlags: ['rainbow', 'trans', 'bi', 'nonbinary', 'pan', 'lesbian', 'gay'],\n  },\n};\n```\n\n## CSS Generation\n\n```python\ndef generate_css_variables(theme: dict) -> str:\n    \"\"\"\n    Generate CSS custom properties from theme.\n    \"\"\"\n    css = \":root {\\n\"\n\n    # Colors\n    for name, value in theme['colors'].items():\n        css += f\"  --color-{name}: {value};\\n\"\n\n    # Gradients\n    for name, value in theme.get('gradients', {}).items():\n        css += f\"  --gradient-{name}: {value};\\n\"\n\n    # Typography\n    for name, value in theme.get('typography', {}).items():\n        css += f\"  --font-{name}: {value};\\n\"\n\n    # Effects\n    for name, value in theme.get('effects', {}).items():\n        css_name = name.replace('_', '-')\n        css += f\"  --{css_name}: {value};\\n\"\n\n    # Animation\n    if 'animations' in theme:\n        css += f\"  --animation-easing: {theme['animations']['easing']};\\n\"\n        css += f\"  --animation-duration: {theme['animations']['duration']};\\n\"\n\n    css += \"}\\n\"\n\n    return css\n```\n\n## Theme Application\n\n```typescript\n// ThemeProvider.tsx\nimport { createContext, useContext, ReactNode } from 'react';\n\ninterface ThemeContextValue {\n  theme: WeddingTheme;\n  setTheme: (theme: WeddingTheme) => void;\n}\n\nconst ThemeContext = createContext<ThemeContextValue | null>(null);\n\nexport function WeddingThemeProvider({\n  children,\n  extractedTheme\n}: {\n  children: ReactNode;\n  extractedTheme: WeddingTheme;\n}) {\n  const [theme, setTheme] = useState(extractedTheme);\n\n  // Apply CSS variables\n  useEffect(() => {\n    const root = document.documentElement;\n\n    Object.entries(theme.colors).forEach(([key, value]) => {\n      root.style.setProperty(`--color-${key}`, value);\n    });\n\n    Object.entries(theme.gradients || {}).forEach(([key, value]) => {\n      root.style.setProperty(`--gradient-${key}`, value);\n    });\n\n    // Apply font imports\n    const fontLink = document.createElement('link');\n    fontLink.href = generateGoogleFontsUrl(theme.typography);\n    fontLink.rel = 'stylesheet';\n    document.head.appendChild(fontLink);\n\n    return () => {\n      document.head.removeChild(fontLink);\n    };\n  }, [theme]);\n\n  return (\n    <ThemeContext.Provider value={{ theme, setTheme }}>\n      {children}\n    </ThemeContext.Provider>\n  );\n}\n\nexport function useWeddingTheme() {\n  const context = useContext(ThemeContext);\n  if (!context) {\n    throw new Error('useWeddingTheme must be used within WeddingThemeProvider');\n  }\n  return context;\n}\n```\n"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "wedding-immortalist/SKILL.md",
      "size": 15356,
      "content": "---\nname: wedding-immortalist\ndescription: Transform thousands of wedding photos and hours of footage into an immersive 3D Gaussian Splatting experience with theatre mode replay, face-clustered guest roster, and AI-curated best photos per person. Expert in 3DGS pipelines, face clustering, aesthetic scoring, and adaptive design matching the couple's wedding theme (disco, rustic, modern, LGBTQ+ celebrations). Activate on \"wedding photos\", \"wedding video\", \"3D wedding\", \"Gaussian Splatting wedding\", \"wedding memory\", \"wedding immortalize\", \"face clustering wedding\", \"best wedding photos\". NOT for general photo editing (use native-app-designer), non-wedding 3DGS (use drone-inspection-specialist), or event planning (not a wedding planner).\nallowed-tools: Read,Write,Edit,Bash,Grep,Glob,WebFetch\ncategory: AI & Machine Learning\ntags:\n  - wedding\n  - 3dgs\n  - gaussian-splatting\n  - face-clustering\n  - memories\npairs-with:\n  - skill: photo-content-recognition-curation-expert\n    reason: Curate wedding photos\n  - skill: event-detection-temporal-intelligence-expert\n    reason: Detect wedding events\n---\n\n# Wedding Immortalist\n\nTransform wedding photos and video into an eternal, immersive 3D experience. Create living memories that let couples and guests relive the magic forever.\n\n## When to Use This Skill\n\n**Use for:**\n- Processing thousands of wedding photos into 3DGS scenes\n- Creating theatre-mode experiences where ceremony/reception moments play in-place\n- Building face-clustered guest rosters with best-photo selection\n- Matching design aesthetics to wedding themes (disco, rustic, beach, modern, queer celebrations)\n- AI-curated photo selection per guest with aesthetic scoring\n\n**NOT for:**\n- General photo editing → use native-app-designer\n- Non-wedding 3DGS → use drone-inspection-specialist\n- Event planning → not a wedding planner\n- Video editing without 3D reconstruction\n\n## Core Pipeline\n\n```\n┌─────────────────────────────────────────────────────────────────┐\n│                    WEDDING IMMORTALIST PIPELINE                  │\n├─────────────────────────────────────────────────────────────────┤\n│                                                                  │\n│  1. INGEST                2. RECONSTRUCT        3. CLUSTER       │\n│  ├─ Photos (1000s)        ├─ COLMAP SfM         ├─ Face detect   │\n│  ├─ Video (hours)         ├─ 3DGS training      ├─ Embeddings    │\n│  └─ Audio/speeches        └─ Scene merge        └─ Identity link │\n│                                                                  │\n│  4. CURATE                5. DESIGN             6. PRESENT       │\n│  ├─ Aesthetic score       ├─ Theme extract      ├─ Web viewer    │\n│  ├─ Per-person best       ├─ Color palette      ├─ Theatre mode  │\n│  └─ Moment detect         └─ Typography         └─ Guest roster  │\n│                                                                  │\n└─────────────────────────────────────────────────────────────────┘\n```\n\n## Theme-Adaptive Design\n\n### Theme Detection & Matching\n\nEvery wedding has a unique aesthetic. Extract and honor it:\n\n| Theme Type | Color Palette | Typography | UI Elements |\n|------------|---------------|------------|-------------|\n| **70s Disco** | Gold, orange, burnt sienna, deep purple | Groovy script, bold sans | Mirror balls, starbursts, warm gradients |\n| **Rustic/Barn** | Earth tones, sage, cream, wood | Serif, hand-lettered | Burlap textures, wildflower accents |\n| **Beach/Coastal** | Ocean blues, sand, coral, seafoam | Light sans, script | Shell motifs, wave patterns |\n| **Modern Minimal** | Black, white, metallics | Clean geometric sans | Sharp lines, negative space |\n| **Queer Joy** | Rainbow spectrums, bold colors | Expressive, varied | Pride elements, celebration maximalism |\n| **Cultural Fusion** | Per tradition | Traditional + modern | Cultural motifs, heritage patterns |\n\n### Extracting Theme from Photos\n\n```python\n# Theme extraction signals\nTHEME_SIGNALS = {\n    'color_palette': 'Dominant colors from venue, florals, attire',\n    'lighting_mood': 'Warm/cool, natural/dramatic, string lights/chandeliers',\n    'decor_elements': 'Rustic/modern/vintage/eclectic',\n    'attire_style': 'Traditional/non-traditional, formal/casual',\n    'cultural_markers': 'Religious symbols, cultural traditions',\n    'era_aesthetic': '70s disco, 20s gatsby, etc.'\n}\n```\n\n## 3D Gaussian Splatting Pipeline\n\n### Photo/Video Ingestion\n\n```\nOptimal Input Strategy:\n├── Video: Extract 2-3 fps (80% overlap minimum)\n├── Photos: Include ALL photographer shots\n├── Phone photos: Guest uploads (georeferenced bonus)\n└── Coverage: Ceremony + reception + all spaces\n\nQuality Thresholds:\n├── Minimum images per space: 50-100\n├── Overlap requirement: 60-80%\n├── Blur rejection: Laplacian variance < 100 = skip\n└── Exposure: Reject severe over/underexposure\n```\n\n### COLMAP Structure from Motion\n\n```bash\n# Feature extraction\ncolmap feature_extractor \\\n  --database_path database.db \\\n  --image_path images/ \\\n  --ImageReader.single_camera 0 \\\n  --SiftExtraction.max_image_size 3200\n\n# Exhaustive matching for comprehensive coverage\ncolmap exhaustive_matcher \\\n  --database_path database.db \\\n  --SiftMatching.guided_matching 1\n\n# Sparse reconstruction\ncolmap mapper \\\n  --database_path database.db \\\n  --image_path images/ \\\n  --output_path sparse/\n\n# Dense reconstruction (optional, for mesh)\ncolmap image_undistorter ...\ncolmap patch_match_stereo ...\n```\n\n### 3DGS Training\n\n```python\n# Wedding-optimized 3DGS settings\nWEDDING_3DGS_CONFIG = {\n    'iterations': 50_000,          # High quality for permanent archive\n    'densify_from_iter': 500,\n    'densify_until_iter': 15_000,\n    'densification_interval': 100,\n    'opacity_reset_interval': 3000,\n    'sh_degree': 3,                # Full spherical harmonics for lighting\n    'percent_dense': 0.01,\n    'densify_grad_threshold': 0.0002,\n}\n\n# Multi-space merge strategy\nSPACES = ['ceremony', 'cocktail_hour', 'reception', 'photo_booth', 'dance_floor']\n# Train each separately, then create unified navigation\n```\n\n## Face Clustering System\n\n### Pipeline\n\n```\n┌────────────────────────────────────────────────────────┐\n│               FACE CLUSTERING PIPELINE                  │\n├────────────────────────────────────────────────────────┤\n│  1. Detection (RetinaFace/MTCNN)                       │\n│     └─ All faces in all photos                         │\n│  2. Alignment (5-point landmark)                       │\n│     └─ Standardize for embedding                       │\n│  3. Embedding (ArcFace/AdaFace)                        │\n│     └─ 512-dim identity vector per face                │\n│  4. Clustering (HDBSCAN)                               │\n│     └─ Group by identity, handle edge cases            │\n│  5. Identity Linking                                   │\n│     └─ Match to couple, wedding party, family, guests  │\n│  6. Best Photo Selection                               │\n│     └─ Aesthetic scoring per cluster                   │\n└────────────────────────────────────────────────────────┘\n```\n\n### Clustering Parameters\n\n```python\nCLUSTERING_CONFIG = {\n    'min_cluster_size': 3,         # At least 3 photos to form identity\n    'min_samples': 2,\n    'metric': 'cosine',\n    'cluster_selection_epsilon': 0.3,\n    'cluster_selection_method': 'eom',\n}\n\n# Identity priority for naming\nIDENTITY_PRIORITY = [\n    'couple_1', 'couple_2',        # The married couple\n    'wedding_party',               # Bridesmaids, groomspeople\n    'parents',                     # Parents of the couple\n    'grandparents',\n    'siblings',\n    'extended_family',\n    'friends',\n    'vendors',                     # Photographer, DJ, etc.\n]\n```\n\n### Identity Linking Workflow\n\n1. **Couple identification**: User tags couple in 2-3 photos\n2. **Wedding party**: User identifies key people\n3. **Auto-propagation**: Embeddings match across all photos\n4. **Guest matching**: Optional guest list import for name assignment\n5. **Manual corrections**: UI for fixing mismatches\n\n## Aesthetic Scoring\n\n### Per-Photo Quality Metrics\n\n```python\nAESTHETIC_FEATURES = {\n    # Technical quality\n    'sharpness': 'Laplacian variance, MTF analysis',\n    'exposure': 'Histogram analysis, dynamic range',\n    'noise': 'High-ISO detection, grain analysis',\n\n    # Composition\n    'rule_of_thirds': 'Subject placement scoring',\n    'symmetry': 'For venue/group shots',\n    'framing': 'Negative space, balance',\n\n    # Face-specific\n    'expression': 'Smile detection, eye openness',\n    'blink_detection': 'Eyes closed penalty',\n    'gaze_direction': 'Looking at camera vs. candid',\n    'face_occlusion': 'Nothing blocking the face',\n    'face_lighting': 'Even illumination, no harsh shadows',\n\n    # Emotional\n    'genuine_smile': 'Duchenne marker detection',\n    'moment_quality': 'Laughter, tears, embraces',\n}\n```\n\n### Best Photo Selection Per Person\n\n```python\ndef select_best_photos(cluster_photos, n=5):\n    \"\"\"Select top N photos for a person across all their appearances.\"\"\"\n\n    scores = []\n    for photo in cluster_photos:\n        score = (\n            0.25 * technical_quality(photo) +\n            0.25 * composition_score(photo) +\n            0.30 * expression_quality(photo) +\n            0.20 * context_diversity(photo, scores)  # Avoid all similar shots\n        )\n        scores.append((photo, score))\n\n    # Select top N with diversity constraint\n    return diverse_top_n(scores, n, diversity_threshold=0.7)\n```\n\n## Theatre Mode\n\n### Moment Detection & Playback\n\n```\nKEY MOMENTS (auto-detected + user-tagged):\n├── Ceremony\n│   ├── Processional\n│   ├── Vows exchange\n│   ├── Ring ceremony\n│   ├── First kiss\n│   └── Recessional\n├── Reception\n│   ├── Grand entrance\n│   ├── First dance\n│   ├── Parent dances\n│   ├── Toasts/speeches\n│   ├── Cake cutting\n│   └── Bouquet/garter\n├── Party\n│   ├── Dance floor highlights\n│   └── Exit/sendoff\n└── Candids\n    ├── Emotional moments (tears, laughter)\n    └── Spontaneous joy\n```\n\n### In-Scene Video Projection\n\n```\nTheatre Mode Rendering:\n1. User navigates 3DGS scene freely\n2. Approaches \"moment marker\" (glowing orb/frame)\n3. Video/slideshow plays IN the 3D space\n   ├── On walls where projector was\n   ├── Floating frames in dance floor area\n   └── Photo booth backdrop location\n4. Spatial audio for speeches/music\n5. User can pause, scrub, exit to continue exploring\n```\n\n## Web Viewer Architecture\n\n```javascript\n// Wedding Immortalist Viewer Components\nconst VIEWER_FEATURES = {\n  // 3DGS Navigation\n  gaussianSplatting: {\n    renderer: 'three-gaussian-splat',\n    navigation: 'orbit + first-person',\n    qualityLevels: ['preview', 'standard', 'maximum'],\n  },\n\n  // Theatre Mode\n  theatreMode: {\n    momentMarkers: true,\n    videoInScene: true,\n    spatialAudio: true,\n    transitionEffects: 'theme-matched',\n  },\n\n  // Guest Roster\n  guestRoster: {\n    faceGrid: 'clustered by identity',\n    photoGallery: 'per-person best shots',\n    searchByName: true,\n    shareableLinks: 'per-guest galleries',\n  },\n\n  // Theme\n  theming: {\n    colorPalette: 'extracted from wedding',\n    typography: 'theme-matched',\n    uiElements: 'aesthetic-consistent',\n  },\n};\n```\n\n## Anti-Patterns\n\n### \"All Frames, All the Time\"\n**Wrong**: Extracting every video frame for 3DGS.\n**Why**: Redundant data, 10x slower processing, no quality improvement.\n**Right**: 2-3 fps extraction with motion-based keyframe selection.\n\n### \"One Giant Scene\"\n**Wrong**: Training single 3DGS for entire venue.\n**Why**: Memory explosion, quality degradation, impossible on consumer hardware.\n**Right**: Train per-space, create unified navigation with seamless transitions.\n\n### \"Default Clustering Threshold\"\n**Wrong**: Using default HDBSCAN settings.\n**Why**: Wedding photos have varying lighting, makeup, angles—need tuning.\n**Right**: Tune per-wedding based on photo count and quality variance.\n\n### \"Ignoring Theme\"\n**Wrong**: Generic white/gray viewer UI for disco wedding.\n**Why**: Destroys the personality and joy of the event.\n**Right**: Extract and honor the couple's aesthetic choices.\n\n### \"Photographer Only\"\n**Wrong**: Using only professional photos.\n**Why**: Misses candid moments, guest perspectives, coverage gaps.\n**Right**: Merge professional + guest photos for complete coverage.\n\n## Guest Experience Features\n\n### Shareable Guest Galleries\n\n```\nPer-Guest Experience:\n├── Personalized link: yourwedding.com/guests/aunt-martha\n├── Their best photos (AI-curated)\n├── Photos with the couple\n├── Group photos they appear in\n├── Download options (full-res)\n└── \"Add to my memories\" for their own archives\n```\n\n### Collaborative Enhancement\n\n```\nGuest Contribution Portal:\n├── Upload their own photos\n├── Tag themselves in unidentified clusters\n├── Correct misidentifications\n├── Add names to unknown guests\n└── Submit video moments they captured\n```\n\n## Output Deliverables\n\n```\nwedding-immortalist-output/\n├── 3dgs-scenes/\n│   ├── ceremony/\n│   ├── cocktail/\n│   ├── reception/\n│   └── unified-navigation.json\n├── guest-roster/\n│   ├── face-clusters/\n│   ├── identity-mapping.json\n│   └── per-person-galleries/\n├── theatre-mode/\n│   ├── moment-markers.json\n│   ├── video-segments/\n│   └── spatial-audio/\n├── web-viewer/\n│   ├── index.html\n│   ├── theme-config.json\n│   └── assets/\n└── exports/\n    ├── full-resolution-photos/\n    ├── guest-gallery-zips/\n    └── video-compilations/\n```\n\n## Integration Points\n\n- **drone-inspection-specialist**: 3DGS techniques, COLMAP pipeline\n- **collage-layout-expert**: Photo arrangement, aesthetic composition\n- **color-theory-palette-harmony-expert**: Theme color extraction\n- **clip-aware-embeddings**: Photo-text matching for search\n- **photo-composition-critic**: Aesthetic quality scoring\n\n---\n\n**Core Philosophy**: A wedding happens once. The memories should live forever. This skill transforms ephemeral moments into an eternal, explorable experience that honors the couple's unique celebration—whether it's a disco dance party, a rustic barn gathering, or two grooms celebrating their love with chosen family.\n"
    }
  ]
}