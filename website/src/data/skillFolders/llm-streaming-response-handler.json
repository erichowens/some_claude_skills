{
  "name": "llm-streaming-response-handler",
  "type": "folder",
  "path": "llm-streaming-response-handler",
  "children": [
    {
      "name": "assets",
      "type": "folder",
      "path": "llm-streaming-response-handler/assets",
      "children": []
    },
    {
      "name": "references",
      "type": "folder",
      "path": "llm-streaming-response-handler/references",
      "children": [
        {
          "name": "error-recovery.md",
          "type": "file",
          "path": "llm-streaming-response-handler/references/error-recovery.md",
          "size": 11913,
          "content": "# Stream Error Recovery Strategies\n\nProduction patterns for handling errors during LLM streaming and providing graceful recovery.\n\n## Error Categories\n\n### 1. Network Errors\n- Connection timeouts\n- DNS failures\n- Lost connectivity mid-stream\n\n### 2. API Errors\n- Rate limits (429)\n- Authentication (401, 403)\n- Server errors (500, 503)\n- Invalid requests (400)\n\n### 3. Stream-Specific Errors\n- Malformed SSE data\n- Unexpected stream termination\n- Backpressure overload\n- Client-side cancellation\n\n---\n\n## Pattern 1: Retry with Exponential Backoff\n\n```typescript\nasync function streamWithRetry(\n  url: string,\n  options: RequestInit,\n  maxRetries = 3\n): Promise<Response> {\n  let attempt = 0;\n\n  while (attempt < maxRetries) {\n    try {\n      const response = await fetch(url, options);\n\n      // Don't retry on client errors (4xx except rate limit)\n      if (response.status >= 400 && response.status < 500 && response.status !== 429) {\n        throw new Error(`Client error: ${response.status}`);\n      }\n\n      // Don't retry on success\n      if (response.ok) {\n        return response;\n      }\n\n      // Retry on server errors and rate limits\n      throw new Error(`Server error: ${response.status}`);\n\n    } catch (error) {\n      attempt++;\n\n      if (attempt >= maxRetries) {\n        throw error;\n      }\n\n      // Exponential backoff: 1s, 2s, 4s\n      const delay = Math.pow(2, attempt) * 1000;\n      console.log(`Retry ${attempt}/${maxRetries} after ${delay}ms`);\n\n      await new Promise(resolve => setTimeout(resolve, delay));\n    }\n  }\n\n  throw new Error('Max retries exceeded');\n}\n\n// Usage\nconst response = await streamWithRetry('/api/chat', {\n  method: 'POST',\n  body: JSON.stringify({ prompt })\n});\n```\n\n---\n\n## Pattern 2: Rate Limit Handling\n\n```typescript\nasync function handleRateLimit(response: Response): Promise<void> {\n  if (response.status === 429) {\n    const retryAfter = response.headers.get('Retry-After');\n\n    if (retryAfter) {\n      const delay = parseInt(retryAfter) * 1000;\n      console.log(`Rate limited. Retrying after ${delay}ms`);\n      await new Promise(resolve => setTimeout(resolve, delay));\n    } else {\n      // No Retry-After header, use default backoff\n      await new Promise(resolve => setTimeout(resolve, 60000)); // 1 minute\n    }\n  }\n}\n\n// Usage\nconst response = await fetch('/api/chat', { ... });\n\nif (response.status === 429) {\n  await handleRateLimit(response);\n  // Retry request\n  return fetch('/api/chat', { ... });\n}\n```\n\n---\n\n## Pattern 3: Graceful Degradation\n\nShow partial response if stream fails mid-way.\n\n```typescript\nfunction useStreamingWithFallback() {\n  const [content, setContent] = useState('');\n  const [error, setError] = useState<Error | null>(null);\n  const [isComplete, setIsComplete] = useState(false);\n\n  const stream = async (prompt: string) => {\n    setError(null);\n    setIsComplete(false);\n\n    try {\n      const response = await fetch('/api/chat', {\n        method: 'POST',\n        body: JSON.stringify({ prompt })\n      });\n\n      const reader = response.body!.getReader();\n      const decoder = new TextDecoder();\n\n      while (true) {\n        const { done, value } = await reader.read();\n\n        if (done) {\n          setIsComplete(true);\n          break;\n        }\n\n        const chunk = decoder.decode(value);\n        const lines = chunk.split('\\n');\n\n        for (const line of lines) {\n          if (line.startsWith('data: ')) {\n            const data = JSON.parse(line.slice(6));\n            if (data.content) {\n              setContent(prev => prev + data.content);\n            }\n          }\n        }\n      }\n    } catch (err) {\n      setError(err as Error);\n      // Keep partial content visible\n      console.error('Stream failed, showing partial response:', content);\n    }\n  };\n\n  return { content, error, isComplete, stream };\n}\n\n// UI shows partial content + error\n{content && (\n  <div>\n    {content}\n    {error && !isComplete && (\n      <div className=\"error-banner\">\n        ‚ö†Ô∏è Connection lost. Showing partial response.\n        <button onClick={() => stream(lastPrompt)}>Resume</button>\n      </div>\n    )}\n  </div>\n)}\n```\n\n---\n\n## Pattern 4: Resume from Last Token\n\nFor very long streams, save checkpoints.\n\n```typescript\ninterface StreamCheckpoint {\n  prompt: string;\n  content: string;\n  lastTokenIndex: number;\n}\n\nasync function streamWithCheckpoints(prompt: string) {\n  let checkpoint: StreamCheckpoint | null = loadCheckpoint(prompt);\n\n  if (checkpoint) {\n    console.log('Resuming from checkpoint:', checkpoint.lastTokenIndex);\n  }\n\n  const response = await fetch('/api/chat', {\n    method: 'POST',\n    body: JSON.stringify({\n      prompt,\n      resumeFrom: checkpoint?.lastTokenIndex || 0\n    })\n  });\n\n  const reader = response.body!.getReader();\n  const decoder = new TextDecoder();\n\n  let tokenIndex = checkpoint?.lastTokenIndex || 0;\n  let content = checkpoint?.content || '';\n\n  try {\n    while (true) {\n      const { done, value } = await reader.read();\n      if (done) break;\n\n      const chunk = decoder.decode(value);\n      // Process chunk...\n\n      content += newToken;\n      tokenIndex++;\n\n      // Save checkpoint every 100 tokens\n      if (tokenIndex % 100 === 0) {\n        saveCheckpoint({ prompt, content, lastTokenIndex: tokenIndex });\n      }\n    }\n  } catch (error) {\n    // Checkpoint saved, can resume later\n    console.error('Stream interrupted at token', tokenIndex);\n    throw error;\n  }\n}\n```\n\n---\n\n## Pattern 5: Client-Side Timeout\n\nPrevent infinite hanging.\n\n```typescript\nasync function streamWithTimeout(\n  url: string,\n  options: RequestInit,\n  timeoutMs = 30000\n): Promise<void> {\n  const controller = new AbortController();\n\n  // Set timeout\n  const timeoutId = setTimeout(() => {\n    controller.abort();\n  }, timeoutMs);\n\n  try {\n    const response = await fetch(url, {\n      ...options,\n      signal: controller.signal\n    });\n\n    // Reset timeout on each chunk\n    const reader = response.body!.getReader();\n    const decoder = new TextDecoder();\n\n    while (true) {\n      // Clear and reset timeout\n      clearTimeout(timeoutId);\n      const chunkTimeoutId = setTimeout(() => controller.abort(), timeoutMs);\n\n      const { done, value } = await reader.read();\n\n      clearTimeout(chunkTimeoutId);\n\n      if (done) break;\n\n      // Process chunk...\n    }\n  } catch (error) {\n    if (error.name === 'AbortError') {\n      throw new Error('Request timed out');\n    }\n    throw error;\n  } finally {\n    clearTimeout(timeoutId);\n  }\n}\n```\n\n---\n\n## Pattern 6: User-Friendly Error Messages\n\n```typescript\nfunction getErrorMessage(error: unknown): string {\n  if (error instanceof Error) {\n    // Network errors\n    if (error.message.includes('Failed to fetch')) {\n      return 'Network error. Please check your connection.';\n    }\n\n    // Timeout\n    if (error.name === 'AbortError') {\n      return 'Request timed out. Please try again.';\n    }\n\n    // Generic\n    return error.message;\n  }\n\n  // HTTP errors\n  if (typeof error === 'object' && error !== null && 'status' in error) {\n    const status = (error as any).status;\n\n    switch (status) {\n      case 400:\n        return 'Invalid request. Please try rephrasing.';\n      case 401:\n        return 'Please log in to continue.';\n      case 403:\n        return 'You don\\'t have permission to do this.';\n      case 429:\n        return 'Too many requests. Please wait a moment.';\n      case 500:\n        return 'Server error. Please try again later.';\n      case 503:\n        return 'Service temporarily unavailable.';\n      default:\n        return `Error ${status}: Something went wrong.`;\n    }\n  }\n\n  return 'An unexpected error occurred.';\n}\n\n// UI\n{error && (\n  <div className=\"error-message\">\n    <span>{getErrorMessage(error)}</span>\n    <button onClick={retry}>Try Again</button>\n  </div>\n)}\n```\n\n---\n\n## Pattern 7: Fallback to Non-Streaming\n\nIf streaming fails repeatedly, fall back to standard request.\n\n```typescript\nasync function adaptiveStream(prompt: string): Promise<string> {\n  try {\n    // Try streaming first\n    return await streamResponse(prompt);\n  } catch (error) {\n    console.log('Streaming failed, falling back to standard request');\n\n    // Fallback to non-streaming\n    const response = await fetch('/api/chat', {\n      method: 'POST',\n      body: JSON.stringify({ prompt, stream: false })\n    });\n\n    const data = await response.json();\n    return data.content;\n  }\n}\n```\n\n---\n\n## Pattern 8: Error Logging & Monitoring\n\n```typescript\nasync function streamWithMonitoring(prompt: string) {\n  const startTime = Date.now();\n\n  try {\n    await streamResponse(prompt);\n\n    // Log success\n    await logMetric({\n      event: 'stream_success',\n      duration: Date.now() - startTime,\n      prompt\n    });\n\n  } catch (error) {\n    const duration = Date.now() - startTime;\n\n    // Log error with context\n    await logError({\n      event: 'stream_error',\n      error: error.message,\n      duration,\n      prompt,\n      userAgent: navigator.userAgent,\n      timestamp: new Date().toISOString()\n    });\n\n    // Send to error tracking (Sentry, etc.)\n    if (window.Sentry) {\n      window.Sentry.captureException(error, {\n        tags: {\n          component: 'chat-stream',\n          duration\n        }\n      });\n    }\n\n    throw error;\n  }\n}\n```\n\n---\n\n## Pattern 9: Circuit Breaker\n\nStop trying after repeated failures.\n\n```typescript\nclass CircuitBreaker {\n  private failures = 0;\n  private lastFailureTime = 0;\n  private state: 'closed' | 'open' | 'half-open' = 'closed';\n\n  async execute<T>(fn: () => Promise<T>): Promise<T> {\n    // Circuit open: reject immediately\n    if (this.state === 'open') {\n      const timeSinceFailure = Date.now() - this.lastFailureTime;\n\n      if (timeSinceFailure < 60000) { // 1 minute cooldown\n        throw new Error('Circuit breaker open. Try again later.');\n      }\n\n      // Try half-open\n      this.state = 'half-open';\n    }\n\n    try {\n      const result = await fn();\n\n      // Success: reset\n      this.failures = 0;\n      this.state = 'closed';\n\n      return result;\n    } catch (error) {\n      this.failures++;\n      this.lastFailureTime = Date.now();\n\n      // Trip circuit after 3 failures\n      if (this.failures >= 3) {\n        this.state = 'open';\n      }\n\n      throw error;\n    }\n  }\n}\n\nconst breaker = new CircuitBreaker();\n\n// Usage\ntry {\n  await breaker.execute(() => streamResponse(prompt));\n} catch (error) {\n  // Handle error or show cached response\n}\n```\n\n---\n\n## Production Checklist\n\n```\n‚ñ° Retry logic with exponential backoff\n‚ñ° Rate limit handling (Retry-After header)\n‚ñ° Graceful degradation (show partial)\n‚ñ° User-friendly error messages\n‚ñ° Timeout enforcement\n‚ñ° Error logging to monitoring service\n‚ñ° Circuit breaker for repeated failures\n‚ñ° Fallback to non-streaming\n‚ñ° Checkpoint saving for long streams\n‚ñ° Network status detection\n‚ñ° Offline mode (show cached)\n‚ñ° Cancel button always works\n```\n\n---\n\n## Testing Error Scenarios\n\n### Simulate Network Failure\n\n```typescript\n// Test: Network interruption\nif (process.env.NODE_ENV === 'development') {\n  // Randomly fail 10% of requests\n  if (Math.random() < 0.1) {\n    throw new Error('Simulated network failure');\n  }\n}\n```\n\n### Simulate Rate Limit\n\n```typescript\n// Test: Rate limit response\nreturn new Response('Too many requests', {\n  status: 429,\n  headers: {\n    'Retry-After': '5' // 5 seconds\n  }\n});\n```\n\n### Simulate Partial Stream\n\n```typescript\n// Test: Stream fails halfway\ncontroller.enqueue(encoder.encode('data: First half\\n\\n'));\nawait new Promise(resolve => setTimeout(resolve, 1000));\ncontroller.error(new Error('Connection lost'));\n```\n\n---\n\n## Resources\n\n- [MDN: Error Handling](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Control_flow_and_error_handling)\n- [Circuit Breaker Pattern](https://martinfowler.com/bliki/CircuitBreaker.html)\n- [Retry Best Practices](https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/)\n"
        },
        {
          "name": "sse-protocol.md",
          "type": "file",
          "path": "llm-streaming-response-handler/references/sse-protocol.md",
          "size": 8948,
          "content": "# Server-Sent Events (SSE) Protocol\n\nDeep dive into the SSE specification and implementation details for LLM streaming.\n\n## Protocol Basics\n\nSSE is a simple HTTP-based protocol for server-to-client streaming.\n\n**Request**:\n```http\nGET /api/stream HTTP/1.1\nAccept: text/event-stream\nCache-Control: no-cache\n```\n\n**Response**:\n```http\nHTTP/1.1 200 OK\nContent-Type: text/event-stream\nCache-Control: no-cache\nConnection: keep-alive\n\ndata: First message\n\ndata: Second message\n\ndata: {\"type\":\"completion\",\"text\":\"Token\"}\n\n```\n\n## Message Format\n\n### Data Field\n\nThe only required field. Can span multiple lines.\n\n```\ndata: Simple message\n\ndata: Multi-line\ndata: message\ndata: here\n\ndata: {\"json\":\"works too\"}\n\n```\n\n### Event Field\n\nCustom event types (default is \"message\").\n\n```\nevent: status\ndata: {\"state\":\"processing\"}\n\nevent: completion\ndata: {\"content\":\"Hello\"}\n\n```\n\n### ID Field\n\nEvent identifier for reconnection.\n\n```\nid: 1\ndata: First event\n\nid: 2\ndata: Second event\n\n```\n\nClient can reconnect with `Last-Event-ID` header:\n```http\nGET /api/stream HTTP/1.1\nLast-Event-ID: 2\n```\n\n### Retry Field\n\nReconnection delay in milliseconds.\n\n```\nretry: 3000\ndata: Reconnect after 3 seconds if disconnected\n\n```\n\n## Client Implementation\n\n### EventSource API (Browser)\n\n```typescript\nconst eventSource = new EventSource('/api/stream');\n\neventSource.onmessage = (event) => {\n  console.log('Data:', event.data);\n};\n\neventSource.addEventListener('status', (event) => {\n  console.log('Status:', JSON.parse(event.data));\n});\n\neventSource.onerror = (error) => {\n  console.error('SSE error:', error);\n  eventSource.close();\n};\n\n// Close connection\neventSource.close();\n```\n\n**Limitations**:\n- No custom headers (can't send Authorization)\n- No POST requests (GET only)\n- Limited error handling\n\n### Fetch API (More Control)\n\n```typescript\nconst response = await fetch('/api/stream', {\n  method: 'POST',\n  headers: {\n    'Content-Type': 'application/json',\n    'Authorization': `Bearer ${token}`\n  },\n  body: JSON.stringify({ prompt: 'Hello' })\n});\n\nconst reader = response.body!.getReader();\nconst decoder = new TextDecoder();\n\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n\n  const chunk = decoder.decode(value);\n  console.log(chunk);\n}\n```\n\n**Advantages**:\n- Custom headers\n- POST requests\n- AbortController support\n- Better error handling\n\n## Server Implementation\n\n### Node.js (Express)\n\n```typescript\nimport express from 'express';\n\napp.get('/api/stream', (req, res) => {\n  // Set SSE headers\n  res.setHeader('Content-Type', 'text/event-stream');\n  res.setHeader('Cache-Control', 'no-cache');\n  res.setHeader('Connection', 'keep-alive');\n\n  // Send initial message\n  res.write('data: Connected\\n\\n');\n\n  // Send periodic updates\n  const interval = setInterval(() => {\n    res.write(`data: ${Date.now()}\\n\\n`);\n  }, 1000);\n\n  // Cleanup on disconnect\n  req.on('close', () => {\n    clearInterval(interval);\n    res.end();\n  });\n});\n```\n\n### Next.js (App Router)\n\n```typescript\n// app/api/stream/route.ts\nexport async function GET() {\n  const encoder = new TextEncoder();\n\n  const stream = new ReadableStream({\n    async start(controller) {\n      // Send messages\n      controller.enqueue(encoder.encode('data: Hello\\n\\n'));\n      controller.enqueue(encoder.encode('data: World\\n\\n'));\n\n      // Close stream\n      controller.close();\n    }\n  });\n\n  return new Response(stream, {\n    headers: {\n      'Content-Type': 'text/event-stream',\n      'Cache-Control': 'no-cache',\n      'Connection': 'keep-alive'\n    }\n  });\n}\n```\n\n### Edge Runtime (Vercel/Cloudflare)\n\n```typescript\nexport const runtime = 'edge';\n\nexport async function POST(req: Request) {\n  const { prompt } = await req.json();\n\n  const stream = new ReadableStream({\n    async start(controller) {\n      const encoder = new TextEncoder();\n\n      // Stream from LLM\n      for await (const chunk of llmStream(prompt)) {\n        const message = `data: ${JSON.stringify({ content: chunk })}\\n\\n`;\n        controller.enqueue(encoder.encode(message));\n      }\n\n      controller.close();\n    }\n  });\n\n  return new Response(stream, {\n    headers: {\n      'Content-Type': 'text/event-stream',\n      'Cache-Control': 'no-cache'\n    }\n  });\n}\n```\n\n## Reconnection Logic\n\nSSE has built-in reconnection, but Fetch API requires manual implementation.\n\n```typescript\nclass SSEClient {\n  private reconnectDelay = 1000;\n  private maxReconnectDelay = 30000;\n  private reconnectAttempts = 0;\n\n  async connect(url: string, onMessage: (data: string) => void) {\n    try {\n      const response = await fetch(url);\n      const reader = response.body!.getReader();\n      const decoder = new TextDecoder();\n\n      // Reset reconnect state on successful connection\n      this.reconnectAttempts = 0;\n      this.reconnectDelay = 1000;\n\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n\n        const chunk = decoder.decode(value);\n        const lines = chunk.split('\\n');\n\n        for (const line of lines) {\n          if (line.startsWith('data: ')) {\n            onMessage(line.slice(6));\n          }\n        }\n      }\n    } catch (error) {\n      // Exponential backoff\n      this.reconnectAttempts++;\n      this.reconnectDelay = Math.min(\n        this.reconnectDelay * 2,\n        this.maxReconnectDelay\n      );\n\n      console.log(`Reconnecting in ${this.reconnectDelay}ms...`);\n\n      setTimeout(() => {\n        this.connect(url, onMessage);\n      }, this.reconnectDelay);\n    }\n  }\n}\n```\n\n## CORS Configuration\n\nSSE requires proper CORS headers.\n\n```typescript\n// Server-side\napp.use((req, res, next) => {\n  res.setHeader('Access-Control-Allow-Origin', '*');\n  res.setHeader('Access-Control-Allow-Methods', 'GET, POST');\n  res.setHeader('Access-Control-Allow-Headers', 'Content-Type');\n  next();\n});\n```\n\n## Heartbeat Pattern\n\nSend periodic pings to keep connection alive.\n\n```typescript\n// Server\nconst heartbeat = setInterval(() => {\n  res.write(': heartbeat\\n\\n'); // Comment, ignored by client\n}, 15000);\n\nreq.on('close', () => {\n  clearInterval(heartbeat);\n});\n```\n\n**Why**: Some proxies close idle connections after 30 seconds.\n\n## Error Handling\n\n### Client-Side Errors\n\n```typescript\ntry {\n  const response = await fetch('/api/stream');\n\n  if (!response.ok) {\n    throw new Error(`HTTP ${response.status}`);\n  }\n\n  if (!response.body) {\n    throw new Error('No response body');\n  }\n\n  // Stream handling...\n} catch (error) {\n  if (error.name === 'AbortError') {\n    console.log('Stream cancelled');\n  } else if (error.message.includes('Failed to fetch')) {\n    console.error('Network error');\n  } else {\n    console.error('Unknown error:', error);\n  }\n}\n```\n\n### Server-Side Errors\n\n```typescript\n// Send error as SSE event\nasync start(controller) {\n  try {\n    // Streaming logic...\n  } catch (error) {\n    const errorMessage = `event: error\\ndata: ${JSON.stringify({\n      message: error.message\n    })}\\n\\n`;\n    controller.enqueue(encoder.encode(errorMessage));\n    controller.close();\n  }\n}\n```\n\n## Performance Considerations\n\n### Backpressure\n\nSlow clients can overwhelm server. Handle backpressure:\n\n```typescript\nconst stream = new ReadableStream({\n  async start(controller) {\n    for await (const chunk of dataSource) {\n      // Wait if client is slow\n      if (controller.desiredSize !== null && controller.desiredSize <= 0) {\n        await new Promise(resolve => setTimeout(resolve, 100));\n      }\n\n      controller.enqueue(encoder.encode(`data: ${chunk}\\n\\n`));\n    }\n  }\n});\n```\n\n### Memory Management\n\nClose connections when done:\n\n```typescript\n// Client\nuseEffect(() => {\n  const controller = new AbortController();\n\n  fetch('/api/stream', { signal: controller.signal })\n    .then(/* ... */);\n\n  return () => {\n    controller.abort();\n  };\n}, []);\n```\n\n## Security\n\n### Rate Limiting\n\n```typescript\nimport rateLimit from 'express-rate-limit';\n\nconst limiter = rateLimit({\n  windowMs: 60 * 1000, // 1 minute\n  max: 10, // 10 requests per minute\n  message: 'Too many requests'\n});\n\napp.get('/api/stream', limiter, (req, res) => {\n  // Stream handler...\n});\n```\n\n### Authentication\n\n```typescript\n// Server\nconst token = req.headers.authorization?.split(' ')[1];\nif (!verifyToken(token)) {\n  return res.status(401).send('Unauthorized');\n}\n```\n\n## Production Checklist\n\n```\n‚ñ° Content-Type: text/event-stream header set\n‚ñ° Cache-Control: no-cache header set\n‚ñ° CORS configured correctly\n‚ñ° Heartbeat implemented (for proxies)\n‚ñ° Reconnection logic with exponential backoff\n‚ñ° Error events sent to client\n‚ñ° Rate limiting enabled\n‚ñ° Authentication required\n‚ñ° Memory cleanup on disconnect\n‚ñ° Timeout for long-running streams\n```\n\n## Resources\n\n- [MDN: Server-Sent Events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)\n- [SSE Specification](https://html.spec.whatwg.org/multipage/server-sent-events.html)\n- [EventSource API](https://developer.mozilla.org/en-US/docs/Web/API/EventSource)\n"
        },
        {
          "name": "vercel-ai-sdk.md",
          "type": "file",
          "path": "llm-streaming-response-handler/references/vercel-ai-sdk.md",
          "size": 9899,
          "content": "# Vercel AI SDK Integration Patterns\n\nProduction patterns for using Vercel AI SDK to handle LLM streaming across providers.\n\n## Why Vercel AI SDK?\n\n**Unified API** across OpenAI, Anthropic, Google, Cohere, Hugging Face:\n- Same streaming interface\n- Automatic retries\n- Built-in token counting\n- React hooks\n\n**Timeline**:\n- 2023: Released as `ai` package\n- 2024: Became de facto standard for Next.js AI apps\n- 2024+: Supports 15+ LLM providers\n\n---\n\n## Installation\n\n```bash\nnpm install ai @ai-sdk/openai @ai-sdk/anthropic\n```\n\n---\n\n## Pattern 1: Basic Streaming (useChat Hook)\n\n```typescript\n'use client';\n\nimport { useChat } from 'ai/react';\n\nexport function ChatInterface() {\n  const { messages, input, handleInputChange, handleSubmit, isLoading } = useChat({\n    api: '/api/chat'\n  });\n\n  return (\n    <div>\n      <div className=\"messages\">\n        {messages.map((message) => (\n          <div key={message.id} className={`message ${message.role}`}>\n            {message.content}\n          </div>\n        ))}\n      </div>\n\n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          onChange={handleInputChange}\n          disabled={isLoading}\n          placeholder=\"Type a message...\"\n        />\n        <button type=\"submit\" disabled={isLoading}>\n          Send\n        </button>\n      </form>\n    </div>\n  );\n}\n```\n\n**Server Route** (app/api/chat/route.ts):\n```typescript\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nexport const runtime = 'edge';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = await streamText({\n    model: openai('gpt-4-turbo'),\n    messages\n  });\n\n  return result.toAIStreamResponse();\n}\n```\n\n**What you get**:\n- ‚úÖ Streaming automatically handled\n- ‚úÖ Message history managed\n- ‚úÖ Loading states\n- ‚úÖ Error handling\n- ‚úÖ Optimistic updates\n\n---\n\n## Pattern 2: Streaming with Tools (Function Calling)\n\n```typescript\nimport { openai } from '@ai-sdk/openai';\nimport { streamText, tool } from 'ai';\nimport { z } from 'zod';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = await streamText({\n    model: openai('gpt-4-turbo'),\n    messages,\n    tools: {\n      getWeather: tool({\n        description: 'Get weather for a location',\n        parameters: z.object({\n          location: z.string().describe('City name')\n        }),\n        execute: async ({ location }) => {\n          const weather = await fetchWeather(location);\n          return weather;\n        }\n      }),\n      createReminder: tool({\n        description: 'Create a reminder',\n        parameters: z.object({\n          text: z.string(),\n          time: z.string()\n        }),\n        execute: async ({ text, time }) => {\n          await db.reminders.create({ text, time });\n          return { success: true };\n        }\n      })\n    }\n  });\n\n  return result.toAIStreamResponse();\n}\n```\n\n**Client** (automatic tool execution):\n```typescript\nconst { messages } = useChat({\n  api: '/api/chat',\n  onToolCall: ({ toolCall }) => {\n    console.log('Tool called:', toolCall.toolName, toolCall.args);\n  }\n});\n```\n\n---\n\n## Pattern 3: Multi-Provider Fallback\n\n```typescript\nimport { openai } from '@ai-sdk/openai';\nimport { anthropic } from '@ai-sdk/anthropic';\nimport { streamText } from 'ai';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  try {\n    // Try OpenAI first\n    const result = await streamText({\n      model: openai('gpt-4-turbo'),\n      messages\n    });\n\n    return result.toAIStreamResponse();\n  } catch (error) {\n    // Fallback to Anthropic\n    console.log('OpenAI failed, falling back to Claude');\n\n    const result = await streamText({\n      model: anthropic('claude-3-sonnet-20240229'),\n      messages\n    });\n\n    return result.toAIStreamResponse();\n  }\n}\n```\n\n---\n\n## Pattern 4: Custom useChat with Abort\n\n```typescript\nimport { useChat } from 'ai/react';\n\nexport function ChatWithAbort() {\n  const {\n    messages,\n    input,\n    handleInputChange,\n    handleSubmit,\n    isLoading,\n    stop\n  } = useChat({\n    api: '/api/chat'\n  });\n\n  return (\n    <div>\n      {/* Messages */}\n      {messages.map((m) => (\n        <div key={m.id}>{m.content}</div>\n      ))}\n\n      {/* Form */}\n      <form onSubmit={handleSubmit}>\n        <input value={input} onChange={handleInputChange} />\n        <button type=\"submit\" disabled={isLoading}>\n          Send\n        </button>\n        {isLoading && (\n          <button type=\"button\" onClick={stop}>\n            Stop\n          </button>\n        )}\n      </form>\n    </div>\n  );\n}\n```\n\n---\n\n## Pattern 5: Streaming Object (Structured Output)\n\nFor structured data (not just text).\n\n```typescript\nimport { openai } from '@ai-sdk/openai';\nimport { streamObject } from 'ai';\nimport { z } from 'zod';\n\nconst recipeSchema = z.object({\n  name: z.string(),\n  ingredients: z.array(z.object({\n    name: z.string(),\n    amount: z.string()\n  })),\n  instructions: z.array(z.string())\n});\n\nexport async function POST(req: Request) {\n  const { prompt } = await req.json();\n\n  const result = await streamObject({\n    model: openai('gpt-4-turbo'),\n    schema: recipeSchema,\n    prompt: `Generate a recipe for: ${prompt}`\n  });\n\n  return result.toTextStreamResponse();\n}\n```\n\n**Client**:\n```typescript\n'use client';\n\nimport { experimental_useObject as useObject } from 'ai/react';\n\nexport function RecipeGenerator() {\n  const { object, submit, isLoading } = useObject({\n    api: '/api/generate-recipe',\n    schema: recipeSchema\n  });\n\n  return (\n    <div>\n      <button onClick={() => submit('chocolate cake')}>\n        Generate Recipe\n      </button>\n\n      {object && (\n        <div>\n          <h2>{object.name}</h2>\n          <h3>Ingredients:</h3>\n          <ul>\n            {object.ingredients?.map((ing, i) => (\n              <li key={i}>{ing.amount} {ing.name}</li>\n            ))}\n          </ul>\n          <h3>Instructions:</h3>\n          <ol>\n            {object.instructions?.map((step, i) => (\n              <li key={i}>{step}</li>\n            ))}\n          </ol>\n        </div>\n      )}\n    </div>\n  );\n}\n```\n\n---\n\n## Pattern 6: Token Counting & Cost Tracking\n\n```typescript\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nexport async function POST(req: Request) {\n  const { messages } = await req.json();\n\n  const result = await streamText({\n    model: openai('gpt-4-turbo'),\n    messages,\n    onFinish: ({ usage }) => {\n      console.log('Token usage:', usage);\n      // { promptTokens: 50, completionTokens: 100, totalTokens: 150 }\n\n      const cost = calculateCost(usage, 'gpt-4-turbo');\n      await db.usage.create({ cost, tokens: usage.totalTokens });\n    }\n  });\n\n  return result.toAIStreamResponse();\n}\n\nfunction calculateCost(usage: any, model: string): number {\n  const pricing = {\n    'gpt-4-turbo': { input: 0.00001, output: 0.00003 } // per token\n  };\n\n  const rates = pricing[model];\n  return (\n    usage.promptTokens * rates.input +\n    usage.completionTokens * rates.output\n  );\n}\n```\n\n---\n\n## Pattern 7: Middleware (Logging, Auth)\n\n```typescript\nimport { openai } from '@ai-sdk/openai';\nimport { streamText } from 'ai';\n\nexport async function POST(req: Request) {\n  // Authentication\n  const session = await getSession(req);\n  if (!session) {\n    return new Response('Unauthorized', { status: 401 });\n  }\n\n  // Rate limiting\n  const { allowed } = await rateLimit(session.user.id);\n  if (!allowed) {\n    return new Response('Rate limit exceeded', { status: 429 });\n  }\n\n  const { messages } = await req.json();\n\n  // Log request\n  await db.chatLog.create({\n    userId: session.user.id,\n    prompt: messages[messages.length - 1].content,\n    timestamp: new Date()\n  });\n\n  const result = await streamText({\n    model: openai('gpt-4-turbo'),\n    messages,\n    onFinish: async ({ text, usage }) => {\n      // Log response\n      await db.chatLog.update({\n        response: text,\n        tokens: usage.totalTokens\n      });\n    }\n  });\n\n  return result.toAIStreamResponse();\n}\n```\n\n---\n\n## Pattern 8: Custom System Prompts\n\n```typescript\nconst result = await streamText({\n  model: openai('gpt-4-turbo'),\n  system: `You are a helpful assistant for a cooking app.\n           Only provide recipes and cooking advice.\n           Keep responses under 200 words.`,\n  messages\n});\n```\n\n---\n\n## Pattern 9: Temperature & Model Parameters\n\n```typescript\nconst result = await streamText({\n  model: openai('gpt-4-turbo'),\n  messages,\n  temperature: 0.7,  // Creativity (0-2)\n  maxTokens: 500,    // Response length limit\n  topP: 0.9,         // Nucleus sampling\n  frequencyPenalty: 0.5,  // Reduce repetition\n  presencePenalty: 0.5    // Encourage new topics\n});\n```\n\n---\n\n## Production Checklist\n\n```\n‚ñ° Rate limiting per user\n‚ñ° Token usage tracking\n‚ñ° Cost monitoring\n‚ñ° Error logging\n‚ñ° Authentication required\n‚ñ° System prompt defined\n‚ñ° Max tokens set\n‚ñ° onFinish handler for analytics\n‚ñ° Multi-provider fallback\n‚ñ° Tool calling validated (if used)\n```\n\n---\n\n## Comparison: Vercel AI SDK vs Raw API\n\n| Feature | Vercel AI SDK | Raw OpenAI API |\n|---------|---------------|----------------|\n| Setup complexity | Low | Medium |\n| Provider switching | Easy | Manual |\n| React hooks | Built-in | Custom |\n| Error handling | Automatic | Manual |\n| Retries | Yes | No |\n| Token counting | Built-in | Manual |\n| Type safety | ‚úÖ | ‚ö†Ô∏è |\n| Bundle size | +50KB | +10KB (minimal) |\n\n**Use Vercel AI SDK when**:\n- Building with Next.js\n- Need React hooks\n- Want provider flexibility\n- Prefer TypeScript safety\n\n**Use raw API when**:\n- Non-React framework\n- Bundle size critical\n- Need custom streaming logic\n- Provider-specific features\n\n---\n\n## Resources\n\n- [Vercel AI SDK Docs](https://sdk.vercel.ai/docs)\n- [Examples](https://github.com/vercel/ai/tree/main/examples)\n- [API Reference](https://sdk.vercel.ai/docs/reference)\n"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "folder",
      "path": "llm-streaming-response-handler/scripts",
      "children": [
        {
          "name": "stream_tester.ts",
          "type": "file",
          "path": "llm-streaming-response-handler/scripts/stream_tester.ts",
          "size": 5174,
          "content": "#!/usr/bin/env node\n/**\n * SSE Stream Tester\n *\n * Test Server-Sent Events endpoints locally without UI.\n *\n * Usage: npx tsx stream_tester.ts <endpoint> [prompt]\n *\n * Examples:\n *   npx tsx stream_tester.ts http://localhost:3000/api/chat \"Hello AI\"\n *   npx tsx stream_tester.ts https://api.example.com/stream\n *\n * Dependencies: npm install node-fetch\n */\n\ninterface StreamTestOptions {\n  endpoint: string;\n  prompt?: string;\n  method?: 'GET' | 'POST';\n  headers?: Record<string, string>;\n  timeout?: number;\n}\n\nasync function testSSEStream(options: StreamTestOptions) {\n  const {\n    endpoint,\n    prompt = 'Tell me a short story',\n    method = 'POST',\n    headers = {},\n    timeout = 30000\n  } = options;\n\n  console.log(`\\nüß™ Testing SSE endpoint: ${endpoint}\\n`);\n  console.log(`üìù Prompt: \"${prompt}\"\\n`);\n  console.log('‚îÄ'.repeat(60));\n\n  const startTime = Date.now();\n  let tokenCount = 0;\n  let fullResponse = '';\n  let lastEventTime = startTime;\n\n  try {\n    const controller = new AbortController();\n    const timeoutId = setTimeout(() => controller.abort(), timeout);\n\n    const fetchOptions: RequestInit = {\n      method,\n      signal: controller.signal,\n      headers: {\n        'Content-Type': 'application/json',\n        ...headers\n      }\n    };\n\n    if (method === 'POST') {\n      fetchOptions.body = JSON.stringify({ prompt });\n    }\n\n    const response = await fetch(endpoint, fetchOptions);\n\n    if (!response.ok) {\n      throw new Error(`HTTP ${response.status}: ${response.statusText}`);\n    }\n\n    if (!response.body) {\n      throw new Error('Response body is null');\n    }\n\n    console.log(`‚úÖ Connected (${response.status} ${response.statusText})\\n`);\n\n    const reader = response.body.getReader();\n    const decoder = new TextDecoder();\n\n    while (true) {\n      const { done, value } = await reader.read();\n\n      if (done) {\n        console.log('\\n\\n‚úÖ Stream completed');\n        break;\n      }\n\n      const now = Date.now();\n      const chunkLatency = now - lastEventTime;\n      lastEventTime = now;\n\n      const chunk = decoder.decode(value, { stream: true });\n      const lines = chunk.split('\\n').filter(line => line.trim());\n\n      for (const line of lines) {\n        if (line.startsWith('data: ')) {\n          const dataStr = line.slice(6);\n\n          // Handle [DONE] signal (OpenAI format)\n          if (dataStr === '[DONE]') {\n            console.log('\\n\\n‚úÖ Received [DONE] signal');\n            break;\n          }\n\n          try {\n            const data = JSON.parse(dataStr);\n\n            // Handle different response formats\n            let content = null;\n\n            if (data.content) {\n              // Generic format\n              content = data.content;\n            } else if (data.choices?.[0]?.delta?.content) {\n              // OpenAI format\n              content = data.choices[0].delta.content;\n            } else if (data.delta?.text) {\n              // Anthropic format\n              content = data.delta.text;\n            }\n\n            if (content) {\n              process.stdout.write(content);\n              fullResponse += content;\n              tokenCount++;\n\n              // Log latency for slow chunks\n              if (chunkLatency > 500) {\n                console.log(`\\n‚ö†Ô∏è  High latency: ${chunkLatency}ms\\n`);\n              }\n            }\n\n            if (data.done) {\n              console.log('\\n\\n‚úÖ Received done:true signal');\n              break;\n            }\n          } catch (parseError) {\n            console.error(`\\n‚ùå Failed to parse SSE data: ${dataStr}`);\n          }\n        } else if (line.startsWith('event: ')) {\n          const eventType = line.slice(7);\n          console.log(`\\nüì° Event: ${eventType}\\n`);\n        } else if (line.startsWith(':')) {\n          // Comment, ignore\n        } else if (line.trim()) {\n          console.log(`\\n‚ö†Ô∏è  Unexpected line: ${line}\\n`);\n        }\n      }\n    }\n\n    clearTimeout(timeoutId);\n\n    // Print summary\n    const duration = Date.now() - startTime;\n    console.log('\\n' + '‚îÄ'.repeat(60));\n    console.log('\\nüìä Stream Statistics:\\n');\n    console.log(`  Duration: ${duration}ms`);\n    console.log(`  Tokens received: ${tokenCount}`);\n    console.log(`  Average latency: ${Math.round(duration / tokenCount)}ms/token`);\n    console.log(`  Total characters: ${fullResponse.length}`);\n    console.log(`  Words: ${fullResponse.split(/\\s+/).length}`);\n\n  } catch (error: any) {\n    if (error.name === 'AbortError') {\n      console.error('\\n‚ùå Request timed out');\n    } else {\n      console.error('\\n‚ùå Error:', error.message);\n    }\n    process.exit(1);\n  }\n}\n\n// CLI entry point\nif (require.main === module) {\n  const args = process.argv.slice(2);\n\n  if (args.length === 0) {\n    console.log('Usage: npx tsx stream_tester.ts <endpoint> [prompt]');\n    console.log('\\nExamples:');\n    console.log('  npx tsx stream_tester.ts http://localhost:3000/api/chat');\n    console.log('  npx tsx stream_tester.ts https://api.example.com/stream \"Hello\"');\n    process.exit(1);\n  }\n\n  const endpoint = args[0];\n  const prompt = args[1];\n\n  testSSEStream({ endpoint, prompt }).catch(console.error);\n}\n\nexport { testSSEStream };\n"
        },
        {
          "name": "token_counter.ts",
          "type": "file",
          "path": "llm-streaming-response-handler/scripts/token_counter.ts",
          "size": 5611,
          "content": "#!/usr/bin/env node\n/**\n * Token Counter & Cost Estimator\n *\n * Estimate token usage and costs before making API calls.\n *\n * Usage: npx tsx token_counter.ts <text> [--model=gpt-4]\n *\n * Examples:\n *   npx tsx token_counter.ts \"Hello world\"\n *   npx tsx token_counter.ts \"Long text...\" --model=gpt-4\n *   echo \"Text from file\" | npx tsx token_counter.ts\n *\n * Dependencies: npm install tiktoken\n */\n\nimport { encoding_for_model } from 'tiktoken';\n\n// Pricing as of Jan 2024 (per 1M tokens)\nconst PRICING = {\n  'gpt-4': { input: 30, output: 60 },\n  'gpt-4-32k': { input: 60, output: 120 },\n  'gpt-4-turbo': { input: 10, output: 30 },\n  'gpt-3.5-turbo': { input: 0.50, output: 1.50 },\n  'claude-3-opus': { input: 15, output: 75 },\n  'claude-3-sonnet': { input: 3, output: 15 },\n  'claude-3-haiku': { input: 0.25, output: 1.25 }\n} as const;\n\ntype ModelName = keyof typeof PRICING;\n\ninterface TokenEstimate {\n  text: string;\n  model: ModelName;\n  tokens: number;\n  inputCost: number;\n  outputCost: number;\n  totalCost: number;\n}\n\nfunction estimateTokens(text: string, model: ModelName = 'gpt-4'): TokenEstimate {\n  // Map Claude models to closest tiktoken encoding\n  const tikTokenModel = model.startsWith('claude')\n    ? 'gpt-4'\n    : model as any;\n\n  const encoding = encoding_for_model(tikTokenModel);\n  const tokens = encoding.encode(text).length;\n  encoding.free();\n\n  const pricing = PRICING[model];\n  const inputCost = (tokens / 1_000_000) * pricing.input;\n  const outputCost = (tokens / 1_000_000) * pricing.output;\n\n  return {\n    text,\n    model,\n    tokens,\n    inputCost,\n    outputCost,\n    totalCost: inputCost + outputCost\n  };\n}\n\nfunction formatCost(cents: number): string {\n  if (cents < 0.01) {\n    return `$${cents.toFixed(4)}`;\n  }\n  return `$${cents.toFixed(2)}`;\n}\n\nfunction displayEstimate(estimate: TokenEstimate) {\n  console.log('\\nüìä Token Usage Estimate\\n');\n  console.log('‚îÄ'.repeat(50));\n  console.log(`Model: ${estimate.model}`);\n  console.log(`Tokens: ${estimate.tokens.toLocaleString()}`);\n  console.log(`\\nCost Breakdown:`);\n  console.log(`  Input:  ${formatCost(estimate.inputCost)}`);\n  console.log(`  Output: ${formatCost(estimate.outputCost)} (same tokens)`);\n  console.log(`  Total:  ${formatCost(estimate.totalCost)}`);\n  console.log('‚îÄ'.repeat(50));\n\n  // Warnings\n  if (estimate.tokens > 100_000) {\n    console.log('\\n‚ö†Ô∏è  High token count! Consider chunking.');\n  }\n\n  if (estimate.totalCost > 1.0) {\n    console.log('\\n‚ö†Ô∏è  Expensive request! Total cost > $1.00');\n  }\n\n  // Character stats\n  const charCount = estimate.text.length;\n  const wordCount = estimate.text.split(/\\s+/).length;\n\n  console.log(`\\nText Statistics:`);\n  console.log(`  Characters: ${charCount.toLocaleString()}`);\n  console.log(`  Words: ${wordCount.toLocaleString()}`);\n  console.log(`  Tokens/word: ${(estimate.tokens / wordCount).toFixed(2)}`);\n  console.log('');\n}\n\nfunction compareModels(text: string) {\n  console.log('\\nüîç Model Comparison\\n');\n  console.log('‚îÄ'.repeat(70));\n  console.log('Model'.padEnd(20), 'Tokens'.padEnd(12), 'Input'.padEnd(12), 'Output'.padEnd(12), 'Total');\n  console.log('‚îÄ'.repeat(70));\n\n  const models: ModelName[] = [\n    'gpt-4-turbo',\n    'gpt-4',\n    'gpt-3.5-turbo',\n    'claude-3-opus',\n    'claude-3-sonnet',\n    'claude-3-haiku'\n  ];\n\n  const estimates = models.map(model => estimateTokens(text, model));\n\n  estimates.forEach(est => {\n    console.log(\n      est.model.padEnd(20),\n      est.tokens.toString().padEnd(12),\n      formatCost(est.inputCost).padEnd(12),\n      formatCost(est.outputCost).padEnd(12),\n      formatCost(est.totalCost)\n    );\n  });\n\n  console.log('‚îÄ'.repeat(70));\n\n  // Highlight cheapest\n  const cheapest = estimates.reduce((min, est) =>\n    est.totalCost < min.totalCost ? est : min\n  );\n\n  console.log(`\\nüí∞ Cheapest option: ${cheapest.model} (${formatCost(cheapest.totalCost)})`);\n  console.log('');\n}\n\n// CLI entry point\nif (require.main === module) {\n  const args = process.argv.slice(2);\n  let text = '';\n  let model: ModelName = 'gpt-4';\n  let compare = false;\n\n  // Parse args\n  args.forEach(arg => {\n    if (arg.startsWith('--model=')) {\n      model = arg.slice(8) as ModelName;\n      if (!PRICING[model]) {\n        console.error(`‚ùå Unknown model: ${model}`);\n        console.error(`Available models: ${Object.keys(PRICING).join(', ')}`);\n        process.exit(1);\n      }\n    } else if (arg === '--compare') {\n      compare = true;\n    } else {\n      text = arg;\n    }\n  });\n\n  // Read from stdin if no text provided\n  if (!text && !process.stdin.isTTY) {\n    const chunks: Buffer[] = [];\n    process.stdin.on('data', chunk => chunks.push(chunk));\n    process.stdin.on('end', () => {\n      text = Buffer.concat(chunks).toString('utf-8');\n      processText(text, model, compare);\n    });\n  } else if (!text) {\n    console.log('Usage: npx tsx token_counter.ts <text> [--model=gpt-4] [--compare]');\n    console.log('\\nExamples:');\n    console.log('  npx tsx token_counter.ts \"Hello world\"');\n    console.log('  npx tsx token_counter.ts \"Text...\" --model=gpt-3.5-turbo');\n    console.log('  npx tsx token_counter.ts \"Text...\" --compare');\n    console.log('  echo \"Text\" | npx tsx token_counter.ts');\n    console.log(`\\nAvailable models: ${Object.keys(PRICING).join(', ')}`);\n    process.exit(1);\n  } else {\n    processText(text, model, compare);\n  }\n}\n\nfunction processText(text: string, model: ModelName, compare: boolean) {\n  if (compare) {\n    compareModels(text);\n  } else {\n    const estimate = estimateTokens(text, model);\n    displayEstimate(estimate);\n  }\n}\n\nexport { estimateTokens, ModelName };\n"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "llm-streaming-response-handler/SKILL.md",
      "size": 13254,
      "content": "---\nname: llm-streaming-response-handler\ndescription: Build production LLM streaming UIs with Server-Sent Events, real-time token display, cancellation, error recovery. Handles OpenAI/Anthropic/Claude streaming APIs. Use for chatbots, AI assistants, real-time text generation. Activate on \"LLM streaming\", \"SSE\", \"token stream\", \"chat UI\", \"real-time AI\". NOT for batch processing, non-streaming APIs, or WebSocket bidirectional chat.\nallowed-tools: Read,Write,Edit,Bash(npm:*)\n---\n\n# LLM Streaming Response Handler\n\nExpert in building production-grade streaming interfaces for LLM responses that feel instant and responsive.\n\n## When to Use\n\n‚úÖ **Use for**:\n- Chat interfaces with typing animation\n- Real-time AI assistants\n- Code generation with live preview\n- Document summarization with progressive display\n- Any UI where users expect immediate feedback from LLMs\n\n‚ùå **NOT for**:\n- Batch document processing (no user watching)\n- APIs that don't support streaming\n- WebSocket-based bidirectional chat (use Socket.IO)\n- Simple request/response (fetch is fine)\n\n## Quick Decision Tree\n\n```\nDoes your LLM interaction:\n‚îú‚îÄ‚îÄ Need immediate visual feedback? ‚Üí Streaming\n‚îú‚îÄ‚îÄ Display long-form content (&gt;100 words)? ‚Üí Streaming\n‚îú‚îÄ‚îÄ User expects typewriter effect? ‚Üí Streaming\n‚îú‚îÄ‚îÄ Short response (&lt;50 words)? ‚Üí Regular fetch\n‚îî‚îÄ‚îÄ Background processing? ‚Üí Regular fetch\n```\n\n---\n\n## Technology Selection\n\n### Server-Sent Events (SSE) - Recommended\n\n**Why SSE over WebSockets for LLM streaming**:\n- **Simplicity**: HTTP-based, works with existing infrastructure\n- **Auto-reconnect**: Built-in reconnection logic\n- **Firewall-friendly**: Easier than WebSockets through proxies\n- **One-way perfect**: LLMs only stream server ‚Üí client\n\n**Timeline**:\n- 2015-2020: WebSockets for everything\n- 2020: SSE adoption for streaming APIs\n- 2023+: SSE standard for LLM streaming (OpenAI, Anthropic)\n- 2024: Vercel AI SDK popularizes SSE patterns\n\n### Streaming APIs\n\n| Provider | Streaming Method | Response Format |\n|----------|------------------|-----------------|\n| OpenAI | SSE | `data: {\"choices\":[{\"delta\":{\"content\":\"token\"}}]}` |\n| Anthropic | SSE | `data: {\"type\":\"content_block_delta\",\"delta\":{\"text\":\"token\"}}` |\n| Claude (API) | SSE | `data: {\"delta\":{\"text\":\"token\"}}` |\n| Vercel AI SDK | SSE | Normalized across providers |\n\n---\n\n## Common Anti-Patterns\n\n### Anti-Pattern 1: Buffering Before Display\n\n**Novice thinking**: \"Collect all tokens, then show complete response\"\n\n**Problem**: Defeats the entire purpose of streaming.\n\n**Wrong approach**:\n```typescript\n// ‚ùå Waits for entire response before showing anything\nconst response = await fetch('/api/chat', { method: 'POST', body: prompt });\nconst fullText = await response.text();\nsetMessage(fullText); // User sees nothing until done\n```\n\n**Correct approach**:\n```typescript\n// ‚úÖ Display tokens as they arrive\nconst response = await fetch('/api/chat', {\n  method: 'POST',\n  body: JSON.stringify({ prompt })\n});\n\nconst reader = response.body.getReader();\nconst decoder = new TextDecoder();\n\nwhile (true) {\n  const { done, value } = await reader.read();\n  if (done) break;\n\n  const chunk = decoder.decode(value);\n  const lines = chunk.split('\\n').filter(line => line.trim());\n\n  for (const line of lines) {\n    if (line.startsWith('data: ')) {\n      const data = JSON.parse(line.slice(6));\n      setMessage(prev => prev + data.content); // Update immediately\n    }\n  }\n}\n```\n\n**Timeline**:\n- Pre-2023: Many apps buffered entire response\n- 2023+: Token-by-token display expected\n\n---\n\n### Anti-Pattern 2: No Stream Cancellation\n\n**Problem**: User can't stop generation, wasting tokens and money.\n\n**Symptom**: \"Stop\" button doesn't work or doesn't exist.\n\n**Correct approach**:\n```typescript\n// ‚úÖ AbortController for cancellation\nconst [abortController, setAbortController] = useState<AbortController | null>(null);\n\nconst streamResponse = async () => {\n  const controller = new AbortController();\n  setAbortController(controller);\n\n  try {\n    const response = await fetch('/api/chat', {\n      signal: controller.signal,\n      method: 'POST',\n      body: JSON.stringify({ prompt })\n    });\n\n    // Stream handling...\n  } catch (error) {\n    if (error.name === 'AbortError') {\n      console.log('Stream cancelled by user');\n    }\n  } finally {\n    setAbortController(null);\n  }\n};\n\nconst cancelStream = () => {\n  abortController?.abort();\n};\n\nreturn (\n  <button onClick={cancelStream} disabled={!abortController}>\n    Stop Generating\n  </button>\n);\n```\n\n---\n\n### Anti-Pattern 3: No Error Recovery\n\n**Problem**: Stream fails mid-response, user sees partial text with no indication of failure.\n\n**Correct approach**:\n```typescript\n// ‚úÖ Error states and recovery\nconst [streamState, setStreamState] = useState<'idle' | 'streaming' | 'error' | 'complete'>('idle');\nconst [errorMessage, setErrorMessage] = useState<string | null>(null);\n\ntry {\n  setStreamState('streaming');\n\n  // Streaming logic...\n\n  setStreamState('complete');\n} catch (error) {\n  setStreamState('error');\n\n  if (error.name === 'AbortError') {\n    setErrorMessage('Generation stopped');\n  } else if (error.message.includes('429')) {\n    setErrorMessage('Rate limit exceeded. Try again in a moment.');\n  } else {\n    setErrorMessage('Something went wrong. Please retry.');\n  }\n}\n\n// UI feedback\n{streamState === 'error' && (\n  <div className=\"error-banner\">\n    {errorMessage}\n    <button onClick={retryStream}>Retry</button>\n  </div>\n)}\n```\n\n---\n\n### Anti-Pattern 4: Memory Leaks from Unclosed Streams\n\n**Problem**: Streams not cleaned up, causing memory leaks.\n\n**Symptom**: Browser slows down after multiple requests.\n\n**Correct approach**:\n```typescript\n// ‚úÖ Cleanup with useEffect\nuseEffect(() => {\n  let reader: ReadableStreamDefaultReader | null = null;\n\n  const streamResponse = async () => {\n    const response = await fetch('/api/chat', { ... });\n    reader = response.body.getReader();\n\n    // Streaming...\n  };\n\n  streamResponse();\n\n  // Cleanup on unmount\n  return () => {\n    reader?.cancel();\n  };\n}, [prompt]);\n```\n\n---\n\n### Anti-Pattern 5: No Typing Indicator Between Tokens\n\n**Problem**: UI feels frozen between slow tokens.\n\n**Correct approach**:\n```typescript\n// ‚úÖ Animated cursor during generation\n<div className=\"message\">\n  {content}\n  {isStreaming && <span className=\"typing-cursor\">‚ñä</span>}\n</div>\n```\n\n```css\n.typing-cursor {\n  animation: blink 1s step-end infinite;\n}\n\n@keyframes blink {\n  50% { opacity: 0; }\n}\n```\n\n---\n\n## Implementation Patterns\n\n### Pattern 1: Basic SSE Stream Handler\n\n```typescript\nasync function* streamCompletion(prompt: string) {\n  const response = await fetch('/api/chat', {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({ prompt })\n  });\n\n  const reader = response.body!.getReader();\n  const decoder = new TextDecoder();\n\n  while (true) {\n    const { done, value } = await reader.read();\n    if (done) break;\n\n    const chunk = decoder.decode(value);\n    const lines = chunk.split('\\n');\n\n    for (const line of lines) {\n      if (line.startsWith('data: ')) {\n        const data = JSON.parse(line.slice(6));\n\n        if (data.content) {\n          yield data.content;\n        }\n\n        if (data.done) {\n          return;\n        }\n      }\n    }\n  }\n}\n\n// Usage\nfor await (const token of streamCompletion('Hello')) {\n  console.log(token);\n}\n```\n\n### Pattern 2: React Hook for Streaming\n\n```typescript\nimport { useState, useCallback } from 'react';\n\ninterface UseStreamingOptions {\n  onToken?: (token: string) => void;\n  onComplete?: (fullText: string) => void;\n  onError?: (error: Error) => void;\n}\n\nexport function useStreaming(options: UseStreamingOptions = {}) {\n  const [content, setContent] = useState('');\n  const [isStreaming, setIsStreaming] = useState(false);\n  const [error, setError] = useState<Error | null>(null);\n  const [abortController, setAbortController] = useState<AbortController | null>(null);\n\n  const stream = useCallback(async (prompt: string) => {\n    const controller = new AbortController();\n    setAbortController(controller);\n    setIsStreaming(true);\n    setError(null);\n    setContent('');\n\n    try {\n      const response = await fetch('/api/chat', {\n        method: 'POST',\n        signal: controller.signal,\n        headers: { 'Content-Type': 'application/json' },\n        body: JSON.stringify({ prompt })\n      });\n\n      const reader = response.body!.getReader();\n      const decoder = new TextDecoder();\n\n      let accumulated = '';\n\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n\n        const chunk = decoder.decode(value);\n        const lines = chunk.split('\\n').filter(line => line.trim());\n\n        for (const line of lines) {\n          if (line.startsWith('data: ')) {\n            const data = JSON.parse(line.slice(6));\n\n            if (data.content) {\n              accumulated += data.content;\n              setContent(accumulated);\n              options.onToken?.(data.content);\n            }\n          }\n        }\n      }\n\n      options.onComplete?.(accumulated);\n    } catch (err) {\n      if (err.name !== 'AbortError') {\n        setError(err as Error);\n        options.onError?.(err as Error);\n      }\n    } finally {\n      setIsStreaming(false);\n      setAbortController(null);\n    }\n  }, [options]);\n\n  const cancel = useCallback(() => {\n    abortController?.abort();\n  }, [abortController]);\n\n  return { content, isStreaming, error, stream, cancel };\n}\n\n// Usage in component\nfunction ChatInterface() {\n  const { content, isStreaming, stream, cancel } = useStreaming({\n    onToken: (token) => console.log('New token:', token),\n    onComplete: (text) => console.log('Done:', text)\n  });\n\n  return (\n    <div>\n      <div className=\"message\">\n        {content}\n        {isStreaming && <span className=\"cursor\">‚ñä</span>}\n      </div>\n\n      <button onClick={() => stream('Tell me a story')} disabled={isStreaming}>\n        Generate\n      </button>\n\n      {isStreaming && <button onClick={cancel}>Stop</button>}\n    </div>\n  );\n}\n```\n\n### Pattern 3: Server-Side Streaming (Next.js)\n\n```typescript\n// app/api/chat/route.ts\nimport { OpenAI } from 'openai';\n\nexport const runtime = 'edge'; // Required for streaming\n\nexport async function POST(req: Request) {\n  const { prompt } = await req.json();\n\n  const openai = new OpenAI({\n    apiKey: process.env.OPENAI_API_KEY\n  });\n\n  const stream = await openai.chat.completions.create({\n    model: 'gpt-4',\n    messages: [{ role: 'user', content: prompt }],\n    stream: true\n  });\n\n  // Convert OpenAI stream to SSE format\n  const encoder = new TextEncoder();\n\n  const readable = new ReadableStream({\n    async start(controller) {\n      try {\n        for await (const chunk of stream) {\n          const content = chunk.choices[0]?.delta?.content;\n\n          if (content) {\n            const sseMessage = `data: ${JSON.stringify({ content })}\\n\\n`;\n            controller.enqueue(encoder.encode(sseMessage));\n          }\n        }\n\n        // Send completion signal\n        controller.enqueue(encoder.encode('data: {\"done\":true}\\n\\n'));\n        controller.close();\n      } catch (error) {\n        controller.error(error);\n      }\n    }\n  });\n\n  return new Response(readable, {\n    headers: {\n      'Content-Type': 'text/event-stream',\n      'Cache-Control': 'no-cache',\n      'Connection': 'keep-alive'\n    }\n  });\n}\n```\n\n---\n\n## Production Checklist\n\n```\n‚ñ° AbortController for cancellation\n‚ñ° Error states with retry capability\n‚ñ° Typing indicator during generation\n‚ñ° Cleanup on component unmount\n‚ñ° Rate limiting on API route\n‚ñ° Token usage tracking\n‚ñ° Streaming fallback (if API fails)\n‚ñ° Accessibility (screen reader announces updates)\n‚ñ° Mobile-friendly (touch targets for stop button)\n‚ñ° Network error recovery (auto-retry on disconnect)\n‚ñ° Max response length enforcement\n‚ñ° Cost estimation before generation\n```\n\n---\n\n## When to Use vs Avoid\n\n| Scenario | Use Streaming? |\n|----------|----------------|\n| Chat interface | ‚úÖ Yes |\n| Long-form content generation | ‚úÖ Yes |\n| Code generation with preview | ‚úÖ Yes |\n| Short completions (&lt;50 words) | ‚ùå No - regular fetch |\n| Background jobs | ‚ùå No - use job queue |\n| Bidirectional chat | ‚ö†Ô∏è Use WebSockets instead |\n\n---\n\n## Technology Comparison\n\n| Feature | SSE | WebSockets | Long Polling |\n|---------|-----|-----------|--------------|\n| Complexity | Low | Medium | High |\n| Auto-reconnect | ‚úÖ | ‚ùå | ‚ùå |\n| Bidirectional | ‚ùå | ‚úÖ | ‚ùå |\n| Firewall-friendly | ‚úÖ | ‚ö†Ô∏è | ‚úÖ |\n| Browser support | ‚úÖ All modern | ‚úÖ All modern | ‚úÖ Universal |\n| LLM API support | ‚úÖ Standard | ‚ùå Rare | ‚ùå Not used |\n\n---\n\n## References\n\n- `/references/sse-protocol.md` - Server-Sent Events specification details\n- `/references/vercel-ai-sdk.md` - Vercel AI SDK integration patterns\n- `/references/error-recovery.md` - Stream error handling strategies\n\n## Scripts\n\n- `scripts/stream_tester.ts` - Test SSE endpoints locally\n- `scripts/token_counter.ts` - Estimate costs before generation\n\n---\n\n**This skill guides**: LLM streaming implementation | SSE protocol | Real-time UI updates | Cancellation | Error recovery | Token-by-token display\n\n"
    }
  ]
}