{
  "name": "data-pipeline-engineer",
  "type": "folder",
  "path": "data-pipeline-engineer",
  "children": [
    {
      "name": "references",
      "type": "folder",
      "path": "data-pipeline-engineer/references",
      "children": [
        {
          "name": "airflow-dag.py",
          "type": "file",
          "path": "data-pipeline-engineer/references/airflow-dag.py",
          "size": 4063,
          "content": "# Airflow DAG Reference\n# Complete ETL pipeline with sensors, task groups, and quality checks\n\nfrom datetime import datetime, timedelta\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator\nfrom airflow.providers.dbt.cloud.operators.dbt import DbtCloudRunJobOperator\nfrom airflow.sensors.external_task import ExternalTaskSensor\nfrom airflow.utils.task_group import TaskGroup\n\ndefault_args = {\n    'owner': 'data-engineering',\n    'depends_on_past': False,\n    'email_on_failure': True,\n    'email': ['data-alerts@company.com'],\n    'retries': 3,\n    'retry_delay': timedelta(minutes=5),\n    'retry_exponential_backoff': True,\n    'max_retry_delay': timedelta(minutes=30),\n}\n\nwith DAG(\n    dag_id='etl_orders_pipeline',\n    default_args=default_args,\n    description='Daily orders ETL pipeline',\n    schedule_interval='0 6 * * *',  # 6 AM daily\n    start_date=datetime(2024, 1, 1),\n    catchup=False,\n    max_active_runs=1,\n    tags=['etl', 'orders', 'production'],\n) as dag:\n\n    # Wait for upstream data\n    wait_for_source = ExternalTaskSensor(\n        task_id='wait_for_source_data',\n        external_dag_id='source_system_export',\n        external_task_id='export_complete',\n        timeout=3600,\n        poke_interval=60,\n        mode='reschedule',\n    )\n\n    # Extract and load to bronze\n    with TaskGroup(group_id='extract_load') as extract_load:\n        extract_orders = SparkSubmitOperator(\n            task_id='extract_orders',\n            application='/opt/spark/jobs/extract_orders.py',\n            conn_id='spark_default',\n            conf={\n                'spark.sql.shuffle.partitions': '200',\n                'spark.dynamicAllocation.enabled': 'true',\n            },\n            application_args=[\n                '--date', '{{ ds }}',\n                '--source', 's3://source-bucket/orders/',\n                '--target', 's3://bronze-bucket/orders/',\n            ],\n        )\n\n        extract_customers = SparkSubmitOperator(\n            task_id='extract_customers',\n            application='/opt/spark/jobs/extract_customers.py',\n            conn_id='spark_default',\n            application_args=[\n                '--date', '{{ ds }}',\n                '--source', 's3://source-bucket/customers/',\n                '--target', 's3://bronze-bucket/customers/',\n            ],\n        )\n\n    # Data quality checks on bronze\n    def run_bronze_quality_checks(**context):\n        from great_expectations.data_context import DataContext\n\n        ge_context = DataContext('/opt/great_expectations')\n        result = ge_context.run_checkpoint(\n            checkpoint_name='bronze_orders_checkpoint',\n            batch_request={\n                'datasource_name': 's3_bronze',\n                'data_asset_name': 'orders',\n                'data_connector_query': {\n                    'batch_filter_parameters': {\n                        'date': context['ds']\n                    }\n                }\n            }\n        )\n        if not result.success:\n            raise ValueError('Bronze data quality checks failed')\n\n    bronze_quality = PythonOperator(\n        task_id='bronze_quality_checks',\n        python_callable=run_bronze_quality_checks,\n    )\n\n    # Transform with dbt (bronze â†’ silver â†’ gold)\n    dbt_run = DbtCloudRunJobOperator(\n        task_id='dbt_transform',\n        job_id=12345,\n        check_interval=30,\n        timeout=7200,\n        wait_for_termination=True,\n    )\n\n    # Gold layer quality checks\n    gold_quality = PythonOperator(\n        task_id='gold_quality_checks',\n        python_callable=lambda: print('Running gold quality checks'),\n    )\n\n    # Notify downstream\n    def notify_completion(**context):\n        # Send Slack notification, update data catalog, etc.\n        pass\n\n    notify = PythonOperator(\n        task_id='notify_completion',\n        python_callable=notify_completion,\n    )\n\n    # DAG dependencies\n    wait_for_source >> extract_load >> bronze_quality >> dbt_run >> gold_quality >> notify\n"
        },
        {
          "name": "dbt-project-structure.md",
          "type": "file",
          "path": "data-pipeline-engineer/references/dbt-project-structure.md",
          "size": 3570,
          "content": "# dbt Project Structure Reference\n# Complete project layout with staging, intermediate, and marts layers\n\n```\ndbt_project/\nâ”œâ”€â”€ dbt_project.yml\nâ”œâ”€â”€ profiles.yml\nâ”œâ”€â”€ models/\nâ”‚   â”œâ”€â”€ staging/           # Bronze â†’ Silver\nâ”‚   â”‚   â”œâ”€â”€ _staging.yml\nâ”‚   â”‚   â”œâ”€â”€ stg_orders.sql\nâ”‚   â”‚   â”œâ”€â”€ stg_customers.sql\nâ”‚   â”‚   â””â”€â”€ stg_products.sql\nâ”‚   â”œâ”€â”€ intermediate/      # Business logic\nâ”‚   â”‚   â”œâ”€â”€ _intermediate.yml\nâ”‚   â”‚   â”œâ”€â”€ int_orders_enriched.sql\nâ”‚   â”‚   â””â”€â”€ int_customer_orders.sql\nâ”‚   â””â”€â”€ marts/             # Gold layer\nâ”‚       â”œâ”€â”€ core/\nâ”‚       â”‚   â”œâ”€â”€ _core.yml\nâ”‚       â”‚   â”œâ”€â”€ dim_customers.sql\nâ”‚       â”‚   â”œâ”€â”€ dim_products.sql\nâ”‚       â”‚   â””â”€â”€ fct_orders.sql\nâ”‚       â””â”€â”€ marketing/\nâ”‚           â”œâ”€â”€ _marketing.yml\nâ”‚           â””â”€â”€ mrt_customer_ltv.sql\nâ”œâ”€â”€ tests/\nâ”‚   â”œâ”€â”€ generic/\nâ”‚   â”‚   â””â”€â”€ test_positive_value.sql\nâ”‚   â””â”€â”€ singular/\nâ”‚       â””â”€â”€ assert_total_revenue_positive.sql\nâ”œâ”€â”€ macros/\nâ”‚   â”œâ”€â”€ generate_schema_name.sql\nâ”‚   â””â”€â”€ cents_to_dollars.sql\nâ”œâ”€â”€ seeds/\nâ”‚   â””â”€â”€ country_codes.csv\nâ””â”€â”€ snapshots/\n    â””â”€â”€ snap_customers.sql\n```\n\n## Staging Model Example\n\n```sql\n-- models/staging/stg_orders.sql\n{{\n  config(\n    materialized='incremental',\n    unique_key='order_id',\n    partition_by={\n      \"field\": \"order_date\",\n      \"data_type\": \"date\",\n      \"granularity\": \"day\"\n    }\n  )\n}}\n\nwith source as (\n    select * from {{ source('raw', 'orders') }}\n    {% if is_incremental() %}\n    where _loaded_at > (select max(_loaded_at) from {{ this }})\n    {% endif %}\n),\n\ncleaned as (\n    select\n        order_id,\n        customer_id,\n        cast(order_date as date) as order_date,\n        cast(total_cents as numeric) / 100 as total_amount,\n        status,\n        _loaded_at\n    from source\n    where order_id is not null\n)\n\nselect * from cleaned\n```\n\n## Fact Table Example\n\n```sql\n-- models/marts/core/fct_orders.sql\n{{\n  config(\n    materialized='table',\n    cluster_by=['customer_id', 'order_date']\n  )\n}}\n\nwith orders as (\n    select * from {{ ref('stg_orders') }}\n),\n\ncustomers as (\n    select * from {{ ref('dim_customers') }}\n),\n\nproducts as (\n    select * from {{ ref('int_order_items_enriched') }}\n),\n\nfinal as (\n    select\n        o.order_id,\n        o.order_date,\n        c.customer_key,\n        c.customer_segment,\n        p.total_items,\n        p.total_quantity,\n        o.total_amount,\n        o.status,\n        datediff('day', c.first_order_date, o.order_date) as days_since_first_order,\n        row_number() over (\n            partition by o.customer_id\n            order by o.order_date\n        ) as order_sequence_number\n    from orders o\n    left join customers c on o.customer_id = c.customer_id\n    left join products p on o.order_id = p.order_id\n)\n\nselect * from final\n```\n\n## Schema YAML Example\n\n```yaml\n# models/staging/_staging.yml\nversion: 2\n\nmodels:\n  - name: stg_orders\n    description: Cleaned orders from source system\n    columns:\n      - name: order_id\n        description: Primary key\n        tests:\n          - unique\n          - not_null\n      - name: customer_id\n        tests:\n          - not_null\n          - relationships:\n              to: ref('stg_customers')\n              field: customer_id\n      - name: total_amount\n        tests:\n          - not_null\n          - dbt_utils.accepted_range:\n              min_value: 0\n              max_value: 100000\n```\n"
        },
        {
          "name": "great-expectations-suite.json",
          "type": "file",
          "path": "data-pipeline-engineer/references/great-expectations-suite.json",
          "size": 2522,
          "content": "{\n  \"expectation_suite_name\": \"orders_bronze\",\n  \"meta\": {\n    \"great_expectations_version\": \"0.18.0\"\n  },\n  \"expectations\": [\n    {\n      \"expectation_type\": \"expect_table_row_count_to_be_between\",\n      \"kwargs\": {\n        \"min_value\": 1000,\n        \"max_value\": 10000000\n      },\n      \"meta\": {\n        \"notes\": \"Alert if unusually low/high volume\"\n      }\n    },\n    {\n      \"expectation_type\": \"expect_column_values_to_not_be_null\",\n      \"kwargs\": {\n        \"column\": \"order_id\"\n      }\n    },\n    {\n      \"expectation_type\": \"expect_column_values_to_be_unique\",\n      \"kwargs\": {\n        \"column\": \"order_id\"\n      }\n    },\n    {\n      \"expectation_type\": \"expect_column_values_to_not_be_null\",\n      \"kwargs\": {\n        \"column\": \"customer_id\"\n      }\n    },\n    {\n      \"expectation_type\": \"expect_column_values_to_be_between\",\n      \"kwargs\": {\n        \"column\": \"total_amount\",\n        \"min_value\": 0,\n        \"max_value\": 100000\n      },\n      \"meta\": {\n        \"notes\": \"Flag outlier orders for review\"\n      }\n    },\n    {\n      \"expectation_type\": \"expect_column_values_to_match_regex\",\n      \"kwargs\": {\n        \"column\": \"email\",\n        \"regex\": \"^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\\\.[a-zA-Z0-9-.]+$\"\n      }\n    },\n    {\n      \"expectation_type\": \"expect_column_pair_values_a_to_be_greater_than_b\",\n      \"kwargs\": {\n        \"column_A\": \"updated_at\",\n        \"column_B\": \"created_at\",\n        \"or_equal\": true\n      }\n    },\n    {\n      \"expectation_type\": \"expect_column_values_to_be_in_set\",\n      \"kwargs\": {\n        \"column\": \"status\",\n        \"value_set\": [\"pending\", \"processing\", \"shipped\", \"delivered\", \"cancelled\", \"refunded\"]\n      }\n    },\n    {\n      \"expectation_type\": \"expect_column_values_to_be_dateutil_parseable\",\n      \"kwargs\": {\n        \"column\": \"order_date\"\n      }\n    },\n    {\n      \"expectation_type\": \"expect_column_values_to_be_of_type\",\n      \"kwargs\": {\n        \"column\": \"quantity\",\n        \"type_\": \"int\"\n      }\n    },\n    {\n      \"expectation_type\": \"expect_column_values_to_be_between\",\n      \"kwargs\": {\n        \"column\": \"quantity\",\n        \"min_value\": 1,\n        \"max_value\": 1000\n      }\n    },\n    {\n      \"expectation_type\": \"expect_table_columns_to_match_set\",\n      \"kwargs\": {\n        \"column_set\": [\n          \"order_id\",\n          \"customer_id\",\n          \"order_date\",\n          \"total_amount\",\n          \"status\",\n          \"email\",\n          \"quantity\",\n          \"created_at\",\n          \"updated_at\",\n          \"_loaded_at\"\n        ]\n      }\n    }\n  ]\n}\n"
        },
        {
          "name": "spark-streaming.py",
          "type": "file",
          "path": "data-pipeline-engineer/references/spark-streaming.py",
          "size": 2882,
          "content": "# Spark Streaming Reference\n# Complete Kafka-to-Delta Lake streaming processor with windowing\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import (\n    from_json, col, window, sum as spark_sum,\n    count, avg, expr, current_timestamp\n)\nfrom pyspark.sql.types import (\n    StructType, StructField, StringType,\n    DoubleType, TimestampType, IntegerType\n)\n\nspark = SparkSession.builder \\\n    .appName(\"OrderStreamProcessor\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", \"s3://checkpoints/orders\") \\\n    .getOrCreate()\n\n# Define schema for incoming events\norder_schema = StructType([\n    StructField(\"order_id\", StringType(), False),\n    StructField(\"customer_id\", StringType(), False),\n    StructField(\"product_id\", StringType(), False),\n    StructField(\"quantity\", IntegerType(), False),\n    StructField(\"unit_price\", DoubleType(), False),\n    StructField(\"event_time\", TimestampType(), False),\n])\n\n# Read from Kafka\norders_stream = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n    .option(\"subscribe\", \"orders\") \\\n    .option(\"startingOffsets\", \"latest\") \\\n    .option(\"failOnDataLoss\", \"false\") \\\n    .load()\n\n# Parse JSON and apply schema\nparsed_orders = orders_stream \\\n    .select(from_json(col(\"value\").cast(\"string\"), order_schema).alias(\"data\")) \\\n    .select(\"data.*\") \\\n    .withColumn(\"total_amount\", col(\"quantity\") * col(\"unit_price\")) \\\n    .withWatermark(\"event_time\", \"10 minutes\")\n\n# Aggregate by 5-minute windows\nwindowed_metrics = parsed_orders \\\n    .groupBy(\n        window(col(\"event_time\"), \"5 minutes\", \"1 minute\"),\n        col(\"product_id\")\n    ) \\\n    .agg(\n        count(\"order_id\").alias(\"order_count\"),\n        spark_sum(\"quantity\").alias(\"total_quantity\"),\n        spark_sum(\"total_amount\").alias(\"total_revenue\"),\n        avg(\"total_amount\").alias(\"avg_order_value\")\n    ) \\\n    .select(\n        col(\"window.start\").alias(\"window_start\"),\n        col(\"window.end\").alias(\"window_end\"),\n        \"product_id\",\n        \"order_count\",\n        \"total_quantity\",\n        \"total_revenue\",\n        \"avg_order_value\",\n        current_timestamp().alias(\"processed_at\")\n    )\n\n# Write to Delta Lake\nquery = windowed_metrics \\\n    .writeStream \\\n    .format(\"delta\") \\\n    .outputMode(\"append\") \\\n    .option(\"checkpointLocation\", \"s3://checkpoints/metrics\") \\\n    .option(\"mergeSchema\", \"true\") \\\n    .trigger(processingTime=\"30 seconds\") \\\n    .start(\"s3://gold-bucket/order_metrics\")\n\n# Also write to Kafka for real-time dashboards\nkafka_query = windowed_metrics \\\n    .selectExpr(\"to_json(struct(*)) as value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n    .option(\"topic\", \"order_metrics\") \\\n    .option(\"checkpointLocation\", \"s3://checkpoints/kafka-metrics\") \\\n    .start()\n\nspark.streams.awaitAnyTermination()\n"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "folder",
      "path": "data-pipeline-engineer/scripts",
      "children": [
        {
          "name": "validate-pipeline.sh",
          "type": "file",
          "path": "data-pipeline-engineer/scripts/validate-pipeline.sh",
          "size": 6963,
          "content": "#!/bin/bash\n# Data Pipeline Engineer Skill Validation Script\n# Validates data pipeline configurations for best practices\n\nset -e\n\nERRORS=0\nWARNINGS=0\n\necho \"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\"\necho \"Data Pipeline Engineer Skill Validator\"\necho \"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\"\necho \"\"\n\n# Check dbt project\ncheck_dbt() {\n    echo \"ğŸ“Š Checking dbt project...\"\n\n    if [ -f \"dbt_project.yml\" ]; then\n        echo \"  Found dbt_project.yml\"\n\n        # Check for version\n        if ! grep -q \"version:\" dbt_project.yml 2>/dev/null; then\n            echo \"âš ï¸  WARN: dbt_project.yml missing version\"\n            ((WARNINGS++))\n        fi\n\n        # Check for models directory\n        if [ ! -d \"models\" ]; then\n            echo \"âŒ ERROR: models/ directory not found\"\n            ((ERRORS++))\n        fi\n\n        # Check for staging layer\n        if [ ! -d \"models/staging\" ]; then\n            echo \"âš ï¸  WARN: models/staging/ not found (recommended for medallion architecture)\"\n            ((WARNINGS++))\n        fi\n\n        # Check for marts layer\n        if [ ! -d \"models/marts\" ]; then\n            echo \"âš ï¸  WARN: models/marts/ not found (recommended for gold layer)\"\n            ((WARNINGS++))\n        fi\n\n        # Check models for incremental config\n        for model in models/**/*.sql; do\n            [ -f \"$model\" ] || continue\n\n            # Check if large tables are incremental\n            if grep -q \"materialized='table'\" \"$model\" 2>/dev/null; then\n                if ! grep -q \"is_incremental\" \"$model\" 2>/dev/null; then\n                    echo \"âš ï¸  WARN: $model uses table materialization but not incremental\"\n                    ((WARNINGS++))\n                fi\n            fi\n\n            # Check for hardcoded dates\n            if grep -qE \"'202[0-9]-[0-9]{2}-[0-9]{2}'\" \"$model\" 2>/dev/null; then\n                echo \"âš ï¸  WARN: $model contains hardcoded dates\"\n                ((WARNINGS++))\n            fi\n        done\n\n        # Check for schema YAML files\n        schema_files=$(find models -name \"*.yml\" 2>/dev/null | wc -l)\n        if [ \"$schema_files\" -eq 0 ]; then\n            echo \"âš ï¸  WARN: No schema YAML files found (add tests and documentation)\"\n            ((WARNINGS++))\n        fi\n    else\n        echo \"â„¹ï¸  No dbt project found\"\n    fi\n}\n\n# Check Airflow DAGs\ncheck_airflow() {\n    echo \"\"\n    echo \"ğŸ”„ Checking Airflow DAGs...\"\n\n    for dag in dags/*.py; do\n        [ -f \"$dag\" ] || continue\n\n        echo \"  Checking: $dag\"\n\n        # Check for retries\n        if ! grep -q \"retries\" \"$dag\" 2>/dev/null; then\n            echo \"âš ï¸  WARN: $dag missing retry configuration\"\n            ((WARNINGS++))\n        fi\n\n        # Check for catchup=False (usually desired)\n        if grep -q \"catchup=True\" \"$dag\" 2>/dev/null; then\n            echo \"â„¹ï¸  INFO: $dag has catchup=True (ensure this is intentional)\"\n        fi\n\n        # Check for max_active_runs\n        if ! grep -q \"max_active_runs\" \"$dag\" 2>/dev/null; then\n            echo \"âš ï¸  WARN: $dag missing max_active_runs (could cause parallel run issues)\"\n            ((WARNINGS++))\n        fi\n\n        # Check for email_on_failure\n        if ! grep -q \"email_on_failure\" \"$dag\" 2>/dev/null; then\n            echo \"âš ï¸  WARN: $dag missing email_on_failure notification\"\n            ((WARNINGS++))\n        fi\n\n        # Check for hardcoded connections\n        if grep -qE \"host\\s*=\\s*['\\\"]\" \"$dag\" 2>/dev/null; then\n            echo \"âŒ ERROR: $dag contains hardcoded host (use Airflow connections)\"\n            ((ERRORS++))\n        fi\n    done\n}\n\n# Check Spark jobs\ncheck_spark() {\n    echo \"\"\n    echo \"âš¡ Checking Spark jobs...\"\n\n    for spark_job in *.py spark/*.py jobs/*.py; do\n        [ -f \"$spark_job\" ] || continue\n\n        # Check if it's a Spark file\n        if ! grep -q \"SparkSession\\|pyspark\" \"$spark_job\" 2>/dev/null; then\n            continue\n        fi\n\n        echo \"  Checking: $spark_job\"\n\n        # Check for checkpoint location in streaming\n        if grep -q \"readStream\\|writeStream\" \"$spark_job\" 2>/dev/null; then\n            if ! grep -q \"checkpointLocation\" \"$spark_job\" 2>/dev/null; then\n                echo \"âŒ ERROR: $spark_job streaming job missing checkpointLocation\"\n                ((ERRORS++))\n            fi\n        fi\n\n        # Check for watermark in streaming\n        if grep -q \"readStream\" \"$spark_job\" 2>/dev/null; then\n            if ! grep -q \"withWatermark\" \"$spark_job\" 2>/dev/null; then\n                echo \"âš ï¸  WARN: $spark_job streaming job missing watermark (may accumulate state)\"\n                ((WARNINGS++))\n            fi\n        fi\n\n        # Check for shuffle partition tuning\n        if grep -q \"groupBy\\|join\" \"$spark_job\" 2>/dev/null; then\n            if ! grep -q \"shuffle.partitions\" \"$spark_job\" 2>/dev/null; then\n                echo \"â„¹ï¸  INFO: $spark_job may benefit from tuning spark.sql.shuffle.partitions\"\n            fi\n        fi\n    done\n}\n\n# Check data quality\ncheck_data_quality() {\n    echo \"\"\n    echo \"âœ… Checking data quality setup...\"\n\n    # Check for Great Expectations\n    if [ -d \"great_expectations\" ]; then\n        echo \"  Found Great Expectations configuration\"\n\n        if [ ! -d \"great_expectations/expectations\" ]; then\n            echo \"âš ï¸  WARN: No expectation suites found\"\n            ((WARNINGS++))\n        fi\n\n        if [ ! -d \"great_expectations/checkpoints\" ]; then\n            echo \"âš ï¸  WARN: No checkpoints configured\"\n            ((WARNINGS++))\n        fi\n    fi\n\n    # Check for dbt tests\n    if [ -f \"dbt_project.yml\" ]; then\n        test_count=$(grep -r \"tests:\" models/ 2>/dev/null | wc -l)\n        if [ \"$test_count\" -lt 5 ]; then\n            echo \"âš ï¸  WARN: Few dbt tests found (add schema tests)\"\n            ((WARNINGS++))\n        fi\n    fi\n}\n\n# Run all checks\ncheck_dbt\ncheck_airflow\ncheck_spark\ncheck_data_quality\n\n# Summary\necho \"\"\necho \"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\"\necho \"Validation Complete\"\necho \"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\"\necho \"Errors:   $ERRORS\"\necho \"Warnings: $WARNINGS\"\necho \"\"\n\nif [ $ERRORS -gt 0 ]; then\n    echo \"âŒ Validation FAILED - fix errors before deployment\"\n    exit 1\nelif [ $WARNINGS -gt 5 ]; then\n    echo \"âš ï¸  Validation PASSED with warnings - review recommended\"\n    exit 0\nelse\n    echo \"âœ… Validation PASSED\"\n    exit 0\nfi\n"
        }
      ]
    },
    {
      "name": "CHANGELOG.md",
      "type": "file",
      "path": "data-pipeline-engineer/CHANGELOG.md",
      "size": 1287,
      "content": "# Changelog\n\nAll notable changes to the data-pipeline-engineer skill will be documented in this file.\n\n## [2.0.0] - 2024-12-12\n\n### Changed\n- **BREAKING**: Restructured SKILL.md from 590 lines to ~160 lines for progressive disclosure\n- Moved all large code examples to `./references/` directory\n- Expanded anti-patterns section from 5 to 10 patterns\n\n### Added\n- `references/dbt-project-structure.md` - Complete dbt project layout with staging, intermediate, marts examples\n- `references/airflow-dag.py` - Production DAG with sensors, task groups, and quality checks\n- `references/spark-streaming.py` - Kafka-to-Delta streaming processor with windowing\n- `references/great-expectations-suite.json` - Comprehensive data quality expectation suite\n- `scripts/validate-pipeline.sh` - Validation for dbt, Airflow, Spark, and data quality setup\n- Version field in frontmatter for skill tracking\n\n### Improved\n- Anti-patterns section now covers 10 common mistakes with solutions\n- Quality checklist expanded to cover all pipeline components\n- Better cross-references to external documentation\n\n## [1.0.0] - 2024-01-01\n\n### Added\n- Initial data-pipeline-engineer skill\n- dbt medallion architecture guidance\n- Airflow DAG patterns\n- Spark optimization strategies\n- Great Expectations integration\n"
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "data-pipeline-engineer/SKILL.md",
      "size": 5656,
      "content": "---\nname: data-pipeline-engineer\nversion: 2.0.0\ndescription: \"Expert data engineer for ETL/ELT pipelines, streaming, data warehousing. Activate on: data pipeline, ETL, ELT, data warehouse, Spark, Kafka, Airflow, dbt, data modeling, star schema, streaming data, batch processing, data quality. NOT for: API design (use api-architect), ML training (use ML skills), dashboards (use design skills).\"\nallowed-tools: Read,Write,Edit,Bash(dbt:*,spark-submit:*,airflow:*,python:*)\n---\n\n# Data Pipeline Engineer\n\nExpert data engineer specializing in ETL/ELT pipelines, streaming architectures, data warehousing, and modern data stack implementation.\n\n## Quick Start\n\n1. **Identify sources** - data formats, volumes, freshness requirements\n2. **Choose architecture** - Medallion (Bronze/Silver/Gold), Lambda, or Kappa\n3. **Design layers** - staging â†’ intermediate â†’ marts (dbt pattern)\n4. **Add quality gates** - Great Expectations or dbt tests at each layer\n5. **Orchestrate** - Airflow DAGs with sensors and retries\n6. **Monitor** - lineage, freshness, anomaly detection\n\n## Core Capabilities\n\n| Capability | Technologies | Key Patterns |\n|------------|--------------|--------------|\n| **Batch Processing** | Spark, dbt, Databricks | Incremental, partitioning, Delta/Iceberg |\n| **Stream Processing** | Kafka, Flink, Spark Streaming | Watermarks, exactly-once, windowing |\n| **Orchestration** | Airflow, Dagster, Prefect | DAG design, sensors, task groups |\n| **Data Modeling** | dbt, SQL | Kimball, Data Vault, SCD |\n| **Data Quality** | Great Expectations, dbt tests | Validation suites, freshness |\n\n## Architecture Patterns\n\n### Medallion Architecture (Recommended)\n```\nBRONZE (Raw)     â†’ Exact source copy, schema-on-read, partitioned by ingestion\n      â†“ Cleaning, Deduplication\nSILVER (Cleansed) â†’ Validated, standardized, business logic applied\n      â†“ Aggregation, Enrichment\nGOLD (Business)   â†’ Dimensional models, aggregates, ready for BI/ML\n```\n\n### Lambda vs Kappa\n- **Lambda**: Batch + Stream layers â†’ merged serving layer (complex but complete)\n- **Kappa**: Stream-only with replay â†’ simpler but requires robust streaming\n\n## Reference Examples\n\nFull implementation examples in `./references/`:\n\n| File | Description |\n|------|-------------|\n| `dbt-project-structure.md` | Complete dbt layout with staging, intermediate, marts |\n| `airflow-dag.py` | Production DAG with sensors, task groups, quality checks |\n| `spark-streaming.py` | Kafka-to-Delta processor with windowing |\n| `great-expectations-suite.json` | Comprehensive data quality expectation suite |\n\n## Anti-Patterns (10 Critical Mistakes)\n\n### 1. Full Table Refreshes\n**Symptom**: Truncate and rebuild entire tables every run\n**Fix**: Use incremental models with `is_incremental()`, partition by date\n\n### 2. Tight Coupling to Source Schemas\n**Symptom**: Pipeline breaks when upstream adds/removes columns\n**Fix**: Explicit source contracts, select only needed columns in staging\n\n### 3. Monolithic DAGs\n**Symptom**: One 200-task DAG running 8 hours\n**Fix**: Domain-specific DAGs, ExternalTaskSensor for dependencies\n\n### 4. No Data Quality Gates\n**Symptom**: Bad data reaches production before detection\n**Fix**: Great Expectations or dbt tests at each layer, block on failures\n\n### 5. Processing Before Archiving\n**Symptom**: Raw data transformed without preserving original\n**Fix**: Always land raw in Bronze first, make transformations reproducible\n\n### 6. Hardcoded Dates in Queries\n**Symptom**: Manual updates needed for date filters\n**Fix**: Use Airflow templating (e.g., `ds` variable) or dynamic date functions\n\n### 7. Missing Watermarks in Streaming\n**Symptom**: Unbounded state growth, OOM in long-running jobs\n**Fix**: Add `withWatermark()` to handle late-arriving data\n\n### 8. No Retry/Backoff Strategy\n**Symptom**: Transient failures cause DAG failures\n**Fix**: `retries=3`, `retry_exponential_backoff=True`, `max_retry_delay`\n\n### 9. Undocumented Data Lineage\n**Symptom**: No one knows where data comes from or who uses it\n**Fix**: dbt docs, data catalog integration, column-level lineage\n\n### 10. Testing Only in Production\n**Symptom**: Bugs discovered by stakeholders, not engineers\n**Fix**: dbt `--target dev`, sample datasets, CI/CD for models\n\n## Quality Checklist\n\n**Pipeline Design:**\n- [ ] Incremental processing where possible\n- [ ] Idempotent transformations (re-runnable safely)\n- [ ] Partitioning strategy defined and documented\n- [ ] Backfill procedures documented\n\n**Data Quality:**\n- [ ] Tests at Bronze layer (schema, nulls, ranges)\n- [ ] Tests at Silver layer (business rules, referential integrity)\n- [ ] Tests at Gold layer (aggregation checks, trend monitoring)\n- [ ] Anomaly detection for volumes and distributions\n\n**Orchestration:**\n- [ ] Retry and alerting configured\n- [ ] SLAs defined and monitored\n- [ ] Cross-DAG dependencies use sensors\n- [ ] max_active_runs prevents parallel conflicts\n\n**Operations:**\n- [ ] Data lineage documented\n- [ ] Runbooks for common failures\n- [ ] Monitoring dashboards for pipeline health\n- [ ] On-call procedures defined\n\n## Validation Script\n\nRun `./scripts/validate-pipeline.sh` to check:\n- dbt project structure and conventions\n- Airflow DAG best practices\n- Spark job configurations\n- Data quality setup\n\n## External Resources\n\n- [dbt Best Practices](https://docs.getdbt.com/guides/best-practices)\n- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)\n- [Great Expectations Docs](https://docs.greatexpectations.io/)\n- [Delta Lake Guide](https://docs.delta.io/latest/index.html)\n- [Kafka Streams](https://kafka.apache.org/documentation/streams/)\n"
    }
  ]
}