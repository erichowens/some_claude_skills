{
  "name": "computer-vision-pipeline",
  "type": "folder",
  "path": "computer-vision-pipeline",
  "children": [
    {
      "name": "references",
      "type": "folder",
      "path": "computer-vision-pipeline/references",
      "children": [
        {
          "name": "tracking-algorithms.md",
          "type": "file",
          "path": "computer-vision-pipeline/references/tracking-algorithms.md",
          "size": 16708,
          "content": "# Multi-Object Tracking Algorithms\n\nComprehensive guide to tracking algorithms for maintaining object identity across video frames.\n\n---\n\n## Why Tracking Matters\n\n**Without Tracking**:\n```\nFrame 1: Detected 3 dolphins\nFrame 2: Detected 3 dolphins\nFrame 3: Detected 2 dolphins\n```\n**Question**: Are these the same dolphins? Which one left?\n\n**With Tracking**:\n```\nFrame 1: Dolphin #1, #2, #3\nFrame 2: Dolphin #1, #2, #3\nFrame 3: Dolphin #1, #3 (Dolphin #2 disappeared)\n```\n**Answer**: Dolphin #2 left the scene\n\n---\n\n## Algorithm Comparison\n\n| Algorithm | Speed (FPS) | Robustness | Occlusion | Re-ID | Use Case |\n|-----------|-------------|------------|-----------|-------|----------|\n| SORT | 260 | Fair | Poor | No | Simple scenes, speed critical |\n| DeepSORT | 40 | Excellent | Good | Yes | Crowded scenes, re-identification |\n| ByteTrack | 150 | Very Good | Excellent | No | Balanced performance |\n| BotSORT | 45 | Excellent | Excellent | Yes | Complex scenes, high accuracy |\n\n**Key Metrics**:\n- **Speed**: Frames per second (higher = faster)\n- **Robustness**: Handling ID switches\n- **Occlusion**: Tracking through overlaps\n- **Re-ID**: Re-identifying after long absence\n\n---\n\n## SORT (Simple Online and Realtime Tracking)\n\n### Algorithm Overview\n\n**How it works**:\n1. Detect objects in frame (YOLO, etc.)\n2. Associate detections with existing tracks using IoU\n3. Update tracks with Kalman filter\n4. Remove tracks that haven't been seen for N frames\n\n**Strengths**:\n- Extremely fast (260 FPS)\n- Simple to implement\n- Works well for sparse scenes\n\n**Weaknesses**:\n- Many ID switches in crowded scenes\n- Poor occlusion handling\n- No appearance-based matching\n\n---\n\n### SORT Implementation\n\n```python\nfrom sort import Sort\n\n# Initialize tracker\ntracker = Sort(\n    max_age=30,        # Max frames to keep track without detection\n    min_hits=3,        # Min detections before track is confirmed\n    iou_threshold=0.3  # IoU threshold for matching\n)\n\n# Process video\nvideo = cv2.VideoCapture('video.mp4')\nwhile True:\n    ret, frame = video.read()\n    if not ret:\n        break\n\n    # Run detector\n    results = yolo_model(frame)\n\n    # Convert to SORT format: [x1, y1, x2, y2, confidence]\n    detections = []\n    for box in results[0].boxes:\n        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n        conf = float(box.conf[0])\n        detections.append([x1, y1, x2, y2, conf])\n\n    # Update tracker\n    tracks = tracker.update(np.array(detections))\n\n    # tracks: [x1, y1, x2, y2, track_id]\n    for track in tracks:\n        x1, y1, x2, y2, track_id = track\n        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n        cv2.putText(frame, f'ID {int(track_id)}', (int(x1), int(y1)-10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n```\n\n**Installation**:\n```bash\npip install filterpy scikit-learn\ngit clone https://github.com/abewley/sort.git\n```\n\n---\n\n## DeepSORT (Deep Simple Online and Realtime Tracking)\n\n### Algorithm Overview\n\n**How it works**:\n1. Detect objects (YOLO)\n2. Extract appearance features (deep CNN)\n3. Associate using IoU + appearance similarity\n4. Update with Kalman filter\n5. Re-identify objects after long absence\n\n**Strengths**:\n- Excellent robustness in crowded scenes\n- Re-identification after occlusion\n- Appearance-based matching reduces ID switches\n\n**Weaknesses**:\n- Slower than SORT (40 FPS vs 260 FPS)\n- Requires pre-trained re-identification model\n- Higher computational cost\n\n---\n\n### DeepSORT Implementation\n\n```python\nfrom deep_sort_realtime.deepsort_tracker import DeepSort\n\n# Initialize tracker\ntracker = DeepSort(\n    max_age=30,              # Max frames without detection\n    n_init=3,                # Min detections before confirmed\n    max_iou_distance=0.7,    # IoU threshold\n    max_cosine_distance=0.3, # Appearance similarity threshold\n    embedder=\"mobilenet\",    # Feature extractor (mobilenet, resnet50, etc.)\n    half=True,               # Use FP16 for speed\n    bgr=True                 # Input is BGR (OpenCV default)\n)\n\n# Process video\nvideo = cv2.VideoCapture('video.mp4')\nwhile True:\n    ret, frame = video.read()\n    if not ret:\n        break\n\n    # Run detector\n    results = yolo_model(frame)\n\n    # Convert to DeepSORT format: ([x1, y1, w, h], confidence, class)\n    detections = []\n    for box in results[0].boxes:\n        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n        w, h = x2 - x1, y2 - y1\n        conf = float(box.conf[0])\n        cls = int(box.cls[0])\n\n        detections.append(([x1, y1, w, h], conf, cls))\n\n    # Update tracker\n    tracks = tracker.update_tracks(detections, frame=frame)\n\n    # Draw tracks\n    for track in tracks:\n        if not track.is_confirmed():\n            continue\n\n        track_id = track.track_id\n        ltrb = track.to_ltrb()  # [left, top, right, bottom]\n\n        cv2.rectangle(frame, (int(ltrb[0]), int(ltrb[1])),\n                      (int(ltrb[2]), int(ltrb[3])), (0, 255, 0), 2)\n        cv2.putText(frame, f'ID {track_id}', (int(ltrb[0]), int(ltrb[1])-10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n```\n\n**Installation**:\n```bash\npip install deep-sort-realtime\n```\n\n---\n\n## ByteTrack\n\n### Algorithm Overview\n\n**How it works**:\n1. Detect objects with YOLO\n2. Separate detections into high-confidence and low-confidence\n3. Match high-confidence detections first (like SORT)\n4. Use low-confidence detections to recover occluded objects\n5. Update with Kalman filter\n\n**Key Innovation**: Uses low-confidence detections that other trackers ignore\n\n**Strengths**:\n- Fast (150 FPS) - 3.75x faster than DeepSORT\n- Excellent occlusion handling\n- No appearance model needed (no extra GPU memory)\n- SOTA performance on MOT benchmarks\n\n**Weaknesses**:\n- No re-identification after long absence\n- Requires tuning confidence thresholds\n\n---\n\n### ByteTrack Implementation\n\n```python\nfrom ultralytics import YOLO\n\n# YOLOv8 has ByteTrack built-in!\nmodel = YOLO('yolov8n.pt')\n\n# Track with ByteTrack\nresults = model.track(\n    'video.mp4',\n    tracker='bytetrack.yaml',  # Use ByteTrack\n    conf=0.3,                  # Detection confidence (low for ByteTrack)\n    iou=0.5,                   # IoU threshold for NMS\n    persist=True,              # Persist tracks across frames\n    verbose=False\n)\n\n# Process results\nfor result in results:\n    boxes = result.boxes\n\n    if boxes is not None and boxes.id is not None:\n        for box in boxes:\n            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n            track_id = int(box.id[0])\n            conf = float(box.conf[0])\n            cls = int(box.cls[0])\n\n            print(f'Track {track_id}: {result.names[cls]} ({conf:.2f})')\n```\n\n**Custom ByteTrack Config** (`bytetrack.yaml`):\n```yaml\ntracker_type: bytetrack\ntrack_high_thresh: 0.5    # High confidence threshold\ntrack_low_thresh: 0.1     # Low confidence threshold (key!)\nnew_track_thresh: 0.6     # Threshold for new track\ntrack_buffer: 30          # Max frames without detection\nmatch_thresh: 0.8         # Matching threshold\n```\n\n---\n\n## BotSORT\n\n### Algorithm Overview\n\n**How it works**:\n1. ByteTrack foundation (high + low confidence)\n2. Add camera motion compensation\n3. Add appearance-based re-identification\n4. Use more sophisticated motion model\n\n**Strengths**:\n- Best overall accuracy (SOTA on MOT17/20)\n- Excellent occlusion handling (from ByteTrack)\n- Re-identification (from DeepSORT)\n- Camera motion compensation (unique)\n\n**Weaknesses**:\n- Slower than ByteTrack (45 FPS)\n- More complex to tune\n- Requires appearance model\n\n---\n\n### BotSORT Implementation\n\n```python\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8n.pt')\n\n# Track with BotSORT\nresults = model.track(\n    'video.mp4',\n    tracker='botsort.yaml',  # Use BotSORT\n    conf=0.3,\n    persist=True\n)\n\n# Process same as ByteTrack example above\n```\n\n**Custom BotSORT Config** (`botsort.yaml`):\n```yaml\ntracker_type: botsort\ntrack_high_thresh: 0.5\ntrack_low_thresh: 0.1\nnew_track_thresh: 0.6\ntrack_buffer: 30\nmatch_thresh: 0.8\nproximity_thresh: 0.5     # For appearance matching\nappearance_thresh: 0.25   # Appearance similarity\ncmc_method: sparseOptFlow # Camera motion compensation (sparseOptFlow, orb, ecc)\n```\n\n---\n\n## Performance Comparison\n\n### Speed Benchmarks\n\nTested on 1080p video, 30 FPS, 20 objects per frame (NVIDIA RTX 3080):\n\n| Tracker | Inference (ms) | Tracking (ms) | Total (ms) | FPS |\n|---------|----------------|---------------|------------|-----|\n| SORT | 28 | 1 | 29 | 34 |\n| DeepSORT | 28 | 18 | 46 | 22 |\n| ByteTrack | 28 | 3 | 31 | 32 |\n| BotSORT | 28 | 14 | 42 | 24 |\n\n**Key Insight**: Tracking overhead is minimal for SORT/ByteTrack, significant for DeepSORT/BotSORT\n\n---\n\n### Accuracy Benchmarks\n\nMOT17 Dataset (Multiple Object Tracking benchmark):\n\n| Tracker | MOTA | IDF1 | ID Switches | False Positives |\n|---------|------|------|-------------|-----------------|\n| SORT | 64.1% | 62.2% | 1,423 | 12,852 |\n| DeepSORT | 61.4% | 62.2% | 781 | 8,013 |\n| ByteTrack | **80.3%** | 77.3% | 2,196 | 8,112 |\n| BotSORT | **80.5%** | **80.2%** | **1,212** | **7,538** |\n\n**Metrics**:\n- **MOTA**: Multi-Object Tracking Accuracy (higher = better)\n- **IDF1**: ID F1 Score (higher = better, measures ID consistency)\n- **ID Switches**: Number of times IDs change (lower = better)\n\n**Winner**: BotSORT (best overall), ByteTrack (best speed/accuracy)\n\n---\n\n## Use Case Recommendations\n\n### Wildlife Monitoring (Dolphins, Birds, etc.)\n\n**Best Choice**: **ByteTrack**\n\n**Why**:\n- Animals move smoothly (Kalman filter works well)\n- Occlusions are common (ByteTrack excels)\n- No need for re-ID (animals don't leave and return)\n- Speed allows real-time processing\n\n**Config**:\n```yaml\ntracker_type: bytetrack\ntrack_high_thresh: 0.4    # Lower for animals (harder to detect)\ntrack_low_thresh: 0.1\ntrack_buffer: 60          # Longer buffer (animals move slower)\nmatch_thresh: 0.7         # Lower threshold (animal appearance varies)\n```\n\n---\n\n### Crowded Indoor Scenes (Retail, Security)\n\n**Best Choice**: **BotSORT** or **DeepSORT**\n\n**Why**:\n- Many occlusions (need appearance model)\n- People leave and return (need re-ID)\n- Camera is stationary (can use camera motion compensation)\n\n**Config** (BotSORT):\n```yaml\ntracker_type: botsort\ntrack_high_thresh: 0.5\ntrack_low_thresh: 0.1\ntrack_buffer: 30\nappearance_thresh: 0.25   # Strict appearance matching\ncmc_method: sparseOptFlow # Compensate for camera jitter\n```\n\n---\n\n### Drone Footage (Archaeological Surveys, Inspection)\n\n**Best Choice**: **ByteTrack** with custom config\n\n**Why**:\n- Camera moves (simpler motion model better)\n- Objects may be small/low confidence\n- Speed important for large footage volumes\n- No re-ID needed (continuous tracking)\n\n**Config**:\n```yaml\ntracker_type: bytetrack\ntrack_high_thresh: 0.3    # Very low (objects are small)\ntrack_low_thresh: 0.05    # Extremely low (catch faint objects)\ntrack_buffer: 90          # Long buffer (objects move slowly relative to camera)\nmatch_thresh: 0.6         # Lenient matching (camera motion)\n```\n\n---\n\n### Sports Tracking (Soccer, Basketball)\n\n**Best Choice**: **BotSORT**\n\n**Why**:\n- Fast, erratic motion\n- Frequent occlusions (players overlap)\n- Need re-ID (players leave/enter frame)\n- Camera pans/zooms (camera motion compensation helps)\n\n**Config**:\n```yaml\ntracker_type: botsort\ntrack_high_thresh: 0.5\ntrack_low_thresh: 0.1\ntrack_buffer: 20          # Short buffer (fast action)\nappearance_thresh: 0.3\ncmc_method: ecc           # Best for sports (handles zoom)\n```\n\n---\n\n### Real-Time Applications (Edge Devices, Webcams)\n\n**Best Choice**: **SORT**\n\n**Why**:\n- Fastest option (260 FPS)\n- Low memory footprint\n- Good enough for simple scenes\n\n**Config**:\n```python\ntracker = Sort(\n    max_age=15,       # Short buffer for real-time\n    min_hits=2,       # Quick confirmation\n    iou_threshold=0.3\n)\n```\n\n---\n\n## Common Issues and Solutions\n\n### Issue 1: Too Many ID Switches\n\n**Symptoms**: Same object gets new ID every few frames\n\n**Causes**:\n- Detection confidence too low\n- Match threshold too high\n- Track buffer too short\n\n**Solutions**:\n```yaml\n# Increase detection confidence\nconf: 0.5  # Instead of 0.3\n\n# Lower match threshold (more lenient)\nmatch_thresh: 0.6  # Instead of 0.8\n\n# Longer track buffer\ntrack_buffer: 60  # Instead of 30\n```\n\n---\n\n### Issue 2: Lost Tracks During Occlusion\n\n**Symptoms**: Object disappears behind another, gets new ID when reappearing\n\n**Cause**: Tracker doesn't use low-confidence detections\n\n**Solutions**:\n1. **Use ByteTrack or BotSORT** (designed for occlusion)\n2. **Lower track_low_thresh**:\n   ```yaml\n   track_low_thresh: 0.05  # Catch low-confidence detections\n   ```\n3. **Increase track_buffer**:\n   ```yaml\n   track_buffer: 90  # Keep track alive longer\n   ```\n\n---\n\n### Issue 3: Tracks Not Re-identified After Long Absence\n\n**Symptoms**: Object leaves frame, returns later with new ID\n\n**Cause**: SORT/ByteTrack don't support re-identification\n\n**Solution**: Switch to **DeepSORT** or **BotSORT**\n\n---\n\n### Issue 4: Slow Tracking Speed\n\n**Symptoms**: Tracking overhead dominates inference time\n\n**Causes**:\n- Using appearance model (DeepSORT/BotSORT)\n- Too many tracks\n- Too many detections\n\n**Solutions**:\n1. **Switch to ByteTrack or SORT**\n2. **Increase detection confidence**:\n   ```yaml\n   conf: 0.5  # Fewer detections = faster tracking\n   ```\n3. **Use FP16 for appearance model**:\n   ```python\n   tracker = DeepSort(half=True)  # 2x faster\n   ```\n\n---\n\n## Advanced Techniques\n\n### 1. Multi-Camera Tracking\n\nTrack objects across multiple camera views:\n\n```python\nfrom deep_sort_realtime.deepsort_tracker import DeepSort\n\n# Shared appearance database\nglobal_tracker = DeepSort(\n    max_cosine_distance=0.2,  # Strict appearance matching\n    embedder=\"resnet50\"       # Better features for cross-camera\n)\n\n# Camera 1\ntracks_cam1 = global_tracker.update_tracks(detections_cam1, frame_cam1)\n\n# Camera 2 (shares appearance database with cam 1)\ntracks_cam2 = global_tracker.update_tracks(detections_cam2, frame_cam2)\n\n# Match tracks by appearance\nfor t1 in tracks_cam1:\n    for t2 in tracks_cam2:\n        if appearance_similarity(t1, t2) > 0.8:\n            print(f\"Same object: Cam1 ID {t1.track_id} = Cam2 ID {t2.track_id}\")\n```\n\n---\n\n### 2. Track Smoothing\n\nReduce jittery bounding boxes with moving average:\n\n```python\nfrom collections import deque\n\nclass TrackSmoother:\n    def __init__(self, window_size=5):\n        self.tracks = {}  # track_id -> deque of boxes\n        self.window_size = window_size\n\n    def smooth(self, track_id, box):\n        \"\"\"Smooth bounding box with moving average\"\"\"\n        if track_id not in self.tracks:\n            self.tracks[track_id] = deque(maxlen=self.window_size)\n\n        self.tracks[track_id].append(box)\n\n        # Average boxes\n        boxes = np.array(self.tracks[track_id])\n        smoothed = boxes.mean(axis=0)\n\n        return smoothed\n\n# Usage\nsmoother = TrackSmoother(window_size=5)\n\nfor track in tracks:\n    x1, y1, x2, y2, track_id = track\n    smoothed_box = smoother.smooth(int(track_id), [x1, y1, x2, y2])\n```\n\n---\n\n### 3. Track Validation\n\nFilter out false positive tracks:\n\n```python\ndef validate_track(track_history, min_length=10, min_movement=50):\n    \"\"\"\n    Validate track is real (not false positive)\n\n    Args:\n        track_history: List of (x, y) center points\n        min_length: Minimum track length\n        min_movement: Minimum total movement (pixels)\n\n    Returns:\n        True if valid track\n    \"\"\"\n    if len(track_history) < min_length:\n        return False\n\n    # Calculate total movement\n    total_movement = 0\n    for i in range(1, len(track_history)):\n        dx = track_history[i][0] - track_history[i-1][0]\n        dy = track_history[i][1] - track_history[i-1][1]\n        total_movement += np.sqrt(dx**2 + dy**2)\n\n    return total_movement >= min_movement\n\n# Usage\ntrack_histories = {}  # track_id -> list of (x, y)\n\nfor track in tracks:\n    x1, y1, x2, y2, track_id = track\n    center_x, center_y = (x1 + x2) / 2, (y1 + y2) / 2\n\n    if track_id not in track_histories:\n        track_histories[track_id] = []\n\n    track_histories[track_id].append((center_x, center_y))\n\n    # Validate\n    if validate_track(track_histories[track_id]):\n        # Draw only valid tracks\n        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n```\n\n---\n\n## Resources\n\n- [ByteTrack Paper](https://arxiv.org/abs/2110.06864)\n- [DeepSORT Paper](https://arxiv.org/abs/1703.07402)\n- [BotSORT Paper](https://arxiv.org/abs/2206.14651)\n- [SORT Implementation](https://github.com/abewley/sort)\n- [DeepSORT Implementation](https://github.com/levan92/deep_sort_realtime)\n- [YOLOv8 Tracking](https://docs.ultralytics.com/modes/track/)\n- [MOT Challenge](https://motchallenge.net/) - Tracking benchmarks\n"
        },
        {
          "name": "video-processing.md",
          "type": "file",
          "path": "computer-vision-pipeline/references/video-processing.md",
          "size": 12639,
          "content": "# Video Processing for Computer Vision\n\nEfficient video frame extraction, preprocessing, and scene detection for object detection pipelines.\n\n---\n\n## Frame Extraction with FFmpeg\n\n### Basic Frame Extraction\n\n```bash\n# Extract every 30th frame (1 FPS for 30 FPS video)\nffmpeg -i video.mp4 -vf \"select='not(mod(n\\,30))'\" -vsync vfr frames/frame_%06d.jpg\n\n# Extract at specific FPS\nffmpeg -i video.mp4 -vf fps=1 frames/frame_%06d.jpg\n\n# Extract with quality control\nffmpeg -i video.mp4 -vf fps=1 -q:v 2 frames/frame_%06d.jpg\n# q:v range: 1 (best) to 31 (worst)\n```\n\n### Resolution and Aspect Ratio\n\n```bash\n# Resize to 640x640 (for YOLO)\nffmpeg -i video.mp4 -vf \"fps=1,scale=640:640:force_original_aspect_ratio=decrease,pad=640:640:(ow-iw)/2:(oh-ih)/2:color=gray\" frames/frame_%06d.jpg\n\n# Maintain aspect ratio with padding\nffmpeg -i video.mp4 -vf \"fps=1,scale=640:-1\" frames/frame_%06d.jpg\n```\n\n**Explanation**:\n- `scale=640:640:force_original_aspect_ratio=decrease` - Shrink to fit within 640x640\n- `pad=640:640:(ow-iw)/2:(oh-ih)/2` - Center padding to make square\n- `color=gray` - Gray padding (114,114,114 matches YOLO default)\n\n---\n\n## Scene Change Detection\n\n### FFmpeg Scene Detection\n\n```bash\n# Extract keyframes only (scene changes)\nffmpeg -i video.mp4 -vf \"select='gt(scene,0.3)',showinfo\" -vsync vfr frames/scene_%06d.jpg\n\n# Adjust sensitivity (0.0 = all frames, 1.0 = major changes only)\nffmpeg -i video.mp4 -vf \"select='gt(scene,0.4)'\" -vsync vfr frames/scene_%06d.jpg\n```\n\n**Scene threshold guide**:\n- `0.1` - Very sensitive (every small change)\n- `0.3` - Moderate (good for drone footage)\n- `0.5` - Conservative (only major scene changes)\n\n---\n\n### Python Scene Detection with OpenCV\n\n```python\nimport cv2\nimport numpy as np\nfrom typing import List, Tuple\n\ndef detect_scene_changes(\n    video_path: str,\n    threshold: float = 0.3,\n    min_frame_gap: int = 10\n) -> List[int]:\n    \"\"\"\n    Detect scene changes using histogram comparison\n\n    Args:\n        video_path: Path to video file\n        threshold: Scene change threshold (0.0-1.0)\n        min_frame_gap: Minimum frames between scene changes\n\n    Returns:\n        List of frame numbers where scenes change\n    \"\"\"\n    video = cv2.VideoCapture(video_path)\n\n    scene_frames = []\n    prev_hist = None\n    frame_count = 0\n    last_scene_frame = -min_frame_gap\n\n    while True:\n        ret, frame = video.read()\n        if not ret:\n            break\n\n        # Convert to grayscale\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n        # Calculate histogram\n        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n        cv2.normalize(hist, hist)\n\n        if prev_hist is not None:\n            # Compare histograms\n            correlation = cv2.compareHist(\n                prev_hist,\n                hist,\n                cv2.HISTCMP_CORREL\n            )\n\n            # Scene change detected\n            if correlation < (1 - threshold):\n                # Respect minimum gap\n                if frame_count - last_scene_frame >= min_frame_gap:\n                    scene_frames.append(frame_count)\n                    last_scene_frame = frame_count\n\n        prev_hist = hist\n        frame_count += 1\n\n    video.release()\n    return scene_frames\n```\n\n---\n\n### Advanced: Structural Similarity (SSIM)\n\n```python\nfrom skimage.metrics import structural_similarity as ssim\n\ndef detect_scenes_ssim(\n    video_path: str,\n    threshold: float = 0.7\n) -> List[int]:\n    \"\"\"\n    Detect scene changes using SSIM\n\n    More accurate than histogram, but slower\n    \"\"\"\n    video = cv2.VideoCapture(video_path)\n\n    scene_frames = []\n    prev_frame = None\n    frame_count = 0\n\n    while True:\n        ret, frame = video.read()\n        if not ret:\n            break\n\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n        if prev_frame is not None:\n            # Calculate SSIM\n            score = ssim(prev_frame, gray)\n\n            # Low SSIM = scene change\n            if score < threshold:\n                scene_frames.append(frame_count)\n\n        prev_frame = gray\n        frame_count += 1\n\n    video.release()\n    return scene_frames\n```\n\n**SSIM vs Histogram**:\n- **Histogram**: Fast (200 FPS), good for gross changes\n- **SSIM**: Slower (50 FPS), better for subtle changes\n- **Use histogram** for drone footage, wildlife\n- **Use SSIM** for indoor scenes, dialogue cuts\n\n---\n\n## Memory-Efficient Streaming\n\n### Problem: Loading Entire Video\n\n```python\n# ‚ùå WRONG: Loads entire video into memory\nvideo = cv2.VideoCapture('large_video.mp4')\nframes = []\nwhile True:\n    ret, frame = video.read()\n    if not ret:\n        break\n    frames.append(frame)  # 4K frame = 32 MB!\n\n# 30 seconds of 4K @ 30 FPS = 28 GB RAM\n```\n\n---\n\n### Solution: Batch Processing\n\n```python\n# ‚úÖ CORRECT: Process in batches\ndef process_video_batched(\n    video_path: str,\n    model,\n    batch_size: int = 16,\n    sample_rate: int = 30\n):\n    \"\"\"Process video in batches to limit memory usage\"\"\"\n    video = cv2.VideoCapture(video_path)\n\n    batch = []\n    frame_count = 0\n\n    while True:\n        ret, frame = video.read()\n        if not ret:\n            # Process final batch\n            if batch:\n                yield model(batch)\n            break\n\n        frame_count += 1\n\n        # Sample frames\n        if frame_count % sample_rate != 0:\n            continue\n\n        # Preprocess\n        processed = preprocess_frame(frame)\n        batch.append(processed)\n\n        # Process batch when full\n        if len(batch) >= batch_size:\n            results = model(batch)\n            yield results\n            batch = []  # Clear memory\n\n    video.release()\n\n# Usage\nfor batch_results in process_video_batched('video.mp4', yolo_model):\n    # Process results immediately\n    save_results(batch_results)\n```\n\n**Memory savings**:\n- Batch of 16 frames @ 640x640 = 31 MB\n- vs 900 frames @ 4K = 28 GB\n- **900x less memory**\n\n---\n\n## Video Codec Optimization\n\n### Choosing the Right Codec\n\n| Codec | Speed | Size | Quality | Use Case |\n|-------|-------|------|---------|----------|\n| H.264 | Fast | Small | Good | General purpose |\n| H.265 | Slow | Smaller | Better | High quality, storage |\n| VP9 | Medium | Small | Good | Web delivery |\n| ProRes | Very Fast | Large | Excellent | Editing, CV processing |\n\n**For CV pipelines**: Use **H.264** for storage, **extract frames** for processing\n\n---\n\n### Re-encoding for Speed\n\n```bash\n# Re-encode to H.264 for faster seeking\nffmpeg -i input.mp4 -c:v libx264 -preset ultrafast -crf 23 output.mp4\n\n# Preset options:\n# - ultrafast: Fastest encoding, larger files\n# - fast: Good balance\n# - medium: Default\n# - slow: Better compression\n\n# CRF (quality):\n# - 0: Lossless (huge files)\n# - 18-23: High quality (visually lossless)\n# - 28: Acceptable quality\n# - 51: Worst quality\n```\n\n---\n\n## Preprocessing Pipeline\n\n### Complete Pipeline\n\n```python\nimport cv2\nimport numpy as np\n\nclass VideoPreprocessor:\n    \"\"\"Complete preprocessing pipeline for CV\"\"\"\n\n    def __init__(\n        self,\n        target_size: int = 640,\n        normalize: bool = True,\n        enhance_contrast: bool = False\n    ):\n        self.target_size = target_size\n        self.normalize = normalize\n        self.enhance_contrast = enhance_contrast\n\n    def preprocess_frame(self, frame: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Full preprocessing pipeline\n\n        1. Resize to target size\n        2. Pad to square\n        3. Enhance contrast (optional)\n        4. Normalize (optional)\n        \"\"\"\n        # 1. Resize\n        h, w = frame.shape[:2]\n        scale = self.target_size / max(h, w)\n        new_w, new_h = int(w * scale), int(h * scale)\n\n        resized = cv2.resize(\n            frame,\n            (new_w, new_h),\n            interpolation=cv2.INTER_LINEAR\n        )\n\n        # 2. Pad to square\n        pad_w = (self.target_size - new_w) // 2\n        pad_h = (self.target_size - new_h) // 2\n\n        padded = cv2.copyMakeBorder(\n            resized,\n            pad_h, self.target_size - new_h - pad_h,\n            pad_w, self.target_size - new_w - pad_w,\n            cv2.BORDER_CONSTANT,\n            value=(114, 114, 114)  # Gray padding\n        )\n\n        # 3. Enhance contrast (optional)\n        if self.enhance_contrast:\n            lab = cv2.cvtColor(padded, cv2.COLOR_BGR2LAB)\n            l, a, b = cv2.split(lab)\n            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n            l = clahe.apply(l)\n            padded = cv2.merge([l, a, b])\n            padded = cv2.cvtColor(padded, cv2.COLOR_LAB2BGR)\n\n        # 4. Normalize (optional)\n        if self.normalize:\n            padded = padded.astype(np.float32) / 255.0\n\n        return padded\n```\n\n---\n\n## FFmpeg Advanced Filters\n\n### Multi-Stage Filtering\n\n```bash\n# Extract, resize, denoise, and sharpen\nffmpeg -i video.mp4 \\\n  -vf \"fps=1,scale=640:640:force_original_aspect_ratio=decrease,pad=640:640:(ow-iw)/2:(oh-ih)/2,hqdn3d=4:3:6:4.5,unsharp=5:5:1.0:5:5:0.0\" \\\n  -q:v 2 \\\n  frames/frame_%06d.jpg\n```\n\n**Filter breakdown**:\n- `fps=1` - 1 frame per second\n- `scale=640:640:force_original_aspect_ratio=decrease` - Resize\n- `pad=640:640:(ow-iw)/2:(oh-ih)/2` - Center padding\n- `hqdn3d=4:3:6:4.5` - Denoise (luma:chroma:luma_temporal:chroma_temporal)\n- `unsharp=5:5:1.0:5:5:0.0` - Sharpen (luma only)\n\n---\n\n### Extracting Specific Time Ranges\n\n```bash\n# Extract frames from 1:30 to 2:00\nffmpeg -i video.mp4 -ss 00:01:30 -to 00:02:00 -vf fps=1 frames/frame_%06d.jpg\n\n# Extract frames starting at 5:00 for 30 seconds\nffmpeg -i video.mp4 -ss 00:05:00 -t 30 -vf fps=1 frames/frame_%06d.jpg\n```\n\n---\n\n## Performance Benchmarks\n\n### Frame Extraction Speed\n\nTested on 10-minute 4K drone footage (30 FPS, 18,000 frames):\n\n| Method | Time | Frames | Throughput |\n|--------|------|--------|------------|\n| Python (cv2.VideoCapture) | 45s | 600 | 13 FPS |\n| FFmpeg (fps filter) | 12s | 600 | 50 FPS |\n| FFmpeg (select filter) | 8s | 600 | 75 FPS |\n| FFmpeg (scene detection) | 22s | 324 | 15 FPS |\n\n**Winner**: FFmpeg with select filter (6x faster than Python)\n\n---\n\n### Scene Detection Performance\n\n10-minute video, detecting scene changes:\n\n| Method | Time | Scenes | Accuracy |\n|--------|------|--------|----------|\n| Histogram (OpenCV) | 18s | 67 | Good |\n| SSIM (scikit-image) | 98s | 73 | Excellent |\n| FFmpeg scene filter | 22s | 71 | Very Good |\n\n**Winner**: FFmpeg scene filter (fast + accurate)\n\n---\n\n## Best Practices\n\n1. **Always sample frames**\n   - Use `fps=1` for general use\n   - Use scene detection for narrative content\n   - Process every frame only for critical applications\n\n2. **Resize before detection**\n   - YOLO expects 640x640\n   - Resizing during extraction is 3x faster than after\n\n3. **Use FFmpeg for extraction**\n   - 6x faster than Python\n   - Better quality control\n   - GPU acceleration available\n\n4. **Batch process frames**\n   - Load 16-32 frames at once\n   - Process batch together\n   - Clear memory between batches\n\n5. **Choose codec wisely**\n   - H.264 for general use\n   - ProRes for frame-accurate seeking\n   - Avoid H.265 for extraction (slow to decode)\n\n---\n\n## Common Pitfalls\n\n### Pitfall 1: Extracting Every Frame\n\n```bash\n# ‚ùå WRONG: 18,000 frames for 10-minute video\nffmpeg -i video.mp4 frames/frame_%06d.jpg\n```\n\n**Why it's wrong**:\n- 18,000 inferences (slow, expensive)\n- Adjacent frames are nearly identical\n- Wasting GPU cycles on duplicate information\n\n**Solution**:\n```bash\n# ‚úÖ CORRECT: 600 frames (97% reduction)\nffmpeg -i video.mp4 -vf fps=1 frames/frame_%06d.jpg\n```\n\n---\n\n### Pitfall 2: Not Preprocessing\n\n```bash\n# ‚ùå WRONG: Extract at original resolution\nffmpeg -i 4k_video.mp4 -vf fps=1 frames/frame_%06d.jpg\n# Then resize in Python (slow)\n```\n\n**Why it's wrong**:\n- Large files (32 MB per 4K frame)\n- Slower I/O\n- Extra preprocessing step\n\n**Solution**:\n```bash\n# ‚úÖ CORRECT: Resize during extraction\nffmpeg -i 4k_video.mp4 -vf \"fps=1,scale=640:640:force_original_aspect_ratio=decrease,pad=640:640:(ow-iw)/2:(oh-ih)/2\" frames/frame_%06d.jpg\n```\n\n---\n\n### Pitfall 3: Poor Quality Settings\n\n```bash\n# ‚ùå WRONG: Default quality (artifacts)\nffmpeg -i video.mp4 -vf fps=1 frames/frame_%06d.jpg\n```\n\n**Why it's wrong**:\n- JPEG compression artifacts hurt detection accuracy\n- Default quality varies by FFmpeg version\n\n**Solution**:\n```bash\n# ‚úÖ CORRECT: Explicit quality setting\nffmpeg -i video.mp4 -vf fps=1 -q:v 2 frames/frame_%06d.jpg\n# q:v 2 = very high quality\n```\n\n---\n\n## Resources\n\n- [FFmpeg Filters Documentation](https://ffmpeg.org/ffmpeg-filters.html)\n- [OpenCV Video Processing](https://docs.opencv.org/4.x/d8/dfe/classcv_1_1VideoCapture.html)\n- [Scene Detection Library (PySceneDetect)](https://pyscenedetect.readthedocs.io/)\n"
        },
        {
          "name": "yolo-guide.md",
          "type": "file",
          "path": "computer-vision-pipeline/references/yolo-guide.md",
          "size": 10105,
          "content": "# YOLOv8 Guide\n\nComplete guide to YOLOv8 for object detection: setup, training, inference, and optimization.\n\n## Installation\n\n```bash\n# Install ultralytics (includes YOLOv8)\npip install ultralytics\n\n# Verify installation\nyolo version\n\n# Install additional dependencies\npip install opencv-python numpy torch torchvision\n```\n\n**Latest version**: ultralytics 8.1.20 (Jan 2024)\n\n---\n\n## Model Variants\n\n| Model | Size (MB) | mAP50-95 | Speed (ms) | Params (M) | Use Case |\n|-------|-----------|----------|------------|------------|----------|\n| YOLOv8n | 6 | 37.3% | 80 | 3.2 | Mobile, edge devices |\n| YOLOv8s | 22 | 44.9% | 128 | 11.2 | Embedded systems |\n| YOLOv8m | 52 | 50.2% | 234 | 25.9 | Balanced |\n| YOLOv8l | 88 | 52.9% | 375 | 43.7 | High accuracy |\n| YOLOv8x | 136 | 53.9% | 479 | 68.2 | Highest accuracy |\n\n**Naming**:\n- `n` = nano (smallest)\n- `s` = small\n- `m` = medium\n- `l` = large\n- `x` = extra large\n\n**Speed measured on**: NVIDIA T4 GPU, batch size 1, image size 640x640\n\n---\n\n## Basic Inference\n\n### Load Pre-trained Model\n\n```python\nfrom ultralytics import YOLO\n\n# Load model\nmodel = YOLO('yolov8n.pt')  # nano model\n\n# Run inference on single image\nresults = model('image.jpg')\n\n# Access results\nfor result in results:\n    boxes = result.boxes  # Bounding boxes\n    masks = result.masks  # Segmentation masks (if using seg model)\n    probs = result.probs  # Classification probabilities\n```\n\n### Process Results\n\n```python\nimport cv2\n\nresults = model('image.jpg')\n\nfor result in results:\n    # Get bounding boxes\n    boxes = result.boxes\n\n    for box in boxes:\n        # Coordinates\n        x1, y1, x2, y2 = box.xyxy[0]  # Box coordinates\n\n        # Metadata\n        conf = box.conf[0]  # Confidence\n        cls = box.cls[0]    # Class ID\n        label = result.names[int(cls)]  # Class name\n\n        print(f\"{label} {conf:.2f} at ({x1}, {y1}, {x2}, {y2})\")\n\n        # Draw on image\n        img = result.orig_img\n        cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n        cv2.putText(img, f'{label} {conf:.2f}', (int(x1), int(y1)-10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    cv2.imwrite('output.jpg', img)\n```\n\n---\n\n## Inference Options\n\n### Confidence and IoU Thresholds\n\n```python\nresults = model(\n    'image.jpg',\n    conf=0.5,    # Confidence threshold (default: 0.25)\n    iou=0.7      # IoU threshold for NMS (default: 0.7)\n)\n```\n\n### Image Size\n\n```python\nresults = model(\n    'image.jpg',\n    imgsz=640    # Image size (default: 640)\n                  # Can be single int or tuple (height, width)\n)\n```\n\n### Device Selection\n\n```python\n# Use GPU\nresults = model('image.jpg', device=0)  # GPU 0\n\n# Use CPU\nresults = model('image.jpg', device='cpu')\n\n# Multiple GPUs\nresults = model('image.jpg', device=[0, 1])  # GPUs 0 and 1\n```\n\n### Max Detections\n\n```python\nresults = model(\n    'image.jpg',\n    max_det=100  # Maximum detections per image (default: 300)\n)\n```\n\n---\n\n## Batch Inference\n\n```python\n# List of images\nimage_paths = ['img1.jpg', 'img2.jpg', 'img3.jpg']\nresults = model(image_paths)\n\n# Process results\nfor i, result in enumerate(results):\n    print(f\"Image {i}: {len(result.boxes)} detections\")\n```\n\n---\n\n## Training Custom Models\n\n### Dataset Format\n\nYOLO requires this directory structure:\n\n```\ndataset/\n‚îú‚îÄ‚îÄ data.yaml\n‚îú‚îÄ‚îÄ images/\n‚îÇ   ‚îú‚îÄ‚îÄ train/\n‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ img001.jpg\n‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ img002.jpg\n‚îÇ   ‚îî‚îÄ‚îÄ val/\n‚îÇ       ‚îú‚îÄ‚îÄ img101.jpg\n‚îÇ       ‚îî‚îÄ‚îÄ img102.jpg\n‚îî‚îÄ‚îÄ labels/\n    ‚îú‚îÄ‚îÄ train/\n    ‚îÇ   ‚îú‚îÄ‚îÄ img001.txt\n    ‚îÇ   ‚îî‚îÄ‚îÄ img002.txt\n    ‚îî‚îÄ‚îÄ val/\n        ‚îú‚îÄ‚îÄ img101.txt\n        ‚îî‚îÄ‚îÄ img102.txt\n```\n\n**data.yaml**:\n```yaml\npath: /path/to/dataset\ntrain: images/train\nval: images/val\n\nnames:\n  0: dolphin\n  1: whale\n  2: shark\n```\n\n**Label format** (one line per object):\n```\n<class_id> <x_center> <y_center> <width> <height>\n```\n\nAll values normalized to [0, 1].\n\nExample (`img001.txt`):\n```\n0 0.5 0.5 0.3 0.4\n1 0.7 0.3 0.2 0.2\n```\n\n---\n\n### Train Model\n\n```python\nfrom ultralytics import YOLO\n\n# Load pre-trained model\nmodel = YOLO('yolov8n.pt')\n\n# Train\nresults = model.train(\n    data='data.yaml',\n    epochs=100,\n    imgsz=640,\n    batch=16,\n    name='dolphin_detector',\n    device=0\n)\n```\n\n**Training parameters**:\n- `epochs`: Number of training epochs\n- `imgsz`: Image size (640, 1280, etc.)\n- `batch`: Batch size (reduce if OOM errors)\n- `patience`: Early stopping patience (default: 50)\n- `lr0`: Initial learning rate (default: 0.01)\n- `lrf`: Final learning rate (default: 0.01)\n- `momentum`: SGD momentum (default: 0.937)\n- `weight_decay`: Optimizer weight decay (default: 0.0005)\n- `warmup_epochs`: Warmup epochs (default: 3.0)\n- `save`: Save checkpoints (default: True)\n- `device`: GPU device (0, 1, ..., 'cpu')\n\n---\n\n### Resume Training\n\n```python\n# Resume from last checkpoint\nmodel = YOLO('runs/detect/dolphin_detector/weights/last.pt')\nmodel.train(resume=True)\n```\n\n---\n\n### Augmentation\n\nYOLO automatically applies augmentations. You can customize:\n\n```python\nmodel.train(\n    data='data.yaml',\n    epochs=100,\n    # Augmentation parameters\n    hsv_h=0.015,       # HSV-Hue augmentation\n    hsv_s=0.7,         # HSV-Saturation augmentation\n    hsv_v=0.4,         # HSV-Value augmentation\n    degrees=0.0,       # Rotation (+/- deg)\n    translate=0.1,     # Translation (+/- fraction)\n    scale=0.5,         # Scale (+/- gain)\n    shear=0.0,         # Shear (+/- deg)\n    perspective=0.0,   # Perspective (+/- fraction)\n    flipud=0.0,        # Flip up-down (probability)\n    fliplr=0.5,        # Flip left-right (probability)\n    mosaic=1.0,        # Mosaic augmentation (probability)\n    mixup=0.0          # MixUp augmentation (probability)\n)\n```\n\n---\n\n## Validation\n\n```python\n# Validate trained model\nmodel = YOLO('runs/detect/dolphin_detector/weights/best.pt')\n\nmetrics = model.val(data='data.yaml')\n\n# Access metrics\nprint(f\"mAP50: {metrics.box.map50:.4f}\")\nprint(f\"mAP50-95: {metrics.box.map:.4f}\")\nprint(f\"Precision: {metrics.box.mp:.4f}\")\nprint(f\"Recall: {metrics.box.mr:.4f}\")\n\n# Per-class metrics\nfor i, ap in enumerate(metrics.box.ap50):\n    print(f\"Class {i} AP50: {ap:.4f}\")\n```\n\n---\n\n## Export\n\n### ONNX (Recommended for Production)\n\n```python\nmodel = YOLO('best.pt')\n\n# Export to ONNX\nmodel.export(format='onnx', imgsz=640)\n\n# Use exported model\nonnx_model = YOLO('best.onnx')\nresults = onnx_model('image.jpg')\n```\n\n**Benefits**:\n- Faster inference (~2x)\n- Smaller file size\n- Cross-platform compatibility\n\n### Other Formats\n\n```python\n# TorchScript\nmodel.export(format='torchscript')\n\n# CoreML (for iOS)\nmodel.export(format='coreml')\n\n# TensorFlow Lite (for mobile)\nmodel.export(format='tflite')\n\n# TensorFlow.js (for web)\nmodel.export(format='tfjs')\n```\n\n---\n\n## CLI Usage\n\n### Inference\n\n```bash\n# Single image\nyolo detect predict model=yolov8n.pt source=image.jpg\n\n# Video\nyolo detect predict model=yolov8n.pt source=video.mp4\n\n# Webcam\nyolo detect predict model=yolov8n.pt source=0\n\n# Directory\nyolo detect predict model=yolov8n.pt source=./images/\n\n# With options\nyolo detect predict model=yolov8n.pt source=image.jpg conf=0.5 iou=0.7 save=true\n```\n\n### Training\n\n```bash\nyolo detect train data=data.yaml model=yolov8n.pt epochs=100 imgsz=640\n```\n\n### Validation\n\n```bash\nyolo detect val model=best.pt data=data.yaml\n```\n\n---\n\n## Optimization Tips\n\n### 1. Mixed Precision Training (FP16)\n\n```python\nmodel.train(\n    data='data.yaml',\n    epochs=100,\n    amp=True  # Automatic Mixed Precision (faster, less memory)\n)\n```\n\n**Benefits**:\n- 2x faster training\n- 50% less GPU memory\n- Minimal accuracy loss\n\n---\n\n### 2. Optimal Batch Size\n\n```python\n# Find max batch size for your GPU\nfor batch_size in [4, 8, 16, 32, 64]:\n    try:\n        model.train(data='data.yaml', epochs=1, batch=batch_size)\n        print(f\"Batch {batch_size}: OK\")\n    except RuntimeError as e:\n        print(f\"Batch {batch_size}: OOM\")\n        break\n```\n\n---\n\n### 3. Freeze Layers\n\n```python\n# Freeze backbone layers (faster convergence for similar data)\nmodel.train(\n    data='data.yaml',\n    epochs=100,\n    freeze=10  # Freeze first 10 layers\n)\n```\n\n---\n\n### 4. Multi-GPU Training\n\n```python\n# Use all GPUs\nmodel.train(\n    data='data.yaml',\n    epochs=100,\n    device=[0, 1, 2, 3]  # Use GPUs 0-3\n)\n```\n\n---\n\n## Common Issues\n\n### Out of Memory (OOM)\n\n**Solution 1**: Reduce batch size\n```python\nmodel.train(batch=8)  # Instead of 16\n```\n\n**Solution 2**: Reduce image size\n```python\nmodel.train(imgsz=416)  # Instead of 640\n```\n\n**Solution 3**: Use gradient accumulation\n```python\n# Simulate larger batch size\nmodel.train(batch=4, accumulate=4)  # Effective batch size = 16\n```\n\n---\n\n### Slow Training\n\n**Solution 1**: Use smaller model\n```python\nmodel = YOLO('yolov8n.pt')  # Instead of yolov8x.pt\n```\n\n**Solution 2**: Enable AMP\n```python\nmodel.train(amp=True)\n```\n\n**Solution 3**: Use multiple workers\n```python\nmodel.train(workers=8)  # More data loading workers\n```\n\n---\n\n### Poor Accuracy\n\n**Solution 1**: Train longer\n```python\nmodel.train(epochs=300, patience=100)\n```\n\n**Solution 2**: Increase image size\n```python\nmodel.train(imgsz=1280)  # Higher resolution\n```\n\n**Solution 3**: More data augmentation\n```python\nmodel.train(mosaic=1.0, mixup=0.15, copy_paste=0.3)\n```\n\n**Solution 4**: Use larger model\n```python\nmodel = YOLO('yolov8l.pt')  # Instead of yolov8n.pt\n```\n\n---\n\n## Benchmarking\n\n```python\nfrom ultralytics.utils.benchmarks import benchmark\n\n# Benchmark model\nbenchmark(model='yolov8n.pt', imgsz=640, half=False, device=0)\n```\n\nOutput:\n```\nModel          Format    Size (MB)  mAP50-95  Inference (ms)\nyolov8n        PyTorch   6.2        37.3      80.2\nyolov8n        ONNX      12.4       37.3      42.1  (1.9x faster)\nyolov8n        TensorRT  13.1       37.3      28.5  (2.8x faster)\n```\n\n---\n\n## Resources\n\n- [YOLOv8 Documentation](https://docs.ultralytics.com/)\n- [Ultralytics GitHub](https://github.com/ultralytics/ultralytics)\n- [Model Zoo](https://github.com/ultralytics/assets/releases)\n- [Training Tips](https://docs.ultralytics.com/guides/model-training-tips/)\n"
        }
      ]
    },
    {
      "name": "scripts",
      "type": "folder",
      "path": "computer-vision-pipeline/scripts",
      "children": [
        {
          "name": "model_trainer.py",
          "type": "file",
          "path": "computer-vision-pipeline/scripts/model_trainer.py",
          "size": 12740,
          "content": "#!/usr/bin/env python3\n\"\"\"\nYOLO Model Trainer\n\nFine-tune YOLOv8 on custom datasets for specialized object detection.\nSupports data preparation, training, validation, and export.\n\nUsage:\n    python model_trainer.py prepare <images_dir/> <annotations_dir/> <output_dir/>\n    python model_trainer.py train <data.yaml> [--model yolov8n.pt] [--epochs 100]\n    python model_trainer.py validate <model.pt> <data.yaml>\n    python model_trainer.py export <model.pt> [--format onnx]\n\nExamples:\n    python model_trainer.py prepare ./images/ ./annotations/ ./dataset/\n    python model_trainer.py train dataset.yaml --model yolov8n.pt --epochs 100 --imgsz 640\n    python model_trainer.py validate runs/train/exp/weights/best.pt dataset.yaml\n    python model_trainer.py export runs/train/exp/weights/best.pt --format onnx\n\"\"\"\n\nimport os\nimport sys\nimport argparse\nimport yaml\nimport shutil\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\nimport json\n\nfrom ultralytics import YOLO\nimport cv2\n\n\nclass ModelTrainer:\n    \"\"\"Fine-tune YOLO models on custom datasets\"\"\"\n\n    def __init__(self):\n        pass\n\n    def prepare_dataset(\n        self,\n        images_dir: str,\n        annotations_dir: str,\n        output_dir: str,\n        train_split: float = 0.8,\n        class_names: List[str] = None\n    ) -> str:\n        \"\"\"\n        Prepare dataset in YOLO format\n\n        Args:\n            images_dir: Directory containing images\n            annotations_dir: Directory containing YOLO format labels (.txt)\n            output_dir: Output directory for prepared dataset\n            train_split: Fraction of data for training (rest for validation)\n            class_names: List of class names (if None, infer from annotations)\n\n        Returns:\n            Path to generated data.yaml file\n        \"\"\"\n        print(f\"\\nüì¶ Preparing dataset...\")\n        print(f\"   Images: {images_dir}\")\n        print(f\"   Annotations: {annotations_dir}\")\n        print(f\"   Output: {output_dir}\\n\")\n\n        # Create directory structure\n        train_images = os.path.join(output_dir, 'images', 'train')\n        val_images = os.path.join(output_dir, 'images', 'val')\n        train_labels = os.path.join(output_dir, 'labels', 'train')\n        val_labels = os.path.join(output_dir, 'labels', 'val')\n\n        for dir_path in [train_images, val_images, train_labels, val_labels]:\n            os.makedirs(dir_path, exist_ok=True)\n\n        # Get all image files\n        image_files = []\n        for ext in ['*.jpg', '*.jpeg', '*.png']:\n            image_files.extend(Path(images_dir).glob(ext))\n\n        print(f\"Found {len(image_files)} images\")\n\n        # Infer class names if not provided\n        if class_names is None:\n            class_names = self._infer_class_names(annotations_dir)\n            print(f\"Inferred {len(class_names)} classes: {class_names}\")\n\n        # Split into train/val\n        import random\n        random.shuffle(image_files)\n        split_idx = int(len(image_files) * train_split)\n        train_files = image_files[:split_idx]\n        val_files = image_files[split_idx:]\n\n        print(f\"\\nSplit:\")\n        print(f\"  Train: {len(train_files)} images\")\n        print(f\"  Val: {len(val_files)} images\\n\")\n\n        # Copy files\n        for image_path in train_files:\n            # Copy image\n            shutil.copy(image_path, train_images)\n\n            # Copy label\n            label_path = Path(annotations_dir) / f\"{image_path.stem}.txt\"\n            if label_path.exists():\n                shutil.copy(label_path, train_labels)\n\n        for image_path in val_files:\n            shutil.copy(image_path, val_images)\n            label_path = Path(annotations_dir) / f\"{image_path.stem}.txt\"\n            if label_path.exists():\n                shutil.copy(label_path, val_labels)\n\n        # Create data.yaml\n        data_yaml = {\n            'path': os.path.abspath(output_dir),\n            'train': 'images/train',\n            'val': 'images/val',\n            'names': {i: name for i, name in enumerate(class_names)}\n        }\n\n        yaml_path = os.path.join(output_dir, 'data.yaml')\n        with open(yaml_path, 'w') as f:\n            yaml.dump(data_yaml, f)\n\n        print(f\"‚úÖ Dataset prepared: {output_dir}\")\n        print(f\"   Config: {yaml_path}\\n\")\n\n        return yaml_path\n\n    def train(\n        self,\n        data_yaml: str,\n        model: str = 'yolov8n.pt',\n        epochs: int = 100,\n        imgsz: int = 640,\n        batch: int = 16,\n        patience: int = 50,\n        device: str = '0'\n    ) -> Dict:\n        \"\"\"\n        Train YOLO model\n\n        Args:\n            data_yaml: Path to data.yaml config\n            model: Base model to fine-tune\n            epochs: Number of training epochs\n            imgsz: Image size for training\n            batch: Batch size\n            patience: Early stopping patience\n            device: GPU device (0, 1, ...) or 'cpu'\n\n        Returns:\n            Training results dictionary\n        \"\"\"\n        print(f\"\\nüèãÔ∏è Training model...\")\n        print(f\"   Base model: {model}\")\n        print(f\"   Data: {data_yaml}\")\n        print(f\"   Epochs: {epochs}\")\n        print(f\"   Image size: {imgsz}\")\n        print(f\"   Batch size: {batch}\\n\")\n\n        # Load model\n        yolo_model = YOLO(model)\n\n        # Train\n        results = yolo_model.train(\n            data=data_yaml,\n            epochs=epochs,\n            imgsz=imgsz,\n            batch=batch,\n            patience=patience,\n            device=device,\n            project='runs/train',\n            name='exp',\n            exist_ok=True,\n            verbose=True\n        )\n\n        print(f\"\\n‚úÖ Training complete\")\n        print(f\"   Best weights: runs/train/exp/weights/best.pt\")\n        print(f\"   Last weights: runs/train/exp/weights/last.pt\\n\")\n\n        return results\n\n    def validate(\n        self,\n        model_path: str,\n        data_yaml: str,\n        imgsz: int = 640,\n        batch: int = 16\n    ) -> Dict:\n        \"\"\"\n        Validate trained model\n\n        Args:\n            model_path: Path to trained model weights\n            data_yaml: Path to data.yaml config\n            imgsz: Image size for validation\n            batch: Batch size\n\n        Returns:\n            Validation metrics\n        \"\"\"\n        print(f\"\\nüîç Validating model...\")\n        print(f\"   Model: {model_path}\")\n        print(f\"   Data: {data_yaml}\\n\")\n\n        model = YOLO(model_path)\n\n        # Validate\n        metrics = model.val(\n            data=data_yaml,\n            imgsz=imgsz,\n            batch=batch,\n            verbose=True\n        )\n\n        print(f\"\\n‚úÖ Validation complete\")\n        print(f\"\\nMetrics:\")\n        print(f\"   mAP50: {metrics.box.map50:.4f}\")\n        print(f\"   mAP50-95: {metrics.box.map:.4f}\")\n        print(f\"   Precision: {metrics.box.mp:.4f}\")\n        print(f\"   Recall: {metrics.box.mr:.4f}\\n\")\n\n        # Per-class metrics\n        print(\"Per-class AP50:\")\n        for i, ap in enumerate(metrics.box.ap50):\n            print(f\"   Class {i}: {ap:.4f}\")\n\n        return metrics\n\n    def export(\n        self,\n        model_path: str,\n        format: str = 'onnx',\n        imgsz: int = 640\n    ) -> str:\n        \"\"\"\n        Export model to different format\n\n        Args:\n            model_path: Path to trained model weights\n            format: Export format (onnx, torchscript, coreml, tflite, etc.)\n            imgsz: Image size for exported model\n\n        Returns:\n            Path to exported model\n        \"\"\"\n        print(f\"\\nüì¶ Exporting model...\")\n        print(f\"   Model: {model_path}\")\n        print(f\"   Format: {format}\")\n        print(f\"   Image size: {imgsz}\\n\")\n\n        model = YOLO(model_path)\n\n        # Export\n        export_path = model.export(\n            format=format,\n            imgsz=imgsz\n        )\n\n        print(f\"\\n‚úÖ Export complete\")\n        print(f\"   Exported to: {export_path}\\n\")\n\n        return export_path\n\n    def _infer_class_names(self, annotations_dir: str) -> List[str]:\n        \"\"\"Infer class names from annotation files\"\"\"\n        class_ids = set()\n\n        for label_file in Path(annotations_dir).glob('*.txt'):\n            with open(label_file, 'r') as f:\n                for line in f:\n                    parts = line.strip().split()\n                    if parts:\n                        class_ids.add(int(parts[0]))\n\n        # Generate default names\n        return [f'class_{i}' for i in sorted(class_ids)]\n\n    def create_annotation_template(\n        self,\n        image_path: str,\n        output_path: str,\n        class_id: int = 0\n    ):\n        \"\"\"\n        Create annotation template for an image\n\n        YOLO format: <class_id> <x_center> <y_center> <width> <height>\n        All values normalized to [0, 1]\n\n        Args:\n            image_path: Path to image\n            output_path: Path to save annotation .txt\n            class_id: Default class ID\n        \"\"\"\n        # Read image to get dimensions\n        img = cv2.imread(image_path)\n        h, w = img.shape[:2]\n\n        # Example annotation (centered box, 50% of image)\n        x_center = 0.5\n        y_center = 0.5\n        width = 0.5\n        height = 0.5\n\n        annotation = f\"{class_id} {x_center} {y_center} {width} {height}\\n\"\n\n        with open(output_path, 'w') as f:\n            f.write(annotation)\n\n        print(f\"Created annotation template: {output_path}\")\n        print(f\"  Format: class_id x_center y_center width height\")\n        print(f\"  Example: {annotation.strip()}\")\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Train custom YOLO models')\n    subparsers = parser.add_subparsers(dest='command', help='Command to run')\n\n    # Prepare command\n    prepare_parser = subparsers.add_parser('prepare', help='Prepare dataset in YOLO format')\n    prepare_parser.add_argument('images_dir', help='Directory containing images')\n    prepare_parser.add_argument('annotations_dir', help='Directory containing YOLO annotations')\n    prepare_parser.add_argument('output_dir', help='Output directory for prepared dataset')\n    prepare_parser.add_argument('--train-split', type=float, default=0.8, help='Train split ratio')\n    prepare_parser.add_argument('--classes', nargs='+', help='Class names')\n\n    # Train command\n    train_parser = subparsers.add_parser('train', help='Train YOLO model')\n    train_parser.add_argument('data_yaml', help='Path to data.yaml config')\n    train_parser.add_argument('--model', default='yolov8n.pt', help='Base model to fine-tune')\n    train_parser.add_argument('--epochs', type=int, default=100, help='Number of epochs')\n    train_parser.add_argument('--imgsz', type=int, default=640, help='Image size')\n    train_parser.add_argument('--batch', type=int, default=16, help='Batch size')\n    train_parser.add_argument('--patience', type=int, default=50, help='Early stopping patience')\n    train_parser.add_argument('--device', default='0', help='GPU device (0, 1, ...) or cpu')\n\n    # Validate command\n    validate_parser = subparsers.add_parser('validate', help='Validate trained model')\n    validate_parser.add_argument('model', help='Path to trained model weights')\n    validate_parser.add_argument('data_yaml', help='Path to data.yaml config')\n    validate_parser.add_argument('--imgsz', type=int, default=640, help='Image size')\n    validate_parser.add_argument('--batch', type=int, default=16, help='Batch size')\n\n    # Export command\n    export_parser = subparsers.add_parser('export', help='Export trained model')\n    export_parser.add_argument('model', help='Path to trained model weights')\n    export_parser.add_argument('--format', default='onnx', help='Export format')\n    export_parser.add_argument('--imgsz', type=int, default=640, help='Image size')\n\n    args = parser.parse_args()\n\n    if args.command is None:\n        parser.print_help()\n        sys.exit(1)\n\n    trainer = ModelTrainer()\n\n    if args.command == 'prepare':\n        trainer.prepare_dataset(\n            args.images_dir,\n            args.annotations_dir,\n            args.output_dir,\n            train_split=args.train_split,\n            class_names=args.classes\n        )\n\n    elif args.command == 'train':\n        trainer.train(\n            args.data_yaml,\n            model=args.model,\n            epochs=args.epochs,\n            imgsz=args.imgsz,\n            batch=args.batch,\n            patience=args.patience,\n            device=args.device\n        )\n\n    elif args.command == 'validate':\n        trainer.validate(\n            args.model,\n            args.data_yaml,\n            imgsz=args.imgsz,\n            batch=args.batch\n        )\n\n    elif args.command == 'export':\n        trainer.export(\n            args.model,\n            format=args.format,\n            imgsz=args.imgsz\n        )\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "video_analyzer.py",
          "type": "file",
          "path": "computer-vision-pipeline/scripts/video_analyzer.py",
          "size": 15017,
          "content": "#!/usr/bin/env python3\n\"\"\"\nVideo Analyzer\n\nExtract frames from video, run object detection, generate timeline of detections.\nSupports YOLOv8, batch processing, and tracking.\n\nUsage:\n    python video_analyzer.py detect <video.mp4> <output_dir/> [--model yolov8n.pt] [--conf 0.4]\n    python video_analyzer.py track <video.mp4> <output_dir/> [--model yolov8n.pt]\n    python video_analyzer.py extract <video.mp4> <output_dir/> [--sample-rate 30]\n\nExamples:\n    python video_analyzer.py detect drone_footage.mp4 ./detections/ --model yolov8n.pt --conf 0.4\n    python video_analyzer.py track dolphins.mp4 ./tracks/ --model yolov8n.pt\n    python video_analyzer.py extract survey.mp4 ./frames/ --sample-rate 60\n\"\"\"\n\nimport os\nimport sys\nimport argparse\nimport json\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Tuple\nimport time\n\nimport cv2\nimport numpy as np\nfrom ultralytics import YOLO\n\n\nclass VideoAnalyzer:\n    \"\"\"Analyze video with object detection and tracking\"\"\"\n\n    def __init__(self, model_path: str = 'yolov8n.pt'):\n        \"\"\"Initialize with YOLO model\"\"\"\n        print(f\"\\nüîç Loading model: {model_path}\")\n        self.model = YOLO(model_path)\n        print(f\"‚úÖ Model loaded\\n\")\n\n    def extract_frames(\n        self,\n        video_path: str,\n        output_dir: str,\n        sample_rate: int = 30,\n        scene_detection: bool = False\n    ) -> List[str]:\n        \"\"\"\n        Extract frames from video\n\n        Args:\n            video_path: Path to input video\n            output_dir: Directory to save frames\n            sample_rate: Extract every Nth frame (default: 30 = 1 FPS for 30 FPS video)\n            scene_detection: Use adaptive sampling based on scene changes\n\n        Returns:\n            List of extracted frame paths\n        \"\"\"\n        print(f\"üìπ Extracting frames from: {video_path}\")\n        print(f\"   Sample rate: every {sample_rate} frames\")\n        print(f\"   Scene detection: {scene_detection}\\n\")\n\n        os.makedirs(output_dir, exist_ok=True)\n\n        video = cv2.VideoCapture(video_path)\n        fps = video.get(cv2.CAP_PROP_FPS)\n        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        print(f\"Video info:\")\n        print(f\"  FPS: {fps}\")\n        print(f\"  Total frames: {total_frames}\")\n        print(f\"  Duration: {total_frames / fps:.2f}s\\n\")\n\n        frame_count = 0\n        extracted = []\n        prev_frame = None\n\n        while True:\n            ret, frame = video.read()\n            if not ret:\n                break\n\n            frame_count += 1\n\n            # Determine if we should save this frame\n            should_save = False\n\n            if scene_detection:\n                should_save = self._scene_changed(prev_frame, frame)\n                prev_frame = frame.copy()\n            else:\n                should_save = (frame_count % sample_rate == 0)\n\n            if should_save:\n                frame_path = os.path.join(output_dir, f'frame_{frame_count:06d}.jpg')\n                cv2.imwrite(frame_path, frame)\n                extracted.append(frame_path)\n\n                if len(extracted) % 100 == 0:\n                    print(f\"  ‚úì Extracted {len(extracted)} frames...\")\n\n        video.release()\n\n        print(f\"\\n‚úÖ Extracted {len(extracted)} frames\")\n        print(f\"   Reduction: {(1 - len(extracted)/total_frames)*100:.1f}%\\n\")\n\n        return extracted\n\n    def detect_batch(\n        self,\n        image_paths: List[str],\n        output_path: str,\n        conf_threshold: float = 0.4,\n        iou_threshold: float = 0.5,\n        batch_size: int = 16\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Run object detection on batch of images\n\n        Args:\n            image_paths: List of image file paths\n            output_path: Path to save results JSON\n            conf_threshold: Confidence threshold\n            iou_threshold: IoU threshold for NMS\n            batch_size: Number of images to process at once\n\n        Returns:\n            Dictionary with detection results\n        \"\"\"\n        print(f\"üîç Running detection on {len(image_paths)} images\")\n        print(f\"   Confidence threshold: {conf_threshold}\")\n        print(f\"   IoU threshold: {iou_threshold}\")\n        print(f\"   Batch size: {batch_size}\\n\")\n\n        all_results = []\n        start_time = time.time()\n\n        for i in range(0, len(image_paths), batch_size):\n            batch_paths = image_paths[i:i+batch_size]\n\n            # Load batch\n            frames = [cv2.imread(path) for path in batch_paths]\n\n            # Batch inference\n            results = self.model(\n                frames,\n                conf=conf_threshold,\n                iou=iou_threshold,\n                verbose=False\n            )\n\n            # Extract results\n            for path, result in zip(batch_paths, results):\n                detections = []\n\n                for box in result.boxes:\n                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n                    conf = float(box.conf[0])\n                    cls = int(box.cls[0])\n                    label = result.names[cls]\n\n                    detections.append({\n                        'bbox': [float(x1), float(y1), float(x2), float(y2)],\n                        'confidence': conf,\n                        'class': label,\n                        'class_id': cls\n                    })\n\n                all_results.append({\n                    'image': path,\n                    'detections': detections,\n                    'count': len(detections)\n                })\n\n            if (i + batch_size) % 100 < batch_size:\n                print(f\"  ‚úì Processed {min(i + batch_size, len(image_paths))} images...\")\n\n        elapsed = time.time() - start_time\n        fps = len(image_paths) / elapsed\n\n        # Save results\n        results_data = {\n            'metadata': {\n                'total_images': len(image_paths),\n                'total_detections': sum(r['count'] for r in all_results),\n                'conf_threshold': conf_threshold,\n                'iou_threshold': iou_threshold,\n                'processing_time': elapsed,\n                'fps': fps\n            },\n            'results': all_results\n        }\n\n        with open(output_path, 'w') as f:\n            json.dump(results_data, f, indent=2)\n\n        print(f\"\\n‚úÖ Detection complete\")\n        print(f\"   Total detections: {results_data['metadata']['total_detections']}\")\n        print(f\"   Processing time: {elapsed:.2f}s\")\n        print(f\"   Throughput: {fps:.1f} images/s\")\n        print(f\"   Results saved: {output_path}\\n\")\n\n        return results_data\n\n    def track_video(\n        self,\n        video_path: str,\n        output_path: str,\n        conf_threshold: float = 0.4,\n        tracker: str = 'bytetrack.yaml'\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Run object tracking on video\n\n        Args:\n            video_path: Path to input video\n            output_path: Path to save tracking results JSON\n            conf_threshold: Confidence threshold\n            tracker: Tracking algorithm ('bytetrack.yaml', 'botsort.yaml')\n\n        Returns:\n            Dictionary with tracking results\n        \"\"\"\n        print(f\"üéØ Tracking objects in: {video_path}\")\n        print(f\"   Tracker: {tracker}\")\n        print(f\"   Confidence: {conf_threshold}\\n\")\n\n        video = cv2.VideoCapture(video_path)\n        fps = video.get(cv2.CAP_PROP_FPS)\n        total_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n\n        tracks = {}  # track_id -> list of detections\n        frame_count = 0\n        start_time = time.time()\n\n        while True:\n            ret, frame = video.read()\n            if not ret:\n                break\n\n            frame_count += 1\n\n            # Run tracking\n            results = self.model.track(\n                frame,\n                conf=conf_threshold,\n                persist=True,\n                tracker=tracker,\n                verbose=False\n            )\n\n            # Extract tracked objects\n            if results[0].boxes is not None and results[0].boxes.id is not None:\n                for box in results[0].boxes:\n                    track_id = int(box.id[0])\n                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n                    conf = float(box.conf[0])\n                    cls = int(box.cls[0])\n                    label = results[0].names[cls]\n\n                    if track_id not in tracks:\n                        tracks[track_id] = {\n                            'track_id': track_id,\n                            'class': label,\n                            'first_frame': frame_count,\n                            'last_frame': frame_count,\n                            'detections': []\n                        }\n\n                    tracks[track_id]['detections'].append({\n                        'frame': frame_count,\n                        'timestamp': frame_count / fps,\n                        'bbox': [float(x1), float(y1), float(x2), float(y2)],\n                        'confidence': conf\n                    })\n\n                    tracks[track_id]['last_frame'] = frame_count\n\n            if frame_count % 100 == 0:\n                print(f\"  ‚úì Processed {frame_count}/{total_frames} frames...\")\n\n        video.release()\n        elapsed = time.time() - start_time\n\n        # Calculate statistics\n        track_list = list(tracks.values())\n        for track in track_list:\n            track['duration_frames'] = track['last_frame'] - track['first_frame'] + 1\n            track['duration_seconds'] = track['duration_frames'] / fps\n            track['avg_confidence'] = np.mean([d['confidence'] for d in track['detections']])\n\n        # Save results\n        results_data = {\n            'metadata': {\n                'video': video_path,\n                'total_frames': total_frames,\n                'fps': fps,\n                'duration': total_frames / fps,\n                'total_tracks': len(tracks),\n                'processing_time': elapsed,\n                'processing_fps': total_frames / elapsed\n            },\n            'tracks': track_list\n        }\n\n        with open(output_path, 'w') as f:\n            json.dump(results_data, f, indent=2)\n\n        print(f\"\\n‚úÖ Tracking complete\")\n        print(f\"   Unique objects: {len(tracks)}\")\n        print(f\"   Total detections: {sum(len(t['detections']) for t in track_list)}\")\n        print(f\"   Processing time: {elapsed:.2f}s\")\n        print(f\"   Results saved: {output_path}\\n\")\n\n        # Print track summary\n        print(\"Track summary:\")\n        for track in sorted(track_list, key=lambda t: t['duration_frames'], reverse=True)[:10]:\n            print(f\"  Track {track['track_id']:3d} ({track['class']:15s}): \"\n                  f\"{track['duration_frames']:4d} frames, \"\n                  f\"{track['duration_seconds']:6.2f}s, \"\n                  f\"conf={track['avg_confidence']:.2f}\")\n\n        return results_data\n\n    def _scene_changed(\n        self,\n        prev_frame: np.ndarray,\n        curr_frame: np.ndarray,\n        threshold: float = 0.3\n    ) -> bool:\n        \"\"\"Detect scene change using histogram comparison\"\"\"\n        if prev_frame is None:\n            return True\n\n        # Convert to grayscale\n        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n        curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n\n        # Calculate histograms\n        prev_hist = cv2.calcHist([prev_gray], [0], None, [256], [0, 256])\n        curr_hist = cv2.calcHist([curr_gray], [0], None, [256], [0, 256])\n\n        # Normalize\n        cv2.normalize(prev_hist, prev_hist)\n        cv2.normalize(curr_hist, curr_hist)\n\n        # Compare\n        correlation = cv2.compareHist(prev_hist, curr_hist, cv2.HISTCMP_CORREL)\n\n        return correlation < (1 - threshold)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Video analysis with object detection and tracking')\n    subparsers = parser.add_subparsers(dest='command', help='Command to run')\n\n    # Extract command\n    extract_parser = subparsers.add_parser('extract', help='Extract frames from video')\n    extract_parser.add_argument('video', help='Input video path')\n    extract_parser.add_argument('output_dir', help='Output directory for frames')\n    extract_parser.add_argument('--sample-rate', type=int, default=30, help='Extract every Nth frame')\n    extract_parser.add_argument('--scene-detection', action='store_true', help='Use scene change detection')\n\n    # Detect command\n    detect_parser = subparsers.add_parser('detect', help='Run object detection on video')\n    detect_parser.add_argument('video', help='Input video path')\n    detect_parser.add_argument('output_dir', help='Output directory')\n    detect_parser.add_argument('--model', default='yolov8n.pt', help='YOLO model path')\n    detect_parser.add_argument('--conf', type=float, default=0.4, help='Confidence threshold')\n    detect_parser.add_argument('--iou', type=float, default=0.5, help='IoU threshold')\n    detect_parser.add_argument('--batch-size', type=int, default=16, help='Batch size for inference')\n    detect_parser.add_argument('--sample-rate', type=int, default=30, help='Extract every Nth frame')\n\n    # Track command\n    track_parser = subparsers.add_parser('track', help='Track objects in video')\n    track_parser.add_argument('video', help='Input video path')\n    track_parser.add_argument('output_dir', help='Output directory')\n    track_parser.add_argument('--model', default='yolov8n.pt', help='YOLO model path')\n    track_parser.add_argument('--conf', type=float, default=0.4, help='Confidence threshold')\n    track_parser.add_argument('--tracker', default='bytetrack.yaml', help='Tracker config')\n\n    args = parser.parse_args()\n\n    if args.command is None:\n        parser.print_help()\n        sys.exit(1)\n\n    if args.command == 'extract':\n        analyzer = VideoAnalyzer()\n        analyzer.extract_frames(\n            args.video,\n            args.output_dir,\n            sample_rate=args.sample_rate,\n            scene_detection=args.scene_detection\n        )\n\n    elif args.command == 'detect':\n        analyzer = VideoAnalyzer(args.model)\n\n        # Extract frames\n        frames_dir = os.path.join(args.output_dir, 'frames')\n        frame_paths = analyzer.extract_frames(\n            args.video,\n            frames_dir,\n            sample_rate=args.sample_rate\n        )\n\n        # Run detection\n        results_path = os.path.join(args.output_dir, 'detections.json')\n        analyzer.detect_batch(\n            frame_paths,\n            results_path,\n            conf_threshold=args.conf,\n            iou_threshold=args.iou,\n            batch_size=args.batch_size\n        )\n\n    elif args.command == 'track':\n        analyzer = VideoAnalyzer(args.model)\n\n        # Run tracking\n        os.makedirs(args.output_dir, exist_ok=True)\n        results_path = os.path.join(args.output_dir, 'tracks.json')\n        analyzer.track_video(\n            args.video,\n            results_path,\n            conf_threshold=args.conf,\n            tracker=args.tracker\n        )\n\n\nif __name__ == '__main__':\n    main()\n"
        }
      ]
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "computer-vision-pipeline/SKILL.md",
      "size": 15506,
      "content": "---\nname: computer-vision-pipeline\ndescription: Build production computer vision pipelines for object detection, tracking, and video analysis. Handles drone footage, wildlife monitoring, and real-time detection. Supports YOLO, Detectron2, TensorFlow, PyTorch. Use for archaeological surveys, conservation, security. Activate on \"object detection\", \"video analysis\", \"YOLO\", \"tracking\", \"drone footage\". NOT for simple image filters, photo editing, or face recognition APIs.\nallowed-tools: Read,Write,Edit,Bash(python*,pip*,ffmpeg*)\n---\n\n# Computer Vision Pipeline\n\nExpert in building production-ready computer vision systems for object detection, tracking, and video analysis.\n\n## When to Use\n\n‚úÖ **Use for**:\n- Drone footage analysis (archaeological surveys, conservation)\n- Wildlife monitoring and tracking\n- Real-time object detection systems\n- Video preprocessing and analysis\n- Custom model training and inference\n- Multi-object tracking (MOT)\n\n‚ùå **NOT for**:\n- Simple image filters (use Pillow/PIL)\n- Photo editing (use Photoshop/GIMP)\n- Face recognition APIs (use AWS Rekognition)\n- Basic OCR (use Tesseract)\n\n---\n\n## Technology Selection\n\n### Object Detection Models\n\n| Model | Speed (FPS) | Accuracy (mAP) | Use Case |\n|-------|-------------|----------------|----------|\n| YOLOv8 | 140 | 53.9% | Real-time detection |\n| Detectron2 | 25 | 58.7% | High accuracy, research |\n| EfficientDet | 35 | 55.1% | Mobile deployment |\n| Faster R-CNN | 10 | 42.0% | Legacy systems |\n\n**Timeline**:\n- 2015: Faster R-CNN (two-stage detection)\n- 2016: YOLO v1 (one-stage, real-time)\n- 2020: YOLOv5 (PyTorch, production-ready)\n- 2023: YOLOv8 (state-of-the-art)\n- 2024: YOLOv8 is industry standard for real-time\n\n**Decision tree**:\n```\nNeed real-time (&gt;30 FPS)? ‚Üí YOLOv8\nNeed highest accuracy? ‚Üí Detectron2 Mask R-CNN\nNeed mobile deployment? ‚Üí YOLOv8-nano or EfficientDet\nNeed instance segmentation? ‚Üí Detectron2 or YOLOv8-seg\nNeed custom objects? ‚Üí Fine-tune YOLOv8\n```\n\n---\n\n## Common Anti-Patterns\n\n### Anti-Pattern 1: Not Preprocessing Frames Before Detection\n\n**Novice thinking**: \"Just run detection on raw video frames\"\n\n**Problem**: Poor detection accuracy, wasted GPU cycles.\n\n**Wrong approach**:\n```python\n# ‚ùå No preprocessing - poor results\nimport cv2\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8n.pt')\nvideo = cv2.VideoCapture('drone_footage.mp4')\n\nwhile True:\n    ret, frame = video.read()\n    if not ret:\n        break\n\n    # Raw frame detection - no normalization, no resizing\n    results = model(frame)\n    # Poor accuracy, slow inference\n```\n\n**Why wrong**:\n- Video resolution too high (4K = 8.3 megapixels per frame)\n- No normalization (pixel values 0-255 instead of 0-1)\n- Aspect ratio not maintained\n- GPU memory overflow on high-res frames\n\n**Correct approach**:\n```python\n# ‚úÖ Proper preprocessing pipeline\nimport cv2\nimport numpy as np\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8n.pt')\nvideo = cv2.VideoCapture('drone_footage.mp4')\n\n# Model expects 640x640 input\nTARGET_SIZE = 640\n\ndef preprocess_frame(frame):\n    # Resize while maintaining aspect ratio\n    h, w = frame.shape[:2]\n    scale = TARGET_SIZE / max(h, w)\n    new_w, new_h = int(w * scale), int(h * scale)\n\n    resized = cv2.resize(frame, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n\n    # Pad to square\n    pad_w = (TARGET_SIZE - new_w) // 2\n    pad_h = (TARGET_SIZE - new_h) // 2\n\n    padded = cv2.copyMakeBorder(\n        resized,\n        pad_h, TARGET_SIZE - new_h - pad_h,\n        pad_w, TARGET_SIZE - new_w - pad_w,\n        cv2.BORDER_CONSTANT,\n        value=(114, 114, 114)  # Gray padding\n    )\n\n    # Normalize to 0-1 (if model expects it)\n    # normalized = padded.astype(np.float32) / 255.0\n\n    return padded, scale\n\nwhile True:\n    ret, frame = video.read()\n    if not ret:\n        break\n\n    preprocessed, scale = preprocess_frame(frame)\n    results = model(preprocessed)\n\n    # Scale bounding boxes back to original coordinates\n    for box in results[0].boxes:\n        x1, y1, x2, y2 = box.xyxy[0]\n        x1, y1, x2, y2 = x1/scale, y1/scale, x2/scale, y2/scale\n```\n\n**Performance comparison**:\n- Raw 4K frames: 5 FPS, 72% mAP\n- Preprocessed 640x640: 45 FPS, 89% mAP\n\n**Timeline context**:\n- 2015: Manual preprocessing required\n- 2020: YOLOv5 added auto-resize\n- 2023: YOLOv8 has smart preprocessing but explicit control is better\n\n---\n\n### Anti-Pattern 2: Processing Every Frame in Video\n\n**Novice thinking**: \"Run detection on every single frame\"\n\n**Problem**: 99% of frames are redundant, wasting compute.\n\n**Wrong approach**:\n```python\n# ‚ùå Process every frame (30 FPS video = 1800 frames/min)\nimport cv2\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8n.pt')\nvideo = cv2.VideoCapture('drone_footage.mp4')\n\ndetections = []\n\nwhile True:\n    ret, frame = video.read()\n    if not ret:\n        break\n\n    # Run detection on EVERY frame\n    results = model(frame)\n    detections.append(results)\n\n# 10-minute video = 18,000 inferences (15 minutes on GPU)\n```\n\n**Why wrong**:\n- Adjacent frames are nearly identical\n- Wasting 95% of compute on duplicate work\n- Slow processing time\n- Massive storage for results\n\n**Correct approach 1**: Frame sampling\n```python\n# ‚úÖ Sample every Nth frame\nimport cv2\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8n.pt')\nvideo = cv2.VideoCapture('drone_footage.mp4')\n\nSAMPLE_RATE = 30  # Process 1 frame per second (if 30 FPS video)\n\nframe_count = 0\ndetections = []\n\nwhile True:\n    ret, frame = video.read()\n    if not ret:\n        break\n\n    frame_count += 1\n\n    # Only process every 30th frame\n    if frame_count % SAMPLE_RATE == 0:\n        results = model(frame)\n        detections.append({\n            'frame': frame_count,\n            'timestamp': frame_count / 30.0,\n            'results': results\n        })\n\n# 10-minute video = 600 inferences (30 seconds on GPU)\n```\n\n**Correct approach 2**: Adaptive sampling with scene change detection\n```python\n# ‚úÖ Only process when scene changes significantly\nimport cv2\nimport numpy as np\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8n.pt')\nvideo = cv2.VideoCapture('drone_footage.mp4')\n\ndef scene_changed(prev_frame, curr_frame, threshold=0.3):\n    \"\"\"Detect scene change using histogram comparison\"\"\"\n    if prev_frame is None:\n        return True\n\n    # Convert to grayscale\n    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n    curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n\n    # Calculate histograms\n    prev_hist = cv2.calcHist([prev_gray], [0], None, [256], [0, 256])\n    curr_hist = cv2.calcHist([curr_gray], [0], None, [256], [0, 256])\n\n    # Compare histograms\n    correlation = cv2.compareHist(prev_hist, curr_hist, cv2.HISTCMP_CORREL)\n\n    return correlation < (1 - threshold)\n\nprev_frame = None\ndetections = []\n\nwhile True:\n    ret, frame = video.read()\n    if not ret:\n        break\n\n    # Only run detection if scene changed\n    if scene_changed(prev_frame, frame):\n        results = model(frame)\n        detections.append(results)\n\n    prev_frame = frame.copy()\n\n# Adapts to video content - static shots skip frames, action scenes process more\n```\n\n**Savings**:\n- Every frame: 18,000 inferences\n- Sample 1 FPS: 600 inferences (97% reduction)\n- Adaptive: ~1,200 inferences (93% reduction)\n\n---\n\n### Anti-Pattern 3: Not Using Batch Inference\n\n**Novice thinking**: \"Process one image at a time\"\n\n**Problem**: GPU sits idle 80% of the time waiting for data.\n\n**Wrong approach**:\n```python\n# ‚ùå Sequential processing - GPU underutilized\nimport cv2\nfrom ultralytics import YOLO\nimport time\n\nmodel = YOLO('yolov8n.pt')\n\n# 100 images to process\nimage_paths = [f'frame_{i:04d}.jpg' for i in range(100)]\n\nstart = time.time()\n\nfor path in image_paths:\n    frame = cv2.imread(path)\n    results = model(frame)  # Process one at a time\n    # GPU utilization: ~20%\n\nelapsed = time.time() - start\nprint(f\"Processed {len(image_paths)} images in {elapsed:.2f}s\")\n# Output: 45 seconds\n```\n\n**Why wrong**:\n- GPU has to wait for CPU to load each image\n- No parallelization\n- GPU utilization ~20%\n- Slow throughput\n\n**Correct approach**:\n```python\n# ‚úÖ Batch inference - GPU fully utilized\nimport cv2\nfrom ultralytics import YOLO\nimport time\n\nmodel = YOLO('yolov8n.pt')\n\nimage_paths = [f'frame_{i:04d}.jpg' for i in range(100)]\n\nBATCH_SIZE = 16  # Process 16 images at once\n\nstart = time.time()\n\nfor i in range(0, len(image_paths), BATCH_SIZE):\n    batch_paths = image_paths[i:i+BATCH_SIZE]\n\n    # Load batch\n    frames = [cv2.imread(path) for path in batch_paths]\n\n    # Batch inference (single GPU call)\n    results = model(frames)  # Pass list of images\n    # GPU utilization: ~85%\n\nelapsed = time.time() - start\nprint(f\"Processed {len(image_paths)} images in {elapsed:.2f}s\")\n# Output: 8 seconds (5.6x faster!)\n```\n\n**Performance comparison**:\n| Method | Time (100 images) | GPU Util | Throughput |\n|--------|-------------------|----------|------------|\n| Sequential | 45s | 20% | 2.2 img/s |\n| Batch (16) | 8s | 85% | 12.5 img/s |\n| Batch (32) | 6s | 92% | 16.7 img/s |\n\n**Batch size tuning**:\n```python\n# Find optimal batch size for your GPU\nimport torch\n\ndef find_optimal_batch_size(model, image_size=(640, 640)):\n    for batch_size in [1, 2, 4, 8, 16, 32, 64]:\n        try:\n            dummy_input = torch.randn(batch_size, 3, *image_size).cuda()\n\n            start = time.time()\n            with torch.no_grad():\n                _ = model(dummy_input)\n            elapsed = time.time() - start\n\n            throughput = batch_size / elapsed\n            print(f\"Batch {batch_size}: {throughput:.1f} img/s\")\n        except RuntimeError as e:\n            print(f\"Batch {batch_size}: OOM (out of memory)\")\n            break\n\n# Find optimal batch size before production\nfind_optimal_batch_size(model)\n```\n\n---\n\n### Anti-Pattern 4: Ignoring Non-Maximum Suppression (NMS) Tuning\n\n**Problem**: Duplicate detections, missed objects, slow post-processing.\n\n**Wrong approach**:\n```python\n# ‚ùå Use default NMS settings for everything\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8n.pt')\n\n# Default settings (iou_threshold=0.45, conf_threshold=0.25)\nresults = model('crowded_scene.jpg')\n\n# Result: 50 bounding boxes, 30 are duplicates!\n```\n\n**Why wrong**:\n- Default IoU=0.45 is too permissive for dense objects\n- Default conf=0.25 includes low-quality detections\n- No adaptation to use case\n\n**Correct approach**:\n```python\n# ‚úÖ Tune NMS for your use case\nfrom ultralytics import YOLO\n\nmodel = YOLO('yolov8n.pt')\n\n# Sparse objects (dolphins in ocean)\nsparse_results = model(\n    'ocean_footage.jpg',\n    iou=0.5,    # Higher IoU = allow closer boxes\n    conf=0.4    # Higher confidence = fewer false positives\n)\n\n# Dense objects (crowd, flock of birds)\ndense_results = model(\n    'crowded_scene.jpg',\n    iou=0.3,    # Lower IoU = suppress more duplicates\n    conf=0.5    # Higher confidence = filter noise\n)\n\n# High precision needed (legal evidence)\nprecise_results = model(\n    'evidence.jpg',\n    iou=0.5,\n    conf=0.7,   # Very high confidence\n    max_det=50  # Limit max detections\n)\n```\n\n**NMS parameter guide**:\n| Use Case | IoU | Conf | Max Det |\n|----------|-----|------|---------|\n| Sparse objects (wildlife) | 0.5 | 0.4 | 100 |\n| Dense objects (crowd) | 0.3 | 0.5 | 300 |\n| High precision (evidence) | 0.5 | 0.7 | 50 |\n| Real-time (speed priority) | 0.45 | 0.3 | 100 |\n\n---\n\n### Anti-Pattern 5: No Tracking Between Frames\n\n**Novice thinking**: \"Run detection on each frame independently\"\n\n**Problem**: Can't count unique objects, track movement, or build trajectories.\n\n**Wrong approach**:\n```python\n# ‚ùå Independent frame detection - no object identity\nfrom ultralytics import YOLO\nimport cv2\n\nmodel = YOLO('yolov8n.pt')\nvideo = cv2.VideoCapture('dolphins.mp4')\n\ndetections = []\n\nwhile True:\n    ret, frame = video.read()\n    if not ret:\n        break\n\n    results = model(frame)\n    detections.append(results)\n\n# Result: Can't tell if frame 10 dolphin is same as frame 20 dolphin\n# Can't count unique dolphins\n# Can't track trajectories\n```\n\n**Why wrong**:\n- No object identity across frames\n- Can't count unique objects\n- Can't analyze movement patterns\n- Can't build trajectories\n\n**Correct approach**: Use tracking (ByteTrack)\n```python\n# ‚úÖ Multi-object tracking with ByteTrack\nfrom ultralytics import YOLO\nimport cv2\n\n# YOLO with tracking\nmodel = YOLO('yolov8n.pt')\nvideo = cv2.VideoCapture('dolphins.mp4')\n\n# Track objects across frames\ntracks = {}\n\nwhile True:\n    ret, frame = video.read()\n    if not ret:\n        break\n\n    # Run detection + tracking\n    results = model.track(\n        frame,\n        persist=True,     # Maintain IDs across frames\n        tracker='bytetrack.yaml'  # ByteTrack algorithm\n    )\n\n    # Each detection now has persistent ID\n    for box in results[0].boxes:\n        track_id = int(box.id[0])  # Unique ID across frames\n        x1, y1, x2, y2 = box.xyxy[0]\n\n        # Store trajectory\n        if track_id not in tracks:\n            tracks[track_id] = []\n\n        tracks[track_id].append({\n            'frame': len(tracks[track_id]),\n            'bbox': (x1, y1, x2, y2),\n            'conf': box.conf[0]\n        })\n\n# Now we can analyze:\nprint(f\"Unique dolphins detected: {len(tracks)}\")\n\n# Trajectory analysis\nfor track_id, trajectory in tracks.items():\n    if len(trajectory) > 30:  # Only long tracks\n        print(f\"Dolphin {track_id} appeared in {len(trajectory)} frames\")\n        # Calculate movement, speed, etc.\n```\n\n**Tracking benefits**:\n- Count unique objects (not just detections per frame)\n- Build trajectories and movement patterns\n- Analyze behavior over time\n- Filter out brief false positives\n\n**Tracking algorithms**:\n| Algorithm | Speed | Robustness | Occlusion Handling |\n|-----------|-------|------------|---------------------|\n| ByteTrack | Fast | Good | Excellent |\n| SORT | Very Fast | Fair | Fair |\n| DeepSORT | Medium | Excellent | Good |\n| BotSORT | Medium | Excellent | Excellent |\n\n---\n\n## Production Checklist\n\n```\n‚ñ° Preprocess frames (resize, pad, normalize)\n‚ñ° Sample frames intelligently (1 FPS or scene change detection)\n‚ñ° Use batch inference (16-32 images per batch)\n‚ñ° Tune NMS thresholds for your use case\n‚ñ° Implement tracking if analyzing video\n‚ñ° Log inference time and GPU utilization\n‚ñ° Handle edge cases (empty frames, corrupted video)\n‚ñ° Save results in structured format (JSON, CSV)\n‚ñ° Visualize detections for debugging\n‚ñ° Benchmark on representative data\n```\n\n---\n\n## When to Use vs Avoid\n\n| Scenario | Appropriate? |\n|----------|--------------|\n| Analyze drone footage for archaeology | ‚úÖ Yes - custom object detection |\n| Track wildlife in video | ‚úÖ Yes - detection + tracking |\n| Count people in crowd | ‚úÖ Yes - dense object detection |\n| Real-time security camera | ‚úÖ Yes - YOLOv8 real-time |\n| Filter vacation photos | ‚ùå No - use photo management apps |\n| Face recognition login | ‚ùå No - use AWS Rekognition API |\n| Read license plates | ‚ùå No - use specialized OCR |\n\n---\n\n## References\n\n- `/references/yolo-guide.md` - YOLOv8 setup, training, inference patterns\n- `/references/video-processing.md` - Frame extraction, scene detection, optimization\n- `/references/tracking-algorithms.md` - ByteTrack, SORT, DeepSORT comparison\n\n## Scripts\n\n- `scripts/video_analyzer.py` - Extract frames, run detection, generate timeline\n- `scripts/model_trainer.py` - Fine-tune YOLO on custom dataset, export weights\n\n---\n\n**This skill guides**: Computer vision | Object detection | Video analysis | YOLO | Tracking | Drone footage | Wildlife monitoring\n"
    }
  ]
}