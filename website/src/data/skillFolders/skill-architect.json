{
  "name": "skill-architect",
  "type": "folder",
  "path": "skill-architect",
  "children": [
    {
      "name": "references",
      "type": "folder",
      "path": "skill-architect/references",
      "children": [
        {
          "name": "antipatterns.md",
          "type": "file",
          "path": "skill-architect/references/antipatterns.md",
          "size": 14614,
          "content": "# Skill Anti-Patterns: The Shibboleths\n\nThis document catalogs **domain-specific knowledge that separates novices from experts** - the things LLMs get wrong because their training data includes outdated patterns, oversimplified tutorials, or cargo-culted code.\n\n## Table of Contents\n\n1. [ML/AI Model Selection](#mlai-model-selection)\n2. [Framework Evolution](#framework-evolution)\n3. [Tool Architecture](#tool-architecture)\n4. [Skill Design](#skill-design)\n\n---\n\n## ML/AI Model Selection\n\n### Anti-Pattern: CLIP for Everything\n\n**Novice thinking**: \"CLIP is pre-trained on 400M image-text pairs and does zero-shot classification. Use it for all image-text tasks!\"\n\n**Reality**: CLIP has **fundamental geometric limitations**. Research from 2023-2025 proves it cannot simultaneously handle:\n\n1. Basic descriptions\n2. Attribute binding (\"red car AND blue truck\" vs \"blue car AND red truck\")\n3. Spatial relationships (\"cat left of dog\" vs \"dog left of cat\")\n4. Negation (\"not a cat\")\n\n**What CLIP fails at**:\n- ❌ Counting objects in images\n- ❌ Fine-grained classification (celebrity ID, car models, flower species)\n- ❌ Compositional reasoning\n- ❌ Spatial understanding\n- ❌ Handwritten text (MNIST-style)\n\n**When to use alternatives**:\n\n| Task | Use Instead | Why |\n|------|-------------|-----|\n| Counting objects | DETR, Faster R-CNN | Object detection models built for counting |\n| Fine-grained classification | EfficientNet + task head | Transfer learning on specific domain |\n| Compositional reasoning | DCSMs, PC-CLIP | Preserve patch/token topology |\n| Spatial relationships | GQA models, SWIG | Built for spatial understanding |\n| Attribute binding | PC-CLIP (pairwise) | Trained on comparative data |\n\n**Timeline**:\n- 2021: Original CLIP released\n- 2022-2023: Limitations discovered in research\n- 2024: DCSMs (Dense Cosine Similarity Maps) paper\n- 2024: PC-CLIP (Pairwise Comparison CLIP)\n- 2025: SpLiCE (Sparse Linear Concept Embeddings)\n\n**LLM mistake**: LLMs trained on 2021-2023 data will suggest CLIP for everything because limitations weren't widely known yet.\n\n---\n\n### Anti-Pattern: Single Embedding Model\n\n**Novice thinking**: \"Pick one embedding model and use it everywhere\"\n\n**Expert knowledge**: Different tasks need different models:\n\n**Text embeddings**:\n- Semantic search: `text-embedding-3-large`, `voyage-2`\n- Code search: `voyage-code-2`, `text-embedding-ada-002`\n- Multi-lingual: `multilingual-e5-large`\n- Long documents: `jina-embeddings-v2` (8k tokens)\n\n**Image embeddings**:\n- General: CLIP ViT-L/14\n- Fine-grained: DINOv2\n- Medical: BiomedCLIP\n- Faces: ArcFace, CosFace\n\n**Multi-modal**:\n- Image-text: CLIP, BLIP-2\n- Video: X-CLIP, VideoCLIP\n- 3D: ULIP, PointCLIP\n\n**Why this matters**: Embedding quality directly impacts retrieval accuracy. Using the wrong model can drop accuracy by 20-40%.\n\n---\n\n### Anti-Pattern: Ignoring Model Versioning\n\n**Problem**: \"We're using `text-embedding-ada-002`\" (doesn't specify when)\n\n**Why wrong**: Models evolve:\n- `text-embedding-ada-002` (Dec 2022) vs `text-embedding-3-small` (Jan 2024)\n- CLIP ViT-B/32 vs ViT-L/14 vs ViT-g-14\n- Different training data, different capabilities\n\n**Best practice**: Pin versions, document when you adopted them:\n```python\n# embeddings.py\nMODEL = \"text-embedding-3-large\"  # Adopted: 2024-03-15\nMODEL_DIMENSIONS = 3072\nTRAINING_CUTOFF = \"2023-09\"  # Approximate\n```\n\n---\n\n## Framework Evolution\n\n### Anti-Pattern: Pages Router in App Router Projects\n\n**Context**: Next.js 13 (Oct 2022) introduced App Router, fundamentally changing architecture.\n\n**Outdated pattern** (Pages Router):\n```javascript\n// pages/api/users.js\nexport default function handler(req, res) {\n  res.json({ users: [] })\n}\n\n// pages/users.js\nexport async function getServerSideProps() {\n  return { props: { users: [] } }\n}\n```\n\n**Current pattern** (App Router):\n```javascript\n// app/api/users/route.js\nexport async function GET() {\n  return Response.json({ users: [] })\n}\n\n// app/users/page.js\nasync function UsersPage() {\n  const users = await fetchUsers()  // Server Component\n  return <UserList users={users} />\n}\n```\n\n**Why it matters**: Pages Router patterns don't work in App Router and vice versa.\n\n**LLM mistake**: Training data from 2020-2023 overwhelmingly shows Pages Router. LLMs will default to old patterns unless specifically prompted.\n\n**Timeline**:\n- 2016-2022: Pages Router only\n- Oct 2022: App Router introduced (beta)\n- May 2023: App Router stable\n- 2024+: App Router is default\n\n---\n\n### Anti-Pattern: Redux for Everything\n\n**Novice thinking**: \"Global state needs Redux\"\n\n**Timeline**:\n- 2015-2020: Redux dominated\n- 2019: Context API improved in React 16.3\n- 2020: Zustand, Jotai emerged\n- 2023: React Server Components changed the game\n\n**Current wisdom**:\n- **Local UI state**: `useState`, `useReducer`\n- **Derived state**: `useMemo`, selectors\n- **Global state (simple)**: Context API\n- **Global state (complex)**: Zustand, Jotai\n- **Server state**: React Query, SWR\n- **URL state**: Next.js searchParams\n- **Redux**: Only if you need time-travel debugging or complex middleware\n\n**Why Redux fell out of favor**:\n- Boilerplate heavy\n- Server Components make much state \"server-native\"\n- Simpler alternatives emerged\n\n**LLM mistake**: LLMs will suggest Redux by default because 80% of training data predates alternatives.\n\n---\n\n### Anti-Pattern: Class Components\n\n**Timeline**:\n- 2013-2018: Class components only\n- Feb 2019: Hooks introduced (React 16.8)\n- 2020+: Functional components are standard\n\n**Outdated**:\n```javascript\nclass UserProfile extends React.Component {\n  state = { user: null }\n  \n  componentDidMount() {\n    fetchUser().then(user => this.setState({ user }))\n  }\n  \n  render() {\n    return <div>{this.state.user?.name}</div>\n  }\n}\n```\n\n**Current**:\n```javascript\nfunction UserProfile() {\n  const [user, setUser] = useState(null)\n  \n  useEffect(() => {\n    fetchUser().then(setUser)\n  }, [])\n  \n  return <div>{user?.name}</div>\n}\n```\n\n**When class components are still valid**:\n- Error boundaries (no hook equivalent yet)\n- Legacy codebases\n\n**LLM mistake**: Will generate class components for complex state management\n\n---\n\n## Tool Architecture\n\n### Anti-Pattern: MCP for Everything\n\n**Novice thinking**: \"MCP is the new standard, make everything an MCP!\"\n\n**Expert reality**: MCPs have overhead. Use them strategically.\n\n**Use MCP when**:\n- ✅ External API with authentication\n- ✅ Stateful connections (WebSocket, database)\n- ✅ Real-time data streams\n- ✅ Security boundaries (credentials, OAuth)\n\n**Use Scripts when**:\n- ✅ Local file operations\n- ✅ Batch transformations\n- ✅ Stateless computations\n- ✅ CLI wrappers\n\n**Example - Wrong**:\n```python\n# mcp_server_for_json_parsing.py - OVERKILL!\n@mcp.tool()\ndef parse_json(file_path: str):\n    with open(file_path) as f:\n        return json.load(f)\n```\n\n**Example - Right**:\n```python\n# scripts/parse_json.py - Simple script!\nimport json\nimport sys\n\nwith open(sys.argv[1]) as f:\n    data = json.load(f)\n    print(json.dumps(data, indent=2))\n```\n\n**Philosophy**: \"MCP's job isn't to abstract reality for the agent; its job is to manage the auth, networking, and security boundaries and then get out of the way.\"\n\n---\n\n### Anti-Pattern: Premature Abstraction\n\n**Problem**: Building a complex MCP before understanding the use case\n\n**Better approach**: Start with scripts, graduate to MCP when you need:\n1. Auth/security boundaries\n2. Multiple tools in same domain\n3. State management\n4. Error handling standardization\n\n**Evolution path**:\n```\nScript → Multiple Scripts → Helper Library → MCP Server\n```\n\nOnly promote to MCP when complexity justifies it.\n\n---\n\n## Skill Design\n\n### Anti-Pattern: Skill as Documentation Dump\n\n**Bad**:\n```markdown\n---\nname: react-guide\ndescription: Everything about React\n---\n\n# React Guide\n\nReact is a JavaScript library for building user interfaces...\n[50 pages of tutorial content]\n```\n\n**Why wrong**: Not progressive disclosure, not actionable, not targeted.\n\n**Good**:\n```markdown\n---\nname: react-server-components\ndescription: Use React Server Components correctly. Use when working with Next.js App Router, async components, or server-side data fetching.\n---\n\n# React Server Components\n\n## Quick Decision Tree\n\nIs your component:\n- Fetching data? → Server Component\n- Using hooks/events? → Client Component\n- Both? → Server Component wrapper + Client Component child\n\n## Common Anti-Pattern: Everything is 'use client'\n\n❌ **Wrong**:\n```jsx\n'use client'\nasync function Page() {  // This doesn't work!\n  const data = await fetch(...)\n  return <div>{data}</div>\n}\n```\n\n✅ **Right**:\n```jsx\n// Server Component (default)\nasync function Page() {\n  const data = await fetchData()\n  return <ClientComponent data={data} />\n}\n\n// client-component.jsx\n'use client'\nfunction ClientComponent({ data }) {\n  const [count, setCount] = useState(0)\n  return <div onClick={() => setCount(count + 1)}>{data}</div>\n}\n```\n\n## When This Pattern Changed\n\n- Pre-Next.js 13: All components are client-side\n- Next.js 13+: Server Components by default\n- LLM confusion: Will add 'use client' everywhere because older patterns\n\nSee /references/server-components-deep-dive.md for more.\n```\n\n---\n\n### Anti-Pattern: Missing \"When NOT to Use\"\n\n**Problem**: Skills activate on false positives\n\n**Example - Without negatives**:\n```yaml\ndescription: Processes images using computer vision techniques\n```\nActivates for: image resizing, image generation, image editing, OCR, face detection, etc.\n\n**Example - With negatives**:\n```yaml\ndescription: Semantic image search using CLIP embeddings. Use for finding similar images, zero-shot classification. NOT for image generation, editing, or OCR. NOT for counting objects or fine-grained classification.\n```\n\n**Pattern**: Always include \"NOT for X, Y, Z\" to prevent false activation.\n\n---\n\n### Anti-Pattern: No Validation Script\n\n**Problem**: Skill gives instructions but no way to check correctness\n\n**Better**: Include validation\n\n```python\n# scripts/validate.py\ndef validate_setup():\n    \"\"\"Check if environment is configured correctly.\"\"\"\n    checks = {\n        \"Node version\": check_node_version(),\n        \"Dependencies\": check_dependencies(),\n        \"API keys\": check_api_keys(),\n    }\n    \n    for name, passed in checks.items():\n        print(f\"{'✅' if passed else '❌'} {name}\")\n    \n    return all(checks.values())\n```\n\n---\n\n### Anti-Pattern: Overly Permissive Tools\n\n**Bad**:\n```yaml\nallowed-tools: Bash\n```\n\n**Why**: Can execute ANY bash command\n\n**Better**:\n```yaml\nallowed-tools: Bash(git:*,npm:run,npm:install),Read,Write\n```\n\n**Principle**: Least privilege - only grant what's needed\n\n---\n\n## Temporal Knowledge Patterns\n\nWhen documenting anti-patterns, always include:\n\n1. **Timeline**: When was this practice common?\n2. **Why deprecated**: What replaced it and why?\n3. **LLM confusion**: Why will LLMs suggest the old pattern?\n4. **Migration path**: How to update from old to new?\n\n**Template**:\n```markdown\n### Anti-Pattern: [Pattern Name]\n\n**Used**: [Date range]\n**Replaced by**: [New approach]\n**Why deprecated**: [Reason]\n\n**Old way**:\n[code example]\n\n**New way**:\n[code example]\n\n**LLM mistake**: [Why LLM suggests old pattern]\n**How to detect**: [Validation rule]\n```\n\n---\n\n---\n\n## Real-World Failure Case Studies\n\n### Case Study 1: The Photo Expert Explosion\n\n**Skill**: `photo-expert` (v1.0)\n**Problem**: Single skill for ALL photo operations\n\n**Symptoms**:\n- Activated on \"photo\" anywhere in query\n- 800+ lines of instructions\n- Slow loading, high token usage\n- Wrong advice given (composition advice when user wanted color theory)\n\n**Root Cause**: Everything Skill anti-pattern\n\n**Resolution**: Split into 5 focused skills:\n- `clip-aware-embeddings` - semantic search\n- `photo-composition-critic` - aesthetic analysis\n- `color-theory-palette-harmony-expert` - color science\n- `collage-layout-expert` - arrangement algorithms\n- `event-detection-temporal-intelligence-expert` - clustering\n\n**Lesson**: One domain ≠ one skill. Split by expertise type.\n\n---\n\n### Case Study 2: The Phantom MCP\n\n**Skill**: `github-workflow-helper` (v1.1)\n**Problem**: Referenced MCP server that didn't exist\n\n**SKILL.md said**:\n```markdown\nUse the included MCP server for GitHub API access.\nRun: `npx github-helper-mcp`\n```\n\n**Reality**: No `mcp-server/` directory existed\n\n**Symptoms**:\n- Claude confidently told users to run non-existent commands\n- Users filed bug reports\n- Trust in skill ecosystem damaged\n\n**Root Cause**: Reference Illusion anti-pattern\n\n**Resolution**:\n1. Added `check_self_contained.py` to detect phantom tools\n2. Either create the MCP or remove the reference\n3. Added validation to CI\n\n**Lesson**: Don't promise tools you don't deliver.\n\n---\n\n### Case Study 3: The Time Bomb\n\n**Skill**: `react-hooks-expert` (v2.0)\n**Problem**: Temporal knowledge became stale\n\n**Original content (2023)**:\n```markdown\nUse useEffect with empty deps for componentDidMount behavior\n```\n\n**By 2024**: This caused issues with React 18 Strict Mode double-mounting\n\n**Symptoms**:\n- Users followed advice → got bugs\n- Skill became actively harmful\n- No CHANGELOG to track when content was written\n\n**Root Cause**: Missing temporal knowledge markers\n\n**Resolution**:\n```markdown\n## Temporal Context\n- **Pre-React 18**: useEffect with [] = componentDidMount\n- **React 18+**: useEffect with [] runs TWICE in dev (Strict Mode)\n- **Current best practice**: Use refs for \"run once\" patterns\n```\n\n**Lesson**: Date your knowledge. Update quarterly.\n\n---\n\n### Case Study 4: The Activation Black Hole\n\n**Skill**: `api-design-expert` (v1.0)\n**Problem**: Never activated when needed\n\n**Description**:\n```yaml\ndescription: Expert guidance for API design\n```\n\n**Symptoms**:\n- User: \"How should I structure my REST endpoints?\"\n- Skill: *silence*\n- User confused why skill existed but never helped\n\n**Root Cause**: Missing Exclusions + no keywords\n\n**Resolution**:\n```yaml\ndescription: REST/GraphQL API design patterns. Activate on \"API design\",\n\"endpoint structure\", \"REST architecture\", \"GraphQL schema\".\nNOT for API implementation, SDK generation, or documentation.\n```\n\n**Lesson**: Generic descriptions = zero activations\n\n---\n\n## Contributing\n\nWhen you discover a new anti-pattern:\n\n1. Document what looks right but is wrong\n2. Explain the fundamental reason it's wrong\n3. Show the correct approach\n4. Include temporal context (when did this change?)\n5. Note why LLMs make this mistake\n6. Add detection/validation if possible\n\n**Remember**: The goal is to encode the knowledge that separates \"it compiles\" from \"it's correct\" - the shibboleths that reveal expertise.\n"
        },
        {
          "name": "description-guide.md",
          "type": "file",
          "path": "skill-architect/references/description-guide.md",
          "size": 8790,
          "content": "# Skill Description Writing Guide\n\nThe `description` field in SKILL.md frontmatter is the single most important line for activation. Claude's runtime scans descriptions at startup to build a catalog. When a user's query arrives, the runtime matches against these descriptions to decide which skills to load. A weak description means zero activations or constant false positives.\n\n---\n\n## The Formula\n\n**`[What it does] [When to use it] [Trigger keywords]. NOT for [Exclusions].`**\n\nEvery description should answer:\n1. **What**: What does this skill do? (specific verb + domain noun)\n2. **When**: In what situations should it activate?\n3. **Keywords**: What words in a user's query should trigger it?\n4. **NOT for**: What should explicitly NOT trigger it?\n\n---\n\n## Bad → Good Examples\n\n### 1. Too Vague / Generic\n\n**Bad**:\n> \"This skill helps with writing and improving content.\"\n\n**Problems**: No task type, no audience, no trigger conditions, overlaps with every writing-related skill.\n\n**Good**:\n> \"Drafts and revises long-form technical blog posts for software engineers, including structure, headings, and examples. Use when creating or improving an in-depth engineering blog post. NOT for short replies, casual notes, or marketing copy.\"\n\n**Why it works**: Specific audience (software engineers), specific format (long-form blog posts), clear exclusions.\n\n---\n\n### 2. Overlapping with Other Skills\n\n**Bad**:\n> \"This skill writes documents and summaries for business use.\"\n\n**Problems**: Collides with marketing, ops, product, and general summarization skills. Which one should activate?\n\n**Good**:\n> \"Creates and updates quarterly business review (QBR) slide decks for executives, using a standard section layout (executive summary, KPIs, highlights, risks, next steps). Use when preparing or revising a QBR or leadership performance review. NOT for internal status docs, detailed PRDs, or marketing materials.\"\n\n**Why it works**: Specific deliverable (QBR decks), specific audience (executives), clear format, explicit exclusions.\n\n---\n\n### 3. Description is a Mini-Manual\n\n**Bad**:\n> \"This skill helps you research and summarize complex topics. First it collects requirements, then it searches, then it writes an outline, then it drafts a report, then it revises based on feedback, and it always uses clear language and bullet points with citations and examples and...\"\n\n**Problems**: Too long, procedures belong in the SKILL.md body, risks truncation in the catalog scan. The runtime only reads the description for matching — process details don't help activation.\n\n**Good**:\n> \"Performs structured research and writes 1-3 page synthesis reports on technical or business topics for non-expert readers. Use when requesting a researched overview or briefing document. NOT for quick factual questions, casual brainstorming, or academic papers.\"\n\n**Why it works**: Concise, specifies output format (1-3 pages), names the audience, excludes adjacent tasks.\n\n---\n\n### 4. Missing \"When Not to Use\"\n\n**Bad**:\n> \"This skill reviews code changes and suggests improvements.\"\n\n**Problems**: Will activate for every coding request — writing new features, debugging, refactoring, reviewing PRs. Way too broad.\n\n**Good**:\n> \"Reviews existing code changes (diffs, pull requests) in TypeScript and React projects, providing structured feedback on correctness, readability, performance, and tests. Use when sharing diffs or PRs for review. NOT for implementing new features from scratch, debugging runtime errors, or general coding advice.\"\n\n**Why it works**: Specific input (diffs/PRs), specific tech stack (TypeScript/React), clear boundary (review vs. implementation).\n\n---\n\n### 5. Not Using User Language / Domain Keywords\n\n**Bad**:\n> \"This skill manages agile processes for teams and helps with planning and coordination.\"\n\n**Problems**: \"Agile processes\" is a category, not a trigger. No mention of the specific artifacts users actually ask about.\n\n**Good**:\n> \"Plans and updates agile sprints in tools like Jira or Linear, including writing user stories, prioritizing the backlog, and drafting sprint goals. Use when planning a sprint, grooming a backlog, or turning product ideas into user stories. NOT for low-level coding tasks, architecture decisions, or retrospective facilitation.\"\n\n**Why it works**: Names the tools (Jira, Linear), names the artifacts (user stories, backlog, sprint goals), uses verbs users would actually type.\n\n---\n\n### 6. Misaligned Name and Description\n\n**Bad**:\n```yaml\nname: database-migration-skill\ndescription: This skill writes marketing emails to customers.\n```\n\n**Problems**: Name says infrastructure, description says marketing. The runtime and human readers will both be confused.\n\n**Good**:\n```yaml\nname: database-migration-skill\ndescription: Plans and reviews database schema and data migrations, focusing on safety, rollout strategy, and rollback plans. Use when designing or validating a database migration. NOT for general application feature design or marketing content.\n```\n\n---\n\n### 7. Overly Broad \"Catch-All\" Skills\n\n**Bad**:\n> \"This skill helps the user with anything related to product management, including discovery, strategy, roadmapping, writing, and stakeholder communication.\"\n\n**Problems**: Becomes a generic PM agent that competes with every other skill, easy to misfire, impossible to test activation precisely.\n\n**Good** (narrowed to one deliverable):\n> \"Structures and writes product requirement documents (PRDs) for new or existing features, including problem statement, goals, user stories, and acceptance criteria. Use when drafting or refining a PRD. NOT for high-level strategy decks, user interview notes, or OKR planning.\"\n\n**Why it works**: One specific deliverable (PRDs), clear trigger (\"draft a PRD\"), explicit boundaries.\n\n---\n\n## Activation Keyword Strategy\n\n### Use Domain-Specific Terms\n\nInclude the exact words users type:\n- ✅ \"CLIP\", \"embeddings\", \"similarity search\"\n- ❌ \"computer vision techniques\" (too abstract)\n\n### Include Verb + Noun Combinations\n\nUsers ask for actions on objects:\n- ✅ \"create skill\", \"improve skill\", \"debug activation\"\n- ❌ \"skill-related activities\"\n\n### Add Common Synonyms\n\nUsers phrase things differently:\n- ✅ \"review code\", \"code review\", \"PR review\", \"diff review\"\n- ❌ Just \"review\" (too generic)\n\n### Test with Anti-Queries\n\nFor every keyword that should trigger, think of a query with that word that should NOT trigger:\n- \"CLIP\" → triggers: \"Use CLIP for image search\"\n- \"CLIP\" → should NOT trigger: \"Clip the audio at 30 seconds\" (different meaning)\n\nIf anti-queries would false-positive, add them to the NOT clause.\n\n---\n\n## Description Length Guidelines\n\n- **Minimum**: 15 words (enough for What + When + NOT)\n- **Ideal**: 25-50 words\n- **Maximum**: ~100 words (longer descriptions get truncated in catalog scans)\n- **Process details**: Never in description. Put in SKILL.md body.\n- **Examples**: Never in description. Put in SKILL.md body.\n\n---\n\n## Common Description Patterns by Skill Type\n\n### Domain Expertise Skills\n```\n[Domain] expertise for [specific area]. Use when [trigger situations].\nActivate on [keywords]. NOT for [adjacent domains].\n```\n\n### Tool/Script Skills\n```\n[Action verb] [objects] using [method/tool]. Use when [trigger situations].\nNOT for [related but different tasks].\n```\n\n### Process/Workflow Skills\n```\n[Multi-step process name] for [deliverable]. Use when [trigger situations].\nNOT for [simpler/different processes].\n```\n\n### Audit/Review Skills\n```\nAudits/reviews [what] for [quality criteria]. Use when [trigger situations].\nNOT for [creating/implementing the thing being reviewed].\n```\n\n---\n\n## Testing a Description\n\nAfter writing a description, validate with this checklist:\n\n```\n□ Contains at least one specific verb (creates, reviews, plans, debugs)\n□ Names a specific deliverable or domain (PRDs, TypeScript diffs, CLIP embeddings)\n□ Includes keywords users would actually type in a query\n□ Has a NOT clause with 2-5 explicit exclusions\n□ Name and description are aligned (no contradictions)\n□ Under 100 words (ideally 25-50)\n□ No process/workflow details (those go in SKILL.md body)\n□ Doesn't overlap with other skills in the same repo\n```\n\n---\n\n## Rewriting Exercise\n\nWhen improving an existing description, use this process:\n\n1. **List 5 queries that should trigger** this skill\n2. **List 5 queries that should NOT trigger** (but are in a similar domain)\n3. **Extract keywords** from the \"should trigger\" list\n4. **Extract exclusions** from the \"should NOT trigger\" list\n5. **Write**: `[What from step 1 patterns] [When from step 1 patterns] [Keywords from step 3]. NOT for [Exclusions from step 4].`\n6. **Test**: Re-read each of the 10 queries. Would the description correctly match/reject each one?\n"
        },
        {
          "name": "mcp-template.md",
          "type": "file",
          "path": "skill-architect/references/mcp-template.md",
          "size": 5274,
          "content": "# Minimal MCP Server Template\n\nProduction-ready starter template for MCP servers.\n\n## File Structure\n\n```\nmcp-server/\n├── src/\n│   └── index.ts       # Server implementation\n├── package.json       # Dependencies and scripts\n├── tsconfig.json      # TypeScript configuration\n└── README.md          # Installation instructions\n```\n\n## src/index.ts\n\n```typescript\nimport { Server } from \"@modelcontextprotocol/sdk/server/index.js\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\n\n// Server metadata\nconst server = new Server(\n  {\n    name: \"my-skill-mcp\",\n    version: \"1.0.0\"\n  },\n  {\n    capabilities: {\n      tools: {}\n    }\n  }\n);\n\n// Define available tools\nserver.setRequestHandler(\"tools/list\", async () => ({\n  tools: [\n    {\n      name: \"example_tool\",\n      description: \"Example tool that demonstrates the pattern\",\n      inputSchema: {\n        type: \"object\",\n        properties: {\n          input: {\n            type: \"string\",\n            description: \"Input parameter description\"\n          }\n        },\n        required: [\"input\"]\n      }\n    }\n  ]\n}));\n\n// Handle tool calls\nserver.setRequestHandler(\"tools/call\", async (request) => {\n  const { name, arguments: args } = request.params;\n\n  if (name === \"example_tool\") {\n    try {\n      // Your tool implementation here\n      const result = await processInput(args.input);\n\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: JSON.stringify(result, null, 2)\n          }\n        ]\n      };\n    } catch (error) {\n      throw new Error(`Failed to process: ${error.message}`);\n    }\n  }\n\n  throw new Error(`Unknown tool: ${name}`);\n});\n\n// Helper function (example)\nasync function processInput(input: string): Promise<any> {\n  // Implement your logic here\n  return {\n    processed: input,\n    timestamp: new Date().toISOString()\n  };\n}\n\n// Start server\nconst transport = new StdioServerTransport();\nawait server.connect(transport);\n```\n\n## package.json\n\n```json\n{\n  \"name\": \"my-skill-mcp\",\n  \"version\": \"1.0.0\",\n  \"description\": \"MCP server for [domain] operations\",\n  \"type\": \"module\",\n  \"bin\": {\n    \"my-skill-mcp\": \"dist/index.js\"\n  },\n  \"scripts\": {\n    \"build\": \"tsc\",\n    \"start\": \"node dist/index.js\",\n    \"watch\": \"tsc --watch\"\n  },\n  \"dependencies\": {\n    \"@modelcontextprotocol/sdk\": \"^1.0.0\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^20.0.0\",\n    \"typescript\": \"^5.0.0\"\n  }\n}\n```\n\n## tsconfig.json\n\n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"Node16\",\n    \"moduleResolution\": \"Node16\",\n    \"outDir\": \"dist\",\n    \"rootDir\": \"src\",\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"resolveJsonModule\": true\n  },\n  \"include\": [\"src/**/*\"],\n  \"exclude\": [\"node_modules\", \"dist\"]\n}\n```\n\n## README.md\n\n```markdown\n# My Skill MCP Server\n\nMCP server for [domain] operations.\n\n## Features\n\n- [Feature 1]\n- [Feature 2]\n- [Feature 3]\n\n## Installation\n\n\\`\\`\\`bash\ncd mcp-server\nnpm install\nnpm run build\n\\`\\`\\`\n\n## Configuration\n\nAdd to your Claude Code MCP settings (`~/.config/claude/config.json`):\n\n\\`\\`\\`json\n{\n  \"mcpServers\": {\n    \"my-skill\": {\n      \"command\": \"node\",\n      \"args\": [\"/absolute/path/to/mcp-server/dist/index.js\"],\n      \"env\": {\n        \"API_KEY\": \"your-api-key-here\"\n      }\n    }\n  }\n}\n\\`\\`\\`\n\n## Tools\n\n### example_tool\n\nDescription of what this tool does.\n\n**Parameters**:\n- `input` (string, required): Description of input parameter\n\n**Example**:\n\\`\\`\\`json\n{\n  \"input\": \"test value\"\n}\n\\`\\`\\`\n\n## Development\n\n\\`\\`\\`bash\nnpm run watch  # Auto-rebuild on changes\n\\`\\`\\`\n\n## Testing\n\nTest the server manually:\n\\`\\`\\`bash\necho '{\"jsonrpc\":\"2.0\",\"method\":\"tools/list\",\"id\":1}' | npm start\n\\`\\`\\`\n```\n\n## Best Practices\n\n1. **Error Handling**: Always wrap tool implementations in try-catch\n2. **Validation**: Validate inputs before processing\n3. **Logging**: Use structured logging for debugging\n4. **Secrets**: Use environment variables for API keys\n5. **Types**: Use TypeScript for type safety\n6. **Documentation**: Keep README up to date with tool changes\n\n## Common Patterns\n\n### Authentication\n\n```typescript\nconst API_KEY = process.env.API_KEY;\nif (!API_KEY) {\n  throw new Error(\"API_KEY environment variable required\");\n}\n```\n\n### Rate Limiting\n\n```typescript\nimport pLimit from 'p-limit';\nconst limit = pLimit(5); // Max 5 concurrent requests\n\nasync function processWithLimit(items: string[]) {\n  return Promise.all(\n    items.map(item => limit(() => processItem(item)))\n  );\n}\n```\n\n### Caching\n\n```typescript\nconst cache = new Map<string, any>();\n\nasync function getCached(key: string, fetcher: () => Promise<any>) {\n  if (cache.has(key)) {\n    return cache.get(key);\n  }\n  const value = await fetcher();\n  cache.set(key, value);\n  return value;\n}\n```\n\n## Troubleshooting\n\n**Server won't start**:\n- Check that `npm run build` completed successfully\n- Verify absolute path in config.json\n- Check environment variables are set\n\n**Tool not found**:\n- Ensure tool name in `tools/list` matches `tools/call` handler\n- Check for typos in tool names\n\n**Authentication errors**:\n- Verify API_KEY environment variable is set correctly\n- Check API key has necessary permissions\n"
        },
        {
          "name": "self-contained-tools.md",
          "type": "file",
          "path": "skill-architect/references/self-contained-tools.md",
          "size": 8974,
          "content": "# Self-Contained Tools\n\nImplementation patterns for scripts, MCP servers, and subagents that make skills immediately useful.\n\n## Philosophy\n\n**The best skill is one where the user can start working immediately.**\n\n| Approach | Result |\n|----------|--------|\n| \"Here's how to build a CLIP embedder\" | User spends 2 hours implementing |\n| \"Here's a working CLIP embedder, run it\" | User is productive in 2 minutes |\n\nSkills should encode expertise AND provide working tools to apply that expertise.\n\n---\n\n## Scripts\n\n### When to Include Scripts\n\n- Skill describes repeatable operations (analysis, validation, transformation)\n- Domain has specific algorithms that should be implemented correctly\n- Pre-flight checks would prevent common errors\n\n### Script Requirements\n\n1. **Actually work** - Not templates, not pseudocode\n2. **Minimal dependencies** - Prefer stdlib, document any pip/npm installs\n3. **Clear interface** - CLI args or stdin/stdout\n4. **Error handling** - Graceful failures with helpful messages\n5. **README** - How to install and run\n\n### Example: Domain Analysis Script\n\n```python\n#!/usr/bin/env python3\n\"\"\"\nPhoto Composition Analyzer\nAnalyzes images for composition quality using rule of thirds,\nvisual weight distribution, and color harmony.\n\nUsage: python analyze_composition.py <image_path>\nDependencies: pip install pillow numpy\n\"\"\"\n\nimport sys\nfrom pathlib import Path\n\ndef analyze_composition(image_path: str) -> dict:\n    \"\"\"Analyze composition and return scores.\"\"\"\n    # Import here to give helpful error if missing\n    try:\n        from PIL import Image\n        import numpy as np\n    except ImportError:\n        print(\"Install dependencies: pip install pillow numpy\")\n        sys.exit(1)\n\n    img = Image.open(image_path)\n    # ... actual implementation ...\n\n    return {\n        \"rule_of_thirds\": 0.85,\n        \"visual_balance\": 0.72,\n        \"color_harmony\": 0.91,\n        \"overall\": 0.83\n    }\n\nif __name__ == \"__main__\":\n    if len(sys.argv) != 2:\n        print(f\"Usage: {sys.argv[0]} <image_path>\")\n        sys.exit(1)\n\n    result = analyze_composition(sys.argv[1])\n    for metric, score in result.items():\n        print(f\"{metric}: {score:.2f}\")\n```\n\n### Example: Validation Script\n\n```bash\n#!/bin/bash\n# validate_skill.sh - Pre-flight checks for skill quality\n# Usage: ./validate_skill.sh /path/to/skill\n\nSKILL_DIR=\"$1\"\n\nif [ -z \"$SKILL_DIR\" ]; then\n    echo \"Usage: $0 <skill_directory>\"\n    exit 1\nfi\n\nerrors=0\n\n# Check SKILL.md exists\nif [ ! -f \"$SKILL_DIR/SKILL.md\" ]; then\n    echo \"❌ Missing SKILL.md\"\n    ((errors++))\nelse\n    echo \"✅ SKILL.md exists\"\nfi\n\n# Check line count\nlines=$(wc -l < \"$SKILL_DIR/SKILL.md\")\nif [ \"$lines\" -gt 500 ]; then\n    echo \"⚠️  SKILL.md is $lines lines (target: &lt;500)\"\nelse\n    echo \"✅ SKILL.md is $lines lines\"\nfi\n\n# Check for NOT clause in description\nif grep -q \"NOT for\" \"$SKILL_DIR/SKILL.md\"; then\n    echo \"✅ Description has NOT clause\"\nelse\n    echo \"❌ Missing NOT clause in description\"\n    ((errors++))\nfi\n\nexit $errors\n```\n\n---\n\n## MCP Servers\n\n### When to Build an MCP\n\n- Skill needs external API access (GitHub, Figma, databases, etc.)\n- OAuth or API key authentication required\n- Stateful connections (websockets, streaming)\n- Rate limiting or caching needed\n\n### MCP Server Structure\n\n```\nmcp-server/\n├── src/\n│   └── index.ts       # Server implementation\n├── package.json       # Dependencies and scripts\n├── tsconfig.json      # TypeScript config\n└── README.md          # Installation instructions\n```\n\n### Example: Minimal MCP Server\n\n```typescript\n// src/index.ts\nimport { Server } from \"@modelcontextprotocol/sdk/server/index.js\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\n\nconst server = new Server(\n  { name: \"my-skill-mcp\", version: \"1.0.0\" },\n  { capabilities: { tools: {} } }\n);\n\n// Define tools\nserver.setRequestHandler(\"tools/list\", async () => ({\n  tools: [\n    {\n      name: \"analyze_repo\",\n      description: \"Analyze a GitHub repository structure\",\n      inputSchema: {\n        type: \"object\",\n        properties: {\n          repo: { type: \"string\", description: \"owner/repo format\" }\n        },\n        required: [\"repo\"]\n      }\n    }\n  ]\n}));\n\nserver.setRequestHandler(\"tools/call\", async (request) => {\n  const { name, arguments: args } = request.params;\n\n  if (name === \"analyze_repo\") {\n    // Actual implementation\n    const result = await analyzeRepo(args.repo);\n    return { content: [{ type: \"text\", text: JSON.stringify(result) }] };\n  }\n\n  throw new Error(`Unknown tool: ${name}`);\n});\n\n// Start server\nconst transport = new StdioServerTransport();\nawait server.connect(transport);\n```\n\n### package.json\n\n```json\n{\n  \"name\": \"my-skill-mcp\",\n  \"version\": \"1.0.0\",\n  \"type\": \"module\",\n  \"bin\": { \"my-skill-mcp\": \"dist/index.js\" },\n  \"scripts\": {\n    \"build\": \"tsc\",\n    \"start\": \"node dist/index.js\"\n  },\n  \"dependencies\": {\n    \"@modelcontextprotocol/sdk\": \"^1.0.0\"\n  },\n  \"devDependencies\": {\n    \"typescript\": \"^5.0.0\"\n  }\n}\n```\n\n### README Template\n\n```markdown\n# My Skill MCP Server\n\nMCP server for [domain] operations.\n\n## Installation\n\n\\`\\`\\`bash\ncd mcp-server\nnpm install\nnpm run build\n\\`\\`\\`\n\n## Configuration\n\nAdd to your Claude Code MCP settings:\n\n\\`\\`\\`json\n{\n  \"mcpServers\": {\n    \"my-skill\": {\n      \"command\": \"node\",\n      \"args\": [\"/path/to/mcp-server/dist/index.js\"],\n      \"env\": {\n        \"API_KEY\": \"your-api-key\"\n      }\n    }\n  }\n}\n\\`\\`\\`\n\n## Tools\n\n- `analyze_repo` - Analyze a GitHub repository structure\n- `fetch_issues` - Get open issues with labels\n```\n\n---\n\n## Subagents\n\n### When to Define Subagents\n\n- Skill involves multi-step workflows\n- Different phases need different tool access\n- Orchestration logic is complex enough to warrant isolation\n\n### Subagent Definition Format\n\n```markdown\n# agents/research-workflow.md\n\n## Agent: Research Coordinator\n\n### Purpose\nOrchestrate multi-source research with synthesis.\n\n### System Prompt\nYou are a research coordinator. Your job is to:\n1. Break down research questions into searchable queries\n2. Dispatch searches to appropriate sources\n3. Synthesize findings into coherent answers\n\n### Tools Required\n- WebSearch\n- WebFetch\n- Read\n- Write\n\n### Workflow\n1. Receive research question\n2. Generate 3-5 search queries\n3. Execute searches in parallel\n4. Read and extract relevant content\n5. Synthesize into final answer\n\n### Success Criteria\n- All claims have citations\n- Multiple sources corroborate findings\n- Contradictions are explicitly noted\n```\n\n### Multi-Agent Orchestration Pattern\n\n```markdown\n# agents/orchestrator.md\n\n## Pipeline: Code Review\n\n### Agents\n1. **security-scanner** - Check for vulnerabilities\n2. **style-checker** - Verify code style\n3. **architecture-reviewer** - Assess design patterns\n\n### Orchestration\n\\`\\`\\`\nparallel:\n  - security-scanner → security_report\n  - style-checker → style_report\nthen:\n  - architecture-reviewer(security_report, style_report) → final_review\n\\`\\`\\`\n\n### Handoff Protocol\nEach agent produces structured output:\n- `status`: pass | warn | fail\n- `findings`: list of issues\n- `recommendations`: suggested fixes\n```\n\n---\n\n## Anti-Patterns\n\n### Phantom Tools\n**What it looks like**: SKILL.md references `scripts/analyze.py` but file doesn't exist\n\n**Why it's wrong**: Users try to run non-existent code, lose trust in skill\n\n**Fix**: Only reference tools that actually exist and work\n\n### Template Soup\n**What it looks like**: Scripts are templates with `# TODO: implement` comments\n\n**Why it's wrong**: User still has to do the implementation work\n\n**Fix**: Ship working code or don't ship at all\n\n### Dependency Hell\n**What it looks like**: Script requires 15 pip packages, specific Python version, system libraries\n\n**Why it's wrong**: Most users won't complete setup\n\n**Fix**: Minimize dependencies, prefer stdlib, document clearly\n\n### MCP Without Purpose\n**What it looks like**: MCP server for operations that could be a simple script\n\n**Why it's wrong**: Over-engineering; MCP has setup overhead\n\n**Fix**: Use MCP only when you need: auth, state, external APIs, or caching\n\n---\n\n## Checklist: Is My Skill Self-Contained?\n\n```\n□ Can a user start using this skill immediately?\n□ Are all referenced scripts/tools actually present and working?\n□ Do scripts have clear installation instructions?\n□ Do scripts handle errors gracefully?\n□ If MCP needed, is server implementation complete?\n□ If subagents needed, are prompts and workflows defined?\n□ Is there a validation script to check environment?\n□ Does README explain how to set everything up?\n```\n\n---\n\n## Examples of Self-Contained Skills\n\n| Skill | Tools Included |\n|-------|----------------|\n| clip-aware-embeddings | `scripts/validate_clip_usage.py` |\n| site-reliability-engineer | `scripts/validate-brackets.js`, `scripts/validate-liquid.js` |\n| skill-coach | `scripts/validate_skill.py` |\n\n**Goal**: Every skill with repeatable operations should have working tools.\n"
        },
        {
          "name": "subagent-design.md",
          "type": "file",
          "path": "skill-architect/references/subagent-design.md",
          "size": 11804,
          "content": "# Designing Skills for Subagent Consumption\n\nThis guide covers how to design skills that subagents can load and use effectively. A Claude subagent that \"loads up\" skills well is: (1) a very focused role, (2) with a curated skill set pre-injected, and (3) a clear internal workflow for applying those skills to the user's task.\n\n---\n\n## High-Level Architecture\n\nThink of one subagent as **\"a specialist with a toolkit\"**:\n\n| Component | Purpose | Example |\n|-----------|---------|---------|\n| **Role/system prompt** | Defines domain and responsibilities | \"You are a refactoring engineer for TypeScript monorepos\" |\n| **Attached skills** | Small, explicit set encoding methods/checklists/templates | `refactor-plan-skill`, `code-review-skill` |\n| **Tool and memory policy** | What tools it may use, whether it keeps long-term memory | Code tools + tests, project memory |\n| **Communication protocol** | How it receives tasks and reports back | Summary + artifacts + open questions |\n\nThe orchestrator hands the subagent a concrete sub-goal plus relevant context. The subagent's job is to solve it *using its skills as standard operating procedures* rather than improvising a new process each time.\n\n---\n\n## Three Skill-Loading Layers\n\n### Layer 1: Preloaded (Always in Context)\n\nFor core behaviors, inject the full content of 2-5 key skills directly into the subagent's system context. These are always \"present\" — the subagent doesn't need to discover them mid-run.\n\n**When to preload**:\n- The skill defines the subagent's primary workflow\n- The skill is needed for >80% of tasks the subagent handles\n- The skill is small enough (<5k tokens) to keep in context\n\n### Layer 2: Dynamically Selected (Catalog-Based)\n\nIf you have many skills, don't load all of them. Instead, give the subagent:\n- A **short catalog** (name + 1-line description for each skill)\n- Instructions: \"Before starting, scan the skill catalog and choose 1-3 skills whose purpose matches this task. If none match, fall back to generic reasoning.\"\n\nThe orchestrator can also pre-filter and only pass a relevant subset of skills along with the task.\n\n### Layer 3: Execution-Time (Protocol-Based)\n\nThe subagent treats each selected skill like a mini-protocol:\n1. Read the skill's \"When to use / When not to use\" section → confirm applicability\n2. Follow its numbered steps in order (adapt only if task constraints force it)\n3. Respect its output contract (templates, JSON shapes, required headings)\n4. Apply its QA/validation section last (run checklist over own output)\n\n**Make this explicit in the prompt**: \"When using a skill, reference its steps by number and confirm you've completed each one before returning your result.\"\n\n---\n\n## Subagent Prompt Structure\n\nInside the subagent's prompt, maintain this stable four-section structure:\n\n### 1. Identity and Purpose\n\n```\nYou are the **[role]** for this system. You handle [narrow domain of tasks].\nWhen a task is outside this scope, explicitly say so and ask the orchestrator\nfor a different agent.\n```\n\nKeep the role narrow. \"Refactoring engineer for TypeScript monorepos\" is better than \"code helper.\"\n\n### 2. Skill Usage Meta-Rules\n\n```\nYou have access to the following skills, which define your methods:\n  - Skill A: for doing X\n  - Skill B: for doing Y\n  - Skill C: for doing Z\n\nWhen tackling a task, you must:\n  - Decide which skill(s) apply\n  - Follow their step-by-step workflow\n  - Use their output formats and checklists\n```\n\nThis tells the subagent that skills are **standard operating procedures**, not optional hints.\n\n### 3. Task-Handling Loop\n\n```\nFor each task you receive:\n  1) Restate the task in your own words\n  2) Select one or more skills that fit. If none fit well, say so.\n  3) If needed, ask 2-5 clarifying questions\n  4) Produce an internal plan (short, not user-visible unless asked)\n  5) Execute the skill workflow step by step\n  6) Run any validation / QA steps from the skill\n  7) Return:\n     (a) final answer/artifacts\n     (b) what skills you used\n     (c) assumptions and remaining risks\n```\n\n### 4. Constraints and Priorities\n\n```\nQuality bar: [e.g., \"never knowingly leave tests failing\"]\nSafety rules: [e.g., \"never execute destructive operations without confirmation\"]\nTie-breaking: [e.g., \"if speed vs robustness conflict, pick robustness\"]\n```\n\n---\n\n## Orchestrator + Subagent Interaction Patterns\n\n### Single-Specialist Pattern\n\nOrchestrator identifies that the request maps to one domain and routes entirely to that subagent:\n\n```\nUser: \"Refactor this module\"\n  → Orchestrator routes to Refactorer subagent\n  → Refactorer uses refactor-plan-skill + code-review-skill\n  → Returns refactored code + review summary\n```\n\n### Chain Pattern\n\nSequential handoff between specialized subagents:\n\n```\nDesign API → Implement → Test\n\n  1. API-Designer subagent (design skills) → API spec\n  2. Implementer subagent (coding skills) → working code\n  3. QA subagent (testing skills) → test results + coverage\n```\n\nEach receives the prior subagent's artifacts and uses its own skills to transform them.\n\n### Parallel Pattern\n\nIndependent subagents work concurrently:\n\n```\nparallel:\n  - Auth subagent → auth implementation\n  - Billing subagent → billing implementation\n  - UI subagent → frontend components\n\nthen:\n  - Orchestrator merges outputs, resolves conflicts\n```\n\n---\n\n## Designing Skills That Subagents Consume Well\n\n### 1. Explicit \"When to Use\" / \"When Not to Use\"\n\nSubagents need clear applicability signals. A skill without these forces the subagent to guess:\n\n```markdown\n## When to Use\n✅ Existing code needs restructuring for maintainability\n✅ Module has grown beyond 500 lines\n✅ Tests exist and pass (safe to refactor)\n\n## When NOT to Use\n❌ Greenfield development (nothing to refactor)\n❌ No test coverage (too risky without safety net)\n❌ Performance optimization (different skill)\n```\n\n### 2. Numbered Steps (Not Prose)\n\nSubagents follow steps better than paragraphs. Steps are referenceable: \"Completed step 3 of refactor-plan-skill.\"\n\n```markdown\n## Process\n1. Read the target module and identify code smells\n2. Categorize smells by type (duplication, coupling, complexity)\n3. Propose a refactor plan with before/after signatures\n4. Execute changes in atomic commits\n5. Run test suite after each commit\n6. Self-review the diff using code-review-skill\n```\n\n### 3. Output Contracts\n\nDefine what the skill produces so downstream agents or the orchestrator can consume it:\n\n```markdown\n## Output Format\nReturn a JSON object:\n{\n  \"status\": \"pass\" | \"warn\" | \"fail\",\n  \"changes\": [\"list of files changed\"],\n  \"tests_passing\": true | false,\n  \"risks\": [\"list of remaining risks\"],\n  \"summary\": \"1-2 sentence description of what changed\"\n}\n```\n\n### 4. QA/Validation Section\n\nEvery skill should end with a self-check:\n\n```markdown\n## Validation\nBefore returning results, verify:\n□ All tests still pass\n□ No TODO comments left behind\n□ Changes match the original plan\n□ No unrelated files were modified\n```\n\n### 5. Minimal Context Assumptions\n\nDon't assume the subagent knows your project structure. Include paths, conventions, and setup steps in the skill itself or its references.\n\n---\n\n## Concrete Example: Refactorer Subagent\n\n### Config\n\n```yaml\nname: refactorer\ndescription: \"Use for non-trivial refactors or large cleanups in TypeScript.\"\ntools: [Read, Write, Edit, Bash(npm:test, git:*)]\nskills:\n  - refactor-plan-skill\n  - code-review-skill\n  - safe-refactor-skill\nmemory: project\n```\n\n### Prompt Body\n\n```\nYou are the **Refactorer** subagent for this repo. You:\n- Design safe refactors\n- Implement them in small, atomic steps\n- Keep tests passing at every step\n\nYou have the following skills and must rely on them as your standard process:\n- `refactor-plan-skill`: analyze current code and design a stepwise refactor plan\n- `code-review-skill`: review diffs for correctness, style, and risk\n- `safe-refactor-skill`: apply changes incrementally, validate after each change\n\nFor every task:\n1) Restate the requested refactor\n2) If unclear, ask 2-3 clarifying questions\n3) Use `refactor-plan-skill` to propose a stepwise plan\n4) Execute the plan, running tests after each logical chunk\n5) Use `code-review-skill` to self-review your changes\n6) Summarize: what changed, what skills you used, remaining risks\n```\n\n---\n\n## Context Loading Best Practices\n\n### Keep References Separate\n\n- Put only high-level process and triggers in SKILL.md\n- Move bulky content (API specs, FAQs, examples, style guides) to `references/` files\n- Reference by path and purpose: \"see `references/api-guide.md` for full endpoints\"\n\n### Teach Lazy Loading\n\nIn the subagent's system prompt, make reference loading explicit:\n\n```\nWhen you need detailed information, read the specific reference file.\nNever read large reference files \"just in case.\" Only open them when\ndirectly relevant to the current step of your plan.\nIf a file looks huge, skim the headings first and jump to the relevant section.\n```\n\n### Scope Narrowly\n\nProgressive loading works best when each subagent:\n- Has a narrow responsibility (not \"do all marketing\" but \"draft landing pages\")\n- Points to its own small set of reference files\n- Only loads focused subsets, keeping context lean\n\n### Use Summarized Intermediates\n\nHave the subagent produce short internal summaries from large references (\"key API constraints,\" \"brand voice bullets\") and work from those. Only re-open the original reference if something seems missing.\n\n### Avoid Eager Meta-Prompts\n\nNever: \"Read all reference files before you start.\"\nInstead: \"Read only the minimal set of files required to answer the current question accurately.\"\n\n---\n\n## Input/Output Contracts for Multi-Agent Pipelines\n\nDefine what each subagent expects and produces so agents can be composed:\n\n### Input Contract\n\n```markdown\n## Expected Input\n- `files`: List of file paths to analyze\n- `focus`: One of \"security\" | \"performance\" | \"readability\"\n- `prior_findings`: (optional) Output from a previous agent\n```\n\n### Output Contract\n\n```markdown\n## Output Format\n{\n  \"status\": \"pass\" | \"warn\" | \"fail\",\n  \"findings\": [\n    {\n      \"severity\": \"high\" | \"medium\" | \"low\",\n      \"file\": \"path/to/file.ts\",\n      \"line\": 42,\n      \"message\": \"Description of finding\",\n      \"recommendation\": \"How to fix\"\n    }\n  ],\n  \"summary\": {\n    \"total_issues\": 5,\n    \"high\": 1,\n    \"medium\": 2,\n    \"low\": 2\n  }\n}\n```\n\n### Handoff Protocol\n\nEach agent in a pipeline should produce structured output that the next agent can consume without transformation. Standardize on:\n- `status` field for pass/fail signaling\n- `findings` array for detailed results\n- `summary` object for quick assessment\n- `metadata` object for timing, agent name, skills used\n\n---\n\n## Anti-Patterns in Subagent Skill Design\n\n### 1. Skill Without Output Contract\n\n**Problem**: Subagent produces free-form text that downstream agents can't parse.\n**Fix**: Define explicit output format (JSON schema, markdown template with required sections).\n\n### 2. Skill That Assumes Context\n\n**Problem**: Skill says \"check the config file\" without specifying which one or where.\n**Fix**: Include paths, conventions, and any setup requirements.\n\n### 3. Overly Broad Skill for Subagent\n\n**Problem**: Subagent has a 50-skill catalog and spends half its context selecting.\n**Fix**: Orchestrator pre-filters to 2-5 relevant skills before dispatching.\n\n### 4. No Applicability Check\n\n**Problem**: Subagent blindly follows a skill even when it doesn't fit.\n**Fix**: Every skill needs \"When to Use / When NOT to Use\" so the subagent can check before committing.\n\n### 5. Eager Reference Loading\n\n**Problem**: Subagent loads all reference files \"just in case\" and blows context.\n**Fix**: Teach lazy loading in the subagent prompt; reference files loaded per-step, not upfront.\n"
        },
        {
          "name": "subagent-template.md",
          "type": "file",
          "path": "skill-architect/references/subagent-template.md",
          "size": 10416,
          "content": "# Subagent Definition Template\n\nTemplate for defining specialized subagents for complex workflows. A subagent is \"a specialist with a toolkit\" — a focused role with curated skills pre-injected and a clear internal workflow for applying those skills.\n\n## Single Agent Definition\n\nThe prompt has four required sections: Identity, Skill Usage Rules, Task-Handling Loop, and Constraints.\n\n```markdown\n# agents/agent-name.md\n\n## Agent: [Agent Name]\n\n### Purpose\n[What this agent does in 1-2 sentences]\n\n### Identity (Section 1)\nYou are the **[role]** for this system. You handle [narrow domain of tasks].\nWhen a task is outside this scope, explicitly say so and ask the orchestrator\nfor a different agent.\n\n### Skill Usage Rules (Section 2)\nYou have access to the following skills, which define your methods:\n- Skill A: for doing X\n- Skill B: for doing Y\n- Skill C: for doing Z\n\nWhen tackling a task, you must:\n- Decide which skill(s) apply\n- Follow their step-by-step workflow\n- Use their output formats and checklists\n\n### Task-Handling Loop (Section 3)\nFor each task you receive:\n1. Restate the task in your own words\n2. Select one or more skills that fit. If none fit well, say so.\n3. If needed, ask 2-5 clarifying questions\n4. Produce an internal plan (short, not user-visible unless asked)\n5. Execute the skill workflow step by step\n6. Run any validation / QA steps from the skill\n7. Return:\n   (a) final answer/artifacts\n   (b) what skills you used\n   (c) assumptions and remaining risks\n\n### Constraints and Priorities (Section 4)\n- Quality bar: [e.g., \"never knowingly leave tests failing\"]\n- Safety: [e.g., \"no destructive operations without confirmation\"]\n- Tie-breaking: [e.g., \"if speed vs robustness conflict, pick robustness\"]\n\n### Tools Required\n- [Tool 1]\n- [Tool 2]\n- [Tool 3]\n\n### Success Criteria\n- [Criterion 1]\n- [Criterion 2]\n- [Criterion 3]\n\n### Output Format\n[Expected output structure — define a JSON schema or markdown template\nso downstream agents/orchestrator can consume it]\n\n### Example\n**Input**: [Example input]\n**Skills Selected**: [Which skills and why]\n**Process**: [How it's processed, referencing skill steps by number]\n**Output**: [Expected output matching the Output Format]\n```\n\n## Subagent YAML Config (Skill-Aware)\n\n```yaml\nname: refactorer\ndescription: \"Use for non-trivial refactors or large cleanups.\"\nmodel: inherit  # or specific model name\ntools:\n  - Read\n  - Write\n  - Edit\n  - Bash(npm:test, git:*)\nskills:\n  - refactor-plan-skill    # How to analyze code and design a refactor plan\n  - code-review-skill       # How to review diffs for correctness and risk\n  - safe-refactor-skill     # How to apply changes incrementally and validate\nmemory: project\n```\n\nThe `skills` field lists the skills preloaded into the subagent's context. Keep to 2-5 core skills; use dynamic selection for larger catalogs.\n\n---\n\n## Multi-Agent Orchestration Pattern\n\n```markdown\n# agents/orchestrator.md\n\n## Pipeline: [Pipeline Name]\n\n### Agents\n\n1. **agent-1** - [Brief description]\n   - Tools: [Tool list]\n   - Input: [What it receives]\n   - Output: [What it produces]\n\n2. **agent-2** - [Brief description]\n   - Tools: [Tool list]\n   - Input: [What it receives]\n   - Output: [What it produces]\n\n3. **agent-3** - [Brief description]\n   - Tools: [Tool list]\n   - Input: [What it receives]\n   - Output: [What it produces]\n\n### Orchestration Flow\n\n\\`\\`\\`\nparallel:\n  - agent-1 → output_1\n  - agent-2 → output_2\n\nsequential:\n  - agent-3(output_1, output_2) → final_result\n\\`\\`\\`\n\n### Handoff Protocol\n\nEach agent produces structured output:\n- `status`: [Status values]\n- `data`: [Data structure]\n- `metadata`: [Metadata structure]\n\n### Error Handling\n\nIf any agent fails:\n1. [Retry strategy]\n2. [Fallback approach]\n3. [User notification]\n\n### Example Execution\n\n**Input**: [Example input]\n\n**Agent 1 Output**:\n\\`\\`\\`json\n{\n  \"status\": \"success\",\n  \"data\": {...}\n}\n\\`\\`\\`\n\n**Agent 2 Output**:\n\\`\\`\\`json\n{\n  \"status\": \"success\",\n  \"data\": {...}\n}\n\\`\\`\\`\n\n**Final Result**:\n\\`\\`\\`json\n{\n  \"status\": \"success\",\n  \"findings\": [...],\n  \"recommendations\": [...]\n}\n\\`\\`\\`\n```\n\n## Real-World Example: Code Review Pipeline\n\n```markdown\n# agents/code-review-pipeline.md\n\n## Pipeline: Code Review\n\n### Purpose\nAutomated code review with security, style, and architecture analysis.\n\n### Agents\n\n1. **security-scanner**\n   - Purpose: Check for vulnerabilities\n   - Tools: Read, Grep, Bash(semgrep:*)\n   - Output: Security report with severity ratings\n\n2. **style-checker**\n   - Purpose: Verify code style compliance\n   - Tools: Read, Bash(eslint:*, prettier:*)\n   - Output: Style violations with fix suggestions\n\n3. **architecture-reviewer**\n   - Purpose: Assess design patterns and maintainability\n   - Tools: Read, Grep, Glob\n   - Output: Architecture recommendations\n\n### Orchestration Flow\n\n\\`\\`\\`\nparallel:\n  - security-scanner → security_report\n  - style-checker → style_report\n\nthen:\n  - architecture-reviewer(security_report, style_report) → final_review\n\\`\\`\\`\n\n### Handoff Protocol\n\nEach agent produces:\n\\`\\`\\`json\n{\n  \"status\": \"pass\" | \"warn\" | \"fail\",\n  \"findings\": [\n    {\n      \"severity\": \"high\" | \"medium\" | \"low\",\n      \"file\": \"path/to/file.ts\",\n      \"line\": 42,\n      \"message\": \"Description of issue\",\n      \"recommendation\": \"How to fix\"\n    }\n  ],\n  \"summary\": {\n    \"total_issues\": 5,\n    \"high\": 1,\n    \"medium\": 2,\n    \"low\": 2\n  }\n}\n\\`\\`\\`\n\n### Success Criteria\n- All high-severity issues resolved\n- No security vulnerabilities\n- Code style compliance &gt;95%\n- Architecture review approves design\n\n### Example Execution\n\n**Input**: Pull request with 5 changed files\n\n**Security Scanner Output**:\n\\`\\`\\`json\n{\n  \"status\": \"warn\",\n  \"findings\": [\n    {\n      \"severity\": \"medium\",\n      \"file\": \"src/auth.ts\",\n      \"line\": 23,\n      \"message\": \"Hardcoded secret detected\",\n      \"recommendation\": \"Use environment variable\"\n    }\n  ],\n  \"summary\": {\"total_issues\": 1, \"medium\": 1}\n}\n\\`\\`\\`\n\n**Style Checker Output**:\n\\`\\`\\`json\n{\n  \"status\": \"pass\",\n  \"findings\": [],\n  \"summary\": {\"total_issues\": 0}\n}\n\\`\\`\\`\n\n**Architecture Reviewer Output**:\n\\`\\`\\`json\n{\n  \"status\": \"pass\",\n  \"findings\": [\n    {\n      \"severity\": \"low\",\n      \"file\": \"src/auth.ts\",\n      \"message\": \"Consider extracting validation to separate module\",\n      \"recommendation\": \"Create src/validators/auth.ts\"\n    }\n  ],\n  \"summary\": {\"total_issues\": 1, \"low\": 1}\n}\n\\`\\`\\`\n\n**Final Review**: \"APPROVED with recommendations - Fix hardcoded secret before merge\"\n```\n\n## Research Workflow Example\n\n```markdown\n# agents/research-workflow.md\n\n## Agent: Research Coordinator\n\n### Purpose\nOrchestrate multi-source research with synthesis and fact-checking.\n\n### System Prompt\nYou are a research coordinator. Your job is to:\n1. Break down research questions into searchable queries\n2. Dispatch searches to appropriate sources (web, docs, code)\n3. Cross-reference findings for accuracy\n4. Synthesize coherent answers with citations\n\nAlways cite sources. Flag contradictory information.\n\n### Tools Required\n- WebSearch\n- WebFetch\n- Read\n- Grep\n- Write\n\n### Workflow\n1. Receive research question from user\n2. Generate 3-5 targeted search queries\n3. Execute searches in parallel\n4. Read and extract relevant content\n5. Cross-check facts across sources\n6. Synthesize findings into coherent answer\n7. Add citations for all claims\n\n### Success Criteria\n- All claims have citations\n- Multiple sources corroborate key findings\n- Contradictions are explicitly noted\n- Answer is comprehensive but concise\n\n### Output Format\n\\`\\`\\`markdown\n# [Question]\n\n## Summary\n[2-3 sentence answer]\n\n## Detailed Findings\n\n### [Topic 1]\n[Explanation with citations]\n\nSources:\n- [Source 1 with link]\n- [Source 2 with link]\n\n### [Topic 2]\n[Explanation with citations]\n\nSources:\n- [Source 3 with link]\n\n## Contradictory Information\n[If any sources disagree, explain the discrepancy]\n\n## Confidence Assessment\n[High/Medium/Low] - [Reasoning]\n\\`\\`\\`\n\n### Example\n**Input**: \"How do React Server Components affect bundle size?\"\n\n**Process**:\n1. Generate queries: \"React Server Components bundle size\", \"RSC vs Client Components\", \"Next.js App Router performance\"\n2. Search web + official docs\n3. Extract findings from 5+ sources\n4. Cross-reference performance claims\n5. Note any contradictions\n\n**Output**:\n\\`\\`\\`markdown\n# How React Server Components Affect Bundle Size\n\n## Summary\nReact Server Components reduce client-side bundle size by 20-40% on average by rendering components on the server and sending only HTML to the client. Dynamic interactivity requires selective use of Client Components.\n\n## Detailed Findings\n\n### Bundle Size Reduction\nServer Components don't include their code in the client bundle. A typical conversion reduces bundle by 30% (Vercel case study, Next.js docs).\n\nSources:\n- [Next.js App Router Documentation](https://nextjs.org/docs/app)\n- [Vercel Case Study: 33% Reduction](https://vercel.com/blog/rsc-performance)\n\n### Trade-offs\nClient Components still require client-side JavaScript. Overuse of 'use client' negates benefits.\n\nSources:\n- [React RFC: Server Components](https://github.com/reactjs/rfcs/blob/main/text/0188-server-components.md)\n\n## Confidence Assessment\nHigh - Multiple authoritative sources (React team, Next.js team, real-world case studies) corroborate findings.\n\\`\\`\\`\n```\n\n## Agent Communication Patterns\n\n### Request-Response\n\n```typescript\n// Parent to Agent\n{\n  \"task\": \"analyze_code\",\n  \"input\": {\n    \"files\": [\"src/auth.ts\", \"src/users.ts\"],\n    \"focus\": \"security\"\n  }\n}\n\n// Agent to Parent\n{\n  \"status\": \"complete\",\n  \"results\": {...},\n  \"metadata\": {\"duration_ms\": 2500}\n}\n```\n\n### Streaming Updates\n\n```typescript\n// Agent sends incremental updates\n{\n  \"type\": \"progress\",\n  \"step\": \"analyzing_file\",\n  \"file\": \"src/auth.ts\",\n  \"progress\": 0.5\n}\n\n{\n  \"type\": \"finding\",\n  \"severity\": \"high\",\n  \"message\": \"SQL injection risk\"\n}\n\n{\n  \"type\": \"complete\",\n  \"summary\": {...}\n}\n```\n\n## Best Practices\n\n1. **Clear Responsibilities**: Each agent should have a single, well-defined purpose\n2. **Structured Output**: Use consistent JSON schemas for agent communication\n3. **Error Handling**: Define fallback strategies for agent failures\n4. **Parallelization**: Run independent agents concurrently\n5. **Handoff Protocol**: Standardize how agents pass data\n6. **Success Criteria**: Define measurable completion conditions\n7. **Documentation**: Keep agent definitions up to date\n"
        },
        {
          "name": "visual-artifacts.md",
          "type": "file",
          "path": "skill-architect/references/visual-artifacts.md",
          "size": 19682,
          "content": "# Visual Artifacts in Skills: Mermaid Diagrams & Code\n\nSkills that produce visual artifacts — Mermaid diagrams, structured code blocks, annotated tables — are dramatically more useful than skills that produce only prose. A decision tree rendered as a flowchart is parsed instantly; the same logic in paragraph form forces the reader to mentally reconstruct the graph.\n\n**Rule of thumb**: If a skill describes a process, decision tree, architecture, state machine, timeline, or data relationship, it should include or generate a Mermaid diagram for it.\n\n---\n\n## Why Mermaid in Skills\n\nMermaid diagrams are text-based, version-controllable, and render natively in GitHub, Docusaurus, most Markdown renderers, and Claude's own output. They cost very few tokens relative to their information density.\n\n| Medium | Tokens | Comprehension | Version-controllable |\n|--------|--------|---------------|---------------------|\n| Prose paragraph | ~200 | Slow, sequential | Yes but hard to diff |\n| Markdown table | ~80 | Fast for comparisons | Yes |\n| Mermaid diagram | ~60 | Instant for relationships | Yes, text-based |\n| ASCII art | ~150 | Fragile, breaks easily | Painful to maintain |\n\n**Prefer Mermaid over ASCII art.** Mermaid is semantic; ASCII art is visual noise that breaks on re-flow.\n\n---\n\n## Mermaid YAML Frontmatter Configuration\n\nEvery Mermaid diagram can include a YAML frontmatter block (delimited by `---`) before the diagram definition. This controls theme, layout direction, and per-diagram-type settings.\n\n### Basic Structure\n\n````markdown\n```mermaid\n---\ntitle: My Diagram Title\nconfig:\n  theme: default\n  themeVariables:\n    primaryColor: \"#4a86c8\"\n---\nflowchart LR\n  A[Start] --> B[End]\n```\n````\n\n### Theme Options\n\n| Theme | Use When |\n|-------|----------|\n| `default` | General purpose, clean look |\n| `dark` | Dark-mode environments |\n| `forest` | Green-toned, organic feel |\n| `neutral` | Minimal, grayscale |\n| `base` | Starting point for full customization via `themeVariables` |\n\n### Common `themeVariables`\n\n```yaml\nconfig:\n  themeVariables:\n    primaryColor: \"#4a86c8\"        # Main node fill\n    primaryTextColor: \"#fff\"       # Text on primary nodes\n    primaryBorderColor: \"#3a76b8\"  # Border of primary nodes\n    secondaryColor: \"#f4f4f4\"      # Secondary node fill\n    tertiaryColor: \"#e8e8e8\"       # Tertiary node fill\n    lineColor: \"#333\"              # Edge/arrow color\n    fontSize: \"16px\"               # Base font size\n    fontFamily: \"monospace\"        # Font family\n    background: \"#ffffff\"          # Diagram background\n    nodeBorder: \"#333\"             # Default node border\n    noteTextColor: \"#333\"          # Note text color\n    noteBkgColor: \"#fff5ad\"        # Note background\n    edgeLabelBackground: \"#fff\"    # Edge label background\n```\n\n### Per-Diagram Configuration\n\nThe `config` block can include diagram-specific keys:\n\n```yaml\n---\nconfig:\n  flowchart:\n    htmlLabels: true\n    curve: basis           # basis, linear, stepBefore, stepAfter\n    padding: 15\n    nodeSpacing: 50\n    rankSpacing: 50\n    defaultRenderer: dagre  # dagre or elk\n  sequence:\n    mirrorActors: false\n    bottomMarginAdj: 1\n    actorFontSize: 14\n    noteFontSize: 12\n    messageFontSize: 14\n    diagramMarginX: 50\n    diagramMarginY: 10\n    useMaxWidth: true\n  gantt:\n    titleTopMargin: 25\n    barHeight: 20\n    barGap: 4\n    topPadding: 50\n    sectionFontSize: 16\n  er:\n    layoutDirection: TB    # TB or LR\n    minEntityWidth: 100\n    minEntityHeight: 75\n    entityPadding: 15\n    fontSize: 12\n  mindmap:\n    padding: 10\n    maxNodeWidth: 200\n  timeline:\n    padding: 50\n    useMaxWidth: true\n---\n```\n\nFull configuration reference: https://mermaid.ai/open-source/config/configuration.html\n\n---\n\n## Diagram Type Catalog\n\nMermaid supports a rich taxonomy of diagram types. Choose based on what you're modeling.\n\n### Flowchart / Graph — Decision Trees & Processes\n\n**Use when**: A skill encodes a decision tree, troubleshooting flowchart, or branching process.\n\n**This is the most common diagram type for skills** — most skills have some \"If X then A, if Y then B\" logic that belongs in a flowchart.\n\n````markdown\n```mermaid\nflowchart TD\n  A[User asks to create skill] --> B{Existing skill?}\n  B -->|Yes| C[Audit & Improve]\n  B -->|No| D[Gather 3-5 examples]\n  D --> E[Plan reusable contents]\n  E --> F[Initialize skill folder]\n  F --> G[Write scripts → refs → SKILL.md]\n  G --> H[Validate]\n  H --> I{Errors?}\n  I -->|Yes| G\n  I -->|No| J[Ship it]\n```\n````\n\n**Direction options**: `TD` (top-down), `LR` (left-right), `BT` (bottom-top), `RL` (right-left)\n\n**Node shapes**:\n- `[text]` — rectangle\n- `(text)` — rounded rectangle\n- `{text}` — diamond (decision)\n- `([text])` — stadium/pill\n- `[[text]]` — subroutine\n- `[(text)]` — cylinder (database)\n- `((text))` — circle\n- `>text]` — flag/asymmetric\n- `[/text/]` — parallelogram\n- `[\\text\\]` — reverse parallelogram\n- `[/text\\]` — trapezoid\n- `[\\text/]` — reverse trapezoid\n- `{{text}}` — hexagon\n\n**Edge styles**:\n- `-->` solid arrow\n- `---` solid line\n- `-.->` dotted arrow\n- `==>` thick arrow\n- `--text-->` labeled edge\n- `~~~` invisible link (for layout control)\n\n---\n\n### Sequence Diagram — Interactions & Protocols\n\n**Use when**: A skill describes communication between agents, APIs, services, or any request/response protocol.\n\n````markdown\n```mermaid\nsequenceDiagram\n  participant O as Orchestrator\n  participant S as Subagent\n  participant SK as Skill\n\n  O->>S: Assign task + context\n  S->>SK: Load skill, check applicability\n  SK-->>S: \"When to use\" matches ✓\n  S->>S: Follow skill steps 1-5\n  S->>SK: Run QA checklist\n  SK-->>S: Validation passed ✓\n  S->>O: Return artifacts + skills used + risks\n```\n````\n\n**Features**:\n- `->` solid line, `->>` solid arrow, `-->` dotted line, `-->>` dotted arrow\n- `activate` / `deactivate` for lifeline boxes\n- `Note right of X: text` for annotations\n- `alt` / `else` / `end` for conditional blocks\n- `loop` / `end` for repetition\n- `par` / `and` / `end` for parallel execution\n- `critical` / `option` / `end` for critical sections\n- `break` / `end` for break-out flows\n- `rect rgb(...)` / `end` for colored background regions\n- `autonumber` for automatic message numbering\n\n---\n\n### State Diagram — Lifecycle & Status Machines\n\n**Use when**: A skill manages something with distinct states and transitions (build pipelines, document lifecycle, feature flags, deployment stages).\n\n````markdown\n```mermaid\nstateDiagram-v2\n  [*] --> Draft\n  Draft --> InReview: Submit for review\n  InReview --> Approved: Pass validation\n  InReview --> Draft: Revisions needed\n  Approved --> Published: Deploy\n  Published --> Deprecated: Sunset\n  Deprecated --> [*]\n\n  state InReview {\n    [*] --> StructureCheck\n    StructureCheck --> ContentCheck\n    ContentCheck --> ActivationTest\n    ActivationTest --> [*]\n  }\n```\n````\n\n**Features**:\n- `[*]` for start/end states\n- Nested states with `state Name { ... }`\n- `<<choice>>` for conditional branching\n- `<<fork>>` / `<<join>>` for parallel states\n- Notes with `note right of StateName`\n\n---\n\n### Entity-Relationship Diagram — Data Models\n\n**Use when**: A skill works with structured data, database schemas, API shapes, or any domain where entities have relationships.\n\n````markdown\n```mermaid\nerDiagram\n  SKILL ||--o{ REFERENCE : contains\n  SKILL ||--o{ SCRIPT : bundles\n  SKILL {\n    string name PK\n    string description\n    string allowed_tools\n    int line_count\n  }\n  REFERENCE {\n    string filename PK\n    string purpose\n    int size_bytes\n  }\n  SCRIPT {\n    string filename PK\n    string language\n    boolean works\n  }\n  SKILL }o--|| CHANGELOG : tracks\n```\n````\n\n**Relationship cardinality**:\n- `||--||` exactly one to exactly one\n- `||--o{` one to zero-or-many\n- `}o--o{` zero-or-many to zero-or-many\n- `||--|{` one to one-or-many\n\n---\n\n### Gantt Chart — Timelines & Project Plans\n\n**Use when**: A skill involves phased rollouts, migration plans, sprint planning, or any time-sequenced work.\n\n````markdown\n```mermaid\ngantt\n  title Skill Creation Timeline\n  dateFormat YYYY-MM-DD\n  axisFormat %b %d\n\n  section Research\n    Gather examples       :a1, 2026-02-01, 2d\n    Identify shibboleths  :a2, after a1, 1d\n\n  section Build\n    Write scripts         :b1, after a2, 3d\n    Write references      :b2, after a2, 2d\n    Write SKILL.md        :b3, after b1, 2d\n\n  section Validate\n    Run validation        :c1, after b3, 1d\n    Fix issues            :c2, after c1, 2d\n    Ship                  :milestone, after c2, 0d\n```\n````\n\n**Features**:\n- `done`, `active`, `crit` tags for status/priority\n- `milestone` for zero-duration markers\n- Dependencies with `after taskId`\n- Sections for logical grouping\n\n---\n\n### Mindmap — Concept Hierarchies\n\n**Use when**: A skill covers a domain taxonomy, feature map, brainstorm output, or any hierarchical concept space.\n\n````markdown\n```mermaid\nmindmap\n  root((Skill Architecture))\n    Metadata Layer\n      name\n      description\n      allowed-tools\n    SKILL.md Layer\n      Decision trees\n      Anti-patterns\n      Reference index\n    Reference Layer\n      Domain guides\n      Code examples\n      Templates\n    Self-Contained Tools\n      Scripts\n      MCP Servers\n      Subagents\n```\n````\n\n**Features**:\n- Root node shapes: `((circle))`, `(rounded)`, `[square]`, `{{hexagon}}`\n- Automatic layout based on indentation\n- Icon support with `::icon(fa fa-book)`\n\n---\n\n### Timeline — Historical / Temporal Knowledge\n\n**Use when**: A skill encodes temporal knowledge (framework evolution, API deprecations, \"what changed when\").\n\n````markdown\n```mermaid\ntimeline\n  title React State Management Evolution\n  2015 : Redux released\n       : Became default for global state\n  2019 : Context API improved (React 16.3)\n       : Hooks introduced (React 16.8)\n  2020 : Zustand released\n       : Jotai released\n  2023 : React Server Components stable\n       : Server state moves to server\n  2024 : Redux only for time-travel debugging\n       : Most apps use Zustand or React Query\n```\n````\n\nThis is particularly valuable for **shibboleth encoding** — the temporal evolution that LLMs get wrong.\n\n---\n\n### Pie Chart — Proportions & Distributions\n\n**Use when**: Showing relative sizes, coverage breakdowns, or category distributions.\n\n````markdown\n```mermaid\npie title Skill Token Budget\n  \"Metadata (Level 1)\" : 5\n  \"SKILL.md (Level 2)\" : 25\n  \"References (Level 3)\" : 70\n```\n````\n\n---\n\n### Quadrant Chart — 2x2 Decision Matrices\n\n**Use when**: A skill needs to position options along two axes (effort vs. impact, risk vs. reward, urgency vs. importance).\n\n````markdown\n```mermaid\nquadrantChart\n  title Skill Improvement Priority\n  x-axis Low Effort --> High Effort\n  y-axis Low Impact --> High Impact\n  quadrant-1 Do First\n  quadrant-2 Plan Carefully\n  quadrant-3 Delegate or Skip\n  quadrant-4 Quick Wins\n  Add NOT clause: [0.2, 0.8]\n  Tighten description: [0.3, 0.9]\n  Add Mermaid diagrams: [0.4, 0.6]\n  Build MCP server: [0.9, 0.7]\n  Rewrite from scratch: [0.8, 0.5]\n```\n````\n\n---\n\n### Git Graph — Branching & Merge Strategies\n\n**Use when**: A skill involves version control workflows, release strategies, or branch management.\n\n````markdown\n```mermaid\ngitGraph\n  commit id: \"v1.0.0\"\n  branch feature/description-guide\n  commit id: \"Add description examples\"\n  commit id: \"Add keyword strategy\"\n  checkout main\n  merge feature/description-guide id: \"v1.1.0\"\n  branch feature/subagent-design\n  commit id: \"Add loading layers\"\n  commit id: \"Add prompt structure\"\n  checkout main\n  merge feature/subagent-design id: \"v2.0.0\"\n```\n````\n\n---\n\n### Class Diagram — Object Models & Type Hierarchies\n\n**Use when**: A skill involves type systems, class hierarchies, interfaces, or any OO/structural modeling.\n\n````markdown\n```mermaid\nclassDiagram\n  class Skill {\n    +String name\n    +String description\n    +String[] allowedTools\n    +validate() bool\n    +activate(query) Response\n  }\n  class Reference {\n    +String filename\n    +String purpose\n    +load() Content\n  }\n  class Script {\n    +String filename\n    +String language\n    +run(args) Result\n  }\n  Skill \"1\" --> \"*\" Reference : contains\n  Skill \"1\" --> \"*\" Script : bundles\n  Skill <|-- MetaSkill : extends\n```\n````\n\n---\n\n### User Journey — Experience Mapping\n\n**Use when**: A skill models a user flow, onboarding experience, or multi-step interaction.\n\n````markdown\n```mermaid\njourney\n  title First-Time Skill User\n  section Discovery\n    Find skill in catalog: 3: User\n    Read description: 4: User\n    Skill activates on query: 5: System\n  section Usage\n    Follow core process: 4: User, System\n    Hit anti-pattern warning: 5: System\n    Correct approach used: 5: User\n  section Mastery\n    Consult references: 3: User\n    Customize for project: 4: User\n    Contribute improvements: 5: User\n```\n````\n\nScores are satisfaction ratings (1-5). Actors are labeled after the colon.\n\n---\n\n### Sankey Diagram — Flow Quantities\n\n**Use when**: Showing how quantities flow between categories (token budgets, request routing, resource allocation).\n\n````markdown\n```mermaid\nsankey-beta\n  User Query,Metadata Scan,100\n  Metadata Scan,Skill Match,80\n  Metadata Scan,No Match,20\n  Skill Match,SKILL.md Loaded,80\n  SKILL.md Loaded,Task Complete,60\n  SKILL.md Loaded,Reference Needed,20\n  Reference Needed,Single Ref Loaded,15\n  Reference Needed,Multiple Refs Loaded,5\n```\n````\n\n---\n\n### XY Chart — Data Visualization\n\n**Use when**: Plotting metrics, benchmarks, performance data, or any numeric comparison.\n\n````markdown\n```mermaid\nxychart-beta\n  title \"Activation Rate by Description Quality\"\n  x-axis [\"Vague\", \"Keywords Only\", \"Keywords+NOT\", \"Full Formula\"]\n  y-axis \"Activation %\" 0 --> 100\n  bar [12, 45, 78, 94]\n  line [12, 45, 78, 94]\n```\n````\n\n---\n\n### Block Diagram — System Architecture\n\n**Use when**: Modeling system components, infrastructure layouts, or architectural blocks.\n\n````markdown\n```mermaid\nblock-beta\n  columns 3\n  Orchestrator:3\n  space\n  block:subagents:3\n    columns 3\n    Refactorer Reviewer Tester\n  end\n  space\n  block:skills:3\n    columns 3\n    RefactorPlan CodeReview SafeRefactor\n  end\n```\n````\n\n---\n\n### Architecture Diagram — Infrastructure & Deployment\n\n**Use when**: Modeling cloud architecture, service topology, or deployment infrastructure.\n\n````markdown\n```mermaid\narchitecture-beta\n  group api(cloud)[API Layer]\n  group workers(server)[Worker Layer]\n\n  service gateway(internet)[API Gateway] in api\n  service auth(server)[Auth Service] in api\n  service orchestrator(server)[Orchestrator] in workers\n  service subagent1(server)[Refactorer] in workers\n  service subagent2(server)[Reviewer] in workers\n\n  gateway:R --> L:auth\n  auth:B --> T:orchestrator\n  orchestrator:R --> L:subagent1\n  orchestrator:R --> L:subagent2\n```\n````\n\n---\n\n### Kanban — Task/Status Boards\n\n**Use when**: Modeling workflow stages, task statuses, or any column-based status tracking.\n\n````markdown\n```mermaid\nkanban\n  column1[Backlog]\n    task1[Write description guide]\n    task2[Add Mermaid diagrams]\n  column2[In Progress]\n    task3[Subagent design patterns]\n  column3[Done]\n    task4[Progressive disclosure]\n    task5[Frontmatter docs]\n```\n````\n\n---\n\n## Which Diagram Type for Which Skill Content?\n\n| Skill Content | Best Diagram Type | Why |\n|---------------|-------------------|-----|\n| Decision trees / troubleshooting | **Flowchart** | Branching logic is what flowcharts do |\n| Agent/API communication | **Sequence** | Shows request/response over time |\n| Lifecycle / status transitions | **State** | Explicitly models valid transitions |\n| Data models / schemas | **ER Diagram** | Purpose-built for entities + relationships |\n| Framework evolution / temporal knowledge | **Timeline** | Chronological shibboleth encoding |\n| Domain taxonomy / concept maps | **Mindmap** | Hierarchical at a glance |\n| Priority / effort-vs-impact | **Quadrant** | 2x2 matrix is instantly parseable |\n| Project phases / rollout plans | **Gantt** | Time-sequenced dependencies |\n| Branching strategies | **Git Graph** | Models branch/merge visually |\n| Type hierarchies / interfaces | **Class Diagram** | OO structural relationships |\n| User experience flows | **User Journey** | Maps satisfaction across steps |\n| Quantity flows / budgets | **Sankey** | Shows proportional flow between categories |\n| Metrics / benchmarks | **XY Chart** | Numeric data visualization |\n| System architecture | **Block** or **Architecture** | Component layout and connections |\n| Task status tracking | **Kanban** | Column-based workflow visualization |\n| Proportional breakdowns | **Pie** | Simple category proportions |\n\n---\n\n## Best Practices for Mermaid in Skills\n\n### 1. Put Diagrams in the Right Layer\n\n- **SKILL.md**: Include 1-3 key diagrams (decision trees, core workflow). These are loaded on activation and should be high-value, low-token.\n- **References**: Include detailed diagrams (full ER models, comprehensive state machines, architecture layouts). These are loaded on demand.\n- **Never**: Overload SKILL.md with 10 diagrams. That defeats progressive disclosure.\n\n### 2. Use Mermaid for Decision Trees Instead of Prose\n\n**Bad (prose)**:\n> \"First check if the skill exists. If it does, audit it. If not, gather examples, then plan contents, then initialize, then write, then validate.\"\n\n**Good (flowchart)**:\n```mermaid\nflowchart TD\n  A{Skill exists?} -->|Yes| B[Audit & Improve]\n  A -->|No| C[Gather examples]\n  C --> D[Plan contents]\n  D --> E[Initialize]\n  E --> F[Write]\n  F --> G[Validate]\n```\n\nThe flowchart is ~40 tokens. The prose is ~35 tokens. The flowchart is instantly parseable. Always prefer the diagram.\n\n### 3. Prefer Specific Diagram Types Over Generic Flowcharts\n\nA sequence diagram for protocol interactions is more informative than a flowchart of the same protocol. A state diagram for lifecycle management is clearer than a flowchart with \"go back to step 2\" arrows. Choose the diagram type that matches the underlying structure.\n\n### 4. Use YAML Frontmatter for Consistent Styling\n\nIf a skill produces multiple diagrams, use the same theme configuration so they look cohesive:\n\n````markdown\n```mermaid\n---\nconfig:\n  theme: neutral\n  themeVariables:\n    fontSize: \"14px\"\n---\nflowchart LR\n  A --> B\n```\n````\n\n### 5. Keep Diagrams Self-Contained\n\nEach diagram should be understandable without reading the surrounding prose. Use descriptive node labels, not cryptic abbreviations:\n\n- ✅ `A[Check description has NOT clause]`\n- ❌ `A[Step 2.3]`\n\n### 6. Code Blocks Are Visual Artifacts Too\n\nDon't neglect inline code examples as visual artifacts. A 5-line code snippet is worth 50 words of description:\n\n```yaml\n# Good: concrete, copy-pasteable\ndescription: CLIP semantic search for image-text matching.\n  NOT for counting, spatial reasoning, or generation.\n```\n\n> Bad: \"Write a description that mentions what the skill does, when it should be used, and includes a NOT clause with things it should not be used for.\"\n\n---\n\n## Encouraging Visual Artifacts in Skills You Create\n\nWhen creating or auditing a skill, ask:\n\n1. **Does this skill have a decision tree?** → Render it as a flowchart\n2. **Does it describe a multi-step protocol?** → Render it as a sequence diagram\n3. **Does it manage states/lifecycle?** → Render it as a state diagram\n4. **Does it encode temporal knowledge?** → Render it as a timeline\n5. **Does it model data relationships?** → Render it as an ER diagram\n6. **Does it prioritize options on two axes?** → Render it as a quadrant chart\n7. **Does it describe system architecture?** → Render it as a block/architecture diagram\n\nIf the answer to any of these is \"yes\" and the skill only uses prose, **that's an improvement opportunity**.\n"
        }
      ]
    },
    {
      "name": "CHANGELOG.md",
      "type": "file",
      "path": "skill-architect/CHANGELOG.md",
      "size": 5066,
      "content": "# Changelog: skill-architect\n\n## v2.1.0 (2026-02-05)\n\n### Visual Artifacts\n\n**New section in SKILL.md**: \"Visual Artifacts: Mermaid Diagrams & Code\" — encourages skills to render decision trees, workflows, architectures, timelines, and data models as Mermaid diagrams instead of prose. Includes a quick-reference table mapping skill content types to optimal diagram types, plus Mermaid YAML frontmatter configuration syntax.\n\n**New reference**: `references/visual-artifacts.md` — comprehensive guide to all 16+ Mermaid diagram types with:\n- Full YAML frontmatter configuration (themes, themeVariables, per-diagram config)\n- Concrete examples for every diagram type: flowchart, sequence, state, ER, gantt, mindmap, timeline, pie, quadrant, gitgraph, class, user journey, sankey, XY chart, block, architecture, kanban\n- Node shapes, edge styles, and features for each diagram type\n- Decision matrix: which diagram type for which skill content\n- Best practices for Mermaid in progressive-disclosure skills\n\n**Anti-pattern #10**: \"Prose-Only Processes\" — if a skill describes a decision tree or workflow in paragraph form when it could be a Mermaid diagram, that's an improvement opportunity.\n\n**Updated validation checklist**: Now includes \"Decision trees/workflows use Mermaid diagrams, not prose.\"\n\n**Updated Step 4**: Skill creation now explicitly calls out visual artifacts and Mermaid as part of the writing process.\n\n---\n\n## v2.0.0 (2026-02-05)\n\n### Major Improvements\n\n**SKILL.md rewrite** — Reduced from 637 lines to 350 lines (was violating its own <500 line rule). Restructured for clarity and actionability.\n\n**Description Formula** — Expanded with concrete bad→good examples covering 7 common failure modes: too vague, overlapping, mini-manual, missing exclusions, wrong keywords, name mismatch, catch-all. Full guide moved to `references/description-guide.md`.\n\n**Frontmatter Documentation** — Added newly documented optional fields: `argument-hint`, `disable-model-invocation`, `user-invocable`, `context` (fork), and `metadata`. Previous version was incomplete about what's valid.\n\n**Subagent-Aware Skill Design** — New section covering how to design skills that subagents consume effectively: three loading layers (preloaded, dynamic, execution-time), subagent prompt structure (identity, skill rules, task loop, constraints), and orchestrator patterns (single-specialist, chain, parallel).\n\n**Progressive Disclosure** — Enhanced with specific lazy-loading rules: reference files are NOT auto-loaded, teach agents to load on-demand per-step, never instruct \"read all files first.\"\n\n**Anti-Pattern #9** — Added \"Eager Loading\" to the anti-pattern catalog.\n\n### New Reference Files\n\n- `references/description-guide.md` — Comprehensive guide to writing skill descriptions with bad→good examples, keyword strategy, length guidelines, and testing checklist\n- `references/subagent-design.md` — Full guide to designing skills for subagent consumption, including three loading layers, subagent prompt structure, orchestrator patterns, input/output contracts, and lazy-loading best practices\n\n### Updated Reference Files\n\n- `references/subagent-template.md` — Added four-section prompt structure (Identity, Skill Usage Rules, Task-Handling Loop, Constraints), YAML config with skill references, and skill-aware example patterns\n\n### Removed (Deduplicated)\n\n- Case studies removed from SKILL.md (were already duplicated in `references/antipatterns.md`)\n- Verbose code examples moved to reference files where they belong\n- Redundant script example removed (already in `references/self-contained-tools.md`)\n\n### Philosophy Update\n\nFrom \"progressive disclosure machines\" to \"progressive disclosure machines with lazy-loaded references\" — emphasizing that reference files are only loaded when the agent decides they're relevant to the current step, not eagerly.\n\n---\n\n## v1.0.0 (2026-01-14)\n\n### Created\n- **Unified meta-skill** combining skill-coach and skill-creator\n- Merged systematic workflow from skill-creator\n- Merged domain expertise encoding from skill-coach\n- Consolidated best practices from both skills\n\n### Features\n- 6-step skill creation process\n- Shibboleth encoding (expert knowledge patterns)\n- Anti-pattern catalog with case studies\n- Self-contained tool implementation (scripts, MCP, subagents)\n- Progressive disclosure design principles\n- Activation debugging workflows\n- Comprehensive validation checklists\n\n### References Added\n- `antipatterns.md` - Shibboleths and anti-pattern catalog\n- `self-contained-tools.md` - Scripts, MCP, and subagent patterns\n- `mcp-template.md` - Minimal MCP server starter\n- `subagent-template.md` - Agent definition format\n\n### Philosophy\n\"Great skills are progressive disclosure machines that encode real domain expertise, not just surface instructions.\"\n\n### Replaces\n- skill-coach (v2.x) - Expertise encoding focus\n- skill-creator (v1.x) - Systematic workflow focus\n\n### Migration\nUsers of skill-coach or skill-creator should switch to skill-architect for the unified experience.\n"
    },
    {
      "name": "README.md",
      "type": "file",
      "path": "skill-architect/README.md",
      "size": 5397,
      "content": "# Skill Architect\n\nThe authoritative meta-skill for creating, auditing, and improving Agent Skills.\n\n## What It Does\n\nSkill Architect is a meta-skill that teaches Claude how to build other skills well. It combines:\n- **Systematic workflow** (6-step creation process)\n- **Domain expertise encoding** (shibboleths, anti-patterns, temporal knowledge)\n- **Progressive disclosure architecture** (three-layer loading with lazy references)\n- **Subagent-aware design** (skills that work well when consumed by subagents)\n\n## Quick Start\n\n**Creating a new skill**:\n1. Gather 3-5 concrete example queries (what should/shouldn't trigger)\n2. Plan reusable contents (scripts, references, assets)\n3. Initialize: `scripts/init_skill.py <skill-name>`\n4. Write scripts first, references next, SKILL.md last\n5. Validate: `scripts/validate_skill.py <path>`\n6. Iterate based on real-world use\n\n**Improving an existing skill**:\n1. Tighten description: `[What] [When] [Keywords]. NOT for [Exclusions]`\n2. Check line count (<500 lines in SKILL.md)\n3. Add anti-patterns with shibboleth template\n4. Remove phantom references (files that don't exist)\n5. Test activation with 5 should-trigger + 5 shouldn't-trigger queries\n\n## Key Concepts\n\n### Progressive Disclosure (Three Layers)\n\n| Layer | Content | When Loaded |\n|-------|---------|-------------|\n| 1. Metadata | `name` + `description` | Always (catalog scan) |\n| 2. SKILL.md | Core process, decision trees | On skill activation |\n| 3. References | Deep dives, examples, specs | On-demand, per-file, lazy |\n\nReference files are NOT auto-loaded. The agent reads them only when relevant to the current step.\n\n### Description Formula\n\n`[What it does] [When to use] [Trigger keywords]. NOT for [Exclusions].`\n\nThe description is the single most important line for activation. See `references/description-guide.md` for 7 bad→good examples.\n\n### Frontmatter Fields\n\nRequired: `name`, `description`\n\nOptional: `allowed-tools`, `argument-hint`, `license`, `disable-model-invocation`, `user-invocable`, `context`, `metadata`\n\n### Visual Artifacts\n\nSkills should render processes, decision trees, architectures, and temporal knowledge as **Mermaid diagrams** instead of prose. Mermaid is text-based, version-controllable, and renders natively in GitHub, Docusaurus, and Claude's output.\n\n16+ diagram types are available: flowchart, sequence, state, ER, timeline, mindmap, quadrant, gantt, gitgraph, class, user journey, sankey, XY chart, block, architecture, kanban, pie.\n\nSee `references/visual-artifacts.md` for the full catalog with examples and YAML configuration.\n\n### Subagent-Aware Design\n\nSkills consumed by subagents should have:\n- Explicit \"When to Use / NOT\" sections\n- Numbered steps (not prose)\n- Output contracts (JSON schema or markdown template)\n- QA/validation checklists\n\nSee `references/subagent-design.md` for full patterns.\n\n### Shibboleths\n\nExpert knowledge that separates novices from experts:\n- Framework evolution (React: Classes → Hooks → Server Components)\n- Model limitations (CLIP can't count objects)\n- Tool architecture (Script → MCP graduation path)\n- Temporal traps (advice correct in 2023, harmful in 2025)\n\n## Structure\n\n```\nskill-architect/\n├── SKILL.md                          # Core instructions (<500 lines)\n├── CHANGELOG.md                      # Version history\n├── README.md                         # This file\n└── references/\n    ├── description-guide.md          # How to write effective descriptions\n    ├── visual-artifacts.md           # Mermaid diagram catalog & configuration\n    ├── antipatterns.md               # Shibboleths and case studies\n    ├── self-contained-tools.md       # Scripts, MCP, subagent patterns\n    ├── subagent-design.md            # Designing skills for subagent consumption\n    ├── mcp-template.md               # Minimal MCP server starter\n    └── subagent-template.md          # Agent definition format\n```\n\n## Anti-Patterns (Summary)\n\n| # | Anti-Pattern | Fix |\n|---|-------------|-----|\n| 1 | Documentation Dump | Decision trees in SKILL.md, depth in references |\n| 2 | Missing NOT clause | Always include exclusions in description |\n| 3 | Phantom Tools | Only reference files that exist and work |\n| 4 | Template Soup | Ship working code or nothing |\n| 5 | Overly Permissive Tools | Least privilege, scoped Bash |\n| 6 | Stale Temporal Knowledge | Date all advice, update quarterly |\n| 7 | Catch-All Skill | Split by expertise type |\n| 8 | Vague Description | Use the description formula |\n| 9 | Eager Loading | Lazy-load references, never \"read all first\" |\n| 10 | Prose-Only Processes | Use Mermaid for decision trees, workflows, architectures |\n\n## Success Metrics\n\n| Metric | Target |\n|--------|--------|\n| Correct activation | >90% |\n| False positive rate | <5% |\n| Token usage | <5k |\n| Time to productive | <5 min |\n\n## Version History\n\n- **v2.1.0** (2026-02-05) — Visual artifacts: Mermaid diagram guide, 16+ diagram types, YAML config, anti-pattern #10\n- **v2.0.0** (2026-02-05) — Major rewrite: description guide, subagent design, frontmatter fields, lazy loading, trimmed to 350 lines\n- **v1.0.0** (2026-01-14) — Initial unified meta-skill combining skill-coach + skill-creator\n\n## Replaces\n\nThis skill unifies and replaces:\n- **skill-coach** — Expertise encoding\n- **skill-creator** — Systematic workflow\n"
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "skill-architect/SKILL.md",
      "size": 16518,
      "content": "---\nname: skill-architect\ndescription: Design, create, audit, and improve Claude Agent Skills with expert-level progressive disclosure. Use when building new skills, reviewing existing skills, debugging activation failures, encoding domain expertise, or designing skills for subagent consumption. Activate on \"create skill\", \"improve skill\", \"skill audit\", \"skill review\", \"activation debugging\", \"shibboleth\", \"progressive disclosure\", \"skill description\". NOT for general Claude Code features, runtime debugging, non-skill coding, or MCP server implementation.\nargument-hint: \"[skill-path-or-name] [action: create|audit|improve|debug]\"\nallowed-tools: Read,Write,Edit,Bash,Grep,Glob\n---\n\n# Skill Architect: The Authoritative Meta-Skill\n\nThe unified authority for creating expert-level Agent Skills. Encodes the knowledge that separates a skill that *merely exists* from one that *activates precisely, teaches efficiently, and makes users productive immediately*.\n\n## Philosophy\n\n**Great skills are progressive disclosure machines.** They encode real domain expertise (shibboleths), not surface instructions. They follow a three-layer architecture: lightweight metadata for discovery, lean SKILL.md for core process, and reference files for deep dives loaded only on demand.\n\n---\n\n## When to Use This Skill\n\n✅ **Use for**:\n- Creating new skills from scratch or from existing expertise\n- Auditing/reviewing skills for quality, activation, and progressive disclosure\n- Improving activation rates and reducing false positives\n- Encoding domain expertise (shibboleths, anti-patterns, temporal knowledge)\n- Designing skills that subagents consume effectively\n- Building self-contained tools (scripts, MCPs, subagents)\n- Debugging why skills don't activate or activate incorrectly\n\n❌ **NOT for**:\n- General Claude Code features (slash commands, MCP server implementation)\n- Non-skill coding advice or code review\n- Debugging runtime errors (use domain-specific skills)\n- Template generation without real domain expertise to encode\n\n---\n\n## Quick Wins (Immediate Improvements)\n\nFor existing skills, apply in priority order:\n\n1. **Tighten description** → Follow `[What] [When] [Keywords]. NOT for [Exclusions]` formula\n2. **Check line count** → SKILL.md must be <500 lines; move depth to `/references`\n3. **Add NOT clause** → Prevent false activation with explicit exclusions\n4. **Add 1-2 anti-patterns** → Use shibboleth template (Novice/Expert/Timeline)\n5. **Remove dead files** → Delete unreferenced scripts/references (no phantoms)\n6. **Test activation** → Write 5 queries that should trigger and 5 that shouldn't\n\n---\n\n## Progressive Disclosure Architecture\n\nSkills use three-layer loading. The runtime scans metadata at startup, loads SKILL.md on activation, and pulls reference files *only when the agent decides it needs them*.\n\n| Layer | Content | Size | Loading |\n|-------|---------|------|---------|\n| 1. Metadata | `name` + `description` in frontmatter | ~100 tokens | Always in context (catalog scan) |\n| 2. SKILL.md | Core process, decision trees, brief anti-patterns | <5k tokens | On skill activation |\n| 3. References | Deep dives, examples, templates, specs | Unlimited | On-demand, per-file, only when relevant |\n\n**Critical rules**:\n- Keep SKILL.md under 500 lines. Move depth to `/references`.\n- Reference files are NOT auto-loaded. Only SKILL.md enters context on activation.\n- In SKILL.md, list each reference file with a 1-line description of when to consult it. This teaches the agent what's available without loading it.\n- Never instruct \"read all reference files before starting.\" Instead: \"Read only the files relevant to the current step.\"\n- If a reference file is large, the agent should skim headings first, then drill into the relevant section.\n\n---\n\n## Frontmatter Rules\n\n### Required Fields\n\n| Key | Purpose | Example |\n|-----|---------|---------|\n| `name` | Lowercase-hyphenated identifier | `react-server-components` |\n| `description` | Activation trigger: `[What] [When] [Keywords]. NOT for [Exclusions]` | See Description Formula |\n\n### Optional Fields\n\n| Key | Purpose | Example |\n|-----|---------|---------|\n| `allowed-tools` | Comma-separated tool names (least privilege) | `Read,Write,Grep` |\n| `argument-hint` | Hint shown in autocomplete for expected arguments | `\"[path] [format]\"` |\n| `license` | License identifier | `MIT` |\n| `disable-model-invocation` | If `true`, only user-triggered via `/skill-name` | `true` |\n| `user-invocable` | Controls whether skill appears in UI menus | `true` |\n| `context` | Execution context; `fork` runs skill in isolated subagent | `fork` |\n| `metadata` | Arbitrary key-value map for tooling/dashboards | `author: your-org` |\n\n### Invalid Keys (Will Fail Upload)\n\n```yaml\n# ❌ These are NOT valid frontmatter — move info to SKILL.md body\nintegrates_with: [...]\ntriggers: [...]\ntools: Read,Write        # Use 'allowed-tools' instead\noutputs: [...]\ncoordinates_with: [...]\npython_dependencies: [...]\n```\n\n---\n\n## Description Formula\n\n**Pattern**: `[What it does] [When to use] [Trigger keywords]. NOT for [Exclusions].`\n\nThe description is the most important line for activation. Claude's runtime scans descriptions to decide which skill to load. A weak description means zero activations or constant false positives.\n\n| Problem | Bad | Good |\n|---------|-----|------|\n| Too vague | \"Helps with images\" | \"CLIP semantic search for image-text matching and zero-shot classification. NOT for counting, spatial reasoning, or generation.\" |\n| No exclusions | \"Reviews code changes\" | \"Reviews TypeScript/React diffs and PRs for correctness. NOT for writing new features.\" |\n| Mini-manual | \"Researches, then outlines, then drafts...\" | \"Structured research producing 1-3 page synthesis reports. NOT for quick factual questions.\" |\n| Catch-all | \"Helps with product management\" | \"Writes and refines product requirement documents (PRDs). NOT for strategy decks.\" |\n| Name mismatch | name: `db-migration` / desc: \"writes marketing emails\" | name: `db-migration` / desc: \"Plans database schema migrations with rollback strategies.\" |\n\n**Full guide with more examples**: See `references/description-guide.md`\n\n---\n\n## SKILL.md Template\n\n```markdown\n---\nname: your-skill-name\ndescription: [What] [When] [Keywords]. NOT for [Exclusions].\nallowed-tools: Read,Write\n---\n\n# Skill Name\n[One sentence purpose]\n\n## When to Use\n✅ Use for: [A, B, C with specific trigger keywords]\n❌ NOT for: [D, E, F — explicit boundaries]\n\n## Core Process\n[Decision trees as Mermaid flowcharts, not prose. See visual-artifacts.md]\n\n## Anti-Patterns\n### [Pattern Name]\n**Novice**: [Wrong assumption]\n**Expert**: [Why it's wrong + correct approach]\n**Timeline**: [When this changed, if temporal]\n\n## References\n- `references/guide.md` — Consult when [specific situation]\n- `references/examples.md` — Consult for [worked examples of X]\n```\n\n---\n\n## The 6-Step Skill Creation Process\n\n### Step 1: Gather Concrete Examples\n\nCollect 3-5 real queries that should trigger this skill, and 3-5 that should NOT.\n\n### Step 2: Plan Reusable Contents\n\nFor each example, identify what scripts, references, or assets would prevent re-work. Also identify shibboleths: domain algorithms, temporal knowledge, framework evolution, common pitfalls.\n\n### Step 3: Initialize\n\n```bash\nscripts/init_skill.py <skill-name> --path <output-directory>\n```\n\nFor existing skills, skip to Step 4.\n\n### Step 4: Write the Skill\n\nOrder of implementation:\n1. **Scripts first** (`scripts/`) — Working code, not templates\n2. **References next** (`references/`) — Domain knowledge, schemas, guides\n3. **SKILL.md last** — Core process, anti-patterns, reference index\n\nWrite in imperative form: \"To accomplish X, do Y\" not \"You should do X.\"\n\nAnswer these questions in SKILL.md:\n1. **Purpose**: What is this skill for? (1-2 sentences)\n2. **Activation**: What triggers it? What shouldn't?\n3. **Process**: Step-by-step decision trees — use Mermaid flowcharts, not prose\n4. **Anti-patterns**: What do novices get wrong?\n5. **Visual artifacts**: Render workflows, architectures, timelines as Mermaid diagrams (see `references/visual-artifacts.md`)\n6. **References**: What files exist and when to consult them?\n\n### Step 5: Validate\n\n```bash\npython scripts/validate_skill.py <path>\npython scripts/check_self_contained.py <path>\n```\n\nFix ERRORS → WARNINGS → SUGGESTIONS.\n\n### Step 6: Iterate\n\nAfter real-world use: notice struggles, improve SKILL.md and resources, update CHANGELOG.md.\n\n---\n\n## Designing Skills for Subagent Consumption\n\nWhen skills will be loaded by subagents (not just direct user invocation), apply these patterns:\n\n### Three Skill-Loading Layers\n\n1. **Preloaded** (2-5 core skills): Injected into the subagent's system context. These are its standard operating procedures — always present.\n2. **Dynamically selected**: Subagent receives a catalog (name + 1-line description) and picks 1-3 matching skills before starting. The orchestrator can also pre-filter.\n3. **Execution-time**: Subagent reads each skill's \"When to use\" section, follows numbered steps in order, respects output contracts, and runs QA checks.\n\n### How Subagents Should Use Skills\n\nTeach the subagent to treat each skill like a mini-protocol:\n- Check the \"When to use / When not to use\" section for applicability\n- Follow numbered steps in order (adapt only if task constraints force it)\n- Respect the skill's output contract (templates, JSON shapes, required sections)\n- Apply QA/validation steps last\n- Reference skill steps by number: \"Completed step 3 of refactor-plan-skill\"\n\n### Subagent Prompt Structure\n\nThe subagent's prompt should have four sections:\n1. **Identity**: \"You are the [role]. You handle [narrow domain]. If outside scope, say so.\"\n2. **Skill usage rules**: \"Your skills define your methods. Decide which apply, follow their workflows.\"\n3. **Task loop**: Restate → Select skills → Clarify → Plan → Execute step-by-step → Validate → Return (artifacts + skills used + remaining risks).\n4. **Constraints**: Quality bar, safety rules, tie-breaking priorities.\n\n**Full templates and orchestration patterns**: See `references/subagent-design.md`\n\n---\n\n## Visual Artifacts: Mermaid Diagrams & Code\n\nSkills that produce visual artifacts — Mermaid diagrams, structured code blocks, annotated tables — are dramatically more useful than prose-only skills. A decision tree rendered as a flowchart is parsed instantly; the same logic in paragraph form forces mental graph reconstruction.\n\n**Rule**: If a skill describes a process, decision tree, architecture, state machine, timeline, or data relationship, render it as a Mermaid diagram.\n\n### Which Diagram for Which Content\n\n| Skill Content | Diagram Type |\n|---------------|-------------|\n| Decision trees / troubleshooting | `flowchart` |\n| Agent/API communication | `sequenceDiagram` |\n| Lifecycle / status transitions | `stateDiagram-v2` |\n| Data models / schemas | `erDiagram` |\n| Temporal knowledge / evolution | `timeline` |\n| Domain taxonomy / concept maps | `mindmap` |\n| Priority matrices (effort vs. impact) | `quadrantChart` |\n| System architecture | `block-beta` or `architecture-beta` |\n| Project timelines | `gantt` |\n\n### Mermaid YAML Frontmatter\n\nEvery Mermaid diagram supports a YAML config block for theming and layout:\n\n````markdown\n```mermaid\n---\ntitle: Decision Flow\nconfig:\n  theme: neutral\n  flowchart:\n    curve: basis\n    nodeSpacing: 50\n---\nflowchart TD\n  A{Skill exists?} -->|Yes| B[Audit]\n  A -->|No| C[Create]\n```\n````\n\nThemes: `default`, `dark`, `forest`, `neutral`, `base` (for full customization via `themeVariables`).\n\n**Full diagram catalog, node shapes, edge styles, and configuration reference**: See `references/visual-artifacts.md`\n\n---\n\n## Encoding Shibboleths\n\nExpert knowledge that separates novices from experts. Things LLMs get wrong due to outdated training data or cargo-culted patterns.\n\n### Shibboleth Template\n\n```markdown\n### Anti-Pattern: [Name]\n**Novice**: \"[Wrong assumption]\"\n**Expert**: [Why it's wrong, with evidence]\n**Timeline**: [Date]: [Old way] → [Date]: [New way]\n**LLM mistake**: [Why LLMs suggest the old pattern]\n**Detection**: [How to spot this in code/config]\n```\n\n### What to Encode\n\n- Framework evolution (React Classes → Hooks → Server Components)\n- Model limitations (CLIP can't count; embedding models are task-specific)\n- Tool architecture (Script → MCP graduation path)\n- API versioning (ada-002 → text-embedding-3-large)\n- Temporal traps (advice that was correct in 2023 but harmful in 2025)\n\n**Full catalog with case studies**: See `references/antipatterns.md`\n\n---\n\n## Self-Contained Tool Decision Matrix\n\n| Need | Tool Type | Key Requirement |\n|------|-----------|-----------------|\n| External API + auth | MCP Server | Working server + setup README |\n| Multi-step orchestration | Subagent | Defined prompt, tools, workflow |\n| Repeatable operation | Script | Actually runs (not a template) |\n| Domain validation | Script | CLI args, error handling, minimal deps |\n| Templates/boilerplate | Assets | Ready to use, no placeholders |\n| Deep reference docs | References | Separate files, loaded on demand |\n\n**Evolution path**: Script → Multiple Scripts → Helper Library → MCP Server. Only promote when complexity justifies it.\n\n**Detailed patterns**: See `references/self-contained-tools.md`\n\n---\n\n## Tool Permissions\n\n**Principle**: Least privilege — only grant what's needed.\n\n| Access Level | `allowed-tools` |\n|-------------|-----------------|\n| Read-only | `Read,Grep,Glob` |\n| File modifier | `Read,Write,Edit` |\n| Build integration | `Read,Write,Bash(npm:*,git:*)` |\n| ⚠️ Never for untrusted | Unrestricted `Bash` |\n\n---\n\n## Anti-Pattern Summary\n\n| # | Anti-Pattern | Fix |\n|---|-------------|-----|\n| 1 | Documentation Dump | Decision trees in SKILL.md, depth in `/references` |\n| 2 | Missing NOT clause | Always include \"NOT for X, Y, Z\" in description |\n| 3 | Phantom Tools | Only reference files that exist and work |\n| 4 | Template Soup | Ship working code or nothing |\n| 5 | Overly Permissive Tools | Least privilege: specific tool list, scoped Bash |\n| 6 | Stale Temporal Knowledge | Date all advice, update quarterly |\n| 7 | Catch-All Skill | Split by expertise type, not domain |\n| 8 | Vague Description | Use `[What] [When] [Keywords]. NOT for [Exclusions]` |\n| 9 | Eager Loading | Never \"read all files first\"; lazy-load references |\n| 10 | Prose-Only Processes | Use Mermaid diagrams for decision trees, workflows, architectures |\n\n**Full case studies**: See `references/antipatterns.md`\n\n---\n\n## Validation Checklist\n\n```\n□ SKILL.md exists and is <500 lines\n□ Frontmatter has name + description (minimum required)\n□ Description follows [What][When][Keywords] NOT [Exclusions] formula\n□ Description uses keywords users would actually type\n□ Name and description are aligned (not contradictory)\n□ At least 1 anti-pattern with shibboleth template\n□ All referenced files actually exist (no phantoms)\n□ Scripts work (not templates), have clear CLI, handle errors\n□ Reference files each have a 1-line purpose in SKILL.md\n□ Decision trees/workflows use Mermaid diagrams, not prose\n□ CHANGELOG.md tracks version history\n□ If subagent-consumed: output contracts are defined\n```\n\n---\n\n## Success Metrics\n\n| Metric | Target | How to Measure |\n|--------|--------|----------------|\n| Correct activation | >90% | Test queries that should trigger |\n| False positive rate | <5% | Test queries that shouldn't trigger |\n| Token usage | <5k | SKILL.md size + typical reference loads |\n| Time to productive | <5 min | User starts working immediately |\n| Anti-pattern prevention | >80% | Users avoid documented mistakes |\n\n---\n\n## Reference Files\n\nConsult these for deep dives — they are NOT loaded by default:\n\n| File | Consult When |\n|------|-------------|\n| `references/description-guide.md` | Writing or rewriting a skill description |\n| `references/antipatterns.md` | Looking for shibboleths, case studies, or temporal patterns |\n| `references/self-contained-tools.md` | Adding scripts, MCP servers, or subagents to a skill |\n| `references/subagent-design.md` | Designing skills for subagent consumption or orchestration |\n| `references/visual-artifacts.md` | Adding Mermaid diagrams: type catalog, YAML config, best practices |\n| `references/mcp-template.md` | Building an MCP server for a skill |\n| `references/subagent-template.md` | Defining subagent prompts and multi-agent pipelines |\n"
    }
  ]
}