{
  "name": "event-detection-temporal-intelligence-expert",
  "type": "folder",
  "path": "event-detection-temporal-intelligence-expert",
  "children": [
    {
      "name": "references",
      "type": "folder",
      "path": "event-detection-temporal-intelligence-expert/references",
      "children": [
        {
          "name": "event-scoring-shareability.md",
          "type": "file",
          "path": "event-detection-temporal-intelligence-expert/references/event-scoring-shareability.md",
          "size": 11000,
          "content": "# Event Significance Scoring & Shareability Prediction\n\n## Event Significance Scoring\n\n**Goal:** Not all events are equal. Birthday party > Daily commute photos.\n\n### Multi-Factor Scoring Model\n\n```python\nclass EventSignificanceScorer:\n    \"\"\"\n    Score how significant/memorable an event is.\n    \"\"\"\n\n    def score_event(self, event_photos, global_corpus):\n        \"\"\"\n        Compute event significance (0-1 scale).\n\n        Args:\n            event_photos: Photos in this event\n            global_corpus: All photos (for rarity comparison)\n\n        Returns:\n            float: Significance score\n            dict: Breakdown of factors\n        \"\"\"\n        factors = {}\n\n        # 1. DURATION: Longer events are more significant\n        duration_hours = self.compute_duration(event_photos)\n        factors['duration'] = min(1.0, duration_hours / 24)  # Cap at 1 day\n\n        # 2. PHOTO DENSITY: More photos = more memorable\n        photos_per_hour = len(event_photos) / max(1, duration_hours)\n        factors['density'] = min(1.0, photos_per_hour / 10)  # Cap at 10/hour\n\n        # 3. VISUAL DIVERSITY: Special events have varied shots\n        visual_diversity = self.compute_visual_diversity(event_photos)\n        factors['diversity'] = visual_diversity\n\n        # 4. PEOPLE PRESENCE: Events with people > landscapes\n        people_ratio = self.count_people_photos(event_photos) / len(event_photos)\n        factors['people'] = people_ratio\n\n        # 5. LOCATION RARITY: Exotic locations > home\n        location_rarity = self.compute_location_rarity(event_photos, global_corpus)\n        factors['location_rarity'] = location_rarity\n\n        # 6. CONTENT RARITY: Landmarks, weddings, celebrations\n        content_rarity = self.detect_special_content(event_photos)\n        factors['content'] = content_rarity\n\n        # 7. USER ENGAGEMENT: Shared/edited photos matter more\n        engagement = self.compute_engagement(event_photos)\n        factors['engagement'] = engagement\n\n        # 8. TEMPORAL RARITY: Annual events (birthdays, holidays)\n        temporal_rarity = self.detect_annual_patterns(event_photos, global_corpus)\n        factors['temporal'] = temporal_rarity\n\n        # Weighted combination\n        significance = (\n            factors['duration'] * 0.10 +\n            factors['density'] * 0.15 +\n            factors['diversity'] * 0.10 +\n            factors['people'] * 0.15 +\n            factors['location_rarity'] * 0.20 +\n            factors['content'] * 0.15 +\n            factors['engagement'] * 0.10 +\n            factors['temporal'] * 0.05\n        )\n\n        return significance, factors\n\n    def compute_visual_diversity(self, event_photos):\n        \"\"\"\n        Measure visual diversity using CLIP embeddings.\n\n        High diversity = special event (many different scenes)\n        Low diversity = mundane (all photos look similar)\n        \"\"\"\n        if len(event_photos) < 2:\n            return 0.0\n\n        embeddings = np.array([p.clip_embedding for p in event_photos])\n\n        # Compute pairwise cosine distances\n        from scipy.spatial.distance import pdist\n        distances = pdist(embeddings, metric='cosine')\n\n        # Mean distance = diversity\n        diversity = np.mean(distances)\n\n        return min(1.0, diversity / 0.5)  # Normalize (0.5 = highly diverse)\n\n    def compute_location_rarity(self, event_photos, global_corpus):\n        \"\"\"\n        How rare is this location in user's photo history?\n\n        Exotic travel locations are rare, home is common.\n        \"\"\"\n        # Get location cluster of event\n        event_location = self.get_median_location(event_photos)\n\n        # Count photos within 10km of this location in entire corpus\n        nearby_count = sum(\n            1 for p in global_corpus\n            if haversine_distance(p.lat, p.lon,\n                                 event_location[0], event_location[1]) < 10000\n        )\n\n        # Rarity = inverse frequency\n        rarity = 1.0 - min(1.0, nearby_count / len(global_corpus))\n\n        return rarity\n\n    def detect_special_content(self, event_photos):\n        \"\"\"\n        Detect special content using CLIP zero-shot classification.\n\n        Special categories: landmarks, weddings, birthdays, concerts, etc.\n        \"\"\"\n        special_categories = {\n            'famous landmark': 0.9,\n            'wedding ceremony': 0.95,\n            'birthday party': 0.85,\n            'concert performance': 0.8,\n            'graduation ceremony': 0.9,\n            'fireworks display': 0.85,\n            'rainbow': 0.8,\n            'northern lights': 0.95,\n            'wildlife': 0.75,\n            'sports event': 0.7,\n        }\n\n        max_score = 0\n        for photo in event_photos[:10]:  # Sample first 10\n            # CLIP zero-shot classification\n            probs = clip_classify(photo.image, list(special_categories.keys()))\n\n            for category, prob in probs.items():\n                if prob > 0.3:  # Confidence threshold\n                    score = special_categories[category] * prob\n                    max_score = max(max_score, score)\n\n        return max_score\n```\n\n### Weight Customization\n\n| Factor | Default Weight | Increase If | Decrease If |\n|--------|---------------|-------------|-------------|\n| duration | 0.10 | User prefers longer trips | Quick events matter more |\n| density | 0.15 | High-activity events | Sparse documentation OK |\n| diversity | 0.10 | Visual variety important | Consistent themes |\n| people | 0.15 | Social photos prioritized | Solo/landscape focus |\n| location_rarity | 0.20 | Travel photos important | Local events matter |\n| content | 0.15 | Special occasions | Everyday moments |\n| engagement | 0.10 | Social media signals | Raw photos only |\n| temporal | 0.05 | Annual patterns important | Random events |\n\n---\n\n## Shareability Prediction\n\n**Goal:** Predict which photos are likely to be shared on social media.\n\n### Feature Categories\n\n1. **Visual Features**: Aesthetic quality, composition, vibrancy, sharpness\n2. **Emotional Features**: Facial expressions, emotion recognition\n3. **Content Features**: People count, landmarks, food, pets\n4. **Temporal Features**: Recency, special dates\n5. **Complexity Features**: Moderate complexity most shareable (2025 research)\n\n### Model Implementation\n\n```python\nclass ShareabilityPredictor:\n    \"\"\"\n    Predict likelihood of photo being shared on social media.\n\n    Based on: \"Predicting Social Media Engagement from Emotional and\n              Temporal Features\" (arXiv 2025)\n    \"\"\"\n\n    def predict(self, photo, event_context=None):\n        \"\"\"\n        Predict shareability score (0-1).\n\n        Args:\n            photo: PhotoPoint with metadata\n            event_context: Optional Event object for context\n\n        Returns:\n            float: Shareability score\n            dict: Feature contributions\n        \"\"\"\n        features = {}\n\n        # VISUAL FEATURES\n        features['aesthetic'] = photo.aesthetic_score\n        features['composition'] = photo.composition_score\n        features['vibrancy'] = self.compute_vibrancy(photo.image)\n        features['sharpness'] = self.compute_sharpness(photo.image)\n\n        # EMOTIONAL FEATURES\n        if photo.has_faces:\n            features['emotion_positive'] = self.detect_positive_emotion(photo)\n        else:\n            features['emotion_positive'] = 0.5  # Neutral\n\n        # CONTENT FEATURES\n        features['people_count'] = min(photo.face_count / 5, 1.0)\n        features['has_landmark'] = 1.0 if photo.has_landmark else 0.0\n        features['has_food'] = 1.0 if self.detect_food(photo) else 0.0\n        features['has_pet'] = 1.0 if photo.has_pet else 0.0\n\n        # TEMPORAL FEATURES\n        days_old = (datetime.now() - photo.timestamp).days\n        features['recency'] = max(0, 1 - days_old / 30)  # Decay over 30 days\n\n        if event_context:\n            features['event_significance'] = event_context.significance_score\n            features['is_special_date'] = 1.0 if self.is_special_date(photo.timestamp) else 0.0\n        else:\n            features['event_significance'] = 0.5\n            features['is_special_date'] = 0.0\n\n        # COMPLEXITY (2025 research finding)\n        complexity = self.compute_visual_complexity(photo.image)\n        # Moderate complexity most shareable (inverted U-curve)\n        features['optimal_complexity'] = 1.0 - abs(complexity - 0.5) * 2\n\n        # Convert to feature vector\n        feature_vector = np.array(list(features.values()))\n\n        # Predict using trained model\n        shareability = self.model.predict(feature_vector.reshape(1, -1))[0]\n\n        return shareability, features\n\n    def compute_visual_complexity(self, image):\n        \"\"\"\n        Compute visual complexity using edge density.\n\n        Research finding: Moderate complexity (0.4-0.6) most shareable.\n        \"\"\"\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n        edges = cv2.Canny(gray, 50, 150)\n        complexity = edges.mean() / 255\n        return complexity\n```\n\n### Training the Model\n\n```python\ndef create_shareability_dataset(user_photos):\n    \"\"\"\n    Create training dataset from user's sharing history.\n\n    Positive examples: Photos user actually shared\n    Negative examples: Photos from same events that weren't shared\n    \"\"\"\n    X = []  # Feature vectors\n    y = []  # 1 = shared, 0 = not shared\n\n    for photo in user_photos:\n        features = extract_features(photo)\n        X.append(features)\n        y.append(1 if photo.was_shared else 0)\n\n    return np.array(X), np.array(y)\n\n\ndef train_shareability_model(X, y):\n    \"\"\"\n    Train gradient boosting model for shareability prediction.\n\n    Uses XGBoost for interpretability and performance.\n    \"\"\"\n    from xgboost import XGBClassifier\n\n    model = XGBClassifier(\n        n_estimators=100,\n        max_depth=5,\n        learning_rate=0.1,\n        objective='binary:logistic'\n    )\n\n    model.fit(X, y)\n    return model\n```\n\n### Shareability Decision Tree\n\n```\nPhoto Shareability Assessment:\n│\n├─ Has smiling faces? ─────────────────────── +0.3 base score\n│   └─ Group photo (3+ people)? ───────────── +0.2 bonus\n│\n├─ Famous landmark detected? ──────────────── +0.25\n│\n├─ Food/dining scene? ─────────────────────── +0.15\n│\n├─ Aesthetic quality > 7/10? ──────────────── +0.2\n│\n├─ Taken within last 7 days? ──────────────── +0.1 recency\n│\n├─ Part of significant event? ─────────────── +0.15\n│\n└─ Moderate visual complexity (0.4-0.6)? ──── +0.1\n\nThreshold: > 0.6 = \"Highly Shareable\"\n```\n\n---\n\n## References\n\n1. \"Predicting Social Media Engagement from Emotional and Temporal Features\" (arXiv, August 2025)\n2. Pinterest engagement prediction research (2025)\n3. Meta intent modeling (2025)\n4. Visual content persuasiveness features (2024)\n"
        },
        {
          "name": "place-recognition-life-events.md",
          "type": "file",
          "path": "event-detection-temporal-intelligence-expert/references/place-recognition-life-events.md",
          "size": 10835,
          "content": "# Place Recognition & Life Event Detection\n\n## Place Recognition & Semantic Location\n\n**Goal:** Understand WHERE photos were taken beyond GPS coordinates.\n\n### Levels of Location Abstraction\n\n1. **Raw GPS:** (40.7589, -73.9851)\n2. **Address:** \"Times Square, New York, NY\"\n3. **Semantic Place:** \"Tourist landmark, entertainment district\"\n4. **User Context:** \"Vacation destination\" vs \"Daily commute\"\n\n### Implementation\n\n```python\nclass PlaceRecognizer:\n    \"\"\"\n    Multi-level place understanding from GPS coordinates.\n    \"\"\"\n\n    def __init__(self):\n        self.reverse_geocoder = self.init_geocoder()  # OpenStreetMap Nominatim\n        self.user_location_history = {}  # Track user's common places\n\n    def analyze_location(self, lat, lon, photo_history):\n        \"\"\"\n        Analyze location at multiple levels.\n\n        Args:\n            lat, lon: GPS coordinates\n            photo_history: User's photo corpus for context\n\n        Returns:\n            dict with place analysis\n        \"\"\"\n        analysis = {}\n\n        # Level 1: Reverse geocoding\n        address = self.reverse_geocode(lat, lon)\n        analysis['address'] = address\n\n        # Level 2: Place categorization\n        place_type = self.categorize_place(address)\n        analysis['place_type'] = place_type\n\n        # Level 3: Frequency in user's history\n        frequency = self.compute_location_frequency(lat, lon, photo_history)\n        analysis['frequency'] = frequency\n\n        # Level 4: User context\n        if frequency > 0.1:\n            analysis['user_context'] = 'familiar'  # Home, work, frequent spots\n        elif frequency > 0.01:\n            analysis['user_context'] = 'occasional'\n        else:\n            analysis['user_context'] = 'novel'  # Travel, rare visit\n\n        # Level 5: Semantic richness\n        analysis['is_landmark'] = self.is_famous_landmark(address)\n        analysis['is_natural'] = 'park' in place_type or 'beach' in place_type\n        analysis['is_urban'] = 'city' in address.lower() or 'downtown' in address.lower()\n\n        return analysis\n\n    def categorize_place(self, address):\n        \"\"\"Categorize place type from address keywords.\"\"\"\n        address_lower = address.lower()\n\n        place_keywords = {\n            'landmark': ['tower', 'monument', 'statue', 'palace', 'temple'],\n            'restaurant': ['restaurant', 'cafe', 'bistro', 'diner'],\n            'park': ['park', 'garden', 'trail', 'forest'],\n            'beach': ['beach', 'coast', 'shore'],\n            'museum': ['museum', 'gallery', 'exhibition'],\n            'venue': ['stadium', 'arena', 'theater', 'concert hall'],\n            'transit': ['airport', 'station', 'terminal'],\n        }\n\n        for place_type, keywords in place_keywords.items():\n            if any(kw in address_lower for kw in keywords):\n                return place_type\n\n        return 'generic'\n```\n\n### Location-Based Event Labeling\n\n```python\ndef label_event_by_location(event_photos, place_recognizer):\n    \"\"\"\n    Automatically label event based on location.\n\n    Examples:\n    - \"Trip to Paris\"\n    - \"Visit to Grandma's House\"\n    - \"Yellowstone National Park\"\n    \"\"\"\n    # Get median location\n    median_lat = np.median([p.lat for p in event_photos])\n    median_lon = np.median([p.lon for p in event_photos])\n\n    # Analyze place\n    place_analysis = place_recognizer.analyze_location(\n        median_lat, median_lon, event_photos\n    )\n\n    # Generate label\n    if place_analysis['is_landmark']:\n        landmark_name = extract_landmark_name(place_analysis['address'])\n        return f\"Visit to {landmark_name}\"\n\n    elif place_analysis['user_context'] == 'novel':\n        city = extract_city(place_analysis['address'])\n        return f\"Trip to {city}\"\n\n    elif place_analysis['user_context'] == 'familiar':\n        return \"At Home\"\n\n    else:\n        return place_analysis['place_type'].title()\n```\n\n---\n\n## Life Event Detection\n\n**Goal:** Automatically detect major life events (graduations, weddings, births, etc.)\n\n### Multi-Signal Detection Approach\n\n```python\nclass LifeEventDetector:\n    \"\"\"\n    Detect major life events from photo collection.\n    \"\"\"\n\n    def detect_life_events(self, photo_corpus):\n        \"\"\"\n        Scan corpus for life events.\n\n        Returns:\n            List of LifeEvent objects\n        \"\"\"\n        life_events = []\n\n        life_events.extend(self.detect_graduations(photo_corpus))\n        life_events.extend(self.detect_weddings(photo_corpus))\n        life_events.extend(self.detect_births(photo_corpus))\n        life_events.extend(self.detect_moves(photo_corpus))\n        life_events.extend(self.detect_travel_milestones(photo_corpus))\n\n        return life_events\n```\n\n### Graduation Detection\n\n**Signals:** Academic regalia, diplomas, ceremony settings\n\n```python\ndef detect_graduations(self, photos):\n    \"\"\"Detect graduation events using CLIP zero-shot classification.\"\"\"\n    graduation_events = []\n\n    for event in self.cluster_events(photos):\n        signals = {\n            'cap_gown': 0,\n            'diploma': 0,\n            'auditorium': 0,\n            'formal_group': 0,\n        }\n\n        for photo in event.photos:\n            probs = clip_classify(photo.image, [\n                'graduation cap and gown',\n                'diploma certificate',\n                'auditorium ceremony',\n                'formal group photo',\n            ])\n\n            for key, prob in zip(signals.keys(), probs):\n                if prob > 0.4:\n                    signals[key] = max(signals[key], prob)\n\n        # Weighted confidence\n        confidence = (\n            signals['cap_gown'] * 0.4 +\n            signals['diploma'] * 0.3 +\n            signals['auditorium'] * 0.2 +\n            signals['formal_group'] * 0.1\n        )\n\n        if confidence > 0.6:\n            graduation_events.append(LifeEvent(\n                type='graduation',\n                timestamp=event.start_time,\n                photos=event.photos,\n                confidence=confidence\n            ))\n\n    return graduation_events\n```\n\n### Wedding Detection\n\n**Signals:** Formal attire, flowers, rings, venue\n\n```python\ndef detect_weddings(self, photos):\n    \"\"\"Detect wedding events.\"\"\"\n    wedding_events = []\n\n    for event in self.cluster_events(photos):\n        signals = clip_classify_batch(event.photos, [\n            'wedding dress and tuxedo',\n            'wedding bouquet',\n            'wedding rings',\n            'wedding ceremony venue',\n            'wedding cake',\n        ])\n\n        avg_signals = np.mean(signals, axis=0)\n        confidence = np.max(avg_signals)\n\n        if confidence > 0.7:\n            wedding_events.append(LifeEvent(\n                type='wedding',\n                timestamp=event.start_time,\n                photos=event.photos,\n                confidence=confidence\n            ))\n\n    return wedding_events\n```\n\n### Birth/Newborn Detection\n\n**Signals:** Hospital setting, newborn, new face cluster appearing\n\n```python\ndef detect_births(self, photos):\n    \"\"\"\n    Detect newborn/birth events.\n\n    Key insight: Look for sudden appearance of new face cluster (newborn)\n    \"\"\"\n    face_clusters = self.face_clusterer.cluster_all_faces(photos)\n    birth_events = []\n\n    for cluster_id, faces in face_clusters.items():\n        first_appearance = min(f.photo.timestamp for f in faces)\n        cluster_duration = (max(f.photo.timestamp for f in faces) -\n                          first_appearance).days\n\n        # Infant detection via CLIP\n        infant_scores = [clip_classify(f.crop, ['infant', 'newborn'])[0]\n                       for f in faces[:10]]\n\n        avg_infant_score = np.mean(infant_scores)\n\n        if avg_infant_score > 0.8 and cluster_duration < 365:\n            birth_events.append(LifeEvent(\n                type='birth',\n                timestamp=first_appearance,\n                photos=[f.photo for f in faces],\n                confidence=avg_infant_score,\n                metadata={'person_cluster_id': cluster_id}\n            ))\n\n    return birth_events\n```\n\n### Residential Move Detection\n\n**Signal:** Sudden permanent shift in common photo location\n\n```python\ndef detect_moves(self, photos):\n    \"\"\"Detect residential moves via location history analysis.\"\"\"\n    location_clusters = self.cluster_by_location(photos)\n    moves = []\n\n    for i in range(len(location_clusters) - 1):\n        cluster_a = location_clusters[i]\n        cluster_b = location_clusters[i + 1]\n\n        distance = haversine_distance(\n            cluster_a.median_location[0], cluster_a.median_location[1],\n            cluster_b.median_location[0], cluster_b.median_location[1]\n        )\n\n        if distance > 50_000:  # 50km = different city\n            duration_b = (cluster_b.photos[-1].timestamp -\n                        cluster_b.photos[0].timestamp).days\n\n            if duration_b > 30:  # Permanent move (&gt;30 days)\n                moves.append(LifeEvent(\n                    type='residential_move',\n                    timestamp=cluster_b.photos[0].timestamp,\n                    photos=cluster_b.photos[:20],\n                    confidence=0.8,\n                    metadata={\n                        'from': self.get_city_name(cluster_a.median_location),\n                        'to': self.get_city_name(cluster_b.median_location),\n                    }\n                ))\n\n    return moves\n```\n\n### Travel Milestone Detection\n\n**Signal:** First visit to new country/continent\n\n```python\ndef detect_travel_milestones(self, photos):\n    \"\"\"Detect first visits to new countries.\"\"\"\n    location_history = {}\n    milestones = []\n\n    for photo in sorted(photos, key=lambda p: p.timestamp):\n        country = self.get_country(photo.lat, photo.lon)\n\n        if country not in location_history:\n            location_history[country] = photo.timestamp\n\n    for country, first_visit in location_history.items():\n        if country != self.user_home_country:\n            milestones.append(LifeEvent(\n                type='travel_milestone',\n                timestamp=first_visit,\n                photos=self.get_photos_in_country(photos, country)[:10],\n                confidence=1.0,\n                metadata={'country': country, 'milestone': 'first_visit'}\n            ))\n\n    return milestones\n```\n\n---\n\n## Life Event Detection Summary\n\n| Event Type | Primary Signals | Confidence Threshold |\n|------------|-----------------|---------------------|\n| Graduation | Cap/gown, diploma, auditorium | 0.6 |\n| Wedding | Formal attire, bouquet, cake | 0.7 |\n| Birth | New infant face cluster, hospital | 0.8 |\n| Residential Move | 50km+ location shift, &gt;30 days | 0.8 |\n| Travel Milestone | First visit to new country | 1.0 |\n\n---\n\n## References\n\n1. GeoNames & OpenStreetMap: Reverse geocoding APIs\n2. Face clustering for person tracking across photos\n3. CLIP zero-shot classification for event content detection\n"
        },
        {
          "name": "st-dbscan-implementation.md",
          "type": "file",
          "path": "event-detection-temporal-intelligence-expert/references/st-dbscan-implementation.md",
          "size": 11114,
          "content": "# ST-DBSCAN Implementation Reference\n\n## Standard DBSCAN Review\n\n**DBSCAN (Density-Based Spatial Clustering of Applications with Noise):**\n\nCore idea: Clusters are dense regions separated by sparse regions.\n\n**Parameters:**\n- ε (epsilon): Maximum distance for neighborhood\n- MinPts: Minimum points to form dense region\n\n```python\ndef dbscan(points, epsilon, min_pts):\n    \"\"\"\n    Standard DBSCAN clustering.\n\n    Args:\n        points: List of data points\n        epsilon: Neighborhood radius\n        min_pts: Minimum points for core point\n\n    Returns:\n        List of cluster labels (-1 = noise)\n    \"\"\"\n    labels = [-1] * len(points)  # -1 = unvisited\n    cluster_id = 0\n\n    for i, point in enumerate(points):\n        if labels[i] != -1:\n            continue  # Already visited\n\n        # Find neighbors within epsilon\n        neighbors = find_neighbors(points, point, epsilon)\n\n        if len(neighbors) < min_pts:\n            labels[i] = -2  # Mark as noise\n        else:\n            # Start new cluster\n            expand_cluster(points, labels, i, neighbors, cluster_id,\n                          epsilon, min_pts)\n            cluster_id += 1\n\n    return labels\n\n\ndef find_neighbors(points, query_point, epsilon):\n    \"\"\"Find all points within epsilon distance.\"\"\"\n    neighbors = []\n    for i, p in enumerate(points):\n        if distance(query_point, p) <= epsilon:\n            neighbors.append(i)\n    return neighbors\n\n\ndef expand_cluster(points, labels, point_idx, neighbors, cluster_id,\n                   epsilon, min_pts):\n    \"\"\"Expand cluster by adding density-reachable points.\"\"\"\n    labels[point_idx] = cluster_id\n\n    queue = list(neighbors)\n    while queue:\n        current_idx = queue.pop(0)\n\n        if labels[current_idx] == -2:  # Was noise\n            labels[current_idx] = cluster_id\n\n        if labels[current_idx] != -1:  # Already processed\n            continue\n\n        labels[current_idx] = cluster_id\n\n        # Find neighbors of current point\n        current_neighbors = find_neighbors(points, points[current_idx], epsilon)\n\n        if len(current_neighbors) >= min_pts:\n            queue.extend(current_neighbors)\n```\n\n## ST-DBSCAN (Spatio-Temporal DBSCAN)\n\n**Innovation:** Separate thresholds for spatial (ε1) and temporal (ε2) dimensions.\n\n**Key Insight:** 100 meters apart in same minute = same event. 100 meters apart 3 days later = different events.\n\n**Modified Neighborhood Definition:**\n\n```\nNε1,ε2(p) = {q | spatial_dist(p, q) ≤ ε1 AND temporal_dist(p, q) ≤ ε2}\n```\n\n**Parameters:**\n- ε1: Maximum spatial distance (meters, e.g., 100m)\n- ε2: Maximum temporal distance (seconds, e.g., 4 hours = 14400s)\n- MinPts: Minimum points for core (e.g., 3 photos)\n\n```python\nfrom datetime import timedelta\nimport numpy as np\n\n@dataclass\nclass PhotoPoint:\n    photo_id: str\n    timestamp: datetime\n    lat: float\n    lon: float\n    # Optional: visual_embedding for content-based clustering\n\n\ndef st_dbscan(photos, eps_spatial_meters, eps_temporal_seconds, min_pts):\n    \"\"\"\n    Spatio-Temporal DBSCAN for photo event detection.\n\n    Based on: \"ST-DBSCAN: An algorithm for clustering spatial-temporal data\"\n              (Birant & Kut, 2007)\n\n    Args:\n        photos: List of PhotoPoint objects\n        eps_spatial_meters: Maximum spatial distance (e.g., 100)\n        eps_temporal_seconds: Maximum temporal distance (e.g., 4 * 3600)\n        min_pts: Minimum photos for event (e.g., 3)\n\n    Returns:\n        List of cluster labels (event IDs), -1 = noise\n    \"\"\"\n    n = len(photos)\n    labels = [-1] * n\n    cluster_id = 0\n\n    for i in range(n):\n        if labels[i] != -1:\n            continue\n\n        # Find spatio-temporal neighbors\n        neighbors = st_neighbors(photos, i, eps_spatial_meters,\n                                eps_temporal_seconds)\n\n        if len(neighbors) < min_pts:\n            labels[i] = -2  # Noise\n        else:\n            expand_st_cluster(photos, labels, i, neighbors, cluster_id,\n                            eps_spatial_meters, eps_temporal_seconds, min_pts)\n            cluster_id += 1\n\n    return labels\n\n\ndef st_neighbors(photos, query_idx, eps_spatial, eps_temporal):\n    \"\"\"\n    Find spatio-temporal neighbors.\n\n    Both spatial AND temporal constraints must be satisfied.\n    \"\"\"\n    query = photos[query_idx]\n    neighbors = []\n\n    for i, photo in enumerate(photos):\n        # Temporal distance\n        time_diff = abs((photo.timestamp - query.timestamp).total_seconds())\n\n        # Spatial distance (Haversine formula for GPS)\n        spatial_dist = haversine_distance(query.lat, query.lon,\n                                         photo.lat, photo.lon)\n\n        # Both constraints must be satisfied\n        if time_diff <= eps_temporal and spatial_dist <= eps_spatial:\n            neighbors.append(i)\n\n    return neighbors\n\n\ndef haversine_distance(lat1, lon1, lat2, lon2):\n    \"\"\"\n    Calculate distance between two GPS coordinates in meters.\n\n    Uses Haversine formula for great-circle distance.\n    \"\"\"\n    R = 6371000  # Earth radius in meters\n\n    phi1 = np.radians(lat1)\n    phi2 = np.radians(lat2)\n    delta_phi = np.radians(lat2 - lat1)\n    delta_lambda = np.radians(lon2 - lon1)\n\n    a = (np.sin(delta_phi / 2) ** 2 +\n         np.cos(phi1) * np.cos(phi2) * np.sin(delta_lambda / 2) ** 2)\n    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n\n    return R * c\n\n\ndef expand_st_cluster(photos, labels, point_idx, neighbors, cluster_id,\n                     eps_spatial, eps_temporal, min_pts):\n    \"\"\"Expand cluster using spatio-temporal connectivity.\"\"\"\n    labels[point_idx] = cluster_id\n\n    queue = list(neighbors)\n    processed = {point_idx}\n\n    while queue:\n        current_idx = queue.pop(0)\n\n        if current_idx in processed:\n            continue\n\n        processed.add(current_idx)\n\n        if labels[current_idx] == -2:  # Was noise, add to cluster\n            labels[current_idx] = cluster_id\n\n        if labels[current_idx] != -1:  # Already in cluster\n            continue\n\n        labels[current_idx] = cluster_id\n\n        # Find neighbors of current point\n        current_neighbors = st_neighbors(photos, current_idx,\n                                        eps_spatial, eps_temporal)\n\n        if len(current_neighbors) >= min_pts:\n            queue.extend(current_neighbors)\n```\n\n## DeepDBSCAN: Integrating Visual Content\n\n**Problem:** ST-DBSCAN only uses time + GPS. What about photos taken at same place/time but of different subjects?\n\n**Example:** Wedding at venue. Some photos are ceremony (important), some are empty chairs during setup (mundane).\n\n**Solution:** Add visual similarity dimension using CLIP embeddings.\n\n**Three-Dimensional Clustering:** Time × Space × Visual Content\n\n```python\ndef deep_st_dbscan(photos, eps_spatial, eps_temporal, eps_visual, min_pts):\n    \"\"\"\n    DeepDBSCAN: ST-DBSCAN + Visual Similarity.\n\n    Based on: \"DeepDBSCAN: Deep Density-Based Clustering for Geo-Tagged Photos\"\n              (ISPRS, 2021)\n\n    Args:\n        photos: List of PhotoPoint with .clip_embedding attribute\n        eps_spatial: Spatial threshold (meters)\n        eps_temporal: Temporal threshold (seconds)\n        eps_visual: Visual similarity threshold (cosine distance)\n        min_pts: Minimum photos for event\n\n    Returns:\n        Cluster labels\n    \"\"\"\n    n = len(photos)\n    labels = [-1] * n\n    cluster_id = 0\n\n    for i in range(n):\n        if labels[i] != -1:\n            continue\n\n        # Find neighbors satisfying ALL THREE constraints\n        neighbors = deep_st_neighbors(photos, i, eps_spatial,\n                                      eps_temporal, eps_visual)\n\n        if len(neighbors) < min_pts:\n            labels[i] = -2  # Noise\n        else:\n            expand_deep_st_cluster(photos, labels, i, neighbors, cluster_id,\n                                  eps_spatial, eps_temporal, eps_visual, min_pts)\n            cluster_id += 1\n\n    return labels\n\n\ndef deep_st_neighbors(photos, query_idx, eps_spatial, eps_temporal, eps_visual):\n    \"\"\"Find neighbors satisfying time, space, AND visual similarity.\"\"\"\n    query = photos[query_idx]\n    neighbors = []\n\n    for i, photo in enumerate(photos):\n        # Temporal constraint\n        time_diff = abs((photo.timestamp - query.timestamp).total_seconds())\n        if time_diff > eps_temporal:\n            continue\n\n        # Spatial constraint\n        spatial_dist = haversine_distance(query.lat, query.lon,\n                                         photo.lat, photo.lon)\n        if spatial_dist > eps_spatial:\n            continue\n\n        # Visual similarity (cosine similarity of CLIP embeddings)\n        visual_sim = cosine_similarity(query.clip_embedding,\n                                       photo.clip_embedding)\n\n        # Convert similarity to distance\n        visual_dist = 1 - visual_sim\n\n        if visual_dist <= eps_visual:\n            neighbors.append(i)\n\n    return neighbors\n\n\ndef cosine_similarity(vec1, vec2):\n    \"\"\"Cosine similarity between two vectors.\"\"\"\n    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n```\n\n## Parameter Tuning Guide\n\n```\neps_spatial:  50m for indoor events, 500m for outdoor festivals, 5km for city tours\neps_temporal: 1 hour for short events, 8 hours for day trips, 24 hours for multi-day\neps_visual:   0.3 for similar subjects (all photos of ceremony), 0.5 for diverse event\nmin_pts:      3 for small gatherings, 10 for large events/trips\n```\n\n## Hierarchical Event Detection\n\n**Problem:** Events have natural hierarchy. \"Paris Vacation\" contains \"Day 1: Louvre Visit\", \"Day 2: Eiffel Tower\", etc.\n\n**Solution:** Multi-level ST-DBSCAN with cascading thresholds.\n\n```python\ndef hierarchical_event_detection(photos):\n    \"\"\"\n    Detect events at multiple temporal scales.\n\n    Returns:\n        Hierarchy of events (tree structure)\n    \"\"\"\n    # Level 1: Multi-day events (vacations, trips)\n    high_level_events = st_dbscan(\n        photos,\n        eps_spatial=50_000,  # 50km (whole city/region)\n        eps_temporal=72 * 3600,  # 3 days\n        min_pts=10\n    )\n\n    event_hierarchy = {}\n\n    # Level 2: Daily events within each high-level event\n    for event_id in set(high_level_events):\n        if event_id == -1:  # Skip noise\n            continue\n\n        # Photos in this high-level event\n        event_photos = [p for i, p in enumerate(photos)\n                       if high_level_events[i] == event_id]\n\n        # Cluster into daily sub-events\n        sub_events = st_dbscan(\n            event_photos,\n            eps_spatial=5000,  # 5km (neighborhood)\n            eps_temporal=12 * 3600,  # 12 hours\n            min_pts=3\n        )\n\n        event_hierarchy[event_id] = {\n            'photos': event_photos,\n            'sub_events': sub_events\n        }\n\n    return event_hierarchy\n```\n\n## References\n\n1. **ST-DBSCAN**: Birant, D., & Kut, A. (2007). \"ST-DBSCAN: An algorithm for clustering spatial-temporal data.\" Data & Knowledge Engineering.\n\n2. **DeepDBSCAN**: \"DeepDBSCAN: Deep Density-Based Clustering for Geo-Tagged Photos\" (ISPRS, 2021)\n\n3. **HDBSCAN**: For hierarchical density-based clustering with automatic parameter selection\n"
        },
        {
          "name": "temporal-diversity-pipeline.md",
          "type": "file",
          "path": "event-detection-temporal-intelligence-expert/references/temporal-diversity-pipeline.md",
          "size": 10899,
          "content": "# Temporal Diversity & Complete Pipeline\n\n## Temporal Diversity for Photo Selection\n\n**Problem:** Without diversity constraints, all photos might come from single event (e.g., all from last vacation).\n\n**Goal:** Ensure temporal spread across photo collection.\n\n### Method 1: Temporal Binning\n\n```python\ndef select_photos_with_temporal_diversity(photos, target_count, bin_size_days=7):\n    \"\"\"\n    Select photos with temporal diversity.\n\n    Ensures photos span entire collection timeframe.\n\n    Args:\n        photos: List of PhotoPoint objects\n        target_count: Number of photos to select\n        bin_size_days: Size of temporal bins (e.g., 7 = one photo per week)\n\n    Returns:\n        Selected photos with temporal spread\n    \"\"\"\n    # Sort by timestamp\n    photos = sorted(photos, key=lambda p: p.timestamp)\n\n    # Find time range\n    min_time = photos[0].timestamp\n    max_time = photos[-1].timestamp\n    total_days = (max_time - min_time).days\n\n    # Create temporal bins\n    num_bins = max(1, total_days // bin_size_days)\n    bins = [[] for _ in range(num_bins)]\n\n    for photo in photos:\n        days_since_start = (photo.timestamp - min_time).days\n        bin_idx = min(days_since_start // bin_size_days, num_bins - 1)\n        bins[bin_idx].append(photo)\n\n    # Select best photo from each bin\n    selected = []\n    photos_per_bin = max(1, target_count // num_bins)\n\n    for bin_photos in bins:\n        if not bin_photos:\n            continue\n\n        # Sort by quality\n        bin_photos.sort(key=lambda p: p.aesthetic_score, reverse=True)\n        selected.extend(bin_photos[:photos_per_bin])\n\n    # If under target, add more from best bins\n    if len(selected) < target_count:\n        remaining = target_count - len(selected)\n        all_remaining = [p for bin in bins for p in bin if p not in selected]\n        all_remaining.sort(key=lambda p: p.aesthetic_score, reverse=True)\n        selected.extend(all_remaining[:remaining])\n\n    return selected[:target_count]\n```\n\n### Method 2: Temporal MMR (Maximal Marginal Relevance)\n\n```python\ndef select_photos_temporal_mmr(photos, target_count, lambda_temporal=0.5):\n    \"\"\"\n    Select photos using MMR with temporal diversity.\n\n    Args:\n        photos: List of PhotoPoint objects\n        target_count: Number to select\n        lambda_temporal: Diversity parameter (0.5 = balanced)\n\n    Returns:\n        Selected photos\n    \"\"\"\n    selected = []\n\n    # Select first photo: highest quality\n    best_photo = max(photos, key=lambda p: p.aesthetic_score)\n    selected.append(best_photo)\n    remaining = [p for p in photos if p != best_photo]\n\n    # Select remaining using MMR\n    for _ in range(target_count - 1):\n        best_score = -float('inf')\n        best_photo = None\n\n        for photo in remaining:\n            # Quality score\n            quality = photo.aesthetic_score\n\n            # Temporal diversity: min distance to selected photos\n            min_time_diff = min(\n                abs((photo.timestamp - s.timestamp).total_seconds())\n                for s in selected\n            )\n\n            # Normalize time diff (closer in time = higher penalty)\n            temporal_diversity = 1 - np.exp(-min_time_diff / (7 * 24 * 3600))\n\n            # MMR score\n            mmr_score = lambda_temporal * quality + (1 - lambda_temporal) * temporal_diversity\n\n            if mmr_score > best_score:\n                best_score = mmr_score\n                best_photo = photo\n\n        if best_photo:\n            selected.append(best_photo)\n            remaining.remove(best_photo)\n\n    return selected\n```\n\n### Method 3: Event-Based Diversity\n\n```python\ndef select_photos_event_diversity(events, photos_per_event=2):\n    \"\"\"\n    Select photos ensuring representation from each significant event.\n\n    Args:\n        events: List of Event objects (from ST-DBSCAN)\n        photos_per_event: Photos to select per event\n\n    Returns:\n        Selected photos\n    \"\"\"\n    selected = []\n\n    # Sort events by significance\n    events.sort(key=lambda e: e.significance_score, reverse=True)\n\n    for event in events:\n        # Sort photos in event by quality\n        event.photos.sort(key=lambda p: p.aesthetic_score, reverse=True)\n        selected.extend(event.photos[:photos_per_event])\n\n    return selected\n```\n\n---\n\n## Complete Event Detection Pipeline\n\n```python\nclass EventDetectionPipeline:\n    \"\"\"\n    End-to-end pipeline for event detection and analysis.\n    \"\"\"\n\n    def __init__(self):\n        self.st_dbscan = ST_DBSCAN()\n        self.event_scorer = EventSignificanceScorer()\n        self.place_recognizer = PlaceRecognizer()\n        self.shareability_predictor = ShareabilityPredictor()\n        self.life_event_detector = LifeEventDetector()\n\n    def process_photo_corpus(self, photos):\n        \"\"\"\n        Process entire photo collection.\n\n        Returns:\n            dict with events, significance scores, shareability, etc.\n        \"\"\"\n        results = {}\n\n        # 1. Cluster photos into events (ST-DBSCAN)\n        event_labels = self.st_dbscan.cluster(\n            photos,\n            eps_spatial=5000,\n            eps_temporal=8 * 3600,\n            min_pts=3\n        )\n\n        # Group photos by event\n        events = self.group_by_event(photos, event_labels)\n\n        # 2. Score each event's significance\n        for event in events:\n            event.significance_score, event.factors = \\\n                self.event_scorer.score_event(event.photos, photos)\n\n            # 3. Analyze location\n            event.place_analysis = self.place_recognizer.analyze_location(\n                event.median_lat, event.median_lon, photos\n            )\n\n            # 4. Generate event label\n            event.label = self.generate_event_label(event)\n\n        # 3. Predict shareability for each photo\n        for photo in photos:\n            event_context = self.find_photo_event(photo, events)\n            photo.shareability, photo.shareability_features = \\\n                self.shareability_predictor.predict(photo, event_context)\n\n        # 4. Detect life events\n        life_events = self.life_event_detector.detect_life_events(photos)\n\n        results['events'] = events\n        results['life_events'] = life_events\n        results['processed_photos'] = photos\n\n        return results\n\n    def select_for_collage(self, processed_results, target_count=100):\n        \"\"\"\n        Select photos for collage using event intelligence.\n\n        Priorities:\n        1. Life events (graduations, weddings, etc.)\n        2. High-significance events (vacations, celebrations)\n        3. High shareability\n        4. Temporal diversity\n        \"\"\"\n        photos = processed_results['processed_photos']\n        events = processed_results['events']\n        life_events = processed_results['life_events']\n\n        selected = []\n\n        # Priority 1: Life events (1-3 photos per life event)\n        for life_event in life_events:\n            life_event.photos.sort(key=lambda p: p.aesthetic_score, reverse=True)\n            selected.extend(life_event.photos[:3])\n\n        # Priority 2: Significant events (2 photos per high-sig event)\n        significant_events = [e for e in events if e.significance_score > 0.7]\n        significant_events.sort(key=lambda e: e.significance_score, reverse=True)\n\n        for event in significant_events[:20]:\n            event.photos.sort(key=lambda p: p.shareability, reverse=True)\n            selected.extend([p for p in event.photos[:2] if p not in selected])\n\n        # Priority 3: Fill remaining with temporal diversity\n        if len(selected) < target_count:\n            remaining_count = target_count - len(selected)\n            remaining_photos = [p for p in photos if p not in selected]\n\n            diverse_photos = select_photos_temporal_mmr(\n                remaining_photos, remaining_count, lambda_temporal=0.7\n            )\n            selected.extend(diverse_photos)\n\n        return selected[:target_count]\n```\n\n---\n\n## Integration with Collage Assembly\n\n**Modify Greedy Edge Growth to Use Event Intelligence:**\n\n```python\ndef assemble_collage_event_aware(photo_database, target_size=(10, 10)):\n    \"\"\"\n    Collage assembly with event-based prioritization.\n    \"\"\"\n    # 1. Run event detection pipeline\n    pipeline = EventDetectionPipeline()\n    event_results = pipeline.process_photo_corpus(photo_database.all_photos)\n\n    # 2. Select diverse photos using event intelligence\n    candidate_photos = pipeline.select_for_collage(event_results, target_count=200)\n\n    # 3. Build collage using greedy edge growth\n    seed = max(candidate_photos, key=lambda p: p.significance * p.aesthetic)\n\n    canvas = Canvas(target_size)\n    canvas.place_photo(seed, position='center')\n\n    placed_events = {seed.event_id}  # Track which events used\n\n    open_edges = PriorityQueue()\n    for edge in seed.edges:\n        open_edges.push(edge, priority=1.0)\n\n    while canvas.coverage < 0.8 and not open_edges.empty():\n        current_edge = open_edges.pop()\n\n        # Find compatible photos, preferring NEW events\n        candidates = photo_database.find_compatible_edges(current_edge, k=50)\n\n        # Filter: prefer photos from events not yet used\n        novel_event_candidates = [c for c in candidates\n                                 if c.event_id not in placed_events]\n\n        if novel_event_candidates:\n            candidates = novel_event_candidates\n\n        # Score candidates\n        for candidate in candidates:\n            local_fit = edge_compatibility(current_edge, candidate.opposite_edge)\n            event_bonus = 1.2 if candidate.event_id not in placed_events else 1.0\n            shareability_bonus = 1.0 + candidate.shareability * 0.2\n\n            total_score = local_fit * event_bonus * shareability_bonus\n\n            if total_score > 0.6:\n                canvas.place_photo(candidate, adjacent_to=current_edge)\n                placed_events.add(candidate.event_id)\n\n                for new_edge in candidate.new_open_edges:\n                    urgency = compute_edge_urgency(new_edge)\n                    open_edges.push(new_edge, priority=urgency)\n\n                break\n\n    canvas.refine_boundaries()\n    return canvas.render()\n```\n\n---\n\n## Performance Benchmarks\n\n**Target Performance (Swift/Metal/Core ML):**\n\n```\nST-DBSCAN (10K photos):          < 2 seconds\nEvent significance scoring:       < 100ms per event\nShareability prediction:          < 50ms per photo\nPlace recognition (cached):       < 10ms per photo\nFull pipeline (10K photos):       < 5 seconds\nEvent-aware collage assembly:     < 15 seconds (100 photos)\n```\n\n---\n\n## Selection Algorithm Comparison\n\n| Method | Best For | Tradeoff |\n|--------|----------|----------|\n| Temporal Binning | Even time coverage | May miss quality |\n| Temporal MMR | Balanced quality + diversity | Slower computation |\n| Event-Based | Event representation | Depends on event quality |\n| Combined Pipeline | Production use | Most comprehensive |\n"
        }
      ]
    },
    {
      "name": "CHANGELOG.md",
      "type": "file",
      "path": "event-detection-temporal-intelligence-expert/CHANGELOG.md",
      "size": 2822,
      "content": "# Changelog: event-detection-temporal-intelligence-expert\n\n## [2.0.0] - 2025-11-26\n\n### Major Refactoring\n- **Reduced SKILL.md from 1662 lines to 310 lines** (81% reduction)\n- Extracted detailed implementations to reference files\n- Added proper skill-coach compliant structure\n\n### Added\n- **Frontmatter**: Updated to `allowed-tools` format with integration points\n- **NOT clause**: Clear boundaries with sister skills\n- **Decision tree**: Quick algorithm selection guide\n- **6 Anti-patterns**: Common mistakes specific to temporal clustering\n  - Time-only clustering\n  - Fixed epsilon values\n  - Ignoring visual content\n  - Euclidean distance for GPS (use Haversine)\n  - No noise handling\n  - Shareability without event context\n- **Quick reference tables**: Parameters, performance targets, method comparisons\n- **Integration points**: Links to collage-layout-expert, photo-content-recognition-curation-expert, color-theory-palette-harmony-expert, clip-aware-embeddings\n\n### Reference Files Created\n- `references/st-dbscan-implementation.md` - Core clustering algorithms\n  - Standard DBSCAN review\n  - ST-DBSCAN (spatio-temporal)\n  - DeepDBSCAN (visual content integration)\n  - Hierarchical event detection\n  - Parameter tuning guide\n\n- `references/event-scoring-shareability.md` - Scoring systems\n  - EventSignificanceScorer class\n  - Multi-factor weighted model (8 factors)\n  - ShareabilityPredictor class\n  - Training methodology\n  - Decision tree for shareability\n\n- `references/place-recognition-life-events.md` - Location intelligence\n  - PlaceRecognizer with multi-level abstraction\n  - Location-based event labeling\n  - LifeEventDetector for major life events\n  - Detection methods: graduation, wedding, birth, moves, travel milestones\n  - CLIP zero-shot classification integration\n\n- `references/temporal-diversity-pipeline.md` - Selection algorithms\n  - Temporal binning method\n  - Temporal MMR (Maximal Marginal Relevance)\n  - Event-based diversity selection\n  - Complete EventDetectionPipeline class\n  - Event-aware collage assembly integration\n  - Performance benchmarks\n\n### Performance Targets (Documented)\n| Operation | Target |\n|-----------|--------|\n| ST-DBSCAN (10K photos) | &lt; 2 seconds |\n| Event significance scoring | &lt; 100ms/event |\n| Shareability prediction | &lt; 50ms/photo |\n| Place recognition (cached) | &lt; 10ms/photo |\n| Full pipeline (10K photos) | &lt; 5 seconds |\n\n### Dependencies\n```\nnumpy scipy scikit-learn hdbscan geopy transformers xgboost pandas opencv-python\n```\n\n## [1.0.0] - 2025-11 (Initial)\n\n### Initial Implementation\n- ST-DBSCAN algorithm for photo event detection\n- Event significance scoring\n- Shareability prediction model\n- Life event detection (graduation, wedding, birth, moves)\n- Place recognition and semantic location\n- Temporal diversity selection methods\n"
    },
    {
      "name": "SKILL.md",
      "type": "file",
      "path": "event-detection-temporal-intelligence-expert/SKILL.md",
      "size": 10792,
      "content": "---\nname: event-detection-temporal-intelligence-expert\ndescription: Expert in temporal event detection, spatio-temporal clustering (ST-DBSCAN), and photo context understanding. Use for detecting photo events, clustering by time/location, shareability prediction, place recognition, event significance scoring, and life event detection. Activate on 'event detection', 'temporal clustering', 'ST-DBSCAN', 'spatio-temporal', 'shareability prediction', 'place recognition', 'life events', 'photo events', 'temporal diversity'. NOT for individual photo aesthetic quality (use photo-composition-critic), color palette analysis (use color-theory-palette-harmony-expert), face recognition implementation (use photo-content-recognition-curation-expert), or basic EXIF timestamp extraction.\nallowed-tools: Read,Write,Edit,Bash,Grep,Glob,mcp__firecrawl__firecrawl_search,WebFetch\ncategory: AI & Machine Learning\ntags:\n  - temporal\n  - clustering\n  - events\n  - spatio-temporal\n  - photo-context\npairs-with:\n  - skill: photo-content-recognition-curation-expert\n    reason: Content + temporal understanding\n  - skill: wedding-immortalist\n    reason: Event detection for wedding albums\n---\n\n# Event Detection & Temporal Intelligence Expert\n\nExpert in detecting meaningful events from photo collections using spatio-temporal clustering, significance scoring, and intelligent photo selection for collages.\n\n## When to Use This Skill\n\n✅ **Use for:**\n- Detecting events from photo timestamps + GPS coordinates\n- Clustering photos by time, location, and visual content (ST-DBSCAN, DeepDBSCAN)\n- Scoring event significance (birthday > commute)\n- Predicting photo shareability for social media\n- Recognizing life events (graduations, weddings, births, moves)\n- Temporal diversity optimization (avoid all photos from one day)\n- Event-aware collage photo selection\n\n❌ **NOT for:**\n- Individual photo aesthetic quality → `photo-composition-critic`\n- Color palette analysis → `color-theory-palette-harmony-expert`\n- Face clustering/recognition → `photo-content-recognition-curation-expert`\n- CLIP embedding generation → `clip-aware-embeddings`\n- Single-photo timestamp extraction (basic EXIF parsing)\n\n## Quick Decision Tree\n\n```\nNeed to group photos into meaningful events?\n├─ Have GPS + timestamps? ──────────────────── ST-DBSCAN\n│   ├─ Also need visual similarity? ────────── DeepDBSCAN (add CLIP)\n│   └─ Need hierarchical events? ───────────── Multi-level cascading\n│\n├─ No GPS, only timestamps? ────────────────── Temporal binning\n│   └─ With visual content? ─────────────────── CLIP + temporal\n│\n└─ Photos have faces + want groups? ─────────── Face clustering first\n    └─ Then event detection per person\n```\n\n## Core Concepts\n\n### 1. ST-DBSCAN: Spatio-Temporal Clustering\n\n**The Problem**: Standard clustering fails for photos—same location on different days shouldn't be grouped.\n\n**Key Insight**: 100 meters apart in same hour = same event. 100 meters apart 3 days later = different events.\n\n**ST-DBSCAN Parameters**:\n```\nε_spatial:   50m (indoor) → 500m (outdoor festival) → 5km (city tour)\nε_temporal:  1hr (short event) → 8hr (day trip) → 24hr (multi-day)\nmin_pts:     3 (small gathering) → 10 (large event)\n```\n\n**Algorithm**: Both spatial AND temporal constraints must be satisfied:\n```\nNeighbor(p) = {q | distance(p,q) ≤ ε_spatial AND |time(p)-time(q)| ≤ ε_temporal}\n```\n\n→ **Deep dive**: `references/st-dbscan-implementation.md`\n\n### 2. DeepDBSCAN: Adding Visual Content\n\n**Problem**: Photos at same time/place can be different subjects (ceremony vs empty chairs).\n\n**Solution**: Add CLIP embeddings as third dimension:\n```\nNeighbor(p) = {q | spatial_ok AND temporal_ok AND cosine_sim(clip_p, clip_q) > threshold}\n```\n\n**eps_visual**: 0.3 (similar subjects) → 0.5 (diverse event content)\n\n### 3. Hierarchical Event Detection\n\n**Use case**: \"Paris Vacation\" contains \"Day 1: Louvre\", \"Day 2: Eiffel Tower\"\n\n**Approach**: Cascade ST-DBSCAN with expanding thresholds:\n1. **High-level** (vacations): eps_spatial=50km, eps_temporal=72hr\n2. **Mid-level** (daily): eps_spatial=5km, eps_temporal=12hr\n3. **Low-level** (moments): eps_spatial=500m, eps_temporal=1hr\n\n---\n\n## Event Significance Scoring\n\n**Goal**: Birthday party > Daily commute photos\n\n**Multi-Factor Model** (weights sum to 1.0):\n\n| Factor | Weight | Description |\n|--------|--------|-------------|\n| location_rarity | 0.20 | Exotic location > home |\n| people_presence | 0.15 | Photos with people score higher |\n| photo_density | 0.15 | More photos/hour = more memorable |\n| content_rarity | 0.15 | Landmarks, celebrations detected via CLIP |\n| visual_diversity | 0.10 | Varied shots = special event |\n| duration | 0.10 | Longer events score higher |\n| engagement | 0.10 | Shared/edited/favorited photos |\n| temporal_rarity | 0.05 | Annual patterns (birthdays, holidays) |\n\n→ **Deep dive**: `references/event-scoring-shareability.md`\n\n---\n\n## Shareability Prediction\n\n**Goal**: Predict which photos will be shared on social media.\n\n**High-Signal Features** (2025 research):\n1. **Smiling faces** (+0.3 base score)\n2. **Group photos** (3+ people, +0.2)\n3. **Famous landmarks** (+0.25)\n4. **Food scenes** (+0.15)\n5. **Moderate visual complexity** (0.4-0.6 optimal)\n6. **Recency** (decays over 30 days)\n\n**Shareability Threshold**: &gt;0.6 = \"Highly Shareable\"\n\n→ **Deep dive**: `references/event-scoring-shareability.md`\n\n---\n\n## Life Event Detection\n\nAutomatically detect major life events using multi-modal signals:\n\n| Event Type | Primary Signals | Threshold |\n|------------|-----------------|-----------|\n| **Graduation** | Cap/gown, diploma, auditorium | 0.6 |\n| **Wedding** | Formal attire, bouquet, cake, rings | 0.7 |\n| **Birth** | New infant face cluster, hospital setting | 0.8 |\n| **Residential Move** | 50km+ location shift, &gt;30 days | 0.8 |\n| **Travel Milestone** | First visit to new country | 1.0 |\n\n→ **Deep dive**: `references/place-recognition-life-events.md`\n\n---\n\n## Temporal Diversity for Selection\n\n**Problem**: Without constraints, collage might be all vacation photos.\n\n### Method Comparison\n\n| Method | Best For | Use When |\n|--------|----------|----------|\n| **Temporal Binning** | Even time coverage | Need chronological spread |\n| **Temporal MMR** | Quality + diversity balance | Balanced selection |\n| **Event-Based** | Event representation | Each event matters |\n\n### Temporal MMR Formula\n\n```\nMMR(photo) = λ × quality + (1-λ) × min_temporal_distance_to_selected\n```\n- λ=0.5: Balanced\n- λ=0.7: Prefer quality\n- λ=0.3: Prefer diversity\n\n→ **Deep dive**: `references/temporal-diversity-pipeline.md`\n\n---\n\n## Common Anti-Patterns\n\n### Anti-Pattern: Time-Only Clustering\n\n**What it looks like**: Using K-means or basic DBSCAN on timestamps only\n```python\nclusters = KMeans(n_clusters=10).fit(timestamps)  # WRONG\n```\n\n**Why it's wrong**: Multi-day trips at same location get split; same-day different-location events get merged.\n\n**What to do instead**: Use ST-DBSCAN with both spatial AND temporal constraints.\n\n### Anti-Pattern: Fixed Epsilon Values\n\n**What it looks like**: Using same eps_spatial=100m for all events\n\n**Why it's wrong**: Indoor events need 50m, city tours need 5km.\n\n**What to do instead**: Adaptive thresholds based on event type detection, or hierarchical clustering with multiple scales.\n\n### Anti-Pattern: Ignoring Visual Content\n\n**What it looks like**: ST-DBSCAN alone for event detection\n\n**Why it's wrong**: Wedding ceremony and empty chairs setup—same time/place, completely different importance.\n\n**What to do instead**: DeepDBSCAN with CLIP embeddings for content-aware clustering.\n\n### Anti-Pattern: Euclidean Distance for GPS\n\n**What it looks like**:\n```python\ndistance = sqrt((lat2-lat1)**2 + (lon2-lon1)**2)  # WRONG\n```\n\n**Why it's wrong**: Degrees ≠ meters. 1° latitude = 111km, but 1° longitude varies by latitude.\n\n**What to do instead**: Haversine formula for great-circle distance:\n```python\nfrom geopy.distance import geodesic\ndistance_meters = geodesic((lat1, lon1), (lat2, lon2)).meters\n```\n\n### Anti-Pattern: No Noise Handling\n\n**What it looks like**: Forcing every photo into a cluster\n\n**Why it's wrong**: Solo commute photos pollute event clusters.\n\n**What to do instead**: DBSCAN naturally identifies noise (label=-1). Keep noise separate—don't force into nearest cluster.\n\n### Anti-Pattern: Shareability Without Event Context\n\n**What it looks like**: Predicting shareability from photo features alone\n\n**Why it's wrong**: A mediocre photo from your wedding is more shareable than a great photo from Tuesday's lunch.\n\n**What to do instead**: Include event significance as feature:\n```python\nfeatures['event_significance'] = photo.event.significance_score\n```\n\n---\n\n## Quick Start: Event Detection Pipeline\n\n```python\nfrom event_detection import EventDetectionPipeline\n\npipeline = EventDetectionPipeline()\n\n# Process photo corpus\nresults = pipeline.process_photo_corpus(photos)\n\n# Access events\nfor event in results['events']:\n    print(f\"{event.label}: {len(event.photos)} photos, significance={event.significance_score:.2f}\")\n\n# Access life events\nfor life_event in results['life_events']:\n    print(f\"{life_event.type} detected on {life_event.timestamp}\")\n\n# Select for collage with diversity\ncollage_photos = pipeline.select_for_collage(results, target_count=100)\n```\n\n---\n\n## Performance Targets\n\n| Operation | Target |\n|-----------|--------|\n| ST-DBSCAN (10K photos) | &lt; 2 seconds |\n| Event significance scoring | &lt; 100ms/event |\n| Shareability prediction | &lt; 50ms/photo |\n| Place recognition (cached) | &lt; 10ms/photo |\n| Full pipeline (10K photos) | &lt; 5 seconds |\n\n---\n\n## Python Dependencies\n\n```\nnumpy scipy scikit-learn hdbscan geopy transformers xgboost pandas opencv-python\n```\n\n---\n\n## Integration Points\n\n- **collage-layout-expert**: Pass event clusters for diversity-aware placement\n- **photo-content-recognition-curation-expert**: Get face clusters before event detection\n- **color-theory-palette-harmony-expert**: Use for visual diversity within events\n- **clip-aware-embeddings**: Generate embeddings for DeepDBSCAN\n\n---\n\n## References\n\n1. **ST-DBSCAN**: Birant & Kut (2007), \"ST-DBSCAN: An algorithm for clustering spatial-temporal data\"\n2. **DeepDBSCAN**: ISPRS 2021, \"Deep Density-Based Clustering for Geo-Tagged Photos\"\n3. **Shareability**: arXiv 2025, \"Predicting Social Media Engagement from Emotional and Temporal Features\"\n4. **GeoNames/OpenStreetMap**: Reverse geocoding for place recognition\n\n---\n\n**Version**: 2.0.0\n**Last Updated**: November 2025\n"
    }
  ]
}