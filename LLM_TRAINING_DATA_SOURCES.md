# Comprehensive Research Report: Training Data Provenance for Modern Large Language Models

## Executive Summary

- **Common data sources** include massive web crawls (Common Crawl, C4), code repositories (The Stack, GitHub), academic papers (ArXiv, S2ORC), books (Books3, Gutenberg), social media platforms (Reddit, X/Twitter), and increasingly, synthetic data generated by LLMs themselves.

- **Transparency varies dramatically** by vendor: Meta's Llama models provide relatively detailed documentation (15T tokens from public sources), while OpenAI, Anthropic, Google, and others disclose minimal specifics about dataset composition, citing competitive concerns.

- **Copyright controversies** have intensified in 2025, particularly around pirated books (Books3 dataset used by Meta, Apple, Salesforce, Snowflake) and opt-out mechanisms, with growing legal challenges and new IETF standards for consent.

- **Data filtering pipelines** have become sophisticated, employing semantic deduplication, PII removal via LLMs, toxicity filtering, and quality classifiers—though research shows significant bias issues persist, particularly in datasets like C4.

- **Synthetic data** has emerged as a dominant source for instruction-tuning, with techniques like distillation, self-instruct, and Evol-Instruct now producing higher-quality training data than human annotations for many tasks.

---

## 1. Common Data Sources for LLM Training

### 1.1 Web Crawls

#### Common Crawl
- The foundational dataset for most modern LLMs
- Freely available web crawl data representing the largest collection of internet text
- Used by OpenAI (GPT-3/4), Google, Anthropic, xAI (Grok), DeepSeek, and many others
- [Confirmed usage by OpenAI up to at least 2023](https://medium.com/@adnanmasood/inside-the-great-ai-data-grab-comprehensive-analysis-of-public-and-proprietary-corpora-utilised-49b4770abc47)
- [CCBot has become the most widely blocked crawler by top 1,000 websites](https://llmrefs.com/blog/cloudflare-blocks-ai-crawlers)

#### C4 (Colossal Clean Crawled Corpus)
- ~750GB of English text from April 2019 Common Crawl snapshot
- Created by Google to train T5 model, now used in LaMDA (12.5% of training corpus)
- [Heavy filtering using 400+ word blocklist from Shutterstock](https://arxiv.org/abs/2104.08758)
- Multilingual variant (mC4) covers 101 languages from 86 Common Crawl dumps
- [Known issues: Disproportionately excludes LGBT+ content and content from Black/Hispanic authors](https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2021/09/dodge2021documentingC4.pdf)

#### RefinedWeb
- 5 trillion tokens extracted from Common Crawl by Technology Innovation Institute (TII)
- Powers Falcon-7B/40B models
- [Public extract of 600B tokens available under ODC-By 1.0 license](https://arxiv.org/abs/2306.01116)
- [Removes ~90% of original web content through Macrodata Refinement Pipeline](https://huggingface.co/papers/2306.01116)
- Demonstrated that web data alone can outperform curated corpora

### 1.2 Books and Published Works

#### Books3 Dataset
- 72,508 pirated ebook titles from Bibliotik torrent tracker
- Part of EleutherAI's The Pile (4.5% of Meta's LLaMA training)
- [Used by Meta (LLaMA), Apple (OpenELM), Salesforce (XGen), Snowflake (Arctic)](https://literaryagentmarkgottlieb.com/blog/the-books3-dataset-controversy-surrounds-unauthorized-use-in-ai-training)
- [EleutherAI removed Books3 from The Pile following 2025 copyright lawsuits](https://aicopyright.substack.com/p/the-books-used-to-train-llms)
- [2025 court ruling found use of pirated materials not fair use](https://www.classaction.org/media/hendrix-et-al-v-apple-inc-complaint_1.pdf)

#### Project Gutenberg
- Public domain works used by Meta's LLaMA and others
- Legitimate source without copyright concerns
- Smaller scale than Books3 but legally unproblematic

#### Licensed Book Datasets
- OpenAI has "data licensed from third-party providers" for GPT-4 (unspecified)
- Anthropic mentions "paid contractors" and user data for Claude
- Specific licensing deals remain undisclosed

### 1.3 Code Repositories

#### The Stack (v1 & v2)
- v1: 6.4TB of permissively licensed code in 384 programming languages, 54GB GitHub issues
- [v2: Seven times larger than v1, largest open code dataset for LLM pretraining](https://github.com/bigcode-project/starcoder)
- Powers StarCoder (15.5B parameters on 1T tokens from 80+ languages)
- [StarCoder training data: 783GB code, 54GB GitHub Issues, 13GB Jupyter notebooks, 32GB commits (~250B tokens)](https://huggingface.co/blog/starcoder)
- Transparent data governance framework with opt-out mechanisms

#### GitHub
- Used by OpenAI for GitHub Copilot (specific dataset undisclosed)
- Code-Cushman-001 (12B parameters) was initial Copilot model
- [Copilot users rely on it for 35% of code written in some languages](https://www.itpro.com/technology/artificial-intelligence/an-open-source-challenger-to-github-copilot-starcoder-2-a-code-generation-tool-backed-by-nvidia-hugging-face-and-servicenow-is-free-to-use-and-offers-support-for-over-600-programming-languages)

#### Code Llama
- Specialized version of Llama 2 on code-specific dataset (composition not disclosed by Meta)

### 1.4 Social Media and Forums

#### Reddit
- [$60M/year licensing deal with Google (February 2024)](https://techcrunch.com/2024/02/22/reddit-says-its-made-203m-so-far-licensing-its-data/)
- [Total licensing contracts worth $203M over 2-3 years](https://www.constellationr.com/blog-news/insights/reddits-data-licensing-play-do-you-want-your-llm-trained-reddit-data)
- Used in OpenAI models (licensed agreement)
- Part of The Pile dataset (used pre-licensing agreements)

#### X (formerly Twitter)
- [xAI's Grok-1 was NOT initially trained on X data (only public internet up to Q3 2023)](https://help.x.com/en/using-x/about-grok)
- [Grok 2/3 now trained on mix of synthetic data and real-world sources including X posts](https://felloai.com/grok-3-by-xai-everything-you-need-to-know-about-elon-musks-latest-ai/)
- [X shares public posts, engagement metrics, Spaces, and profiles with xAI for training](https://x.ai/legal/faq)
- Unique advantage: Real-time access to X posts for up-to-date information

#### Stack Exchange / Stack Overflow
- ~60 million posts from Stack Overflow (pre-June 2023)
- [Public Kaggle dataset: 10% of Stack Overflow Q&A](https://www.kaggle.com/datasets/stackoverflow/stacksample)
- Used in The Pile and multiple coding-focused LLMs
- High-quality technical Q&A data

### 1.5 Academic Papers and Scientific Datasets

#### ArXiv
- 2.4+ million quantitative science articles, mostly LaTeX source
- [Used in RedPajama-Data-1T alongside CommonCrawl, C4, GitHub, Books, Wikipedia, StackExchange](https://arxiv.org/html/2506.05209v1)
- Various licenses depending on author choice

#### S2ORC (Semantic Scholar Open Research Corpus)
- [81.1M English academic papers across disciplines](https://arxiv.org/abs/1911.02782)
- [8.1M open-access papers with structured full text](https://aclanthology.org/2020.acl-main.447.pdf)
- Annotated citations, figures, tables linked to paper objects
- [Privacy concern: Large quantities of email addresses found in dataset](https://arxiv.org/abs/2501.08365)

#### peS2o
- Text extracted from open-access scientific PDFs based on S2ORC
- Used for scientific LLM training

### 1.6 Wikipedia

#### General Wikipedia Datasets
- BERT trained on Wikipedia + Toronto Book Corpus (3.3B tokens)
- Used by virtually all major LLMs for foundational knowledge

#### Wiki-40B
- [~40 billion characters covering 40+ languages from Wikipedia](https://aclanthology.org/2020.lrec-1.297.pdf)
- [Monolingual models trained using Transformer-XL](https://aclanthology.org/2020.lrec-1.297/)
- Accelerates multilingual modeling research

#### Multilingual Training
- [BLOOM uses Wikipedia as part of 366B token multilingual dataset](https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models)

### 1.7 Licensed and Proprietary Datasets

#### News Organizations
- [OpenAI, Anthropic, Meta, Google have licensing deals with major news organizations worth hundreds of millions](https://www.promarket.org/2025/11/19/the-false-hope-of-content-licensing-at-internet-scale/)
- Specific outlets and terms typically undisclosed

#### Shutterstock
- [OpenAI partnership for AI-generated images training](https://cdn.openai.com/gpt-4o-system-card.pdf)

#### Data Partnerships
- [GPT-4o includes "proprietary data from data partnerships" including paywalled content, archives, metadata](https://datasciencedojo.com/blog/gpt4o/)

### 1.8 Synthetic Data

#### Emergence as Primary Source (2025)
- [Synthetic generation has "largely won" for instruction data (SFT)](https://rlhfbook.com/c/15-synthetic)
- Distillation from stronger models produces higher quality than human writers at scale
- DeepSeek incorporated synthetic data extensively

#### Key Techniques

1. **Distillation**: [Larger teacher models (Llama 405B) create training examples for smaller student models](https://www.confident-ai.com/blog/the-definitive-guide-to-synthetic-data-generation-using-llms)

2. **Self-Instruct/SPIN**: [Models generate data from own output iteratively](https://medium.com/foundation-models-deep-dive/synthetic-data-generation-methods-for-llms-3aff1452ce68)

3. **Evol-Instruct**: [Microsoft technique generating 250,000 instructions from 175 human queries](https://www.redhat.com/en/blog/synthetic-data-secret-ingredient-better-language-models)

4. **InstructLab**: [Red Hat/IBM partnership tool for scalable instruction-tuning](https://www.redhat.com/en/blog/synthetic-data-secret-ingredient-better-language-models)

#### Notable Datasets
- [Llama 3.1: 25M synthetically generated examples for fine-tuning](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md)
- OpenThoughts 3 and reasoning datasets for thinking models
- [NVIDIA Nemotron-CC: 1.9T synthetic tokens out of 6.3T total](https://developer.nvidia.com/blog/announcing-nemotron-cc-a-trillion-token-english-language-dataset-for-llm-pretraining/)

### 1.9 Multimodal Data Sources

#### YouTube
- [YouTube-Commons: 2M+ copyright-free video transcripts, ~30B words (Pleias)](https://datainnovation.org/2024/05/transcribing-youtube-videos-for-llm-training/)
- [Live-CC-5M dataset for pre-training, Live-WhisperX-526K for fine-tuning](https://arxiv.org/html/2504.16030v1)
- ASR (automatic speech recognition) transcripts used despite noise vs. visual content

#### Other Multimodal Sources
- Images from Shutterstock (OpenAI partnership)
- [GPT-4o fingerprinted opt-out images and removed from training](https://cdn.openai.com/gpt-4o-system-card.pdf)

### 1.10 Major Curated Collections

#### The Pile
- [886GB diverse English text dataset by EleutherAI](https://pile.eleuther.ai/)
- [22 component sub-datasets including Pile-CC, Wikipedia, PubMed Central, GitHub, Stack Exchange, YouTube, USPTO](https://github.com/EleutherAI/the-pile)
- [Used to train: GPT-Neo, Microsoft Megatron-Turing, Meta's OPT and LLaMA, Galactica, Stanford BioMedLM, Beijing Academy's Chinese-Transformer-XL, Yandex YaLM 100B, Apple OpenELM](https://en.wikipedia.org/wiki/The_Pile_(dataset))

#### Common Corpus
- [2 trillion tokens of permissively licensed multilingual content](https://developer.nvidia.com/blog/announcing-nemotron-cc-a-trillion-token-english-language-dataset-for-llm-pretraining/)
- Largest fully open dataset addressing copyright concerns

#### FineWeb
- [15 trillion tokens from Common Crawl (2013-2024)](https://www.rohan-paul.com/p/selecting-and-preparing-training)
- Advanced processing: aggressive deduplication, heuristic filtering, PII removal

---

## 2. Per-Model Training Data Sources

### 2.1 Anthropic (Claude 3.5/4/Opus 4.5)

#### Disclosed Information
- [Claude Opus 4.5: Training data up to August 2025](https://www.anthropic.com/news/claude-opus-4-5)
- [Claude Sonnet 4.5: Publicly available information through July 2025](https://platform.claude.com/docs/en/about-claude/models/overview)
- [General sources: Internet text, data from paid contractors, Claude user interactions](https://en.wikipedia.org/wiki/Claude_(language_model))

#### Transparency
- [Anthropic has not publicly disclosed granular dataset composition details](https://support.claude.com/en/articles/8114494-how-up-to-date-is-claude-s-training-data)
- Directs to Transparency Hub for more information (specific breakdowns not available)
- More opaque than Meta/Mistral but publishes detailed system cards

#### ClaudeBot Crawler
- [Makes ~71,000 requests for every single referral click sent back](https://llmrefs.com/blog/cloudflare-blocks-ai-crawlers)
- Used for training Claude models on web data

### 2.2 OpenAI (GPT-4/4o/o1/o3)

#### GPT-4 (March 2023)
- [Trained on "large amount of text from public data and data licensed from third-party providers"](https://en.wikipedia.org/wiki/GPT-4)
- [Multimodal data from open internet + paid/licensed data](https://research.aimultiple.com/gpt-5/)
- [Leaked reports suggest: Common Crawl & RefinedWeb totaling 13T tokens](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/)
- Speculation: Twitter, Reddit, YouTube, textbooks (unconfirmed)

#### GPT-4o (August 2024)
- [Pre-trained on data up to October 2023](https://cdn.openai.com/gpt-4o-system-card.pdf)
- [Sources: Industry-standard ML datasets, web crawls, proprietary data partnerships](https://openai.com/index/gpt-4o-system-card/)
- [Includes paywalled content, archives, metadata from partnerships](https://datasciencedojo.com/blog/gpt4o/)
- [Web data from public pages, code and math data for reasoning](https://cdn.openai.com/gpt-4o-system-card.pdf)
- [Image fingerprinting: Removed all instances of user opt-out images](https://openai.com/index/gpt-4o-system-card/)

#### GPTBot Crawler
- [Confirmed use of Common Crawl data up to 2023](https://medium.com/@adnanmasood/inside-the-great-ai-data-grab-comprehensive-analysis-of-public-and-proprietary-corpora-utilised-49b4770abc47)
- Widely blocked by websites

#### Transparency
- [OpenAI has not revealed technical details like model size or precise dataset composition](https://en.wikipedia.org/wiki/GPT-4)
- Most opaque among major providers

### 2.3 Google (Gemini 1.5/2.0)

#### General Training Data
- [Multimodal and multilingual: Web documents, books, code, images, audio, video](https://gemini.google/overview/)
- [Quality filters applied using heuristic rules and model-based classifiers](https://theanilbajar.medium.com/all-about-gemini-models-and-training-process-989fc3e25602)
- [SentencePiece tokenizer, massive scale multimodal data from web documents, books, code + Google internal sources](https://www.linkedin.com/pulse/google-gemini-model-training-dataset-debjyoti-saha-5rowe)

#### Gemini 2.0 (2024)
- [Pre-training data cutoff: June 2024](https://ritvik19.medium.com/papers-explained-393-gemini-2-5-3b8877cf4da9)
- [Large-scale diverse collection: Web-documents, code (multiple languages), images, audio, video](https://ritvik19.medium.com/papers-explained-393-gemini-2-5-3b8877cf4da9)
- [New methods for improved data quality filtering and deduplication vs. Gemini 1.5](https://ritvik19.medium.com/papers-explained-393-gemini-2-5-3b8877cf4da9)
- [Intensified focus on greater volume/diversity of code from repository and web sources](https://ritvik19.medium.com/papers-explained-393-gemini-2-5-3b8877cf4da9)

#### Gemini for Google Cloud
- [Trained on publicly available code, Google Cloud-specific material, technical information + foundation model datasets](https://docs.cloud.google.com/gemini/docs/discover/works)

#### Transparency
- [Specific sources and "publicly accessible" definition remain unclear](https://scalebytech.com/google-gemini-ai-training-dataset-composition/)
- [No details on inclusions/exclusions or PII protection precautions](https://web.swipeinsight.app/posts/google-s-gemini-training-data-sources-a-deep-dive-into-tech-companies-transparency-5846)
- More transparent than OpenAI but less than Meta

### 2.4 xAI (Grok 2/3)

#### Grok-1 (Initial Release)
- [Pre-trained on diverse text from publicly available internet repositories up to Q3 2023](https://help.x.com/en/using-x/about-grok)
- [Curated datasets by human AI Tutors](https://medium.com/@serverwalainfra/grok-ai-and-real-time-learning-how-it-leverages-x-for-up-to-date-responses-01d7148fc041)
- [NOT initially trained on X data, including public posts](https://en.wikipedia.org/wiki/Grok_(chatbot))

#### Grok 2/3
- [Trained on mix of synthetic data and real-world sources including X posts](https://felloai.com/grok-3-by-xai-everything-you-need-to-know-about-elon-musks-latest-ai/)
- [Grok 3: "10x" more computing power than Grok-2, expanded dataset including legal filings](https://felloai.com/grok-3-by-xai-everything-you-need-to-know-about-elon-musks-latest-ai/)

#### X/Twitter Data Usage
- [X shares public posts, metadata (engagement, reposts), public Spaces, public profiles with xAI](https://x.ai/legal/faq)
- [Unique feature: Real-time search of X public posts for up-to-date information](https://owlead.com/what-is-grok-on-x/)

#### Data Processing
- [Quality filters remove violent content](https://www.byteplus.com/en/topic/407704)
- [Attempts to minimize processing of personal/sensitive data](https://www.byteplus.com/en/topic/407704)

### 2.5 Meta (Llama 3/3.1/4)

#### Llama 3 (April 2024)
- [Pre-trained on 15+ trillion tokens from publicly available sources](https://ai.meta.com/blog/meta-llama-3/)
- [7x larger than Llama 2 dataset](https://kili-technology.com/large-language-models-llms/llama-3-guide-everything-you-need-to-know-about-meta-s-new-model-and-its-data)
- [Pre-training data cutoff: December 2023](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)
- [Fine-tuning: Publicly available instruction datasets + 10M+ human-annotated examples](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)

#### Llama 3.1 (July 2024)
- [~15 trillion tokens from publicly available sources](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md)
- [Fine-tuning: Publicly available instruction datasets + 25M+ synthetically generated examples](https://kili-technology.com/large-language-models-llms/llama-3-1-guide-what-to-know-about-meta-s-new-405b-model-and-its-data)

#### Llama 4 (April 2025)
- Recently released, training data documentation not yet available in search results

#### Data Curation
- [Series of data-filtering pipelines: Heuristic filters, NSFW filters, semantic deduplication, text-quality classifiers](https://huggingface.co/meta-llama/Meta-Llama-3-8B)
- [Previous Llama generations used to generate training data for text-quality classifiers](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)

#### Transparency
- [Most transparent among major providers with detailed model cards](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)
- [Criticism: Term "open-source" misleading - "no source to be seen, training data entirely undocumented"](https://en.wikipedia.org/wiki/LLaMA)
- High-level information provided but specific dataset compositions not disclosed

### 2.6 Mistral AI (Large/Medium)

#### Training Data Disclosure
- [Mistral AI does not disclose datasets used to train its models](https://help.mistral.ai/en/articles/347390-does-mistral-ai-disclose-its-training-datasets)
- Maintains proprietary control over training data sources

#### Company Background
- [France-based AI startup, founded April 2023](https://www.ibm.com/think/topics/mistral-ai)
- [Co-founders from Google DeepMind (Arthur Mensch) and Meta AI (Guillaume Lample, Timothée Lacroix)](https://www.ibm.com/think/topics/mistral-ai)
- Known for open-source LLMs and European AI leadership

#### Custom Training Services
- [Offers custom model training/fine-tuning on proprietary datasets](https://mistral.ai/solutions/custom-model-training)
- [Data sovereignty focus: Models can run entirely within customer infrastructure](https://datanorth.ai/blog/the-complete-guide-to-mistral-ai)
- Addresses regulated industries (healthcare, finance, government)

#### European Context
- [EuroLLM initiative supports 24 official EU languages + 11 more](https://www.marktechpost.com/2025/08/15/europes-top-ai-models-of-2025-multilingual-open-and-enterprise-ready/)
- Uses synthetic datasets and EuroFilter technology for language balancing

### 2.7 Cohere (Command R+)

#### Training Data
- [Trained on massive corpus of diverse texts in multiple languages](https://cohere.com/command)
- [Particular focus on critical business use-cases](https://docs.cohere.com/docs/command-r-plus)
- Specific "enterprise sources" in training data not disclosed

#### Multilingual Focus
- [Trained on data covering dozens of languages](https://www.datacamp.com/tutorial/cohere-command-r-tutorial)
- [Optimized for 10 "key business languages": English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Chinese](https://www.geeksforgeeks.org/r-language/cohere-command-r/)

#### Training Methodology
- [Pre-training on broad corpus → Supervised fine-tuning (SFT) → Preference tuning (similar to RLHF)](https://www.maginative.com/article/cohere-unveils-command-r-a-powerful-rag-optimized-llm-for-enterprise-ai/)
- [Fine-tuned for retrieval-augmented generation (RAG) with citation capabilities](https://cohere.com/blog/command-r)

#### Data Privacy
- [Committed to data privacy/security, offers opt-out options](https://www.theregularizer.com/blog/command-r-cohere)
- Exact training data composition not publicly disclosed

### 2.8 Amazon (Titan)

#### Training Data Sources
- [Pre-trained using curated data from variety of sources: Licensed/proprietary data, open source datasets, publicly available data](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-models.html)
- [Trained on many programming languages, rich text formats (tables, JSON, CSV)](https://www.cloudoptimo.com/blog/understanding-amazon-titan-large-language-models-for-aws)

#### Training Process
- [Three stages: Pre-training → Fine-tuning (SFT + RLHF) → Safety filtering](https://docs.aws.amazon.com/ai/responsible-ai/titan-text/overview.html)
- [Safety filters tuned to block harmful prompts/responses (privacy-protecting, profanity-blocking)](https://docs.aws.amazon.com/ai/responsible-ai/titan-text/overview.html)

#### Transparency
- [AWS has not publicly disclosed training datasets or model sizes](https://crfm.stanford.edu/fmti/May-2024/company-reports/Amazon_Titan%20Text%20Express.html)
- Balanced approach prioritizing predictable output quality and responsible deployment
- Emphasis on responsible AI principles for bias reduction and data quality

#### NVIDIA Partnership
- [NVIDIA powers training for some of largest Amazon Titan foundation models](https://blogs.nvidia.com/blog/nemo-amazon-titan/)

### 2.9 DeepSeek (Chinese LLM)

#### Training Data Scale
- [DeepSeek LLM: 2 trillion tokens in English and Chinese](https://github.com/deepseek-ai/DeepSeek-LLM)
- [DeepSeek-V2: 8.1 trillion tokens from high-quality, multi-source corpus](https://www.byteplus.com/en/topic/406251)
- [DeepSeek-Coder-V2: 10.2 trillion tokens specialized for code intelligence](https://www.analyticsvidhya.com/blog/2023/12/deepseek-china-latest-language-model/)

#### Data Composition Strategy
- [General-purpose models: Diverse mix with extensive Chinese data for strong multilingual performance](https://www.secondtalent.com/resources/chinese-open-source-llms-ai-leaders/)
- [Specialized models: Data heavily skewed toward code/math for coding assistants](https://www.secondtalent.com/resources/chinese-open-source-llms-ai-leaders/)
- [DeepSeek-V3: Extensive training in Chinese and English, supports complex cross-lingual tasks](https://www.deepseek.com/en/)

#### Synthetic Data
- [Incorporated synthetic data for training, contributing to low development cost](https://contineo.world/self-training-ai-how-synthetic-data-is-powering-the-next-generation-of-llms/)

#### Transparency
- Bilingual focus documented
- Specific Chinese vs. English content breakdown not disclosed
- Domain-specific curation approaches outlined but source details limited

---

## 3. Data Filtering and Curation Methods

### 3.1 Deduplication

#### Exact and Fuzzy Matching
- [Standard approaches to prevent model over-memorization](https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/)
- [Removes duplicate and highly redundant texts](https://www.rohan-paul.com/p/selecting-and-preparing-training)

#### Semantic Deduplication
- [Ensures models don't overfit on semantically identical but lexically different text](https://www.rohan-paul.com/p/selecting-and-preparing-training)
- More sophisticated than simple string matching

#### SoftDedup (2024)
- [Novel method that reweights recurring data rather than deleting](https://www.rohan-paul.com/p/selecting-and-preparing-training)
- [Achieves same perplexity with 26% fewer training steps](https://www.rohan-paul.com/p/selecting-and-preparing-training)
- [Boosts downstream accuracy by ~1.8%](https://www.researchgate.net/publication/361073536_Deduplicating_Training_Data_Makes_Language_Models_Better)

#### Implementation
- [RefinedWeb removes ~90% of original content through deduplication + filtering](https://arxiv.org/abs/2306.01116)
- [Gemini 2.0 uses new methods for improved deduplication vs. 1.5](https://ritvik19.medium.com/papers-explained-393-gemini-2-5-3b8877cf4da9)

### 3.2 Quality Filtering

#### Heuristic Filters
- [Meta's Llama 3: Heuristic filters, NSFW filters, semantic deduplication, text-quality classifiers](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)
- [Google Gemini: Quality filters using heuristic rules and model-based classifiers](https://theanilbajar.medium.com/all-about-gemini-models-and-training-process-989fc3e25602)

#### Text-Quality Classifiers
- [Meta: Previous Llama generations used to train quality classifiers for Llama 3](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)
- Models themselves used to filter training data

#### FineWeb Pipeline (Hugging Face)
- [Aggressive deduplication, heuristic filtering, PII removal from Common Crawl (2013-2024)](https://www.rohan-paul.com/p/selecting-and-preparing-training)
- Demonstrates comprehensive filtering approach

### 3.3 PII (Personally Identifiable Information) Removal

#### Traditional Methods
- Regular expressions for pattern matching (phone numbers, emails, SSNs)
- [Limited accuracy, prone to false positives/negatives](https://labelbox.com/blog/how-to-use-llms-to-detect-and-extract-personal-data-from-ai-datasets/)

#### LLM-Based PII Detection (2024-2025)
- [Pre-trained LLM models using prompt engineering to detect/remove PII](https://labelbox.com/blog/how-to-use-llms-to-detect-and-extract-personal-data-from-ai-datasets/)
- [More accurate and efficient than traditional regex approaches](https://www.holisticai.com/blog/managing-personal-data-in-large-language-models)

#### NeMo Curator
- [NVIDIA's GPU-accelerated pipeline integrates PII redaction modules](https://cfp.scipy.org/scipy2025/talk/LEHUMF/)
- [Ensures compliance with data privacy regulations](https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/)

#### Known Issues
- [Staggering amounts of personal data remain: Phone numbers in RedPajama, emails in S2ORC/peS2o, IP addresses in The Stack](https://arxiv.org/abs/2501.08365)
- PII removal claims often aspirational rather than fully achieved

#### GPT-4o Approach
- [Fingerprinted images that users opted out, removed all instances from training dataset](https://cdn.openai.com/gpt-4o-system-card.pdf)
- More sophisticated than simple filtering

### 3.4 Toxicity and NSFW Filtering

#### Blocklist Approaches
- [C4: Removed pages containing any of 400+ "dirty words" from Shutterstock blocklist](https://arxiv.org/abs/2104.08758)
- [Resulted in unintended exclusion of LGBT+ content and content by Black/Hispanic authors](https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2021/09/dodge2021documentingC4.pdf)
- [Yet 72,000+ instances of "swastika" remained, along with pornographic/offensive content](https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/c4-dataset)

#### Automatic Toxicity Classifiers
- [Used to remove hate speech, profanity, harassment](https://www.rohan-paul.com/p/selecting-and-preparing-training)
- More nuanced than simple blocklists

#### IBM Granite HAP Filter (2024)
- [Lightweight 38M parameter model runs real-time on CPU](https://www.rohan-paul.com/p/selecting-and-preparing-training)
- Catches Hate/Abuse/Profanity in training data

#### xAI Grok
- [Quality filters remove violent content](https://www.byteplus.com/en/topic/407704)
- [Minimizes processing of personal/sensitive data](https://www.byteplus.com/en/topic/407704)

### 3.5 Content Exclusion Controversies

#### Bias in C4
- [Disproportionately excludes non-sexual, non-offensive LGBT+ content](https://aclanthology.org/2020.lrec-1.297/)
- [Excludes content associated with Black and Hispanic authors](https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2021/09/dodge2021documentingC4.pdf)
- [Source sites include anti-trans perspectives, white supremacists, QAnon, misinformation](https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/c4-dataset)

#### Language and Regional Bias
- Most datasets heavily English-centric
- Multilingual efforts (Wiki-40B, mC4, EuroLLM) attempt to address but imbalances remain

### 3.6 Data Unlearning

#### Emerging Technique
- [Essential privacy safeguarding method to strip away PII post-training](https://www.researching.cn/articles/OJba348e2553344135)
- Allows targeted removal of specific information without full retraining

---

## 4. Opt-Out Mechanisms and Data Governance

### 4.1 robots.txt for AI Crawlers

#### Functionality
- [Defines how web crawlers may access sites, signals AI systems about allowed/disallowed content](https://robotstxt.com/ai)
- [Respected by all well-behaved bots including search engines and many AI companies](https://neilpatel.com/blog/llms-txt-files-for-seo/)

#### Major AI Crawler User-Agents (2025)
- OpenAI: GPTBot
- Google: Google-Extended
- Anthropic: ClaudeBot
- Apple: Applebot-Extended
- Meta: Meta-ExternalAgent
- Others: CCBot, PerplexityBot, and many more

[Source: Privacy Journal](https://www.privacyjournal.net/block-llm-crawlers/)

#### Blocking Methods
```
User-agent: GPTBot
Disallow: /

User-agent: ClaudeBot
Disallow: /
```
[Source: SuperGeekery](https://supergeekery.com/blog/ai-and-your-content-how-to-opt-to-opt-out)

### 4.2 LLMs.txt - Emerging Standard

#### Purpose
- [Controls how AI crawlers use content for model training, focuses on AI data usage rather than traditional crawling](https://neilpatel.com/blog/llms-txt-files-for-seo/)
- [Functions similarly to robots.txt but is advisory signal, not enforcement mechanism](https://www.ndash.com/blog/llms-txt-explained-a-shared-guide-for-marketers-and-freelancers)

#### Adoption Status
- [Major LLM providers rapidly adopting, creating clearer standard for consent](https://medium.com/@franciscokemeny/best-practices-for-ai-oriented-robots-txt-and-llms-txt-configuration-be564ba5a6bd)
- [Google explicitly said it doesn't support llms.txt](https://neilpatel.com/blog/llms-txt-files-for-seo/)
- [No clear evidence AI companies follow llms.txt or honor its rules](https://www.ndash.com/blog/llms-txt-explained-a-shared-guide-for-marketers-and-freelancers)

### 4.3 New Standards (2025)

#### IETF AI Preferences Working Group
- [Launched January 2025 to create standardized, machine-readable rules](https://searchengineland.com/robots-exclusions-new-rules-definitions-ietf-464990)
- Lets site owners spell out how (or if) AI systems can use their content
- Addresses enforcement gaps in current opt-out mechanisms

### 4.4 Compliance Issues

#### robots.txt Limitations
- [Advisory with no enforcement mechanism, malicious crawlers can ignore](https://blog.openreplay.com/ai-crawlers-block-robots-txt/)
- [Multiple reports of AI services ignoring rules in robots.txt](https://blog.openreplay.com/ai-crawlers-block-robots-txt/)
- [OpenAI could be simply ignoring it](https://www.privacyjournal.net/block-llm-crawlers/)

#### Cloudflare Response
- [July 1, 2025: Every new Cloudflare domain blocks all known AI crawlers by default](https://llmrefs.com/blog/cloudflare-blocks-ai-crawlers/)
- [Changed how 20% of public web interacts with AI systems](https://llmrefs.com/blog/cloudflare-blocks-ai-crawlers/)
- Bot Fight Mode tries to block AI bots from scraping

#### Blocking Trends
- [CCBot most widely blocked crawler by top 1,000 websites, surpassing OpenAI's GPTBot](https://medium.com/@adnanmasood/inside-the-great-ai-data-grab-comprehensive-analysis-of-public-and-proprietary-corpora-utilised-49b4770abc47)

### 4.5 Research on Opt-Out Impact

#### Performance Studies (January 2025)
- [Experiments with 1.5B models show opt-out compliance does not degrade general knowledge acquisition (close to 0% DCG)](https://arxiv.org/html/2504.06219v1)
- [Suggests AI companies could respect opt-outs without significant performance impacts](https://arxiv.org/html/2504.06219v1)

---

## 5. Dataset Sizes and Token Counts

### 5.1 Major Model Training Scales (2025)

| Model | Training Tokens | Notes |
|-------|----------------|-------|
| **GPT-4** | ~5T tokens (estimated) | [Leaked: CommonCrawl & RefinedWeb = 13T tokens](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/) |
| **GPT-4o** | Not disclosed | [Cutoff: October 2023](https://cdn.openai.com/gpt-4o-system-card.pdf) |
| **Claude Opus 4.5** | Not disclosed | [Data up to August 2025](https://www.anthropic.com/news/claude-opus-4-5) |
| **Gemini 2.0** | Not disclosed | [Cutoff: June 2024](https://ritvik19.medium.com/papers-explained-393-gemini-2-5-3b8877cf4da9) |
| **Llama 3** | 15T tokens | [7x larger than Llama 2](https://ai.meta.com/blog/meta-llama-3/) |
| **Llama 3.1** | ~15T tokens | [+ 25M synthetic examples for fine-tuning](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md) |
| **DeepSeek LLM** | 2T tokens | [English and Chinese](https://github.com/deepseek-ai/DeepSeek-LLM) |
| **DeepSeek-V2** | 8.1T tokens | [Multi-source corpus](https://www.byteplus.com/en/topic/406251) |
| **DeepSeek-Coder-V2** | 10.2T tokens | [Specialized for code](https://www.analyticsvidhya.com/blog/2023/12/deepseek-china-latest-language-model/) |

### 5.2 Major Dataset Sizes

| Dataset | Size | Notes |
|---------|------|-------|
| **Common Crawl** | Petabytes | [Monthly snapshots of web](https://medium.com/@adnanmasood/inside-the-great-ai-data-grab-comprehensive-analysis-of-public-and-proprietary-corpora-utilised-49b4770abc47) |
| **RefinedWeb** | 5T tokens | [600B public extract](https://arxiv.org/abs/2306.01116) |
| **FineWeb** | 15T tokens | [From Common Crawl 2013-2024](https://www.rohan-paul.com/p/selecting-and-preparing-training) |
| **C4** | ~750GB | [From single April 2019 CC snapshot](https://arxiv.org/abs/2104.08758) |
| **The Pile** | 886GB | [22 component datasets](https://pile.eleuther.ai/) |
| **Common Corpus** | 2T tokens | [Largest fully open multilingual](https://developer.nvidia.com/blog/announcing-nemotron-cc-a-trillion-token-english-language-dataset-for-llm-pretraining/) |
| **Nemotron-CC** | 6.3T tokens | [1.9T synthetic + 4.4T other](https://developer.nvidia.com/blog/announcing-nemotron-cc-a-trillion-token-english-language-dataset-for-llm-pretraining/) |
| **The Stack v1** | 6.4TB | [384 programming languages](https://github.com/bigcode-project/starcoder) |
| **The Stack v2** | ~45TB | [7x larger than v1](https://github.com/bigcode-project/starcoder) |
| **LLaMA (original)** | 1.4T tokens (4.6TB) | [Foundation for series](https://www.glennklockwood.com/garden/LLM-training-datasets) |

### 5.3 Total Available Data Estimates

#### Human-Generated Text
- [Effective stock of quality, repetition-adjusted public text: ~300T tokens](https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data)
- [90% confidence interval: 100T to 1000T tokens](https://arxiv.org/html/2211.04325v2)
- [Language models will fully utilize this stock between 2026-2032](https://www.educatingsilicon.com/2024/05/09/how-much-llm-training-data-is-there-in-the-limit/)

#### Inference Scale (2025)
- [OpenRouter study: 100 trillion token dataset of LLM usage](https://openrouter.ai/state-of-ai)
- Demonstrates massive inference activity versus training

---

## 6. RLHF and Fine-Tuning Datasets

### 6.1 Anthropic's HH-RLHF Dataset

#### Composition
- [161k conversations between humans and AI assistants](https://huggingface.co/datasets/Anthropic/hh-rlhf)
- [Human preference data about helpfulness and harmlessness](https://github.com/anthropics/hh-rlhf)
- Intended for training preference/reward models for subsequent RLHF

#### Availability
- [Publicly available on HuggingFace](https://huggingface.co/datasets/Anthropic/hh-rlhf)
- Includes human-generated red teaming data

#### Usage
- [Anthropic's alignment research used Transformer models from 10M to 52B parameters](https://intuitionlabs.ai/articles/reinforcement-learning-human-feedback)

### 6.2 InstructGPT Dataset (OpenAI)

#### Scale
- [50,000 prompts with 4-9 responses each](https://huyenchip.com/2023/05/02/rlhf.html)
- [6-36 pairs of (winning_response, losing_response) per prompt](https://huyenchip.com/2023/05/02/rlhf.html)
- [300K to 1.8M training examples total](https://huyenchip.com/2023/05/02/rlhf.html)

### 6.3 OpenAI Summary Dataset

#### Composition
- [Reddit posts with two summaries each, human preferences annotated](https://huyenchip.com/2023/05/02/rlhf.html)
- [117k training samples, 13k validation samples](https://huyenchip.com/2023/05/02/rlhf.html)

### 6.4 RLHF Adoption (2025)

#### Widespread Use
- [OpenAI ChatGPT, InstructGPT, DeepMind Sparrow, Google Gemini, Anthropic Claude all use RLHF](https://www.ibm.com/think/topics/rlhf)
- Fundamental technique for aligning LLMs with human preferences

---

## Conclusion

### Key Findings

1. **Web crawls dominate pre-training**: Common Crawl and its derivatives (C4, RefinedWeb, FineWeb) form the backbone of most LLMs, with datasets ranging from hundreds of GB to 15+ trillion tokens.

2. **Transparency is inversely correlated with commercial success**: Meta (Llama) provides the most detailed documentation, while market leaders OpenAI and Anthropic disclose minimal specifics. Google falls in the middle.

3. **Synthetic data has revolutionized fine-tuning**: In 2025, distillation and self-instruct methods produce higher-quality instruction data than human annotations for most tasks, with models like Llama 3.1 using 25M synthetic examples.

4. **Copyright issues remain unresolved**: The Books3 controversy, ongoing lawsuits against Meta/Apple/Salesforce/Snowflake, and questions about fair use continue to create legal uncertainty around training data provenance.

5. **Opt-out mechanisms are ineffective**: Despite robots.txt and emerging llms.txt standards, compliance is voluntary and widely ignored. Research shows opt-out compliance wouldn't degrade model performance, yet adoption remains poor.

6. **Data filtering is imperfect**: PII removal claims often fail (phone numbers, emails, IPs found in major datasets), and toxicity filtering can introduce bias (C4's exclusion of LGBT+ content while retaining extremist material).

7. **Scale continues to increase**: From Llama's 1.4T tokens to DeepSeek-Coder-V2's 10.2T tokens, training datasets have grown 7-10x in just two years. Total human-generated text may be exhausted between 2026-2032.

8. **Multimodal training is expanding**: YouTube transcripts (30B words), images, audio, and video are increasingly incorporated, with ASR-based approaches enabling large-scale video LLM training.

9. **Code data is transparent**: The Stack (v1: 6.4TB, v2: 45TB) demonstrates how open governance and opt-out mechanisms can work when implemented seriously, contrasting sharply with opaque web/text data practices.

10. **Regional differences matter**: Chinese LLMs (DeepSeek) train on massive bilingual datasets, European initiatives (Mistral, EuroLLM) focus on multilingual capabilities and data sovereignty, while US companies prioritize English-first with international expansion.

### Implications

The LLM training data ecosystem is characterized by a tension between the need for massive, diverse datasets and growing concerns about copyright, privacy, consent, and bias. As human-generated text approaches exhaustion, the industry is pivoting toward synthetic data, which introduces new risks of model collapse and amplified biases. The lack of standardized transparency and effective opt-out mechanisms suggests regulatory intervention may be inevitable, potentially reshaping how future models are trained.

---

## Sources

- [Inside the Great AI Data Grab — Comprehensive Analysis | Medium](https://medium.com/@adnanmasood/inside-the-great-ai-data-grab-comprehensive-analysis-of-public-and-proprietary-corpora-utilised-49b4770abc47)
- [What LLM Web Scrapers Are There? | Peak Hour](https://www.peakhour.io/learning/bots/llm-web-scrapers/)
- [A Critical Analysis of the Largest Source for Generative AI Training Data: Common Crawl | ACM](https://dl.acm.org/doi/10.1145/3630106.3659033)
- [Cloudflare Blocks AI Crawlers | LLMRefs](https://llmrefs.com/blog/cloudflare-blocks-ai-crawlers)
- [GPT-4 | Wikipedia](https://en.wikipedia.org/wiki/GPT-4)
- [GPT-4 Research | OpenAI](https://openai.com/index/gpt-4-research/)
- [GPT-4o System Card | OpenAI](https://cdn.openai.com/gpt-4o-system-card.pdf)
- [GPT-4 Technical Report | OpenAI](https://cdn.openai.com/papers/gpt-4.pdf)
- [GPT-4 architecture, datasets, costs and more leaked | The Decoder](https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/)
- [Introducing Claude Opus 4.5 | Anthropic](https://www.anthropic.com/news/claude-opus-4-5)
- [Claude Opus 4.5 System Card | Anthropic](https://assets.anthropic.com/m/64823ba7485345a7/Claude-Opus-4-5-System-Card.pdf)
- [Models overview - Claude Docs | Anthropic](https://platform.claude.com/docs/en/about-claude/models/overview)
- [Claude (language model) | Wikipedia](https://en.wikipedia.org/wiki/Claude_(language_model))
- [Google's Gemini Training Data Sources | Swipe Insight](https://web.swipeinsight.app/posts/google-s-gemini-training-data-sources-a-deep-dive-into-tech-companies-transparency-5846)
- [Gemini (language model) | Wikipedia](https://en.wikipedia.org/wiki/Gemini_(language_model))
- [Papers Explained 393: Gemini 2.5 | Medium](https://ritvik19.medium.com/papers-explained-393-gemini-2-5-3b8877cf4da9)
- [Google Gemini AI training dataset composition | ScaleByTech](https://scalebytech.com/google-gemini-ai-training-dataset-composition/)
- [Introducing Meta Llama 3 | Meta AI](https://ai.meta.com/blog/meta-llama-3/)
- [llama3/MODEL_CARD.md | GitHub](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)
- [Llama 3.1 Guide | Kili Technology](https://kili-technology.com/large-language-models-llms/llama-3-1-guide-what-to-know-about-meta-s-new-405b-model-and-its-data)
- [llama-models/MODEL_CARD.md | GitHub](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md)
- [Llama (language model) | Wikipedia](https://en.wikipedia.org/wiki/LLaMA)
- [About Grok | X Help](https://help.x.com/en/using-x/about-grok)
- [Grok (chatbot) | Wikipedia](https://en.wikipedia.org/wiki/Grok_(chatbot))
- [Grok 3 by xAI: Everything You Need To Know | Fello AI](https://felloai.com/grok-3-by-xai-everything-you-need-to-know-about-elon-musks-latest-ai/)
- [Consumer FAQs | xAI](https://x.ai/legal/faq)
- [Does Mistral AI disclose its training datasets? | Mistral AI Help](https://help.mistral.ai/en/articles/347390-does-mistral-ai-disclose-its-training-datasets)
- [What is Mistral AI? | IBM](https://www.ibm.com/think/topics/mistral-ai)
- [Europe's Top AI Models of 2025 | MarkTechPost](https://www.marktechpost.com/2025/08/15/europes-top-ai-models-of-2025-multilingual-open-and-enterprise-ready/)
- [Cohere's Command R+ Model | Cohere Docs](https://docs.cohere.com/docs/command-r-plus)
- [Cohere Command Models | Cohere](https://cohere.com/command)
- [Command R: RAG at Production Scale | Cohere](https://cohere.com/blog/command-r)
- [Foundation Model for Generative AI - Amazon Titan | AWS](https://aws.amazon.com/bedrock/amazon-models/titan/)
- [Overview of Amazon Titan models | AWS](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-models.html)
- [Amazon Titan Text Lite and Express | AWS AI Service Cards](https://docs.aws.amazon.com/ai/responsible-ai/titan-text/overview.html)
- [Amazon: Titan Text Express | Stanford FMTI](https://crfm.stanford.edu/fmti/May-2024/company-reports/Amazon_Titan%20Text%20Express.html)
- [DeepSeek LLM | GitHub](https://github.com/deepseek-ai/DeepSeek-LLM)
- [DeepSeek | Wikipedia](https://en.wikipedia.org/wiki/DeepSeek)
- [Deepseek Training Data Size Insights 2025 | BytePlus](https://www.byteplus.com/en/topic/406251)
- [DeepSeek LLM: China's Latest Language Model | Analytics Vidhya](https://www.analyticsvidhya.com/blog/2023/12/deepseek-china-latest-language-model/)
- [Selecting and Preparing Training Data for LLMs (2024–2025) | Rohan Paul](https://www.rohan-paul.com/p/selecting-and-preparing-training)
- [Mastering LLM Techniques: Text Data Processing | NVIDIA](https://developer.nvidia.com/blog/mastering-llm-techniques-data-preprocessing/)
- [Technical Approaches to Managing Personal Data in LLMs | Holistic AI](https://www.holisticai.com/blog/managing-personal-data-in-large-language-models)
- [How to use LLMs to detect and extract personal data | Labelbox](https://labelbox.com/blog/how-to-use-llms-to-detect-and-extract-personal-data-from-ai-datasets/)
- [Unlocking AI Performance with NeMo Curator | SciPy 2025](https://cfp.scipy.org/scipy2025/talk/LEHUMF/)
- [Deduplicating Training Data Makes Language Models Better | ResearchGate](https://www.researchgate.net/publication/361073536_Deduplicating_Training_Data_Makes_Language_Models_Better)
- [The Pile (dataset) | Wikipedia](https://en.wikipedia.org/wiki/The_Pile_(dataset))
- [The Pile | EleutherAI](https://pile.eleuther.ai/)
- [EleutherAI/pile | Hugging Face](https://huggingface.co/datasets/EleutherAI/pile)
- [The Pile: An 800GB Dataset | arXiv](https://arxiv.org/abs/2101.00027)
- [GitHub - bigcode-project/starcoder](https://github.com/bigcode-project/starcoder)
- [StarCoder: A State-of-the-Art LLM for Code | Hugging Face](https://huggingface.co/blog/starcoder)
- [An open source challenger to GitHub Copilot? StarCoder2 | IT Pro](https://www.itpro.com/technology/artificial-intelligence/an-open-source-challenger-to-github-copilot-starcoder-2-a-code-generation-tool-backed-by-nvidia-hugging-face-and-servicenow-is-free-to-use-and-offers-support-for-over-600-programming-languages)
- [The books used to train LLMs | AI Copyright Substack](https://aicopyright.substack.com/p/the-books-used-to-train-llms)
- [The Books3 Dataset: Controversy Surrounds Unauthorized Use | Literary Agent Mark Gottlieb](https://literaryagentmarkgottlieb.com/blog/the-books3-dataset-controversy-surrounds-unauthorized-use-in-ai-training)
- [Hendrix v. Apple Inc. Complaint | Class Action](https://www.classaction.org/media/hendrix-et-al-v-apple-inc-complaint_1.pdf)
- [Reddit's data licensing play | Constellation Research](https://www.constellationr.com/blog-news/insights/reddits-data-licensing-play-do-you-want-your-llm-trained-reddit-data)
- [Reddit says it's made $203M licensing its data | TechCrunch](https://techcrunch.com/2024/02/22/reddit-says-its-made-203m-so-far-licensing-its-data/)
- [Public Licensing Deals for AI Training | Pillsbury Law](https://www.pillsburylaw.com/en/news-and-insights/public-licensing-deals-ai-training-multiply.html)
- [The False Hope of Content Licensing at Internet Scale | ProMarket](https://www.promarket.org/2025/11/19/the-false-hope-of-content-licensing-at-internet-scale/)
- [S2ORC: The Semantic Scholar Open Research Corpus | arXiv](https://arxiv.org/abs/1911.02782)
- [S2ORC: The Semantic Scholar Open Research Corpus | ACL Anthology](https://aclanthology.org/2020.acl-main.447.pdf)
- [Towards Best Practices for Open Datasets for LLM Training | arXiv](https://arxiv.org/abs/2501.08365)
- [Common Corpus: The Largest Collection of Ethical Data | arXiv](https://arxiv.org/html/2506.01732v1)
- [Synthetic data: A secret ingredient for better language models | Red Hat](https://www.redhat.com/en/blog/synthetic-data-secret-ingredient-better-language-models)
- [Using LLMs for Synthetic Data Generation | Confident AI](https://www.confident-ai.com/blog/the-definitive-guide-to-synthetic-data-generation-using-llms)
- [Self-Training AI: How Synthetic Data is Powering the Next Generation | Contineo](https://contineo.world/self-training-ai-how-synthetic-data-is-powering-the-next-generation-of-llms/)
- [Synthetic Data & Distillation | RLHF Book](https://rlhfbook.com/c/15-synthetic)
- [Synthetic Data Generation Methods for LLMs | Medium](https://medium.com/foundation-models-deep-dive/synthetic-data-generation-methods-for-llms-3aff1452ce68)
- [Wiki-40B: Multilingual Language Model Dataset | ACL Anthology](https://aclanthology.org/2020.lrec-1.297.pdf)
- [Wikimedia data for AI | arXiv](https://arxiv.org/html/2410.08918v1)
- [9 Open-Sourced Datasets for Training LLMs | Kili Technology](https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models)
- [Documenting Large Webtext Corpora: C4 | arXiv](https://arxiv.org/abs/2104.08758)
- [allenai/c4 | Hugging Face](https://huggingface.co/datasets/allenai/c4)
- [AIAAIC - C4 dataset](https://www.aiaaic.org/aiaaic-repository/ai-algorithmic-and-automation-incidents/c4-dataset)
- [A Case Study on the Colossal Clean Crawled Corpus | Rutgers](https://sites.rutgers.edu/critical-ai/wp-content/uploads/sites/586/2021/09/dodge2021documentingC4.pdf)
- [What Is LLMs.txt? & Do You Need One? | Neil Patel](https://neilpatel.com/blog/llms-txt-files-for-seo/)
- [AI / LLM User-Agents: Blocking Guide | robots.txt](https://robotstxt.com/ai)
- [How to Block LLM Crawlers in 2025 | Privacy Journal](https://www.privacyjournal.net/block-llm-crawlers/)
- [AI and your content. How to opt to opt-out | SuperGeekery](https://supergeekery.com/blog/ai-and-your-content-how-to-opt-to-opt-out)
- [Can Performant LLMs Be Ethical? | arXiv](https://arxiv.org/html/2504.06219v1)
- [New web standards could redefine how AI models use your content | Search Engine Land](https://searchengineland.com/robots-exclusions-new-rules-definitions-ietf-464990)
- [Best Practices for AI-Oriented robots.txt and llms.txt | Medium](https://medium.com/@franciscokemeny/best-practices-for-ai-oriented-robots-txt-and-llms-txt-configuration-be564ba5a6bd)
- [LLMs.txt Explained | nDash](https://www.ndash.com/blog/llms-txt-explained-a-shared-guide-for-marketers-and-freelancers)
- [AI Crawlers and How to Block Them | OpenReplay](https://blog.openreplay.com/ai-crawlers-block-robots-txt/)
- [15+ High-Quality LLM Datasets | ProjectPro](https://www.projectpro.io/article/llm-datasets-for-training/1027)
- [Training a Custom LLM on Stack Overflow Data | Medium](https://medium.com/@gosshhh9/training-a-custom-llm-on-stack-overflow-data-a-comprehensive-guide-4c58bbb1cb32)
- [StackSample: 10% of Stack Overflow Q&A | Kaggle](https://www.kaggle.com/datasets/stackoverflow/stacksample)
- [Transcribing YouTube Videos for LLM Training | Center for Data Innovation](https://datainnovation.org/2024/05/transcribing-youtube-videos-for-llm-training/)
- [Live: Learning Video LLM with Streaming Speech Transcription | arXiv](https://arxiv.org/html/2504.16030v1)
- [Build your own LLM using YouTube Transcript data | Medium](https://medium.com/@shivendrra_/build-your-own-llm-using-youtube-transcript-data-87c04469c5e2)
- [The RefinedWeb Dataset for Falcon LLM | arXiv](https://arxiv.org/abs/2306.01116)
- [tiiuae/falcon-refinedweb | Hugging Face](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)
- [The RefinedWeb Dataset for Falcon LLM | Hugging Face Papers](https://huggingface.co/papers/2306.01116)
- [Will we run out of data? | Epoch AI](https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data)
- [How much LLM training data is there, in the limit? | Educating Silicon](https://www.educatingsilicon.com/2024/05/09/how-much-llm-training-data-is-there-in-the-limit/)
- [Understanding LLMs: Model size, training data, and tokenization | Outshift](https://outshift.cisco.com/blog/understanding-llms-model-size-training-data-tokenization)
- [Announcing Nemotron-CC | NVIDIA](https://developer.nvidia.com/blog/announcing-nemotron-cc-a-trillion-token-english-language-dataset-for-llm-pretraining/)
- [State of AI 2025: 100T Token LLM Usage Study | OpenRouter](https://openrouter.ai/state-of-ai)
- [LLM training datasets | Glenn Klockwood](https://www.glennklockwood.com/garden/LLM-training-datasets)
- [Reinforcement Learning from Human Feedback (RLHF) Explained | IntuitionLabs](https://intuitionlabs.ai/articles/reinforcement-learning-human-feedback)
- [RLHF: Reinforcement Learning from Human Feedback | Chip Huyen](https://huyenchip.com/2023/05/02/rlhf.html)
- [Anthropic/hh-rlhf | Hugging Face](https://huggingface.co/datasets/Anthropic/hh-rlhf)
- [GitHub - anthropics/hh-rlhf](https://github.com/anthropics/hh-rlhf)
- [What Is RLHF? | IBM](https://www.ibm.com/think/topics/rlhf)
- [Illustrating RLHF | Hugging Face](https://huggingface.co/blog/rlhf)
